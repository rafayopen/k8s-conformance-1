I0214 14:20:16.850577      16 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-284296654
I0214 14:20:16.850760      16 e2e.go:224] Starting e2e run "a5e7314e-3063-11e9-a1dd-12216a2f1059" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1550154015 - Will randomize all specs
Will run 201 of 1946 specs

Feb 14 14:20:17.137: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 14:20:17.143: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 14 14:20:17.157: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 14 14:20:17.218: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 14 14:20:17.218: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Feb 14 14:20:17.219: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 14 14:20:17.234: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 14 14:20:17.234: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'ccm-linode' (0 seconds elapsed)
Feb 14 14:20:17.234: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-linode-node' (0 seconds elapsed)
Feb 14 14:20:17.234: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 14 14:20:17.234: INFO: e2e test version: v1.13.0
Feb 14 14:20:17.236: INFO: kube-apiserver version: v1.13.0
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:20:17.236: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename gc
Feb 14 14:20:17.327: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0214 14:20:18.363509      16 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 14 14:20:18.363: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:20:18.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-5pp9m" for this suite.
Feb 14 14:20:24.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:20:24.397: INFO: namespace: e2e-tests-gc-5pp9m, resource: bindings, ignored listing per whitelist
Feb 14 14:20:24.460: INFO: namespace e2e-tests-gc-5pp9m deletion completed in 6.093861803s

• [SLOW TEST:7.223 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:20:24.462: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 14:20:24.538: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab4c2b84-3063-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-lrk6w" to be "success or failure"
Feb 14 14:20:24.562: INFO: Pod "downwardapi-volume-ab4c2b84-3063-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 24.316727ms
Feb 14 14:20:26.566: INFO: Pod "downwardapi-volume-ab4c2b84-3063-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028123078s
STEP: Saw pod success
Feb 14 14:20:26.566: INFO: Pod "downwardapi-volume-ab4c2b84-3063-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:20:26.569: INFO: Trying to get logs from node k8s-conformance-node-3 pod downwardapi-volume-ab4c2b84-3063-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 14:20:26.589: INFO: Waiting for pod downwardapi-volume-ab4c2b84-3063-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:20:26.594: INFO: Pod downwardapi-volume-ab4c2b84-3063-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:20:26.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lrk6w" for this suite.
Feb 14 14:20:32.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:20:32.650: INFO: namespace: e2e-tests-downward-api-lrk6w, resource: bindings, ignored listing per whitelist
Feb 14 14:20:32.696: INFO: namespace e2e-tests-downward-api-lrk6w deletion completed in 6.097709621s

• [SLOW TEST:8.234 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:20:32.696: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0214 14:20:38.788648      16 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 14 14:20:38.788: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:20:38.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-9f2v2" for this suite.
Feb 14 14:20:44.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:20:44.935: INFO: namespace: e2e-tests-gc-9f2v2, resource: bindings, ignored listing per whitelist
Feb 14 14:20:44.959: INFO: namespace e2e-tests-gc-9f2v2 deletion completed in 6.168167657s

• [SLOW TEST:12.263 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:20:44.961: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-t7l4p/configmap-test-b78442a8-3063-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 14:20:45.038: INFO: Waiting up to 5m0s for pod "pod-configmaps-b784b1fa-3063-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-configmap-t7l4p" to be "success or failure"
Feb 14 14:20:45.043: INFO: Pod "pod-configmaps-b784b1fa-3063-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 4.730962ms
Feb 14 14:20:47.048: INFO: Pod "pod-configmaps-b784b1fa-3063-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010115017s
Feb 14 14:20:49.054: INFO: Pod "pod-configmaps-b784b1fa-3063-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016482397s
STEP: Saw pod success
Feb 14 14:20:49.055: INFO: Pod "pod-configmaps-b784b1fa-3063-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:20:49.057: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-configmaps-b784b1fa-3063-11e9-a1dd-12216a2f1059 container env-test: <nil>
STEP: delete the pod
Feb 14 14:20:49.074: INFO: Waiting for pod pod-configmaps-b784b1fa-3063-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:20:49.077: INFO: Pod pod-configmaps-b784b1fa-3063-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:20:49.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-t7l4p" for this suite.
Feb 14 14:20:55.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:20:55.123: INFO: namespace: e2e-tests-configmap-t7l4p, resource: bindings, ignored listing per whitelist
Feb 14 14:20:55.176: INFO: namespace e2e-tests-configmap-t7l4p deletion completed in 6.093126077s

• [SLOW TEST:10.216 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:20:55.179: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 14 14:20:55.250: INFO: Waiting up to 5m0s for pod "pod-bd9aad60-3063-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-hrmrh" to be "success or failure"
Feb 14 14:20:55.262: INFO: Pod "pod-bd9aad60-3063-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 12.04294ms
Feb 14 14:20:57.265: INFO: Pod "pod-bd9aad60-3063-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015396052s
Feb 14 14:20:59.268: INFO: Pod "pod-bd9aad60-3063-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018286415s
STEP: Saw pod success
Feb 14 14:20:59.268: INFO: Pod "pod-bd9aad60-3063-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:20:59.271: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-bd9aad60-3063-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:20:59.288: INFO: Waiting for pod pod-bd9aad60-3063-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:20:59.291: INFO: Pod pod-bd9aad60-3063-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:20:59.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hrmrh" for this suite.
Feb 14 14:21:05.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:21:05.347: INFO: namespace: e2e-tests-emptydir-hrmrh, resource: bindings, ignored listing per whitelist
Feb 14 14:21:05.388: INFO: namespace e2e-tests-emptydir-hrmrh deletion completed in 6.093230264s

• [SLOW TEST:10.208 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:21:05.388: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-c3b0df2b-3063-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 14:21:05.472: INFO: Waiting up to 5m0s for pod "pod-configmaps-c3b163b0-3063-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-configmap-w8d65" to be "success or failure"
Feb 14 14:21:05.482: INFO: Pod "pod-configmaps-c3b163b0-3063-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 9.954765ms
Feb 14 14:21:07.485: INFO: Pod "pod-configmaps-c3b163b0-3063-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013028044s
Feb 14 14:21:09.489: INFO: Pod "pod-configmaps-c3b163b0-3063-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016996214s
STEP: Saw pod success
Feb 14 14:21:09.489: INFO: Pod "pod-configmaps-c3b163b0-3063-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:21:09.492: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-configmaps-c3b163b0-3063-11e9-a1dd-12216a2f1059 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 14:21:09.523: INFO: Waiting for pod pod-configmaps-c3b163b0-3063-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:21:09.528: INFO: Pod pod-configmaps-c3b163b0-3063-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:21:09.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-w8d65" for this suite.
Feb 14 14:21:15.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:21:15.584: INFO: namespace: e2e-tests-configmap-w8d65, resource: bindings, ignored listing per whitelist
Feb 14 14:21:15.634: INFO: namespace e2e-tests-configmap-w8d65 deletion completed in 6.10047228s

• [SLOW TEST:10.246 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:21:15.634: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-c9cc1022-3063-11e9-a1dd-12216a2f1059
STEP: Creating secret with name s-test-opt-upd-c9cc1096-3063-11e9-a1dd-12216a2f1059
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c9cc1022-3063-11e9-a1dd-12216a2f1059
STEP: Updating secret s-test-opt-upd-c9cc1096-3063-11e9-a1dd-12216a2f1059
STEP: Creating secret with name s-test-opt-create-c9cc10b8-3063-11e9-a1dd-12216a2f1059
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:22:48.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4fvcs" for this suite.
Feb 14 14:23:10.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:23:10.372: INFO: namespace: e2e-tests-secrets-4fvcs, resource: bindings, ignored listing per whitelist
Feb 14 14:23:10.417: INFO: namespace e2e-tests-secrets-4fvcs deletion completed in 22.141253095s

• [SLOW TEST:114.783 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:23:10.419: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0214 14:23:20.548190      16 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 14 14:23:20.548: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:23:20.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-9d9hh" for this suite.
Feb 14 14:23:26.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:23:26.607: INFO: namespace: e2e-tests-gc-9d9hh, resource: bindings, ignored listing per whitelist
Feb 14 14:23:26.642: INFO: namespace e2e-tests-gc-9d9hh deletion completed in 6.091658646s

• [SLOW TEST:16.223 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:23:26.642: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 14:23:26.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 version'
Feb 14 14:23:26.815: INFO: stderr: ""
Feb 14 14:23:26.815: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T20:56:12Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:23:26.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rmmtc" for this suite.
Feb 14 14:23:32.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:23:32.903: INFO: namespace: e2e-tests-kubectl-rmmtc, resource: bindings, ignored listing per whitelist
Feb 14 14:23:32.907: INFO: namespace e2e-tests-kubectl-rmmtc deletion completed in 6.087993051s

• [SLOW TEST:6.265 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:23:32.907: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0214 14:24:12.997089      16 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 14 14:24:12.997: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:24:12.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-s6cm9" for this suite.
Feb 14 14:24:19.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:24:19.043: INFO: namespace: e2e-tests-gc-s6cm9, resource: bindings, ignored listing per whitelist
Feb 14 14:24:19.088: INFO: namespace e2e-tests-gc-s6cm9 deletion completed in 6.089085788s

• [SLOW TEST:46.182 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:24:19.089: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-3723fbc2-3064-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 14:24:19.162: INFO: Waiting up to 5m0s for pod "pod-configmaps-37249b52-3064-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-configmap-b9knb" to be "success or failure"
Feb 14 14:24:19.173: INFO: Pod "pod-configmaps-37249b52-3064-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 11.07046ms
Feb 14 14:24:21.176: INFO: Pod "pod-configmaps-37249b52-3064-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013951312s
Feb 14 14:24:23.179: INFO: Pod "pod-configmaps-37249b52-3064-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017259208s
STEP: Saw pod success
Feb 14 14:24:23.179: INFO: Pod "pod-configmaps-37249b52-3064-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:24:23.182: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-configmaps-37249b52-3064-11e9-a1dd-12216a2f1059 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 14:24:23.208: INFO: Waiting for pod pod-configmaps-37249b52-3064-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:24:23.213: INFO: Pod pod-configmaps-37249b52-3064-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:24:23.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-b9knb" for this suite.
Feb 14 14:24:29.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:24:29.265: INFO: namespace: e2e-tests-configmap-b9knb, resource: bindings, ignored listing per whitelist
Feb 14 14:24:29.333: INFO: namespace e2e-tests-configmap-b9knb deletion completed in 6.107711237s

• [SLOW TEST:10.244 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:24:29.333: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:24:33.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-j9k29" for this suite.
Feb 14 14:25:17.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:25:17.451: INFO: namespace: e2e-tests-kubelet-test-j9k29, resource: bindings, ignored listing per whitelist
Feb 14 14:25:17.517: INFO: namespace e2e-tests-kubelet-test-j9k29 deletion completed in 44.086872289s

• [SLOW TEST:48.184 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:25:17.517: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 14 14:25:17.587: INFO: Waiting up to 5m0s for pod "client-containers-59f85568-3064-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-containers-kfhqc" to be "success or failure"
Feb 14 14:25:17.593: INFO: Pod "client-containers-59f85568-3064-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 6.334979ms
Feb 14 14:25:19.599: INFO: Pod "client-containers-59f85568-3064-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012256783s
STEP: Saw pod success
Feb 14 14:25:19.599: INFO: Pod "client-containers-59f85568-3064-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:25:19.602: INFO: Trying to get logs from node k8s-conformance-node-1 pod client-containers-59f85568-3064-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:25:19.625: INFO: Waiting for pod client-containers-59f85568-3064-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:25:19.650: INFO: Pod client-containers-59f85568-3064-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:25:19.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-kfhqc" for this suite.
Feb 14 14:25:25.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:25:25.779: INFO: namespace: e2e-tests-containers-kfhqc, resource: bindings, ignored listing per whitelist
Feb 14 14:25:25.789: INFO: namespace e2e-tests-containers-kfhqc deletion completed in 6.101601462s

• [SLOW TEST:8.272 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:25:25.791: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 14 14:25:25.848: INFO: Waiting up to 5m0s for pod "var-expansion-5ee4daa7-3064-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-var-expansion-8zm4k" to be "success or failure"
Feb 14 14:25:25.856: INFO: Pod "var-expansion-5ee4daa7-3064-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 7.785423ms
Feb 14 14:25:27.864: INFO: Pod "var-expansion-5ee4daa7-3064-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016440081s
STEP: Saw pod success
Feb 14 14:25:27.864: INFO: Pod "var-expansion-5ee4daa7-3064-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:25:27.869: INFO: Trying to get logs from node k8s-conformance-node-3 pod var-expansion-5ee4daa7-3064-11e9-a1dd-12216a2f1059 container dapi-container: <nil>
STEP: delete the pod
Feb 14 14:25:27.893: INFO: Waiting for pod var-expansion-5ee4daa7-3064-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:25:27.895: INFO: Pod var-expansion-5ee4daa7-3064-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:25:27.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-8zm4k" for this suite.
Feb 14 14:25:33.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:25:33.972: INFO: namespace: e2e-tests-var-expansion-8zm4k, resource: bindings, ignored listing per whitelist
Feb 14 14:25:33.990: INFO: namespace e2e-tests-var-expansion-8zm4k deletion completed in 6.09239258s

• [SLOW TEST:8.200 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:25:33.992: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-cffnn
Feb 14 14:25:36.073: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-cffnn
STEP: checking the pod's current state and verifying that restartCount is present
Feb 14 14:25:36.075: INFO: Initial restart count of pod liveness-exec is 0
Feb 14 14:26:24.164: INFO: Restart count of pod e2e-tests-container-probe-cffnn/liveness-exec is now 1 (48.087995275s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:26:24.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cffnn" for this suite.
Feb 14 14:26:30.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:26:30.277: INFO: namespace: e2e-tests-container-probe-cffnn, resource: bindings, ignored listing per whitelist
Feb 14 14:26:30.377: INFO: namespace e2e-tests-container-probe-cffnn deletion completed in 6.186119606s

• [SLOW TEST:56.386 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:26:30.377: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 14 14:26:30.449: INFO: Waiting up to 5m0s for pod "pod-8565dce1-3064-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-j7w8n" to be "success or failure"
Feb 14 14:26:30.458: INFO: Pod "pod-8565dce1-3064-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 9.028322ms
Feb 14 14:26:32.461: INFO: Pod "pod-8565dce1-3064-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012561868s
STEP: Saw pod success
Feb 14 14:26:32.461: INFO: Pod "pod-8565dce1-3064-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:26:32.464: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-8565dce1-3064-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:26:32.485: INFO: Waiting for pod pod-8565dce1-3064-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:26:32.490: INFO: Pod pod-8565dce1-3064-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:26:32.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-j7w8n" for this suite.
Feb 14 14:26:38.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:26:38.544: INFO: namespace: e2e-tests-emptydir-j7w8n, resource: bindings, ignored listing per whitelist
Feb 14 14:26:38.584: INFO: namespace e2e-tests-emptydir-j7w8n deletion completed in 6.090081662s

• [SLOW TEST:8.207 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:26:38.585: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-bfrcl
Feb 14 14:26:40.667: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-bfrcl
STEP: checking the pod's current state and verifying that restartCount is present
Feb 14 14:26:40.669: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:30:41.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-bfrcl" for this suite.
Feb 14 14:30:47.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:30:47.272: INFO: namespace: e2e-tests-container-probe-bfrcl, resource: bindings, ignored listing per whitelist
Feb 14 14:30:47.334: INFO: namespace e2e-tests-container-probe-bfrcl deletion completed in 6.140672774s

• [SLOW TEST:248.749 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:30:47.335: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 14 14:30:47.592: INFO: Pod name wrapped-volume-race-1ea99378-3065-11e9-a1dd-12216a2f1059: Found 0 pods out of 5
Feb 14 14:30:52.599: INFO: Pod name wrapped-volume-race-1ea99378-3065-11e9-a1dd-12216a2f1059: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1ea99378-3065-11e9-a1dd-12216a2f1059 in namespace e2e-tests-emptydir-wrapper-n98wf, will wait for the garbage collector to delete the pods
Feb 14 14:31:02.682: INFO: Deleting ReplicationController wrapped-volume-race-1ea99378-3065-11e9-a1dd-12216a2f1059 took: 7.657849ms
Feb 14 14:31:02.783: INFO: Terminating ReplicationController wrapped-volume-race-1ea99378-3065-11e9-a1dd-12216a2f1059 pods took: 100.495305ms
STEP: Creating RC which spawns configmap-volume pods
Feb 14 14:31:47.601: INFO: Pod name wrapped-volume-race-426de9c5-3065-11e9-a1dd-12216a2f1059: Found 0 pods out of 5
Feb 14 14:31:52.614: INFO: Pod name wrapped-volume-race-426de9c5-3065-11e9-a1dd-12216a2f1059: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-426de9c5-3065-11e9-a1dd-12216a2f1059 in namespace e2e-tests-emptydir-wrapper-n98wf, will wait for the garbage collector to delete the pods
Feb 14 14:32:02.691: INFO: Deleting ReplicationController wrapped-volume-race-426de9c5-3065-11e9-a1dd-12216a2f1059 took: 6.573691ms
Feb 14 14:32:02.792: INFO: Terminating ReplicationController wrapped-volume-race-426de9c5-3065-11e9-a1dd-12216a2f1059 pods took: 100.484427ms
STEP: Creating RC which spawns configmap-volume pods
Feb 14 14:32:48.504: INFO: Pod name wrapped-volume-race-66bbd7dd-3065-11e9-a1dd-12216a2f1059: Found 0 pods out of 5
Feb 14 14:32:53.510: INFO: Pod name wrapped-volume-race-66bbd7dd-3065-11e9-a1dd-12216a2f1059: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-66bbd7dd-3065-11e9-a1dd-12216a2f1059 in namespace e2e-tests-emptydir-wrapper-n98wf, will wait for the garbage collector to delete the pods
Feb 14 14:33:03.605: INFO: Deleting ReplicationController wrapped-volume-race-66bbd7dd-3065-11e9-a1dd-12216a2f1059 took: 11.623446ms
Feb 14 14:33:03.706: INFO: Terminating ReplicationController wrapped-volume-race-66bbd7dd-3065-11e9-a1dd-12216a2f1059 pods took: 100.405842ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:33:48.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-n98wf" for this suite.
Feb 14 14:33:54.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:33:54.175: INFO: namespace: e2e-tests-emptydir-wrapper-n98wf, resource: bindings, ignored listing per whitelist
Feb 14 14:33:54.246: INFO: namespace e2e-tests-emptydir-wrapper-n98wf deletion completed in 6.096174879s

• [SLOW TEST:186.911 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:33:54.246: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 14 14:33:54.316: INFO: Waiting up to 5m0s for pod "pod-8df6f50f-3065-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-msvl5" to be "success or failure"
Feb 14 14:33:54.321: INFO: Pod "pod-8df6f50f-3065-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 4.832245ms
Feb 14 14:33:56.325: INFO: Pod "pod-8df6f50f-3065-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008711881s
Feb 14 14:33:58.329: INFO: Pod "pod-8df6f50f-3065-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012555909s
STEP: Saw pod success
Feb 14 14:33:58.329: INFO: Pod "pod-8df6f50f-3065-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:33:58.333: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-8df6f50f-3065-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:33:58.373: INFO: Waiting for pod pod-8df6f50f-3065-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:33:58.376: INFO: Pod pod-8df6f50f-3065-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:33:58.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-msvl5" for this suite.
Feb 14 14:34:04.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:34:04.446: INFO: namespace: e2e-tests-emptydir-msvl5, resource: bindings, ignored listing per whitelist
Feb 14 14:34:04.472: INFO: namespace e2e-tests-emptydir-msvl5 deletion completed in 6.092004342s

• [SLOW TEST:10.226 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:34:04.473: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-9411de00-3065-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 14:34:04.568: INFO: Waiting up to 5m0s for pod "pod-configmaps-9412a78a-3065-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-configmap-ch295" to be "success or failure"
Feb 14 14:34:04.581: INFO: Pod "pod-configmaps-9412a78a-3065-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 12.075655ms
Feb 14 14:34:06.585: INFO: Pod "pod-configmaps-9412a78a-3065-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016178306s
STEP: Saw pod success
Feb 14 14:34:06.585: INFO: Pod "pod-configmaps-9412a78a-3065-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:34:06.588: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-configmaps-9412a78a-3065-11e9-a1dd-12216a2f1059 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 14:34:06.617: INFO: Waiting for pod pod-configmaps-9412a78a-3065-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:34:06.622: INFO: Pod pod-configmaps-9412a78a-3065-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:34:06.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ch295" for this suite.
Feb 14 14:34:12.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:34:12.709: INFO: namespace: e2e-tests-configmap-ch295, resource: bindings, ignored listing per whitelist
Feb 14 14:34:12.718: INFO: namespace e2e-tests-configmap-ch295 deletion completed in 6.088718367s

• [SLOW TEST:8.245 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:34:12.719: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 14:34:16.833: INFO: Waiting up to 5m0s for pod "client-envvars-9b5f4cb6-3065-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-pods-gfrxl" to be "success or failure"
Feb 14 14:34:16.836: INFO: Pod "client-envvars-9b5f4cb6-3065-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 3.2788ms
Feb 14 14:34:18.840: INFO: Pod "client-envvars-9b5f4cb6-3065-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006562036s
STEP: Saw pod success
Feb 14 14:34:18.840: INFO: Pod "client-envvars-9b5f4cb6-3065-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:34:18.842: INFO: Trying to get logs from node k8s-conformance-node-1 pod client-envvars-9b5f4cb6-3065-11e9-a1dd-12216a2f1059 container env3cont: <nil>
STEP: delete the pod
Feb 14 14:34:18.866: INFO: Waiting for pod client-envvars-9b5f4cb6-3065-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:34:18.884: INFO: Pod client-envvars-9b5f4cb6-3065-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:34:18.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-gfrxl" for this suite.
Feb 14 14:35:08.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:35:08.924: INFO: namespace: e2e-tests-pods-gfrxl, resource: bindings, ignored listing per whitelist
Feb 14 14:35:08.989: INFO: namespace e2e-tests-pods-gfrxl deletion completed in 50.101133859s

• [SLOW TEST:56.270 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:35:08.991: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 14 14:35:11.580: INFO: Successfully updated pod "pod-update-ba838bab-3065-11e9-a1dd-12216a2f1059"
STEP: verifying the updated pod is in kubernetes
Feb 14 14:35:11.584: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:35:11.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-rfb4c" for this suite.
Feb 14 14:35:33.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:35:33.655: INFO: namespace: e2e-tests-pods-rfb4c, resource: bindings, ignored listing per whitelist
Feb 14 14:35:33.682: INFO: namespace e2e-tests-pods-rfb4c deletion completed in 22.094544915s

• [SLOW TEST:24.691 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:35:33.682: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:35:57.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-8zgjp" for this suite.
Feb 14 14:36:03.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:36:03.115: INFO: namespace: e2e-tests-container-runtime-8zgjp, resource: bindings, ignored listing per whitelist
Feb 14 14:36:03.147: INFO: namespace e2e-tests-container-runtime-8zgjp deletion completed in 6.104115634s

• [SLOW TEST:29.464 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:36:03.147: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-dacd1f6e-3065-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 14:36:03.231: INFO: Waiting up to 5m0s for pod "pod-secrets-dacdb5ec-3065-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-secrets-slgjr" to be "success or failure"
Feb 14 14:36:03.239: INFO: Pod "pod-secrets-dacdb5ec-3065-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 7.931501ms
Feb 14 14:36:05.243: INFO: Pod "pod-secrets-dacdb5ec-3065-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012328621s
STEP: Saw pod success
Feb 14 14:36:05.243: INFO: Pod "pod-secrets-dacdb5ec-3065-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:36:05.246: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-secrets-dacdb5ec-3065-11e9-a1dd-12216a2f1059 container secret-volume-test: <nil>
STEP: delete the pod
Feb 14 14:36:05.270: INFO: Waiting for pod pod-secrets-dacdb5ec-3065-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:36:05.272: INFO: Pod pod-secrets-dacdb5ec-3065-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:36:05.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-slgjr" for this suite.
Feb 14 14:36:11.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:36:11.364: INFO: namespace: e2e-tests-secrets-slgjr, resource: bindings, ignored listing per whitelist
Feb 14 14:36:11.389: INFO: namespace e2e-tests-secrets-slgjr deletion completed in 6.11096217s

• [SLOW TEST:8.242 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:36:11.390: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 14 14:36:11.462: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 14 14:36:11.467: INFO: Waiting for terminating namespaces to be deleted...
Feb 14 14:36:11.469: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-node-1 before test
Feb 14 14:36:11.476: INFO: kube-proxy-4ls9n from kube-system started at 2019-02-14 14:17:57 +0000 UTC (1 container statuses recorded)
Feb 14 14:36:11.476: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 14:36:11.476: INFO: csi-linode-node-7zb7d from kube-system started at 2019-02-14 14:18:26 +0000 UTC (2 container statuses recorded)
Feb 14 14:36:11.476: INFO: 	Container csi-linode-plugin ready: true, restart count 0
Feb 14 14:36:11.476: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 14 14:36:11.476: INFO: calico-node-x2wh4 from kube-system started at 2019-02-14 14:17:57 +0000 UTC (2 container statuses recorded)
Feb 14 14:36:11.476: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 14:36:11.476: INFO: 	Container install-cni ready: true, restart count 0
Feb 14 14:36:11.476: INFO: sonobuoy-e2e-job-1123becdb7604801 from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 14:36:11.476: INFO: 	Container e2e ready: true, restart count 0
Feb 14 14:36:11.476: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 14 14:36:11.476: INFO: sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-wx8k7 from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 14:36:11.476: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb 14 14:36:11.477: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 14 14:36:11.477: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-node-2 before test
Feb 14 14:36:11.492: INFO: csi-linode-node-fwkhk from kube-system started at 2019-02-14 14:18:05 +0000 UTC (2 container statuses recorded)
Feb 14 14:36:11.492: INFO: 	Container csi-linode-plugin ready: true, restart count 0
Feb 14 14:36:11.492: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 14 14:36:11.492: INFO: metrics-server-68d85f76bb-tp9rx from kube-system started at 2019-02-14 14:18:05 +0000 UTC (1 container statuses recorded)
Feb 14 14:36:11.492: INFO: 	Container metrics-server ready: true, restart count 0
Feb 14 14:36:11.492: INFO: sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-4ljxn from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 14:36:11.492: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb 14 14:36:11.492: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 14 14:36:11.492: INFO: kube-proxy-wdgrm from kube-system started at 2019-02-14 14:17:45 +0000 UTC (1 container statuses recorded)
Feb 14 14:36:11.493: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 14:36:11.493: INFO: calico-node-kfmp7 from kube-system started at 2019-02-14 14:17:45 +0000 UTC (2 container statuses recorded)
Feb 14 14:36:11.493: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 14:36:11.493: INFO: 	Container install-cni ready: true, restart count 0
Feb 14 14:36:11.493: INFO: csi-linode-controller-0 from kube-system started at 2019-02-14 14:18:05 +0000 UTC (3 container statuses recorded)
Feb 14 14:36:11.493: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 14 14:36:11.493: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 14 14:36:11.493: INFO: 	Container linode-csi-plugin ready: true, restart count 0
Feb 14 14:36:11.493: INFO: external-dns-d4cfd5855-fwcl4 from kube-system started at 2019-02-14 14:18:05 +0000 UTC (1 container statuses recorded)
Feb 14 14:36:11.493: INFO: 	Container external-dns ready: true, restart count 0
Feb 14 14:36:11.493: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-node-3 before test
Feb 14 14:36:11.509: INFO: kube-proxy-hrh6d from kube-system started at 2019-02-14 14:17:47 +0000 UTC (1 container statuses recorded)
Feb 14 14:36:11.510: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 14:36:11.510: INFO: csi-linode-node-2kjrw from kube-system started at 2019-02-14 14:18:07 +0000 UTC (2 container statuses recorded)
Feb 14 14:36:11.510: INFO: 	Container csi-linode-plugin ready: true, restart count 0
Feb 14 14:36:11.510: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 14 14:36:11.510: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-14 14:19:44 +0000 UTC (1 container statuses recorded)
Feb 14 14:36:11.510: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 14 14:36:11.510: INFO: calico-node-rfldn from kube-system started at 2019-02-14 14:17:47 +0000 UTC (2 container statuses recorded)
Feb 14 14:36:11.510: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 14:36:11.510: INFO: 	Container install-cni ready: true, restart count 0
Feb 14 14:36:11.510: INFO: sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-jm8kn from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 14:36:11.510: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb 14 14:36:11.510: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e0f4993d-3065-11e9-a1dd-12216a2f1059 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e0f4993d-3065-11e9-a1dd-12216a2f1059 off the node k8s-conformance-node-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e0f4993d-3065-11e9-a1dd-12216a2f1059
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:36:15.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-rt76x" for this suite.
Feb 14 14:36:27.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:36:27.708: INFO: namespace: e2e-tests-sched-pred-rt76x, resource: bindings, ignored listing per whitelist
Feb 14 14:36:27.723: INFO: namespace e2e-tests-sched-pred-rt76x deletion completed in 12.101197202s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:16.333 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:36:27.723: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-pb9vj
I0214 14:36:27.798811      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-pb9vj, replica count: 1
I0214 14:36:28.849383      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0214 14:36:29.849694      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 14 14:36:29.962: INFO: Created: latency-svc-kv4pr
Feb 14 14:36:29.968: INFO: Got endpoints: latency-svc-kv4pr [18.817327ms]
Feb 14 14:36:29.990: INFO: Created: latency-svc-t8zwl
Feb 14 14:36:29.994: INFO: Got endpoints: latency-svc-t8zwl [25.203262ms]
Feb 14 14:36:30.007: INFO: Created: latency-svc-h429x
Feb 14 14:36:30.008: INFO: Got endpoints: latency-svc-h429x [38.789132ms]
Feb 14 14:36:30.029: INFO: Created: latency-svc-gwlwt
Feb 14 14:36:30.029: INFO: Got endpoints: latency-svc-gwlwt [59.908674ms]
Feb 14 14:36:30.039: INFO: Created: latency-svc-vhgqc
Feb 14 14:36:30.042: INFO: Got endpoints: latency-svc-vhgqc [72.838564ms]
Feb 14 14:36:30.052: INFO: Created: latency-svc-h79jc
Feb 14 14:36:30.057: INFO: Got endpoints: latency-svc-h79jc [87.758171ms]
Feb 14 14:36:30.088: INFO: Created: latency-svc-md4bm
Feb 14 14:36:30.093: INFO: Got endpoints: latency-svc-md4bm [122.978642ms]
Feb 14 14:36:30.114: INFO: Created: latency-svc-4rkpj
Feb 14 14:36:30.125: INFO: Got endpoints: latency-svc-4rkpj [155.004259ms]
Feb 14 14:36:30.126: INFO: Created: latency-svc-sqztj
Feb 14 14:36:30.130: INFO: Got endpoints: latency-svc-sqztj [159.642038ms]
Feb 14 14:36:30.145: INFO: Created: latency-svc-l9d2m
Feb 14 14:36:30.146: INFO: Got endpoints: latency-svc-l9d2m [175.665192ms]
Feb 14 14:36:30.167: INFO: Created: latency-svc-mdnqj
Feb 14 14:36:30.173: INFO: Got endpoints: latency-svc-mdnqj [202.719382ms]
Feb 14 14:36:30.185: INFO: Created: latency-svc-wwdj6
Feb 14 14:36:30.194: INFO: Got endpoints: latency-svc-wwdj6 [222.605156ms]
Feb 14 14:36:30.205: INFO: Created: latency-svc-mrtm7
Feb 14 14:36:30.211: INFO: Got endpoints: latency-svc-mrtm7 [239.830027ms]
Feb 14 14:36:30.229: INFO: Created: latency-svc-gcdjd
Feb 14 14:36:30.233: INFO: Got endpoints: latency-svc-gcdjd [262.369155ms]
Feb 14 14:36:30.271: INFO: Created: latency-svc-66lkj
Feb 14 14:36:30.281: INFO: Got endpoints: latency-svc-66lkj [309.811798ms]
Feb 14 14:36:30.295: INFO: Created: latency-svc-68gw4
Feb 14 14:36:30.300: INFO: Got endpoints: latency-svc-68gw4 [329.343044ms]
Feb 14 14:36:30.335: INFO: Created: latency-svc-lhbg8
Feb 14 14:36:30.352: INFO: Got endpoints: latency-svc-lhbg8 [357.76628ms]
Feb 14 14:36:30.410: INFO: Created: latency-svc-xg7hn
Feb 14 14:36:30.420: INFO: Got endpoints: latency-svc-xg7hn [411.660688ms]
Feb 14 14:36:30.444: INFO: Created: latency-svc-zlr4j
Feb 14 14:36:30.451: INFO: Got endpoints: latency-svc-zlr4j [421.408415ms]
Feb 14 14:36:30.476: INFO: Created: latency-svc-fbdxr
Feb 14 14:36:30.478: INFO: Got endpoints: latency-svc-fbdxr [435.622912ms]
Feb 14 14:36:30.510: INFO: Created: latency-svc-qjwr2
Feb 14 14:36:30.517: INFO: Got endpoints: latency-svc-qjwr2 [459.872937ms]
Feb 14 14:36:30.528: INFO: Created: latency-svc-bx849
Feb 14 14:36:30.532: INFO: Got endpoints: latency-svc-bx849 [438.970954ms]
Feb 14 14:36:30.561: INFO: Created: latency-svc-t5tn7
Feb 14 14:36:30.564: INFO: Got endpoints: latency-svc-t5tn7 [438.883176ms]
Feb 14 14:36:30.579: INFO: Created: latency-svc-tsq7x
Feb 14 14:36:30.581: INFO: Got endpoints: latency-svc-tsq7x [450.861339ms]
Feb 14 14:36:30.593: INFO: Created: latency-svc-c57bw
Feb 14 14:36:30.595: INFO: Got endpoints: latency-svc-c57bw [448.767583ms]
Feb 14 14:36:30.614: INFO: Created: latency-svc-f5x9d
Feb 14 14:36:30.616: INFO: Got endpoints: latency-svc-f5x9d [442.269427ms]
Feb 14 14:36:30.634: INFO: Created: latency-svc-pgv75
Feb 14 14:36:30.637: INFO: Got endpoints: latency-svc-pgv75 [443.108075ms]
Feb 14 14:36:30.671: INFO: Created: latency-svc-fv8w8
Feb 14 14:36:30.671: INFO: Created: latency-svc-68dhg
Feb 14 14:36:30.671: INFO: Got endpoints: latency-svc-68dhg [460.280227ms]
Feb 14 14:36:30.680: INFO: Got endpoints: latency-svc-fv8w8 [446.252708ms]
Feb 14 14:36:30.690: INFO: Created: latency-svc-twrzk
Feb 14 14:36:30.695: INFO: Got endpoints: latency-svc-twrzk [414.367341ms]
Feb 14 14:36:30.710: INFO: Created: latency-svc-4c899
Feb 14 14:36:30.717: INFO: Got endpoints: latency-svc-4c899 [417.591733ms]
Feb 14 14:36:30.733: INFO: Created: latency-svc-vw959
Feb 14 14:36:30.737: INFO: Got endpoints: latency-svc-vw959 [384.909598ms]
Feb 14 14:36:30.754: INFO: Created: latency-svc-69sxh
Feb 14 14:36:30.758: INFO: Got endpoints: latency-svc-69sxh [338.383073ms]
Feb 14 14:36:30.766: INFO: Created: latency-svc-kqzq9
Feb 14 14:36:30.769: INFO: Got endpoints: latency-svc-kqzq9 [317.798209ms]
Feb 14 14:36:30.790: INFO: Created: latency-svc-r6srt
Feb 14 14:36:30.791: INFO: Got endpoints: latency-svc-r6srt [313.29093ms]
Feb 14 14:36:30.809: INFO: Created: latency-svc-z8jb8
Feb 14 14:36:30.809: INFO: Got endpoints: latency-svc-z8jb8 [291.418779ms]
Feb 14 14:36:30.828: INFO: Created: latency-svc-57xf4
Feb 14 14:36:30.829: INFO: Got endpoints: latency-svc-57xf4 [297.182237ms]
Feb 14 14:36:30.840: INFO: Created: latency-svc-9cpks
Feb 14 14:36:30.844: INFO: Got endpoints: latency-svc-9cpks [279.783866ms]
Feb 14 14:36:30.892: INFO: Created: latency-svc-jmmdh
Feb 14 14:36:30.892: INFO: Got endpoints: latency-svc-jmmdh [311.146085ms]
Feb 14 14:36:30.917: INFO: Created: latency-svc-pg5rk
Feb 14 14:36:30.920: INFO: Got endpoints: latency-svc-pg5rk [324.599324ms]
Feb 14 14:36:30.941: INFO: Created: latency-svc-5ddpn
Feb 14 14:36:30.946: INFO: Got endpoints: latency-svc-5ddpn [330.180642ms]
Feb 14 14:36:30.960: INFO: Created: latency-svc-lgkzt
Feb 14 14:36:30.971: INFO: Got endpoints: latency-svc-lgkzt [333.698883ms]
Feb 14 14:36:30.978: INFO: Created: latency-svc-cbbjk
Feb 14 14:36:30.992: INFO: Got endpoints: latency-svc-cbbjk [320.595524ms]
Feb 14 14:36:31.002: INFO: Created: latency-svc-7wp45
Feb 14 14:36:31.008: INFO: Got endpoints: latency-svc-7wp45 [328.189196ms]
Feb 14 14:36:31.018: INFO: Created: latency-svc-w8r9p
Feb 14 14:36:31.032: INFO: Got endpoints: latency-svc-w8r9p [336.474408ms]
Feb 14 14:36:31.036: INFO: Created: latency-svc-bs7lg
Feb 14 14:36:31.043: INFO: Got endpoints: latency-svc-bs7lg [325.256393ms]
Feb 14 14:36:31.055: INFO: Created: latency-svc-chkxn
Feb 14 14:36:31.061: INFO: Got endpoints: latency-svc-chkxn [323.934956ms]
Feb 14 14:36:31.070: INFO: Created: latency-svc-8zgqr
Feb 14 14:36:31.072: INFO: Got endpoints: latency-svc-8zgqr [313.45139ms]
Feb 14 14:36:31.088: INFO: Created: latency-svc-55k8h
Feb 14 14:36:31.089: INFO: Got endpoints: latency-svc-55k8h [320.445504ms]
Feb 14 14:36:31.102: INFO: Created: latency-svc-c9hcl
Feb 14 14:36:31.104: INFO: Got endpoints: latency-svc-c9hcl [312.270333ms]
Feb 14 14:36:31.116: INFO: Created: latency-svc-626qj
Feb 14 14:36:31.127: INFO: Got endpoints: latency-svc-626qj [317.89903ms]
Feb 14 14:36:31.136: INFO: Created: latency-svc-wq6dh
Feb 14 14:36:31.137: INFO: Got endpoints: latency-svc-wq6dh [307.756072ms]
Feb 14 14:36:31.172: INFO: Created: latency-svc-v52ws
Feb 14 14:36:31.174: INFO: Got endpoints: latency-svc-v52ws [329.160136ms]
Feb 14 14:36:31.227: INFO: Created: latency-svc-vmzbh
Feb 14 14:36:31.235: INFO: Created: latency-svc-v8lw7
Feb 14 14:36:31.242: INFO: Got endpoints: latency-svc-v8lw7 [349.782308ms]
Feb 14 14:36:31.242: INFO: Got endpoints: latency-svc-vmzbh [321.770851ms]
Feb 14 14:36:31.243: INFO: Created: latency-svc-r92z6
Feb 14 14:36:31.279: INFO: Got endpoints: latency-svc-r92z6 [332.16769ms]
Feb 14 14:36:31.293: INFO: Created: latency-svc-49smg
Feb 14 14:36:31.308: INFO: Created: latency-svc-7qrtd
Feb 14 14:36:31.316: INFO: Got endpoints: latency-svc-49smg [344.921671ms]
Feb 14 14:36:31.324: INFO: Created: latency-svc-xdj4v
Feb 14 14:36:31.343: INFO: Created: latency-svc-lhk2s
Feb 14 14:36:31.353: INFO: Created: latency-svc-t7xw5
Feb 14 14:36:31.360: INFO: Created: latency-svc-h9rgf
Feb 14 14:36:31.366: INFO: Got endpoints: latency-svc-7qrtd [373.250437ms]
Feb 14 14:36:31.374: INFO: Created: latency-svc-6bfkt
Feb 14 14:36:31.385: INFO: Created: latency-svc-x9zsx
Feb 14 14:36:31.403: INFO: Created: latency-svc-6c79g
Feb 14 14:36:31.413: INFO: Created: latency-svc-fzg5r
Feb 14 14:36:31.414: INFO: Got endpoints: latency-svc-xdj4v [404.675885ms]
Feb 14 14:36:31.437: INFO: Created: latency-svc-gr8v2
Feb 14 14:36:31.452: INFO: Created: latency-svc-jjd8m
Feb 14 14:36:31.464: INFO: Got endpoints: latency-svc-lhk2s [431.777495ms]
Feb 14 14:36:31.467: INFO: Created: latency-svc-6q47x
Feb 14 14:36:31.588: INFO: Got endpoints: latency-svc-h9rgf [526.481691ms]
Feb 14 14:36:31.589: INFO: Got endpoints: latency-svc-t7xw5 [545.142567ms]
Feb 14 14:36:31.619: INFO: Got endpoints: latency-svc-6bfkt [546.561165ms]
Feb 14 14:36:31.621: INFO: Created: latency-svc-ktg4s
Feb 14 14:36:31.642: INFO: Created: latency-svc-b4f2x
Feb 14 14:36:31.649: INFO: Created: latency-svc-n9szj
Feb 14 14:36:31.661: INFO: Created: latency-svc-6n7hf
Feb 14 14:36:31.664: INFO: Got endpoints: latency-svc-x9zsx [574.810371ms]
Feb 14 14:36:31.675: INFO: Created: latency-svc-v6dxd
Feb 14 14:36:31.690: INFO: Created: latency-svc-8nxjm
Feb 14 14:36:31.724: INFO: Got endpoints: latency-svc-6c79g [620.127699ms]
Feb 14 14:36:31.734: INFO: Created: latency-svc-jc7hb
Feb 14 14:36:31.743: INFO: Created: latency-svc-xf78l
Feb 14 14:36:31.755: INFO: Created: latency-svc-r9lx2
Feb 14 14:36:31.768: INFO: Got endpoints: latency-svc-fzg5r [640.418183ms]
Feb 14 14:36:31.786: INFO: Created: latency-svc-vrt4n
Feb 14 14:36:31.786: INFO: Created: latency-svc-mcprg
Feb 14 14:36:31.801: INFO: Created: latency-svc-jp522
Feb 14 14:36:31.815: INFO: Got endpoints: latency-svc-gr8v2 [677.462389ms]
Feb 14 14:36:31.832: INFO: Created: latency-svc-pnt6w
Feb 14 14:36:31.864: INFO: Got endpoints: latency-svc-jjd8m [689.373052ms]
Feb 14 14:36:31.885: INFO: Created: latency-svc-nmmfb
Feb 14 14:36:31.914: INFO: Got endpoints: latency-svc-6q47x [672.015621ms]
Feb 14 14:36:31.931: INFO: Created: latency-svc-ltc5s
Feb 14 14:36:31.964: INFO: Got endpoints: latency-svc-ktg4s [721.72128ms]
Feb 14 14:36:31.978: INFO: Created: latency-svc-5n8b7
Feb 14 14:36:32.014: INFO: Got endpoints: latency-svc-b4f2x [734.875649ms]
Feb 14 14:36:32.029: INFO: Created: latency-svc-nnlpp
Feb 14 14:36:32.063: INFO: Got endpoints: latency-svc-n9szj [746.751603ms]
Feb 14 14:36:32.078: INFO: Created: latency-svc-zskn7
Feb 14 14:36:32.115: INFO: Got endpoints: latency-svc-6n7hf [748.847118ms]
Feb 14 14:36:32.131: INFO: Created: latency-svc-wcjz2
Feb 14 14:36:32.163: INFO: Got endpoints: latency-svc-v6dxd [748.873119ms]
Feb 14 14:36:32.194: INFO: Created: latency-svc-dsms5
Feb 14 14:36:32.214: INFO: Got endpoints: latency-svc-8nxjm [749.251107ms]
Feb 14 14:36:32.235: INFO: Created: latency-svc-6zqx7
Feb 14 14:36:32.265: INFO: Got endpoints: latency-svc-jc7hb [676.245713ms]
Feb 14 14:36:32.280: INFO: Created: latency-svc-gvm6n
Feb 14 14:36:32.320: INFO: Got endpoints: latency-svc-xf78l [731.273279ms]
Feb 14 14:36:32.348: INFO: Created: latency-svc-24sv4
Feb 14 14:36:32.364: INFO: Got endpoints: latency-svc-r9lx2 [745.537977ms]
Feb 14 14:36:32.380: INFO: Created: latency-svc-fvcf7
Feb 14 14:36:32.414: INFO: Got endpoints: latency-svc-mcprg [749.103379ms]
Feb 14 14:36:32.434: INFO: Created: latency-svc-j7nks
Feb 14 14:36:32.466: INFO: Got endpoints: latency-svc-vrt4n [741.241108ms]
Feb 14 14:36:32.490: INFO: Created: latency-svc-wztrk
Feb 14 14:36:32.514: INFO: Got endpoints: latency-svc-jp522 [745.20495ms]
Feb 14 14:36:32.567: INFO: Got endpoints: latency-svc-pnt6w [752.117434ms]
Feb 14 14:36:32.579: INFO: Created: latency-svc-qg7jq
Feb 14 14:36:32.590: INFO: Created: latency-svc-nhvgr
Feb 14 14:36:32.615: INFO: Got endpoints: latency-svc-nmmfb [750.950516ms]
Feb 14 14:36:32.636: INFO: Created: latency-svc-l99h7
Feb 14 14:36:32.668: INFO: Got endpoints: latency-svc-ltc5s [753.131773ms]
Feb 14 14:36:32.688: INFO: Created: latency-svc-rcp4x
Feb 14 14:36:32.714: INFO: Got endpoints: latency-svc-5n8b7 [750.455138ms]
Feb 14 14:36:32.742: INFO: Created: latency-svc-6f5np
Feb 14 14:36:32.764: INFO: Got endpoints: latency-svc-nnlpp [749.98288ms]
Feb 14 14:36:32.783: INFO: Created: latency-svc-crqrl
Feb 14 14:36:32.816: INFO: Got endpoints: latency-svc-zskn7 [752.694284ms]
Feb 14 14:36:32.837: INFO: Created: latency-svc-6f8q7
Feb 14 14:36:32.869: INFO: Got endpoints: latency-svc-wcjz2 [754.127481ms]
Feb 14 14:36:32.887: INFO: Created: latency-svc-jdkwm
Feb 14 14:36:32.914: INFO: Got endpoints: latency-svc-dsms5 [751.212527ms]
Feb 14 14:36:32.936: INFO: Created: latency-svc-cwgrz
Feb 14 14:36:32.965: INFO: Got endpoints: latency-svc-6zqx7 [750.824848ms]
Feb 14 14:36:32.983: INFO: Created: latency-svc-c8vxb
Feb 14 14:36:33.014: INFO: Got endpoints: latency-svc-gvm6n [749.743121ms]
Feb 14 14:36:33.037: INFO: Created: latency-svc-wqfhb
Feb 14 14:36:33.066: INFO: Got endpoints: latency-svc-24sv4 [745.379671ms]
Feb 14 14:36:33.094: INFO: Created: latency-svc-l9ccv
Feb 14 14:36:33.115: INFO: Got endpoints: latency-svc-fvcf7 [750.00246ms]
Feb 14 14:36:33.130: INFO: Created: latency-svc-52bw9
Feb 14 14:36:33.164: INFO: Got endpoints: latency-svc-j7nks [750.791799ms]
Feb 14 14:36:33.184: INFO: Created: latency-svc-5p6nw
Feb 14 14:36:33.217: INFO: Got endpoints: latency-svc-wztrk [750.804709ms]
Feb 14 14:36:33.232: INFO: Created: latency-svc-d7pnk
Feb 14 14:36:33.266: INFO: Got endpoints: latency-svc-qg7jq [751.478629ms]
Feb 14 14:36:33.284: INFO: Created: latency-svc-2b8vp
Feb 14 14:36:33.315: INFO: Got endpoints: latency-svc-nhvgr [747.714558ms]
Feb 14 14:36:33.329: INFO: Created: latency-svc-bkh4p
Feb 14 14:36:33.363: INFO: Got endpoints: latency-svc-l99h7 [747.769086ms]
Feb 14 14:36:33.381: INFO: Created: latency-svc-dvt97
Feb 14 14:36:33.415: INFO: Got endpoints: latency-svc-rcp4x [746.7419ms]
Feb 14 14:36:33.433: INFO: Created: latency-svc-977qs
Feb 14 14:36:33.467: INFO: Got endpoints: latency-svc-6f5np [752.608747ms]
Feb 14 14:36:33.491: INFO: Created: latency-svc-rnp57
Feb 14 14:36:33.513: INFO: Got endpoints: latency-svc-crqrl [748.164257ms]
Feb 14 14:36:33.537: INFO: Created: latency-svc-jvp8d
Feb 14 14:36:33.566: INFO: Got endpoints: latency-svc-6f8q7 [749.412225ms]
Feb 14 14:36:33.609: INFO: Created: latency-svc-9rt7f
Feb 14 14:36:33.614: INFO: Got endpoints: latency-svc-jdkwm [745.024536ms]
Feb 14 14:36:33.633: INFO: Created: latency-svc-mssbt
Feb 14 14:36:33.668: INFO: Got endpoints: latency-svc-cwgrz [753.303316ms]
Feb 14 14:36:33.690: INFO: Created: latency-svc-mpzrs
Feb 14 14:36:33.720: INFO: Got endpoints: latency-svc-c8vxb [754.760884ms]
Feb 14 14:36:33.746: INFO: Created: latency-svc-khdm4
Feb 14 14:36:33.764: INFO: Got endpoints: latency-svc-wqfhb [749.917615ms]
Feb 14 14:36:33.781: INFO: Created: latency-svc-qgkdr
Feb 14 14:36:33.822: INFO: Got endpoints: latency-svc-l9ccv [756.077452ms]
Feb 14 14:36:33.859: INFO: Created: latency-svc-4nzlj
Feb 14 14:36:33.865: INFO: Got endpoints: latency-svc-52bw9 [750.079315ms]
Feb 14 14:36:33.895: INFO: Created: latency-svc-bm2gj
Feb 14 14:36:33.917: INFO: Got endpoints: latency-svc-5p6nw [752.22205ms]
Feb 14 14:36:33.972: INFO: Got endpoints: latency-svc-d7pnk [755.567392ms]
Feb 14 14:36:34.005: INFO: Created: latency-svc-bmsdb
Feb 14 14:36:34.019: INFO: Got endpoints: latency-svc-2b8vp [752.833198ms]
Feb 14 14:36:34.027: INFO: Created: latency-svc-zw45j
Feb 14 14:36:34.036: INFO: Created: latency-svc-7dcvf
Feb 14 14:36:34.064: INFO: Got endpoints: latency-svc-bkh4p [748.422929ms]
Feb 14 14:36:34.078: INFO: Created: latency-svc-p54lb
Feb 14 14:36:34.114: INFO: Got endpoints: latency-svc-dvt97 [750.910234ms]
Feb 14 14:36:34.138: INFO: Created: latency-svc-6rt67
Feb 14 14:36:34.163: INFO: Got endpoints: latency-svc-977qs [748.481339ms]
Feb 14 14:36:34.177: INFO: Created: latency-svc-pj5n9
Feb 14 14:36:34.218: INFO: Got endpoints: latency-svc-rnp57 [750.669155ms]
Feb 14 14:36:34.234: INFO: Created: latency-svc-xclmn
Feb 14 14:36:34.264: INFO: Got endpoints: latency-svc-jvp8d [750.311335ms]
Feb 14 14:36:34.282: INFO: Created: latency-svc-7ng4p
Feb 14 14:36:34.314: INFO: Got endpoints: latency-svc-9rt7f [748.36423ms]
Feb 14 14:36:34.330: INFO: Created: latency-svc-cxc6z
Feb 14 14:36:34.364: INFO: Got endpoints: latency-svc-mssbt [749.903976ms]
Feb 14 14:36:34.377: INFO: Created: latency-svc-tkcrc
Feb 14 14:36:34.419: INFO: Got endpoints: latency-svc-mpzrs [751.127445ms]
Feb 14 14:36:34.438: INFO: Created: latency-svc-xsgnj
Feb 14 14:36:34.464: INFO: Got endpoints: latency-svc-khdm4 [744.107881ms]
Feb 14 14:36:34.485: INFO: Created: latency-svc-xtvw6
Feb 14 14:36:34.518: INFO: Got endpoints: latency-svc-qgkdr [753.56319ms]
Feb 14 14:36:34.544: INFO: Created: latency-svc-hs24x
Feb 14 14:36:34.568: INFO: Got endpoints: latency-svc-4nzlj [745.408029ms]
Feb 14 14:36:34.593: INFO: Created: latency-svc-xc2t4
Feb 14 14:36:34.614: INFO: Got endpoints: latency-svc-bm2gj [748.821962ms]
Feb 14 14:36:34.634: INFO: Created: latency-svc-x7b77
Feb 14 14:36:34.667: INFO: Got endpoints: latency-svc-bmsdb [749.88131ms]
Feb 14 14:36:34.692: INFO: Created: latency-svc-tzlln
Feb 14 14:36:34.715: INFO: Got endpoints: latency-svc-zw45j [742.620086ms]
Feb 14 14:36:34.736: INFO: Created: latency-svc-vhsh9
Feb 14 14:36:34.767: INFO: Got endpoints: latency-svc-7dcvf [747.820025ms]
Feb 14 14:36:34.783: INFO: Created: latency-svc-b7dct
Feb 14 14:36:34.814: INFO: Got endpoints: latency-svc-p54lb [749.80318ms]
Feb 14 14:36:34.835: INFO: Created: latency-svc-lm8gp
Feb 14 14:36:34.866: INFO: Got endpoints: latency-svc-6rt67 [751.836096ms]
Feb 14 14:36:34.890: INFO: Created: latency-svc-8749b
Feb 14 14:36:34.914: INFO: Got endpoints: latency-svc-pj5n9 [750.981287ms]
Feb 14 14:36:35.008: INFO: Created: latency-svc-pj4mx
Feb 14 14:36:35.008: INFO: Got endpoints: latency-svc-xclmn [790.09792ms]
Feb 14 14:36:35.019: INFO: Got endpoints: latency-svc-7ng4p [755.440048ms]
Feb 14 14:36:35.031: INFO: Created: latency-svc-6qdq4
Feb 14 14:36:35.057: INFO: Created: latency-svc-c6wpm
Feb 14 14:36:35.126: INFO: Got endpoints: latency-svc-tkcrc [761.991983ms]
Feb 14 14:36:35.127: INFO: Got endpoints: latency-svc-cxc6z [812.63836ms]
Feb 14 14:36:35.164: INFO: Got endpoints: latency-svc-xsgnj [745.279781ms]
Feb 14 14:36:35.176: INFO: Created: latency-svc-vrhd8
Feb 14 14:36:35.187: INFO: Created: latency-svc-mx4z4
Feb 14 14:36:35.198: INFO: Created: latency-svc-jstcn
Feb 14 14:36:35.213: INFO: Got endpoints: latency-svc-xtvw6 [748.639774ms]
Feb 14 14:36:35.232: INFO: Created: latency-svc-q2rtq
Feb 14 14:36:35.263: INFO: Got endpoints: latency-svc-hs24x [744.750374ms]
Feb 14 14:36:35.278: INFO: Created: latency-svc-dcpjk
Feb 14 14:36:35.316: INFO: Got endpoints: latency-svc-xc2t4 [747.760256ms]
Feb 14 14:36:35.333: INFO: Created: latency-svc-btjhx
Feb 14 14:36:35.364: INFO: Got endpoints: latency-svc-x7b77 [750.241801ms]
Feb 14 14:36:35.377: INFO: Created: latency-svc-4tdvn
Feb 14 14:36:35.413: INFO: Got endpoints: latency-svc-tzlln [746.297241ms]
Feb 14 14:36:35.429: INFO: Created: latency-svc-ng76k
Feb 14 14:36:35.464: INFO: Got endpoints: latency-svc-vhsh9 [748.969675ms]
Feb 14 14:36:35.479: INFO: Created: latency-svc-jqxnv
Feb 14 14:36:35.515: INFO: Got endpoints: latency-svc-b7dct [748.630356ms]
Feb 14 14:36:35.531: INFO: Created: latency-svc-ckqb4
Feb 14 14:36:35.565: INFO: Got endpoints: latency-svc-lm8gp [750.636832ms]
Feb 14 14:36:35.580: INFO: Created: latency-svc-pf4tn
Feb 14 14:36:35.615: INFO: Got endpoints: latency-svc-8749b [748.521296ms]
Feb 14 14:36:35.629: INFO: Created: latency-svc-2vjmd
Feb 14 14:36:35.664: INFO: Got endpoints: latency-svc-pj4mx [748.981805ms]
Feb 14 14:36:35.685: INFO: Created: latency-svc-hhd82
Feb 14 14:36:35.714: INFO: Got endpoints: latency-svc-6qdq4 [706.077312ms]
Feb 14 14:36:35.735: INFO: Created: latency-svc-tp4rr
Feb 14 14:36:35.763: INFO: Got endpoints: latency-svc-c6wpm [743.165539ms]
Feb 14 14:36:35.782: INFO: Created: latency-svc-nkvvg
Feb 14 14:36:35.815: INFO: Got endpoints: latency-svc-vrhd8 [688.506562ms]
Feb 14 14:36:35.831: INFO: Created: latency-svc-t964z
Feb 14 14:36:35.864: INFO: Got endpoints: latency-svc-mx4z4 [737.495782ms]
Feb 14 14:36:35.901: INFO: Created: latency-svc-vrk2q
Feb 14 14:36:35.915: INFO: Got endpoints: latency-svc-jstcn [749.821686ms]
Feb 14 14:36:35.934: INFO: Created: latency-svc-qrzjb
Feb 14 14:36:35.966: INFO: Got endpoints: latency-svc-q2rtq [752.947498ms]
Feb 14 14:36:36.002: INFO: Created: latency-svc-4tjdx
Feb 14 14:36:36.013: INFO: Got endpoints: latency-svc-dcpjk [749.446196ms]
Feb 14 14:36:36.030: INFO: Created: latency-svc-fqg9d
Feb 14 14:36:36.063: INFO: Got endpoints: latency-svc-btjhx [746.851481ms]
Feb 14 14:36:36.078: INFO: Created: latency-svc-hwqsc
Feb 14 14:36:36.114: INFO: Got endpoints: latency-svc-4tdvn [749.193457ms]
Feb 14 14:36:36.133: INFO: Created: latency-svc-t4pfh
Feb 14 14:36:36.166: INFO: Got endpoints: latency-svc-ng76k [752.44721ms]
Feb 14 14:36:36.179: INFO: Created: latency-svc-g7jvr
Feb 14 14:36:36.215: INFO: Got endpoints: latency-svc-jqxnv [750.555345ms]
Feb 14 14:36:36.234: INFO: Created: latency-svc-dt69m
Feb 14 14:36:36.270: INFO: Got endpoints: latency-svc-ckqb4 [754.110317ms]
Feb 14 14:36:36.284: INFO: Created: latency-svc-77s7n
Feb 14 14:36:36.313: INFO: Got endpoints: latency-svc-pf4tn [747.624472ms]
Feb 14 14:36:36.327: INFO: Created: latency-svc-9lfjn
Feb 14 14:36:36.365: INFO: Got endpoints: latency-svc-2vjmd [749.736048ms]
Feb 14 14:36:36.388: INFO: Created: latency-svc-qpcdj
Feb 14 14:36:36.413: INFO: Got endpoints: latency-svc-hhd82 [749.487189ms]
Feb 14 14:36:36.430: INFO: Created: latency-svc-fpb2c
Feb 14 14:36:36.464: INFO: Got endpoints: latency-svc-tp4rr [749.596479ms]
Feb 14 14:36:36.481: INFO: Created: latency-svc-lqbf6
Feb 14 14:36:36.516: INFO: Got endpoints: latency-svc-nkvvg [752.751522ms]
Feb 14 14:36:36.537: INFO: Created: latency-svc-lcprn
Feb 14 14:36:36.565: INFO: Got endpoints: latency-svc-t964z [749.893947ms]
Feb 14 14:36:36.587: INFO: Created: latency-svc-xlbw4
Feb 14 14:36:36.614: INFO: Got endpoints: latency-svc-vrk2q [749.802678ms]
Feb 14 14:36:36.635: INFO: Created: latency-svc-ckszn
Feb 14 14:36:36.664: INFO: Got endpoints: latency-svc-qrzjb [749.40979ms]
Feb 14 14:36:36.680: INFO: Created: latency-svc-ttgvp
Feb 14 14:36:36.714: INFO: Got endpoints: latency-svc-4tjdx [747.799144ms]
Feb 14 14:36:36.732: INFO: Created: latency-svc-vwsrs
Feb 14 14:36:36.764: INFO: Got endpoints: latency-svc-fqg9d [750.555638ms]
Feb 14 14:36:36.788: INFO: Created: latency-svc-t79rz
Feb 14 14:36:36.815: INFO: Got endpoints: latency-svc-hwqsc [751.792945ms]
Feb 14 14:36:36.830: INFO: Created: latency-svc-nkcqx
Feb 14 14:36:36.873: INFO: Got endpoints: latency-svc-t4pfh [759.277858ms]
Feb 14 14:36:36.903: INFO: Created: latency-svc-xnkrw
Feb 14 14:36:36.914: INFO: Got endpoints: latency-svc-g7jvr [747.932733ms]
Feb 14 14:36:36.926: INFO: Created: latency-svc-wkhqd
Feb 14 14:36:36.963: INFO: Got endpoints: latency-svc-dt69m [747.712684ms]
Feb 14 14:36:36.979: INFO: Created: latency-svc-77ppn
Feb 14 14:36:37.013: INFO: Got endpoints: latency-svc-77s7n [743.544414ms]
Feb 14 14:36:37.030: INFO: Created: latency-svc-zqgzm
Feb 14 14:36:37.063: INFO: Got endpoints: latency-svc-9lfjn [750.491798ms]
Feb 14 14:36:37.083: INFO: Created: latency-svc-vtkvg
Feb 14 14:36:37.113: INFO: Got endpoints: latency-svc-qpcdj [747.961714ms]
Feb 14 14:36:37.127: INFO: Created: latency-svc-7mpwj
Feb 14 14:36:37.165: INFO: Got endpoints: latency-svc-fpb2c [751.239688ms]
Feb 14 14:36:37.198: INFO: Created: latency-svc-plm9l
Feb 14 14:36:37.219: INFO: Got endpoints: latency-svc-lqbf6 [755.027469ms]
Feb 14 14:36:37.236: INFO: Created: latency-svc-gfnsj
Feb 14 14:36:37.263: INFO: Got endpoints: latency-svc-lcprn [747.315937ms]
Feb 14 14:36:37.286: INFO: Created: latency-svc-65dx5
Feb 14 14:36:37.313: INFO: Got endpoints: latency-svc-xlbw4 [748.092195ms]
Feb 14 14:36:37.334: INFO: Created: latency-svc-wr2k9
Feb 14 14:36:37.366: INFO: Got endpoints: latency-svc-ckszn [751.406148ms]
Feb 14 14:36:37.380: INFO: Created: latency-svc-rnmtk
Feb 14 14:36:37.417: INFO: Got endpoints: latency-svc-ttgvp [752.564627ms]
Feb 14 14:36:37.436: INFO: Created: latency-svc-k8cmk
Feb 14 14:36:37.463: INFO: Got endpoints: latency-svc-vwsrs [749.106785ms]
Feb 14 14:36:37.480: INFO: Created: latency-svc-xzk8r
Feb 14 14:36:37.514: INFO: Got endpoints: latency-svc-t79rz [749.446345ms]
Feb 14 14:36:37.538: INFO: Created: latency-svc-mn5ld
Feb 14 14:36:37.563: INFO: Got endpoints: latency-svc-nkcqx [747.938298ms]
Feb 14 14:36:37.581: INFO: Created: latency-svc-9pw7l
Feb 14 14:36:37.613: INFO: Got endpoints: latency-svc-xnkrw [739.887835ms]
Feb 14 14:36:37.632: INFO: Created: latency-svc-8lcqn
Feb 14 14:36:37.663: INFO: Got endpoints: latency-svc-wkhqd [749.176875ms]
Feb 14 14:36:37.681: INFO: Created: latency-svc-nhg7g
Feb 14 14:36:37.716: INFO: Got endpoints: latency-svc-77ppn [753.059717ms]
Feb 14 14:36:37.732: INFO: Created: latency-svc-7q52z
Feb 14 14:36:37.763: INFO: Got endpoints: latency-svc-zqgzm [749.465115ms]
Feb 14 14:36:37.782: INFO: Created: latency-svc-qhjj9
Feb 14 14:36:37.814: INFO: Got endpoints: latency-svc-vtkvg [750.709452ms]
Feb 14 14:36:37.872: INFO: Got endpoints: latency-svc-7mpwj [758.445985ms]
Feb 14 14:36:37.913: INFO: Got endpoints: latency-svc-plm9l [748.216429ms]
Feb 14 14:36:37.964: INFO: Got endpoints: latency-svc-gfnsj [744.445926ms]
Feb 14 14:36:38.013: INFO: Got endpoints: latency-svc-65dx5 [750.209484ms]
Feb 14 14:36:38.065: INFO: Got endpoints: latency-svc-wr2k9 [751.167691ms]
Feb 14 14:36:38.115: INFO: Got endpoints: latency-svc-rnmtk [748.694418ms]
Feb 14 14:36:38.164: INFO: Got endpoints: latency-svc-k8cmk [746.908302ms]
Feb 14 14:36:38.214: INFO: Got endpoints: latency-svc-xzk8r [750.110715ms]
Feb 14 14:36:38.263: INFO: Got endpoints: latency-svc-mn5ld [749.141597ms]
Feb 14 14:36:38.313: INFO: Got endpoints: latency-svc-9pw7l [749.904066ms]
Feb 14 14:36:38.370: INFO: Got endpoints: latency-svc-8lcqn [756.743811ms]
Feb 14 14:36:38.416: INFO: Got endpoints: latency-svc-nhg7g [752.564011ms]
Feb 14 14:36:38.464: INFO: Got endpoints: latency-svc-7q52z [747.181023ms]
Feb 14 14:36:38.514: INFO: Got endpoints: latency-svc-qhjj9 [750.095656ms]
Feb 14 14:36:38.515: INFO: Latencies: [25.203262ms 38.789132ms 59.908674ms 72.838564ms 87.758171ms 122.978642ms 155.004259ms 159.642038ms 175.665192ms 202.719382ms 222.605156ms 239.830027ms 262.369155ms 279.783866ms 291.418779ms 297.182237ms 307.756072ms 309.811798ms 311.146085ms 312.270333ms 313.29093ms 313.45139ms 317.798209ms 317.89903ms 320.445504ms 320.595524ms 321.770851ms 323.934956ms 324.599324ms 325.256393ms 328.189196ms 329.160136ms 329.343044ms 330.180642ms 332.16769ms 333.698883ms 336.474408ms 338.383073ms 344.921671ms 349.782308ms 357.76628ms 373.250437ms 384.909598ms 404.675885ms 411.660688ms 414.367341ms 417.591733ms 421.408415ms 431.777495ms 435.622912ms 438.883176ms 438.970954ms 442.269427ms 443.108075ms 446.252708ms 448.767583ms 450.861339ms 459.872937ms 460.280227ms 526.481691ms 545.142567ms 546.561165ms 574.810371ms 620.127699ms 640.418183ms 672.015621ms 676.245713ms 677.462389ms 688.506562ms 689.373052ms 706.077312ms 721.72128ms 731.273279ms 734.875649ms 737.495782ms 739.887835ms 741.241108ms 742.620086ms 743.165539ms 743.544414ms 744.107881ms 744.445926ms 744.750374ms 745.024536ms 745.20495ms 745.279781ms 745.379671ms 745.408029ms 745.537977ms 746.297241ms 746.7419ms 746.751603ms 746.851481ms 746.908302ms 747.181023ms 747.315937ms 747.624472ms 747.712684ms 747.714558ms 747.760256ms 747.769086ms 747.799144ms 747.820025ms 747.932733ms 747.938298ms 747.961714ms 748.092195ms 748.164257ms 748.216429ms 748.36423ms 748.422929ms 748.481339ms 748.521296ms 748.630356ms 748.639774ms 748.694418ms 748.821962ms 748.847118ms 748.873119ms 748.969675ms 748.981805ms 749.103379ms 749.106785ms 749.141597ms 749.176875ms 749.193457ms 749.251107ms 749.40979ms 749.412225ms 749.446196ms 749.446345ms 749.465115ms 749.487189ms 749.596479ms 749.736048ms 749.743121ms 749.802678ms 749.80318ms 749.821686ms 749.88131ms 749.893947ms 749.903976ms 749.904066ms 749.917615ms 749.98288ms 750.00246ms 750.079315ms 750.095656ms 750.110715ms 750.209484ms 750.241801ms 750.311335ms 750.455138ms 750.491798ms 750.555345ms 750.555638ms 750.636832ms 750.669155ms 750.709452ms 750.791799ms 750.804709ms 750.824848ms 750.910234ms 750.950516ms 750.981287ms 751.127445ms 751.167691ms 751.212527ms 751.239688ms 751.406148ms 751.478629ms 751.792945ms 751.836096ms 752.117434ms 752.22205ms 752.44721ms 752.564011ms 752.564627ms 752.608747ms 752.694284ms 752.751522ms 752.833198ms 752.947498ms 753.059717ms 753.131773ms 753.303316ms 753.56319ms 754.110317ms 754.127481ms 754.760884ms 755.027469ms 755.440048ms 755.567392ms 756.077452ms 756.743811ms 758.445985ms 759.277858ms 761.991983ms 790.09792ms 812.63836ms]
Feb 14 14:36:38.515: INFO: 50 %ile: 747.769086ms
Feb 14 14:36:38.515: INFO: 90 %ile: 752.751522ms
Feb 14 14:36:38.515: INFO: 99 %ile: 790.09792ms
Feb 14 14:36:38.515: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:36:38.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-pb9vj" for this suite.
Feb 14 14:36:50.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:36:50.556: INFO: namespace: e2e-tests-svc-latency-pb9vj, resource: bindings, ignored listing per whitelist
Feb 14 14:36:50.636: INFO: namespace e2e-tests-svc-latency-pb9vj deletion completed in 12.112233634s

• [SLOW TEST:22.912 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:36:50.636: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 14 14:36:50.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:36:51.056: INFO: stderr: ""
Feb 14 14:36:51.056: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 14:36:51.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:36:51.174: INFO: stderr: ""
Feb 14 14:36:51.174: INFO: stdout: "update-demo-nautilus-dcc9g update-demo-nautilus-tscpd "
Feb 14 14:36:51.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-dcc9g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:36:51.278: INFO: stderr: ""
Feb 14 14:36:51.278: INFO: stdout: ""
Feb 14 14:36:51.278: INFO: update-demo-nautilus-dcc9g is created but not running
Feb 14 14:36:56.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:36:56.373: INFO: stderr: ""
Feb 14 14:36:56.373: INFO: stdout: "update-demo-nautilus-dcc9g update-demo-nautilus-tscpd "
Feb 14 14:36:56.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-dcc9g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:36:56.466: INFO: stderr: ""
Feb 14 14:36:56.466: INFO: stdout: "true"
Feb 14 14:36:56.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-dcc9g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:36:56.560: INFO: stderr: ""
Feb 14 14:36:56.560: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 14:36:56.560: INFO: validating pod update-demo-nautilus-dcc9g
Feb 14 14:36:56.564: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 14:36:56.564: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 14:36:56.564: INFO: update-demo-nautilus-dcc9g is verified up and running
Feb 14 14:36:56.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-tscpd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:36:56.654: INFO: stderr: ""
Feb 14 14:36:56.654: INFO: stdout: "true"
Feb 14 14:36:56.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-tscpd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:36:56.747: INFO: stderr: ""
Feb 14 14:36:56.747: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 14:36:56.747: INFO: validating pod update-demo-nautilus-tscpd
Feb 14 14:36:56.752: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 14:36:56.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 14:36:56.752: INFO: update-demo-nautilus-tscpd is verified up and running
STEP: rolling-update to new replication controller
Feb 14 14:36:56.754: INFO: scanned /root for discovery docs: <nil>
Feb 14 14:36:56.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:37:19.191: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 14 14:37:19.191: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 14:37:19.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:37:19.302: INFO: stderr: ""
Feb 14 14:37:19.302: INFO: stdout: "update-demo-kitten-ctv4s update-demo-kitten-xkd9c "
Feb 14 14:37:19.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-kitten-ctv4s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:37:19.396: INFO: stderr: ""
Feb 14 14:37:19.396: INFO: stdout: "true"
Feb 14 14:37:19.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-kitten-ctv4s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:37:19.490: INFO: stderr: ""
Feb 14 14:37:19.490: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 14 14:37:19.490: INFO: validating pod update-demo-kitten-ctv4s
Feb 14 14:37:19.498: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 14 14:37:19.499: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 14 14:37:19.499: INFO: update-demo-kitten-ctv4s is verified up and running
Feb 14 14:37:19.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-kitten-xkd9c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:37:19.592: INFO: stderr: ""
Feb 14 14:37:19.592: INFO: stdout: "true"
Feb 14 14:37:19.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-kitten-xkd9c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-kv7vn'
Feb 14 14:37:19.687: INFO: stderr: ""
Feb 14 14:37:19.687: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 14 14:37:19.687: INFO: validating pod update-demo-kitten-xkd9c
Feb 14 14:37:19.692: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 14 14:37:19.692: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 14 14:37:19.692: INFO: update-demo-kitten-xkd9c is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:37:19.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-kv7vn" for this suite.
Feb 14 14:37:41.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:37:41.793: INFO: namespace: e2e-tests-kubectl-kv7vn, resource: bindings, ignored listing per whitelist
Feb 14 14:37:41.814: INFO: namespace e2e-tests-kubectl-kv7vn deletion completed in 22.118265179s

• [SLOW TEST:51.178 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:37:41.818: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 14 14:37:41.917: INFO: Waiting up to 5m0s for pod "client-containers-159f83a3-3066-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-containers-2jmfp" to be "success or failure"
Feb 14 14:37:41.931: INFO: Pod "client-containers-159f83a3-3066-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 13.864954ms
Feb 14 14:37:43.937: INFO: Pod "client-containers-159f83a3-3066-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019839652s
STEP: Saw pod success
Feb 14 14:37:43.937: INFO: Pod "client-containers-159f83a3-3066-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:37:43.941: INFO: Trying to get logs from node k8s-conformance-node-1 pod client-containers-159f83a3-3066-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:37:43.975: INFO: Waiting for pod client-containers-159f83a3-3066-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:37:43.985: INFO: Pod client-containers-159f83a3-3066-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:37:43.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-2jmfp" for this suite.
Feb 14 14:37:50.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:37:50.024: INFO: namespace: e2e-tests-containers-2jmfp, resource: bindings, ignored listing per whitelist
Feb 14 14:37:50.096: INFO: namespace e2e-tests-containers-2jmfp deletion completed in 6.106957484s

• [SLOW TEST:8.278 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:37:50.096: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 14 14:37:50.196: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-8jkll,SelfLink:/api/v1/namespaces/e2e-tests-watch-8jkll/configmaps/e2e-watch-test-resource-version,UID:1a8c8523-3066-11e9-aaef-f23c91bac0b7,ResourceVersion:7418,Generation:0,CreationTimestamp:2019-02-14 14:37:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 14 14:37:50.196: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-8jkll,SelfLink:/api/v1/namespaces/e2e-tests-watch-8jkll/configmaps/e2e-watch-test-resource-version,UID:1a8c8523-3066-11e9-aaef-f23c91bac0b7,ResourceVersion:7419,Generation:0,CreationTimestamp:2019-02-14 14:37:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:37:50.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-8jkll" for this suite.
Feb 14 14:37:56.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:37:56.236: INFO: namespace: e2e-tests-watch-8jkll, resource: bindings, ignored listing per whitelist
Feb 14 14:37:56.334: INFO: namespace e2e-tests-watch-8jkll deletion completed in 6.133823776s

• [SLOW TEST:6.239 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:37:56.336: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 14 14:37:56.457: INFO: Waiting up to 5m0s for pod "pod-1e49b273-3066-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-bwdgj" to be "success or failure"
Feb 14 14:37:56.481: INFO: Pod "pod-1e49b273-3066-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 24.378872ms
Feb 14 14:37:58.486: INFO: Pod "pod-1e49b273-3066-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028787421s
STEP: Saw pod success
Feb 14 14:37:58.486: INFO: Pod "pod-1e49b273-3066-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:37:58.491: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-1e49b273-3066-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:37:58.531: INFO: Waiting for pod pod-1e49b273-3066-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:37:58.535: INFO: Pod pod-1e49b273-3066-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:37:58.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bwdgj" for this suite.
Feb 14 14:38:04.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:38:04.647: INFO: namespace: e2e-tests-emptydir-bwdgj, resource: bindings, ignored listing per whitelist
Feb 14 14:38:04.654: INFO: namespace e2e-tests-emptydir-bwdgj deletion completed in 6.114960684s

• [SLOW TEST:8.319 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:38:04.656: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 14:38:04.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-jqwlp'
Feb 14 14:38:04.831: INFO: stderr: ""
Feb 14 14:38:04.831: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 14 14:38:09.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-jqwlp -o json'
Feb 14 14:38:09.977: INFO: stderr: ""
Feb 14 14:38:09.977: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.3.42/32\"\n        },\n        \"creationTimestamp\": \"2019-02-14T14:38:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-jqwlp\",\n        \"resourceVersion\": \"7494\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-jqwlp/pods/e2e-test-nginx-pod\",\n        \"uid\": \"23456040-3066-11e9-aaef-f23c91bac0b7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-lq4xx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-conformance-node-1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-lq4xx\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-lq4xx\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T14:38:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T14:38:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T14:38:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T14:38:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://8795a16924a4900f55cbbbee9cd6c868a29cb249d992772f6839d5f7f849ce3f\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-14T14:38:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.219.21\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.3.42\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-14T14:38:04Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 14 14:38:09.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 replace -f - --namespace=e2e-tests-kubectl-jqwlp'
Feb 14 14:38:10.148: INFO: stderr: ""
Feb 14 14:38:10.148: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Feb 14 14:38:10.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-jqwlp'
Feb 14 14:38:12.001: INFO: stderr: ""
Feb 14 14:38:12.001: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:38:12.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jqwlp" for this suite.
Feb 14 14:38:18.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:38:18.058: INFO: namespace: e2e-tests-kubectl-jqwlp, resource: bindings, ignored listing per whitelist
Feb 14 14:38:18.122: INFO: namespace e2e-tests-kubectl-jqwlp deletion completed in 6.117084804s

• [SLOW TEST:13.466 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:38:18.123: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 14 14:38:18.207: INFO: Waiting up to 5m0s for pod "client-containers-2b40e1cd-3066-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-containers-n5t5p" to be "success or failure"
Feb 14 14:38:18.214: INFO: Pod "client-containers-2b40e1cd-3066-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 7.338444ms
Feb 14 14:38:20.219: INFO: Pod "client-containers-2b40e1cd-3066-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011962013s
STEP: Saw pod success
Feb 14 14:38:20.219: INFO: Pod "client-containers-2b40e1cd-3066-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:38:20.222: INFO: Trying to get logs from node k8s-conformance-node-3 pod client-containers-2b40e1cd-3066-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:38:20.240: INFO: Waiting for pod client-containers-2b40e1cd-3066-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:38:20.257: INFO: Pod client-containers-2b40e1cd-3066-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:38:20.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-n5t5p" for this suite.
Feb 14 14:38:26.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:38:26.375: INFO: namespace: e2e-tests-containers-n5t5p, resource: bindings, ignored listing per whitelist
Feb 14 14:38:26.423: INFO: namespace e2e-tests-containers-n5t5p deletion completed in 6.158800371s

• [SLOW TEST:8.301 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:38:26.423: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 14:38:48.537: INFO: Container started at 2019-02-14 14:38:27 +0000 UTC, pod became ready at 2019-02-14 14:38:46 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:38:48.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-b78rk" for this suite.
Feb 14 14:39:10.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:39:10.630: INFO: namespace: e2e-tests-container-probe-b78rk, resource: bindings, ignored listing per whitelist
Feb 14 14:39:10.690: INFO: namespace e2e-tests-container-probe-b78rk deletion completed in 22.148730743s

• [SLOW TEST:44.267 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:39:10.690: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-tjtq
STEP: Creating a pod to test atomic-volume-subpath
Feb 14 14:39:10.774: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tjtq" in namespace "e2e-tests-subpath-h7gmq" to be "success or failure"
Feb 14 14:39:10.780: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.137207ms
Feb 14 14:39:12.784: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01034679s
Feb 14 14:39:14.788: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Running", Reason="", readiness=false. Elapsed: 4.014600377s
Feb 14 14:39:16.792: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Running", Reason="", readiness=false. Elapsed: 6.018722648s
Feb 14 14:39:18.797: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Running", Reason="", readiness=false. Elapsed: 8.022981783s
Feb 14 14:39:20.801: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Running", Reason="", readiness=false. Elapsed: 10.027120981s
Feb 14 14:39:22.804: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Running", Reason="", readiness=false. Elapsed: 12.030617995s
Feb 14 14:39:24.809: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Running", Reason="", readiness=false. Elapsed: 14.035009151s
Feb 14 14:39:26.813: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Running", Reason="", readiness=false. Elapsed: 16.039437149s
Feb 14 14:39:28.819: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Running", Reason="", readiness=false. Elapsed: 18.045151109s
Feb 14 14:39:30.823: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Running", Reason="", readiness=false. Elapsed: 20.049798644s
Feb 14 14:39:32.828: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Running", Reason="", readiness=false. Elapsed: 22.054316623s
Feb 14 14:39:34.831: INFO: Pod "pod-subpath-test-configmap-tjtq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.057074798s
STEP: Saw pod success
Feb 14 14:39:34.831: INFO: Pod "pod-subpath-test-configmap-tjtq" satisfied condition "success or failure"
Feb 14 14:39:34.833: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-subpath-test-configmap-tjtq container test-container-subpath-configmap-tjtq: <nil>
STEP: delete the pod
Feb 14 14:39:34.858: INFO: Waiting for pod pod-subpath-test-configmap-tjtq to disappear
Feb 14 14:39:34.861: INFO: Pod pod-subpath-test-configmap-tjtq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tjtq
Feb 14 14:39:34.861: INFO: Deleting pod "pod-subpath-test-configmap-tjtq" in namespace "e2e-tests-subpath-h7gmq"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:39:34.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-h7gmq" for this suite.
Feb 14 14:39:40.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:39:40.894: INFO: namespace: e2e-tests-subpath-h7gmq, resource: bindings, ignored listing per whitelist
Feb 14 14:39:40.997: INFO: namespace e2e-tests-subpath-h7gmq deletion completed in 6.129901548s

• [SLOW TEST:30.307 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:39:40.998: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 14 14:39:41.075: INFO: Waiting up to 5m0s for pod "pod-5ca5def7-3066-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-vvwhk" to be "success or failure"
Feb 14 14:39:41.082: INFO: Pod "pod-5ca5def7-3066-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 6.403277ms
Feb 14 14:39:43.085: INFO: Pod "pod-5ca5def7-3066-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01010303s
STEP: Saw pod success
Feb 14 14:39:43.085: INFO: Pod "pod-5ca5def7-3066-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:39:43.088: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-5ca5def7-3066-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:39:43.136: INFO: Waiting for pod pod-5ca5def7-3066-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:39:43.158: INFO: Pod pod-5ca5def7-3066-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:39:43.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vvwhk" for this suite.
Feb 14 14:39:49.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:39:49.228: INFO: namespace: e2e-tests-emptydir-vvwhk, resource: bindings, ignored listing per whitelist
Feb 14 14:39:49.275: INFO: namespace e2e-tests-emptydir-vvwhk deletion completed in 6.108282448s

• [SLOW TEST:8.277 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:39:49.275: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 14 14:39:51.415: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-619b7ff0-3066-11e9-a1dd-12216a2f1059", GenerateName:"", Namespace:"e2e-tests-pods-lkrld", SelfLink:"/api/v1/namespaces/e2e-tests-pods-lkrld/pods/pod-submit-remove-619b7ff0-3066-11e9-a1dd-12216a2f1059", UID:"619c22aa-3066-11e9-aaef-f23c91bac0b7", ResourceVersion:"7863", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63685751989, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"388905945"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.2.35/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-mhh5d", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000819c80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-mhh5d", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00104eca8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-conformance-node-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001b35800), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00104f6a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00104f6c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00104f6c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00104f6cc)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685751989, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685751991, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685751991, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685751989, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.184.46", PodIP:"10.244.2.35", StartTime:(*v1.Time)(0xc0013db740), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0013db760), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://2230720e2d490a9222767cddfa5febc823e5bc5843c9b6fba4534ae93b04d117"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 14 14:39:56.435: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:39:56.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-lkrld" for this suite.
Feb 14 14:40:02.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:40:02.521: INFO: namespace: e2e-tests-pods-lkrld, resource: bindings, ignored listing per whitelist
Feb 14 14:40:02.542: INFO: namespace e2e-tests-pods-lkrld deletion completed in 6.099381069s

• [SLOW TEST:13.267 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:40:02.543: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 14:40:02.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-z2vs4'
Feb 14 14:40:02.711: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 14 14:40:02.711: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 14 14:40:02.730: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-2pmpv]
Feb 14 14:40:02.730: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-2pmpv" in namespace "e2e-tests-kubectl-z2vs4" to be "running and ready"
Feb 14 14:40:02.734: INFO: Pod "e2e-test-nginx-rc-2pmpv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029522ms
Feb 14 14:40:04.739: INFO: Pod "e2e-test-nginx-rc-2pmpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.008683418s
Feb 14 14:40:04.739: INFO: Pod "e2e-test-nginx-rc-2pmpv" satisfied condition "running and ready"
Feb 14 14:40:04.739: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-2pmpv]
Feb 14 14:40:04.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-z2vs4'
Feb 14 14:40:04.857: INFO: stderr: ""
Feb 14 14:40:04.857: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Feb 14 14:40:04.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-z2vs4'
Feb 14 14:40:04.946: INFO: stderr: ""
Feb 14 14:40:04.946: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:40:04.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-z2vs4" for this suite.
Feb 14 14:40:26.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:40:26.972: INFO: namespace: e2e-tests-kubectl-z2vs4, resource: bindings, ignored listing per whitelist
Feb 14 14:40:27.048: INFO: namespace e2e-tests-kubectl-z2vs4 deletion completed in 22.098483212s

• [SLOW TEST:24.505 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:40:27.049: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-78177747-3066-11e9-a1dd-12216a2f1059
Feb 14 14:40:27.118: INFO: Pod name my-hostname-basic-78177747-3066-11e9-a1dd-12216a2f1059: Found 0 pods out of 1
Feb 14 14:40:32.122: INFO: Pod name my-hostname-basic-78177747-3066-11e9-a1dd-12216a2f1059: Found 1 pods out of 1
Feb 14 14:40:32.122: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-78177747-3066-11e9-a1dd-12216a2f1059" are running
Feb 14 14:40:32.123: INFO: Pod "my-hostname-basic-78177747-3066-11e9-a1dd-12216a2f1059-htngn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 14:40:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 14:40:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 14:40:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 14:40:27 +0000 UTC Reason: Message:}])
Feb 14 14:40:32.123: INFO: Trying to dial the pod
Feb 14 14:40:37.133: INFO: Controller my-hostname-basic-78177747-3066-11e9-a1dd-12216a2f1059: Got expected result from replica 1 [my-hostname-basic-78177747-3066-11e9-a1dd-12216a2f1059-htngn]: "my-hostname-basic-78177747-3066-11e9-a1dd-12216a2f1059-htngn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:40:37.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-4qddx" for this suite.
Feb 14 14:40:43.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:40:43.207: INFO: namespace: e2e-tests-replication-controller-4qddx, resource: bindings, ignored listing per whitelist
Feb 14 14:40:43.231: INFO: namespace e2e-tests-replication-controller-4qddx deletion completed in 6.094792031s

• [SLOW TEST:16.182 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:40:43.233: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 14 14:40:43.311: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-zwv7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-zwv7m/configmaps/e2e-watch-test-label-changed,UID:81bd88aa-3066-11e9-aaef-f23c91bac0b7,ResourceVersion:8053,Generation:0,CreationTimestamp:2019-02-14 14:40:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 14 14:40:43.311: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-zwv7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-zwv7m/configmaps/e2e-watch-test-label-changed,UID:81bd88aa-3066-11e9-aaef-f23c91bac0b7,ResourceVersion:8054,Generation:0,CreationTimestamp:2019-02-14 14:40:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 14 14:40:43.311: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-zwv7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-zwv7m/configmaps/e2e-watch-test-label-changed,UID:81bd88aa-3066-11e9-aaef-f23c91bac0b7,ResourceVersion:8055,Generation:0,CreationTimestamp:2019-02-14 14:40:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 14 14:40:53.332: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-zwv7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-zwv7m/configmaps/e2e-watch-test-label-changed,UID:81bd88aa-3066-11e9-aaef-f23c91bac0b7,ResourceVersion:8078,Generation:0,CreationTimestamp:2019-02-14 14:40:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 14 14:40:53.333: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-zwv7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-zwv7m/configmaps/e2e-watch-test-label-changed,UID:81bd88aa-3066-11e9-aaef-f23c91bac0b7,ResourceVersion:8079,Generation:0,CreationTimestamp:2019-02-14 14:40:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 14 14:40:53.333: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-zwv7m,SelfLink:/api/v1/namespaces/e2e-tests-watch-zwv7m/configmaps/e2e-watch-test-label-changed,UID:81bd88aa-3066-11e9-aaef-f23c91bac0b7,ResourceVersion:8080,Generation:0,CreationTimestamp:2019-02-14 14:40:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:40:53.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-zwv7m" for this suite.
Feb 14 14:40:59.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:40:59.411: INFO: namespace: e2e-tests-watch-zwv7m, resource: bindings, ignored listing per whitelist
Feb 14 14:40:59.445: INFO: namespace e2e-tests-watch-zwv7m deletion completed in 6.10847525s

• [SLOW TEST:16.213 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:40:59.446: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 14 14:40:59.518: INFO: Waiting up to 5m0s for pod "pod-8b679639-3066-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-94sbv" to be "success or failure"
Feb 14 14:40:59.524: INFO: Pod "pod-8b679639-3066-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075289ms
Feb 14 14:41:01.530: INFO: Pod "pod-8b679639-3066-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012410738s
STEP: Saw pod success
Feb 14 14:41:01.530: INFO: Pod "pod-8b679639-3066-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:41:01.561: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-8b679639-3066-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:41:01.617: INFO: Waiting for pod pod-8b679639-3066-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:41:01.626: INFO: Pod pod-8b679639-3066-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:41:01.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-94sbv" for this suite.
Feb 14 14:41:07.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:41:07.692: INFO: namespace: e2e-tests-emptydir-94sbv, resource: bindings, ignored listing per whitelist
Feb 14 14:41:07.718: INFO: namespace e2e-tests-emptydir-94sbv deletion completed in 6.087670207s

• [SLOW TEST:8.273 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:41:07.721: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0214 14:41:17.853129      16 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 14 14:41:17.853: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:41:17.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-l8cns" for this suite.
Feb 14 14:41:23.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:41:23.907: INFO: namespace: e2e-tests-gc-l8cns, resource: bindings, ignored listing per whitelist
Feb 14 14:41:23.982: INFO: namespace e2e-tests-gc-l8cns deletion completed in 6.126738339s

• [SLOW TEST:16.261 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:41:23.983: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9a0897dc-3066-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 14:41:24.065: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a09252f-3066-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-pj7jq" to be "success or failure"
Feb 14 14:41:24.068: INFO: Pod "pod-projected-configmaps-9a09252f-3066-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 3.171475ms
Feb 14 14:41:26.072: INFO: Pod "pod-projected-configmaps-9a09252f-3066-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006564402s
STEP: Saw pod success
Feb 14 14:41:26.072: INFO: Pod "pod-projected-configmaps-9a09252f-3066-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:41:26.075: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-projected-configmaps-9a09252f-3066-11e9-a1dd-12216a2f1059 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 14:41:26.095: INFO: Waiting for pod pod-projected-configmaps-9a09252f-3066-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:41:26.097: INFO: Pod pod-projected-configmaps-9a09252f-3066-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:41:26.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pj7jq" for this suite.
Feb 14 14:41:32.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:41:32.171: INFO: namespace: e2e-tests-projected-pj7jq, resource: bindings, ignored listing per whitelist
Feb 14 14:41:32.188: INFO: namespace e2e-tests-projected-pj7jq deletion completed in 6.087413749s

• [SLOW TEST:8.205 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:41:32.190: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:41:32.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-lwscq" for this suite.
Feb 14 14:41:38.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:41:38.333: INFO: namespace: e2e-tests-services-lwscq, resource: bindings, ignored listing per whitelist
Feb 14 14:41:38.370: INFO: namespace e2e-tests-services-lwscq deletion completed in 6.114569616s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.180 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:41:38.370: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-a29ca8c7-3066-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 14:41:38.461: INFO: Waiting up to 5m0s for pod "pod-configmaps-a29d2bfb-3066-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-configmap-g8bx7" to be "success or failure"
Feb 14 14:41:38.470: INFO: Pod "pod-configmaps-a29d2bfb-3066-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 7.969019ms
Feb 14 14:41:40.474: INFO: Pod "pod-configmaps-a29d2bfb-3066-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01208569s
STEP: Saw pod success
Feb 14 14:41:40.474: INFO: Pod "pod-configmaps-a29d2bfb-3066-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:41:40.476: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-configmaps-a29d2bfb-3066-11e9-a1dd-12216a2f1059 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 14:41:40.500: INFO: Waiting for pod pod-configmaps-a29d2bfb-3066-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:41:40.503: INFO: Pod pod-configmaps-a29d2bfb-3066-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:41:40.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-g8bx7" for this suite.
Feb 14 14:41:46.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:41:46.604: INFO: namespace: e2e-tests-configmap-g8bx7, resource: bindings, ignored listing per whitelist
Feb 14 14:41:46.619: INFO: namespace e2e-tests-configmap-g8bx7 deletion completed in 6.101554499s

• [SLOW TEST:8.248 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:41:46.619: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 14 14:41:48.699: INFO: Pod pod-hostip-a7852486-3066-11e9-a1dd-12216a2f1059 has hostIP: 192.168.184.46
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:41:48.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-5q9xc" for this suite.
Feb 14 14:42:10.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:42:10.757: INFO: namespace: e2e-tests-pods-5q9xc, resource: bindings, ignored listing per whitelist
Feb 14 14:42:10.784: INFO: namespace e2e-tests-pods-5q9xc deletion completed in 22.081428305s

• [SLOW TEST:24.165 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:42:10.785: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:42:12.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-wk6fn" for this suite.
Feb 14 14:42:50.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:42:50.951: INFO: namespace: e2e-tests-kubelet-test-wk6fn, resource: bindings, ignored listing per whitelist
Feb 14 14:42:51.002: INFO: namespace e2e-tests-kubelet-test-wk6fn deletion completed in 38.113531806s

• [SLOW TEST:40.218 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:42:51.004: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 14 14:42:51.076: INFO: Waiting up to 5m0s for pod "pod-cde55191-3066-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-nfpsd" to be "success or failure"
Feb 14 14:42:51.083: INFO: Pod "pod-cde55191-3066-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 6.559229ms
Feb 14 14:42:53.087: INFO: Pod "pod-cde55191-3066-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010418146s
STEP: Saw pod success
Feb 14 14:42:53.087: INFO: Pod "pod-cde55191-3066-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:42:53.089: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-cde55191-3066-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:42:53.109: INFO: Waiting for pod pod-cde55191-3066-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:42:53.113: INFO: Pod pod-cde55191-3066-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:42:53.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nfpsd" for this suite.
Feb 14 14:42:59.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:42:59.184: INFO: namespace: e2e-tests-emptydir-nfpsd, resource: bindings, ignored listing per whitelist
Feb 14 14:42:59.216: INFO: namespace e2e-tests-emptydir-nfpsd deletion completed in 6.09903183s

• [SLOW TEST:8.213 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:42:59.217: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-vxcfl
Feb 14 14:43:01.302: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-vxcfl
STEP: checking the pod's current state and verifying that restartCount is present
Feb 14 14:43:01.306: INFO: Initial restart count of pod liveness-http is 0
Feb 14 14:43:17.336: INFO: Restart count of pod e2e-tests-container-probe-vxcfl/liveness-http is now 1 (16.030285675s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:43:17.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-vxcfl" for this suite.
Feb 14 14:43:23.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:43:23.415: INFO: namespace: e2e-tests-container-probe-vxcfl, resource: bindings, ignored listing per whitelist
Feb 14 14:43:23.437: INFO: namespace e2e-tests-container-probe-vxcfl deletion completed in 6.087639196s

• [SLOW TEST:24.220 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:43:23.438: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 14:43:23.516: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 14 14:43:23.533: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-5jg79/daemonsets","resourceVersion":"8648"},"items":null}

Feb 14 14:43:23.536: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-5jg79/pods","resourceVersion":"8648"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:43:23.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-5jg79" for this suite.
Feb 14 14:43:29.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:43:29.578: INFO: namespace: e2e-tests-daemonsets-5jg79, resource: bindings, ignored listing per whitelist
Feb 14 14:43:29.642: INFO: namespace e2e-tests-daemonsets-5jg79 deletion completed in 6.092540786s

S [SKIPPING] [6.205 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 14 14:43:23.516: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:43:29.645: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-sl7dg
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 14 14:43:29.705: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 14 14:43:51.804: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.244.3.50 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-sl7dg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 14:43:51.804: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 14:43:52.919: INFO: Found all expected endpoints: [netserver-0]
Feb 14 14:43:52.922: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.244.1.11 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-sl7dg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 14:43:52.922: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 14:43:54.039: INFO: Found all expected endpoints: [netserver-1]
Feb 14 14:43:54.041: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.244.2.42 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-sl7dg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 14:43:54.041: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 14:43:55.151: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:43:55.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-sl7dg" for this suite.
Feb 14 14:44:17.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:44:17.368: INFO: namespace: e2e-tests-pod-network-test-sl7dg, resource: bindings, ignored listing per whitelist
Feb 14 14:44:17.396: INFO: namespace e2e-tests-pod-network-test-sl7dg deletion completed in 22.241392515s

• [SLOW TEST:47.751 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:44:17.397: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 14:44:17.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01669b31-3067-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-f8z2v" to be "success or failure"
Feb 14 14:44:17.489: INFO: Pod "downwardapi-volume-01669b31-3067-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 4.494024ms
Feb 14 14:44:19.493: INFO: Pod "downwardapi-volume-01669b31-3067-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008397425s
STEP: Saw pod success
Feb 14 14:44:19.493: INFO: Pod "downwardapi-volume-01669b31-3067-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:44:19.495: INFO: Trying to get logs from node k8s-conformance-node-3 pod downwardapi-volume-01669b31-3067-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 14:44:19.513: INFO: Waiting for pod downwardapi-volume-01669b31-3067-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:44:19.516: INFO: Pod downwardapi-volume-01669b31-3067-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:44:19.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-f8z2v" for this suite.
Feb 14 14:44:25.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:44:25.565: INFO: namespace: e2e-tests-projected-f8z2v, resource: bindings, ignored listing per whitelist
Feb 14 14:44:25.620: INFO: namespace e2e-tests-projected-f8z2v deletion completed in 6.100605821s

• [SLOW TEST:8.224 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:44:25.621: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 14 14:44:25.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:25.936: INFO: stderr: ""
Feb 14 14:44:25.936: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 14:44:25.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:26.043: INFO: stderr: ""
Feb 14 14:44:26.043: INFO: stdout: "update-demo-nautilus-t7ltd update-demo-nautilus-vf6cv "
Feb 14 14:44:26.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-t7ltd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:26.140: INFO: stderr: ""
Feb 14 14:44:26.140: INFO: stdout: ""
Feb 14 14:44:26.140: INFO: update-demo-nautilus-t7ltd is created but not running
Feb 14 14:44:31.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:31.245: INFO: stderr: ""
Feb 14 14:44:31.245: INFO: stdout: "update-demo-nautilus-t7ltd update-demo-nautilus-vf6cv "
Feb 14 14:44:31.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-t7ltd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:31.346: INFO: stderr: ""
Feb 14 14:44:31.346: INFO: stdout: "true"
Feb 14 14:44:31.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-t7ltd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:31.460: INFO: stderr: ""
Feb 14 14:44:31.460: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 14:44:31.460: INFO: validating pod update-demo-nautilus-t7ltd
Feb 14 14:44:31.465: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 14:44:31.465: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 14:44:31.465: INFO: update-demo-nautilus-t7ltd is verified up and running
Feb 14 14:44:31.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-vf6cv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:31.563: INFO: stderr: ""
Feb 14 14:44:31.563: INFO: stdout: "true"
Feb 14 14:44:31.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-vf6cv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:31.668: INFO: stderr: ""
Feb 14 14:44:31.668: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 14:44:31.668: INFO: validating pod update-demo-nautilus-vf6cv
Feb 14 14:44:31.673: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 14:44:31.673: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 14:44:31.673: INFO: update-demo-nautilus-vf6cv is verified up and running
STEP: scaling down the replication controller
Feb 14 14:44:31.675: INFO: scanned /root for discovery docs: <nil>
Feb 14 14:44:31.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:32.802: INFO: stderr: ""
Feb 14 14:44:32.802: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 14:44:32.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:32.900: INFO: stderr: ""
Feb 14 14:44:32.900: INFO: stdout: "update-demo-nautilus-t7ltd update-demo-nautilus-vf6cv "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 14 14:44:37.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:37.994: INFO: stderr: ""
Feb 14 14:44:37.994: INFO: stdout: "update-demo-nautilus-vf6cv "
Feb 14 14:44:37.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-vf6cv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:38.095: INFO: stderr: ""
Feb 14 14:44:38.095: INFO: stdout: "true"
Feb 14 14:44:38.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-vf6cv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:38.199: INFO: stderr: ""
Feb 14 14:44:38.199: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 14:44:38.199: INFO: validating pod update-demo-nautilus-vf6cv
Feb 14 14:44:38.205: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 14:44:38.205: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 14:44:38.205: INFO: update-demo-nautilus-vf6cv is verified up and running
STEP: scaling up the replication controller
Feb 14 14:44:38.206: INFO: scanned /root for discovery docs: <nil>
Feb 14 14:44:38.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:39.349: INFO: stderr: ""
Feb 14 14:44:39.349: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 14:44:39.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:39.463: INFO: stderr: ""
Feb 14 14:44:39.463: INFO: stdout: "update-demo-nautilus-57z69 update-demo-nautilus-vf6cv "
Feb 14 14:44:39.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-57z69 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:39.559: INFO: stderr: ""
Feb 14 14:44:39.559: INFO: stdout: "true"
Feb 14 14:44:39.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-57z69 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:39.660: INFO: stderr: ""
Feb 14 14:44:39.660: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 14:44:39.660: INFO: validating pod update-demo-nautilus-57z69
Feb 14 14:44:39.665: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 14:44:39.665: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 14:44:39.665: INFO: update-demo-nautilus-57z69 is verified up and running
Feb 14 14:44:39.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-vf6cv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:39.758: INFO: stderr: ""
Feb 14 14:44:39.758: INFO: stdout: "true"
Feb 14 14:44:39.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-vf6cv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:39.859: INFO: stderr: ""
Feb 14 14:44:39.859: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 14:44:39.859: INFO: validating pod update-demo-nautilus-vf6cv
Feb 14 14:44:39.863: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 14:44:39.863: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 14:44:39.863: INFO: update-demo-nautilus-vf6cv is verified up and running
STEP: using delete to clean up resources
Feb 14 14:44:39.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:39.962: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 14:44:39.962: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 14 14:44:39.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-xrwzb'
Feb 14 14:44:40.086: INFO: stderr: "No resources found.\n"
Feb 14 14:44:40.086: INFO: stdout: ""
Feb 14 14:44:40.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -l name=update-demo --namespace=e2e-tests-kubectl-xrwzb -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 14 14:44:40.191: INFO: stderr: ""
Feb 14 14:44:40.191: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:44:40.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xrwzb" for this suite.
Feb 14 14:45:02.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:45:02.252: INFO: namespace: e2e-tests-kubectl-xrwzb, resource: bindings, ignored listing per whitelist
Feb 14 14:45:02.289: INFO: namespace e2e-tests-kubectl-xrwzb deletion completed in 22.093566095s

• [SLOW TEST:36.668 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:45:02.290: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 14:45:02.379: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c2933ae-3067-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-m4rl4" to be "success or failure"
Feb 14 14:45:02.402: INFO: Pod "downwardapi-volume-1c2933ae-3067-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 23.056877ms
Feb 14 14:45:04.406: INFO: Pod "downwardapi-volume-1c2933ae-3067-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026657892s
STEP: Saw pod success
Feb 14 14:45:04.406: INFO: Pod "downwardapi-volume-1c2933ae-3067-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:45:04.409: INFO: Trying to get logs from node k8s-conformance-node-3 pod downwardapi-volume-1c2933ae-3067-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 14:45:04.429: INFO: Waiting for pod downwardapi-volume-1c2933ae-3067-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:45:04.431: INFO: Pod downwardapi-volume-1c2933ae-3067-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:45:04.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-m4rl4" for this suite.
Feb 14 14:45:10.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:45:10.514: INFO: namespace: e2e-tests-projected-m4rl4, resource: bindings, ignored listing per whitelist
Feb 14 14:45:10.525: INFO: namespace e2e-tests-projected-m4rl4 deletion completed in 6.09025649s

• [SLOW TEST:8.235 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:45:10.526: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-210ea3f8-3067-11e9-a1dd-12216a2f1059
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:45:12.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bxz6z" for this suite.
Feb 14 14:45:34.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:45:34.647: INFO: namespace: e2e-tests-configmap-bxz6z, resource: bindings, ignored listing per whitelist
Feb 14 14:45:34.710: INFO: namespace e2e-tests-configmap-bxz6z deletion completed in 22.087895211s

• [SLOW TEST:24.184 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:45:34.710: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-tljlv
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-tljlv
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-tljlv
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-tljlv
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-tljlv
Feb 14 14:45:36.820: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-tljlv, name: ss-0, uid: 2f8b1905-3067-11e9-aaef-f23c91bac0b7, status phase: Pending. Waiting for statefulset controller to delete.
Feb 14 14:45:37.435: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-tljlv, name: ss-0, uid: 2f8b1905-3067-11e9-aaef-f23c91bac0b7, status phase: Failed. Waiting for statefulset controller to delete.
Feb 14 14:45:37.442: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-tljlv, name: ss-0, uid: 2f8b1905-3067-11e9-aaef-f23c91bac0b7, status phase: Failed. Waiting for statefulset controller to delete.
Feb 14 14:45:37.457: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-tljlv
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-tljlv
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-tljlv and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 14 14:45:39.516: INFO: Deleting all statefulset in ns e2e-tests-statefulset-tljlv
Feb 14 14:45:39.519: INFO: Scaling statefulset ss to 0
Feb 14 14:45:49.531: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 14:45:49.533: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:45:49.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-tljlv" for this suite.
Feb 14 14:45:55.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:45:55.626: INFO: namespace: e2e-tests-statefulset-tljlv, resource: bindings, ignored listing per whitelist
Feb 14 14:45:55.645: INFO: namespace e2e-tests-statefulset-tljlv deletion completed in 6.091938111s

• [SLOW TEST:20.935 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:45:55.645: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-3bf31608-3067-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 14:45:55.717: INFO: Waiting up to 5m0s for pod "pod-secrets-3bf39543-3067-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-secrets-pgm65" to be "success or failure"
Feb 14 14:45:55.729: INFO: Pod "pod-secrets-3bf39543-3067-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 11.973283ms
Feb 14 14:45:57.734: INFO: Pod "pod-secrets-3bf39543-3067-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016542956s
STEP: Saw pod success
Feb 14 14:45:57.734: INFO: Pod "pod-secrets-3bf39543-3067-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:45:57.736: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-secrets-3bf39543-3067-11e9-a1dd-12216a2f1059 container secret-volume-test: <nil>
STEP: delete the pod
Feb 14 14:45:57.758: INFO: Waiting for pod pod-secrets-3bf39543-3067-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:45:57.760: INFO: Pod pod-secrets-3bf39543-3067-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:45:57.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pgm65" for this suite.
Feb 14 14:46:03.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:46:03.863: INFO: namespace: e2e-tests-secrets-pgm65, resource: bindings, ignored listing per whitelist
Feb 14 14:46:03.866: INFO: namespace e2e-tests-secrets-pgm65 deletion completed in 6.103076108s

• [SLOW TEST:8.222 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:46:03.867: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 14 14:46:10.972: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:46:10.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-xxwmn" for this suite.
Feb 14 14:46:33.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:46:33.044: INFO: namespace: e2e-tests-replicaset-xxwmn, resource: bindings, ignored listing per whitelist
Feb 14 14:46:33.093: INFO: namespace e2e-tests-replicaset-xxwmn deletion completed in 22.087955217s

• [SLOW TEST:29.226 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:46:33.095: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 14 14:46:35.699: INFO: Successfully updated pod "labelsupdate524515b0-3067-11e9-a1dd-12216a2f1059"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:46:39.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mhnm7" for this suite.
Feb 14 14:47:01.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:47:01.801: INFO: namespace: e2e-tests-projected-mhnm7, resource: bindings, ignored listing per whitelist
Feb 14 14:47:01.823: INFO: namespace e2e-tests-projected-mhnm7 deletion completed in 22.085912736s

• [SLOW TEST:28.729 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:47:01.824: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Feb 14 14:47:01.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-j2lx7'
Feb 14 14:47:02.144: INFO: stderr: ""
Feb 14 14:47:02.144: INFO: stdout: "pod/pause created\n"
Feb 14 14:47:02.144: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 14 14:47:02.144: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-j2lx7" to be "running and ready"
Feb 14 14:47:02.149: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.54283ms
Feb 14 14:47:04.152: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007647587s
Feb 14 14:47:04.152: INFO: Pod "pause" satisfied condition "running and ready"
Feb 14 14:47:04.152: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 14 14:47:04.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-j2lx7'
Feb 14 14:47:04.256: INFO: stderr: ""
Feb 14 14:47:04.256: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 14 14:47:04.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pod pause -L testing-label --namespace=e2e-tests-kubectl-j2lx7'
Feb 14 14:47:04.356: INFO: stderr: ""
Feb 14 14:47:04.356: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 14 14:47:04.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 label pods pause testing-label- --namespace=e2e-tests-kubectl-j2lx7'
Feb 14 14:47:04.466: INFO: stderr: ""
Feb 14 14:47:04.466: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 14 14:47:04.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pod pause -L testing-label --namespace=e2e-tests-kubectl-j2lx7'
Feb 14 14:47:04.614: INFO: stderr: ""
Feb 14 14:47:04.614: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Feb 14 14:47:04.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-j2lx7'
Feb 14 14:47:04.700: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 14:47:04.700: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 14 14:47:04.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-j2lx7'
Feb 14 14:47:04.788: INFO: stderr: "No resources found.\n"
Feb 14 14:47:04.788: INFO: stdout: ""
Feb 14 14:47:04.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -l name=pause --namespace=e2e-tests-kubectl-j2lx7 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 14 14:47:04.885: INFO: stderr: ""
Feb 14 14:47:04.885: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:47:04.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-j2lx7" for this suite.
Feb 14 14:47:10.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:47:10.930: INFO: namespace: e2e-tests-kubectl-j2lx7, resource: bindings, ignored listing per whitelist
Feb 14 14:47:10.979: INFO: namespace e2e-tests-kubectl-j2lx7 deletion completed in 6.088060141s

• [SLOW TEST:9.155 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:47:10.979: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-68d947e5-3067-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 14:47:11.046: INFO: Waiting up to 5m0s for pod "pod-secrets-68d9de1d-3067-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-secrets-zblmn" to be "success or failure"
Feb 14 14:47:11.053: INFO: Pod "pod-secrets-68d9de1d-3067-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 7.457938ms
Feb 14 14:47:13.056: INFO: Pod "pod-secrets-68d9de1d-3067-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010507785s
STEP: Saw pod success
Feb 14 14:47:13.056: INFO: Pod "pod-secrets-68d9de1d-3067-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:47:13.058: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-secrets-68d9de1d-3067-11e9-a1dd-12216a2f1059 container secret-volume-test: <nil>
STEP: delete the pod
Feb 14 14:47:13.079: INFO: Waiting for pod pod-secrets-68d9de1d-3067-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:47:13.082: INFO: Pod pod-secrets-68d9de1d-3067-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:47:13.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-zblmn" for this suite.
Feb 14 14:47:19.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:47:19.200: INFO: namespace: e2e-tests-secrets-zblmn, resource: bindings, ignored listing per whitelist
Feb 14 14:47:19.207: INFO: namespace e2e-tests-secrets-zblmn deletion completed in 6.121227679s

• [SLOW TEST:8.228 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:47:19.209: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 14:47:19.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-4z5cz'
Feb 14 14:47:19.371: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 14 14:47:19.371: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 14 14:47:19.382: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 14 14:47:19.386: INFO: scanned /root for discovery docs: <nil>
Feb 14 14:47:19.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-4z5cz'
Feb 14 14:47:35.236: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 14 14:47:35.236: INFO: stdout: "Created e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269\nScaling up e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 14 14:47:35.236: INFO: stdout: "Created e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269\nScaling up e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 14 14:47:35.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-4z5cz'
Feb 14 14:47:35.339: INFO: stderr: ""
Feb 14 14:47:35.339: INFO: stdout: "e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269-ldhs6 e2e-test-nginx-rc-x2s4r "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb 14 14:47:40.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-4z5cz'
Feb 14 14:47:40.444: INFO: stderr: ""
Feb 14 14:47:40.444: INFO: stdout: "e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269-ldhs6 "
Feb 14 14:47:40.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269-ldhs6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4z5cz'
Feb 14 14:47:40.553: INFO: stderr: ""
Feb 14 14:47:40.553: INFO: stdout: "true"
Feb 14 14:47:40.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269-ldhs6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4z5cz'
Feb 14 14:47:40.663: INFO: stderr: ""
Feb 14 14:47:40.663: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb 14 14:47:40.663: INFO: e2e-test-nginx-rc-2c76734cba61f6762d7cc25812a91269-ldhs6 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Feb 14 14:47:40.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-4z5cz'
Feb 14 14:47:40.785: INFO: stderr: ""
Feb 14 14:47:40.785: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:47:40.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4z5cz" for this suite.
Feb 14 14:48:02.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:48:02.879: INFO: namespace: e2e-tests-kubectl-4z5cz, resource: bindings, ignored listing per whitelist
Feb 14 14:48:02.923: INFO: namespace e2e-tests-kubectl-4z5cz deletion completed in 22.116387572s

• [SLOW TEST:43.714 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:48:02.924: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:48:03.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-hkjzh" for this suite.
Feb 14 14:48:09.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:48:09.051: INFO: namespace: e2e-tests-kubelet-test-hkjzh, resource: bindings, ignored listing per whitelist
Feb 14 14:48:09.101: INFO: namespace e2e-tests-kubelet-test-hkjzh deletion completed in 6.083945631s

• [SLOW TEST:6.178 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:48:09.102: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-zq7xq
Feb 14 14:48:11.194: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-zq7xq
STEP: checking the pod's current state and verifying that restartCount is present
Feb 14 14:48:11.197: INFO: Initial restart count of pod liveness-http is 0
Feb 14 14:48:27.228: INFO: Restart count of pod e2e-tests-container-probe-zq7xq/liveness-http is now 1 (16.031607336s elapsed)
Feb 14 14:48:47.264: INFO: Restart count of pod e2e-tests-container-probe-zq7xq/liveness-http is now 2 (36.067489446s elapsed)
Feb 14 14:49:07.301: INFO: Restart count of pod e2e-tests-container-probe-zq7xq/liveness-http is now 3 (56.104147125s elapsed)
Feb 14 14:49:27.335: INFO: Restart count of pod e2e-tests-container-probe-zq7xq/liveness-http is now 4 (1m16.138731933s elapsed)
Feb 14 14:50:29.446: INFO: Restart count of pod e2e-tests-container-probe-zq7xq/liveness-http is now 5 (2m18.249358439s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:50:29.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-zq7xq" for this suite.
Feb 14 14:50:35.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:50:35.551: INFO: namespace: e2e-tests-container-probe-zq7xq, resource: bindings, ignored listing per whitelist
Feb 14 14:50:35.583: INFO: namespace e2e-tests-container-probe-zq7xq deletion completed in 6.114401074s

• [SLOW TEST:146.481 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:50:35.584: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 14 14:50:35.653: INFO: Waiting up to 5m0s for pod "downward-api-e2ce6fc3-3067-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-z7kjc" to be "success or failure"
Feb 14 14:50:35.658: INFO: Pod "downward-api-e2ce6fc3-3067-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 5.096465ms
Feb 14 14:50:37.662: INFO: Pod "downward-api-e2ce6fc3-3067-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009119322s
STEP: Saw pod success
Feb 14 14:50:37.662: INFO: Pod "downward-api-e2ce6fc3-3067-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:50:37.664: INFO: Trying to get logs from node k8s-conformance-node-3 pod downward-api-e2ce6fc3-3067-11e9-a1dd-12216a2f1059 container dapi-container: <nil>
STEP: delete the pod
Feb 14 14:50:37.687: INFO: Waiting for pod downward-api-e2ce6fc3-3067-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:50:37.691: INFO: Pod downward-api-e2ce6fc3-3067-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:50:37.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-z7kjc" for this suite.
Feb 14 14:50:43.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:50:43.721: INFO: namespace: e2e-tests-downward-api-z7kjc, resource: bindings, ignored listing per whitelist
Feb 14 14:50:43.783: INFO: namespace e2e-tests-downward-api-z7kjc deletion completed in 6.087942949s

• [SLOW TEST:8.199 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:50:43.785: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-e7b4de52-3067-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 14:50:43.882: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7b55bf3-3067-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-mcwk4" to be "success or failure"
Feb 14 14:50:43.897: INFO: Pod "pod-projected-configmaps-e7b55bf3-3067-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 15.265063ms
Feb 14 14:50:45.901: INFO: Pod "pod-projected-configmaps-e7b55bf3-3067-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019221565s
Feb 14 14:50:47.905: INFO: Pod "pod-projected-configmaps-e7b55bf3-3067-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023135572s
STEP: Saw pod success
Feb 14 14:50:47.905: INFO: Pod "pod-projected-configmaps-e7b55bf3-3067-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:50:47.909: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-projected-configmaps-e7b55bf3-3067-11e9-a1dd-12216a2f1059 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 14:50:47.949: INFO: Waiting for pod pod-projected-configmaps-e7b55bf3-3067-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:50:47.952: INFO: Pod pod-projected-configmaps-e7b55bf3-3067-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:50:47.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mcwk4" for this suite.
Feb 14 14:50:53.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:50:54.041: INFO: namespace: e2e-tests-projected-mcwk4, resource: bindings, ignored listing per whitelist
Feb 14 14:50:54.042: INFO: namespace e2e-tests-projected-mcwk4 deletion completed in 6.085655008s

• [SLOW TEST:10.257 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:50:54.042: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-edcf5c09-3067-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 14:50:54.117: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-edd002d7-3067-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-ps7z2" to be "success or failure"
Feb 14 14:50:54.119: INFO: Pod "pod-projected-secrets-edd002d7-3067-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.192307ms
Feb 14 14:50:56.190: INFO: Pod "pod-projected-secrets-edd002d7-3067-11e9-a1dd-12216a2f1059": Phase="Running", Reason="", readiness=true. Elapsed: 2.073007167s
Feb 14 14:50:58.195: INFO: Pod "pod-projected-secrets-edd002d7-3067-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077701976s
STEP: Saw pod success
Feb 14 14:50:58.195: INFO: Pod "pod-projected-secrets-edd002d7-3067-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:50:58.198: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-projected-secrets-edd002d7-3067-11e9-a1dd-12216a2f1059 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 14 14:50:58.274: INFO: Waiting for pod pod-projected-secrets-edd002d7-3067-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:50:58.278: INFO: Pod pod-projected-secrets-edd002d7-3067-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:50:58.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ps7z2" for this suite.
Feb 14 14:51:04.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:51:04.342: INFO: namespace: e2e-tests-projected-ps7z2, resource: bindings, ignored listing per whitelist
Feb 14 14:51:04.413: INFO: namespace e2e-tests-projected-ps7z2 deletion completed in 6.131380708s

• [SLOW TEST:10.371 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:51:04.414: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 14 14:51:04.498: INFO: Waiting up to 5m0s for pod "pod-f3ff3f0c-3067-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-kgjss" to be "success or failure"
Feb 14 14:51:04.504: INFO: Pod "pod-f3ff3f0c-3067-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 5.583113ms
Feb 14 14:51:06.507: INFO: Pod "pod-f3ff3f0c-3067-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009102575s
STEP: Saw pod success
Feb 14 14:51:06.508: INFO: Pod "pod-f3ff3f0c-3067-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:51:06.511: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-f3ff3f0c-3067-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:51:06.531: INFO: Waiting for pod pod-f3ff3f0c-3067-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:51:06.534: INFO: Pod pod-f3ff3f0c-3067-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:51:06.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kgjss" for this suite.
Feb 14 14:51:12.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:51:12.613: INFO: namespace: e2e-tests-emptydir-kgjss, resource: bindings, ignored listing per whitelist
Feb 14 14:51:12.638: INFO: namespace e2e-tests-emptydir-kgjss deletion completed in 6.100502954s

• [SLOW TEST:8.224 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:51:12.640: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-f8e75bf5-3067-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 14:51:12.731: INFO: Waiting up to 5m0s for pod "pod-secrets-f8e814af-3067-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-secrets-tnk8r" to be "success or failure"
Feb 14 14:51:12.744: INFO: Pod "pod-secrets-f8e814af-3067-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 12.320818ms
Feb 14 14:51:14.748: INFO: Pod "pod-secrets-f8e814af-3067-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016402186s
STEP: Saw pod success
Feb 14 14:51:14.748: INFO: Pod "pod-secrets-f8e814af-3067-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:51:14.750: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-secrets-f8e814af-3067-11e9-a1dd-12216a2f1059 container secret-env-test: <nil>
STEP: delete the pod
Feb 14 14:51:14.774: INFO: Waiting for pod pod-secrets-f8e814af-3067-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:51:14.776: INFO: Pod pod-secrets-f8e814af-3067-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:51:14.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-tnk8r" for this suite.
Feb 14 14:51:20.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:51:20.829: INFO: namespace: e2e-tests-secrets-tnk8r, resource: bindings, ignored listing per whitelist
Feb 14 14:51:20.883: INFO: namespace e2e-tests-secrets-tnk8r deletion completed in 6.10332878s

• [SLOW TEST:8.243 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:51:20.884: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:51:20.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-c7rtm" for this suite.
Feb 14 14:51:43.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:51:43.058: INFO: namespace: e2e-tests-pods-c7rtm, resource: bindings, ignored listing per whitelist
Feb 14 14:51:43.105: INFO: namespace e2e-tests-pods-c7rtm deletion completed in 22.121005521s

• [SLOW TEST:22.221 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:51:43.107: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 14:51:43.181: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b0ea293-3068-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-vh4bk" to be "success or failure"
Feb 14 14:51:43.184: INFO: Pod "downwardapi-volume-0b0ea293-3068-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.996777ms
Feb 14 14:51:45.188: INFO: Pod "downwardapi-volume-0b0ea293-3068-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007141512s
STEP: Saw pod success
Feb 14 14:51:45.188: INFO: Pod "downwardapi-volume-0b0ea293-3068-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:51:45.192: INFO: Trying to get logs from node k8s-conformance-node-3 pod downwardapi-volume-0b0ea293-3068-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 14:51:45.225: INFO: Waiting for pod downwardapi-volume-0b0ea293-3068-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:51:45.227: INFO: Pod downwardapi-volume-0b0ea293-3068-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:51:45.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vh4bk" for this suite.
Feb 14 14:51:51.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:51:51.302: INFO: namespace: e2e-tests-projected-vh4bk, resource: bindings, ignored listing per whitelist
Feb 14 14:51:51.328: INFO: namespace e2e-tests-projected-vh4bk deletion completed in 6.097131233s

• [SLOW TEST:8.221 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:51:51.329: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 14 14:51:53.934: INFO: Successfully updated pod "labelsupdate0ff41acf-3068-11e9-a1dd-12216a2f1059"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:51:57.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5b5cd" for this suite.
Feb 14 14:52:19.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:52:19.998: INFO: namespace: e2e-tests-downward-api-5b5cd, resource: bindings, ignored listing per whitelist
Feb 14 14:52:20.056: INFO: namespace e2e-tests-downward-api-5b5cd deletion completed in 22.084300789s

• [SLOW TEST:28.727 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:52:20.061: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-21145188-3068-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 14:52:20.133: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2114e56b-3068-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-4v4tv" to be "success or failure"
Feb 14 14:52:20.144: INFO: Pod "pod-projected-configmaps-2114e56b-3068-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 11.33887ms
Feb 14 14:52:22.148: INFO: Pod "pod-projected-configmaps-2114e56b-3068-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014965452s
STEP: Saw pod success
Feb 14 14:52:22.148: INFO: Pod "pod-projected-configmaps-2114e56b-3068-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:52:22.150: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-projected-configmaps-2114e56b-3068-11e9-a1dd-12216a2f1059 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 14:52:22.169: INFO: Waiting for pod pod-projected-configmaps-2114e56b-3068-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:52:22.171: INFO: Pod pod-projected-configmaps-2114e56b-3068-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:52:22.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4v4tv" for this suite.
Feb 14 14:52:28.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:52:28.234: INFO: namespace: e2e-tests-projected-4v4tv, resource: bindings, ignored listing per whitelist
Feb 14 14:52:28.264: INFO: namespace e2e-tests-projected-4v4tv deletion completed in 6.088911556s

• [SLOW TEST:8.203 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:52:28.266: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 14 14:52:32.386: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 14 14:52:32.389: INFO: Pod pod-with-prestop-http-hook still exists
Feb 14 14:52:34.389: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 14 14:52:34.393: INFO: Pod pod-with-prestop-http-hook still exists
Feb 14 14:52:36.389: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 14 14:52:36.392: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:52:36.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-7txsx" for this suite.
Feb 14 14:52:58.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:52:58.542: INFO: namespace: e2e-tests-container-lifecycle-hook-7txsx, resource: bindings, ignored listing per whitelist
Feb 14 14:52:58.548: INFO: namespace e2e-tests-container-lifecycle-hook-7txsx deletion completed in 22.141900354s

• [SLOW TEST:30.282 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:52:58.548: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-7cfvp
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 14 14:52:58.615: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 14 14:53:22.711: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.61:8080/dial?request=hostName&protocol=http&host=10.244.2.60&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-7cfvp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 14:53:22.711: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 14:53:22.861: INFO: Waiting for endpoints: map[]
Feb 14 14:53:22.864: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.61:8080/dial?request=hostName&protocol=http&host=10.244.3.63&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-7cfvp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 14:53:22.864: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 14:53:22.986: INFO: Waiting for endpoints: map[]
Feb 14 14:53:22.989: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.61:8080/dial?request=hostName&protocol=http&host=10.244.1.12&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-7cfvp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 14:53:22.989: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 14:53:23.141: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:53:23.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-7cfvp" for this suite.
Feb 14 14:53:45.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:53:45.194: INFO: namespace: e2e-tests-pod-network-test-7cfvp, resource: bindings, ignored listing per whitelist
Feb 14 14:53:45.232: INFO: namespace e2e-tests-pod-network-test-7cfvp deletion completed in 22.083465655s

• [SLOW TEST:46.684 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:53:45.235: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 14:53:45.305: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53d92dce-3068-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-tmbpr" to be "success or failure"
Feb 14 14:53:45.309: INFO: Pod "downwardapi-volume-53d92dce-3068-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171085ms
Feb 14 14:53:47.313: INFO: Pod "downwardapi-volume-53d92dce-3068-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008064279s
STEP: Saw pod success
Feb 14 14:53:47.313: INFO: Pod "downwardapi-volume-53d92dce-3068-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:53:47.315: INFO: Trying to get logs from node k8s-conformance-node-1 pod downwardapi-volume-53d92dce-3068-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 14:53:47.340: INFO: Waiting for pod downwardapi-volume-53d92dce-3068-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:53:47.342: INFO: Pod downwardapi-volume-53d92dce-3068-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:53:47.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tmbpr" for this suite.
Feb 14 14:53:53.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:53:53.434: INFO: namespace: e2e-tests-downward-api-tmbpr, resource: bindings, ignored listing per whitelist
Feb 14 14:53:53.437: INFO: namespace e2e-tests-downward-api-tmbpr deletion completed in 6.087380607s

• [SLOW TEST:8.202 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:53:53.437: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 14:53:53.505: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58bc2d6e-3068-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-tc7pj" to be "success or failure"
Feb 14 14:53:53.513: INFO: Pod "downwardapi-volume-58bc2d6e-3068-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 8.307002ms
Feb 14 14:53:55.519: INFO: Pod "downwardapi-volume-58bc2d6e-3068-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013735435s
STEP: Saw pod success
Feb 14 14:53:55.519: INFO: Pod "downwardapi-volume-58bc2d6e-3068-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:53:55.521: INFO: Trying to get logs from node k8s-conformance-node-3 pod downwardapi-volume-58bc2d6e-3068-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 14:53:55.543: INFO: Waiting for pod downwardapi-volume-58bc2d6e-3068-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:53:55.545: INFO: Pod downwardapi-volume-58bc2d6e-3068-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:53:55.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tc7pj" for this suite.
Feb 14 14:54:01.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:54:01.624: INFO: namespace: e2e-tests-projected-tc7pj, resource: bindings, ignored listing per whitelist
Feb 14 14:54:01.647: INFO: namespace e2e-tests-projected-tc7pj deletion completed in 6.098408322s

• [SLOW TEST:8.210 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:54:01.650: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 14 14:54:01.712: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-284296654 proxy --unix-socket=/tmp/kubectl-proxy-unix074231509/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:54:01.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-blthk" for this suite.
Feb 14 14:54:07.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:54:07.857: INFO: namespace: e2e-tests-kubectl-blthk, resource: bindings, ignored listing per whitelist
Feb 14 14:54:07.887: INFO: namespace e2e-tests-kubectl-blthk deletion completed in 6.094483991s

• [SLOW TEST:6.238 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:54:07.889: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 14 14:54:11.988: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 14 14:54:11.992: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 14 14:54:13.992: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 14 14:54:13.995: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 14 14:54:15.993: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 14 14:54:15.996: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 14 14:54:17.992: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 14 14:54:17.996: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 14 14:54:19.993: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 14 14:54:19.996: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 14 14:54:21.992: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 14 14:54:21.996: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 14 14:54:23.992: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 14 14:54:23.996: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 14 14:54:25.992: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 14 14:54:25.996: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 14 14:54:27.992: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 14 14:54:27.996: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 14 14:54:29.992: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 14 14:54:29.995: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 14 14:54:31.992: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 14 14:54:31.996: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:54:32.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-sqjsm" for this suite.
Feb 14 14:54:54.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:54:54.094: INFO: namespace: e2e-tests-container-lifecycle-hook-sqjsm, resource: bindings, ignored listing per whitelist
Feb 14 14:54:54.110: INFO: namespace e2e-tests-container-lifecycle-hook-sqjsm deletion completed in 22.10283239s

• [SLOW TEST:46.222 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:54:54.112: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-7ce8cd4b-3068-11e9-a1dd-12216a2f1059
STEP: Creating configMap with name cm-test-opt-upd-7ce8cd82-3068-11e9-a1dd-12216a2f1059
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7ce8cd4b-3068-11e9-a1dd-12216a2f1059
STEP: Updating configmap cm-test-opt-upd-7ce8cd82-3068-11e9-a1dd-12216a2f1059
STEP: Creating configMap with name cm-test-opt-create-7ce8cd9f-3068-11e9-a1dd-12216a2f1059
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:54:58.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-p9zp6" for this suite.
Feb 14 14:55:20.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:55:20.401: INFO: namespace: e2e-tests-configmap-p9zp6, resource: bindings, ignored listing per whitelist
Feb 14 14:55:20.423: INFO: namespace e2e-tests-configmap-p9zp6 deletion completed in 22.142626128s

• [SLOW TEST:26.312 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:55:20.428: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 14:55:20.504: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c971fca-3068-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-6xj9k" to be "success or failure"
Feb 14 14:55:20.513: INFO: Pod "downwardapi-volume-8c971fca-3068-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 8.736132ms
Feb 14 14:55:22.516: INFO: Pod "downwardapi-volume-8c971fca-3068-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012108757s
STEP: Saw pod success
Feb 14 14:55:22.516: INFO: Pod "downwardapi-volume-8c971fca-3068-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:55:22.519: INFO: Trying to get logs from node k8s-conformance-node-3 pod downwardapi-volume-8c971fca-3068-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 14:55:22.544: INFO: Waiting for pod downwardapi-volume-8c971fca-3068-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:55:22.547: INFO: Pod downwardapi-volume-8c971fca-3068-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:55:22.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6xj9k" for this suite.
Feb 14 14:55:28.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:55:28.579: INFO: namespace: e2e-tests-projected-6xj9k, resource: bindings, ignored listing per whitelist
Feb 14 14:55:28.643: INFO: namespace e2e-tests-projected-6xj9k deletion completed in 6.092802907s

• [SLOW TEST:8.217 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:55:28.644: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 14:55:28.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-917b7fae-3068-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-jq6tv" to be "success or failure"
Feb 14 14:55:28.716: INFO: Pod "downwardapi-volume-917b7fae-3068-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 7.102355ms
Feb 14 14:55:30.720: INFO: Pod "downwardapi-volume-917b7fae-3068-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01076393s
STEP: Saw pod success
Feb 14 14:55:30.720: INFO: Pod "downwardapi-volume-917b7fae-3068-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:55:30.722: INFO: Trying to get logs from node k8s-conformance-node-1 pod downwardapi-volume-917b7fae-3068-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 14:55:30.740: INFO: Waiting for pod downwardapi-volume-917b7fae-3068-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:55:30.764: INFO: Pod downwardapi-volume-917b7fae-3068-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:55:30.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jq6tv" for this suite.
Feb 14 14:55:36.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:55:36.822: INFO: namespace: e2e-tests-projected-jq6tv, resource: bindings, ignored listing per whitelist
Feb 14 14:55:36.870: INFO: namespace e2e-tests-projected-jq6tv deletion completed in 6.102083628s

• [SLOW TEST:8.226 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:55:36.870: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-dst8s/configmap-test-9663a316-3068-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 14:55:36.943: INFO: Waiting up to 5m0s for pod "pod-configmaps-96641c01-3068-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-configmap-dst8s" to be "success or failure"
Feb 14 14:55:36.946: INFO: Pod "pod-configmaps-96641c01-3068-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.433148ms
Feb 14 14:55:38.948: INFO: Pod "pod-configmaps-96641c01-3068-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005235573s
STEP: Saw pod success
Feb 14 14:55:38.949: INFO: Pod "pod-configmaps-96641c01-3068-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:55:38.951: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-configmaps-96641c01-3068-11e9-a1dd-12216a2f1059 container env-test: <nil>
STEP: delete the pod
Feb 14 14:55:38.968: INFO: Waiting for pod pod-configmaps-96641c01-3068-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:55:38.972: INFO: Pod pod-configmaps-96641c01-3068-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:55:38.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-dst8s" for this suite.
Feb 14 14:55:44.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:55:45.059: INFO: namespace: e2e-tests-configmap-dst8s, resource: bindings, ignored listing per whitelist
Feb 14 14:55:45.099: INFO: namespace e2e-tests-configmap-dst8s deletion completed in 6.123385486s

• [SLOW TEST:8.229 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:55:45.103: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-kkvkc
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 14 14:55:45.172: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 14 14:56:03.263: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.66:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-kkvkc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 14:56:03.263: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 14:56:03.379: INFO: Found all expected endpoints: [netserver-0]
Feb 14 14:56:03.383: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.13:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-kkvkc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 14:56:03.383: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 14:56:03.499: INFO: Found all expected endpoints: [netserver-1]
Feb 14 14:56:03.502: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.68:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-kkvkc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 14:56:03.502: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 14:56:03.619: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:56:03.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-kkvkc" for this suite.
Feb 14 14:56:17.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:56:17.676: INFO: namespace: e2e-tests-pod-network-test-kkvkc, resource: bindings, ignored listing per whitelist
Feb 14 14:56:17.732: INFO: namespace e2e-tests-pod-network-test-kkvkc deletion completed in 14.108221522s

• [SLOW TEST:32.629 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:56:17.733: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:56:21.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-fzt6d" for this suite.
Feb 14 14:56:27.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:56:27.925: INFO: namespace: e2e-tests-kubelet-test-fzt6d, resource: bindings, ignored listing per whitelist
Feb 14 14:56:27.970: INFO: namespace e2e-tests-kubelet-test-fzt6d deletion completed in 6.131587914s

• [SLOW TEST:10.237 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:56:27.971: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 14 14:56:28.041: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 14 14:56:28.047: INFO: Waiting for terminating namespaces to be deleted...
Feb 14 14:56:28.049: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-node-1 before test
Feb 14 14:56:28.055: INFO: kube-proxy-4ls9n from kube-system started at 2019-02-14 14:17:57 +0000 UTC (1 container statuses recorded)
Feb 14 14:56:28.055: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 14:56:28.055: INFO: csi-linode-node-7zb7d from kube-system started at 2019-02-14 14:18:26 +0000 UTC (2 container statuses recorded)
Feb 14 14:56:28.055: INFO: 	Container csi-linode-plugin ready: true, restart count 0
Feb 14 14:56:28.056: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 14 14:56:28.056: INFO: calico-node-x2wh4 from kube-system started at 2019-02-14 14:17:57 +0000 UTC (2 container statuses recorded)
Feb 14 14:56:28.056: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 14:56:28.056: INFO: 	Container install-cni ready: true, restart count 0
Feb 14 14:56:28.056: INFO: sonobuoy-e2e-job-1123becdb7604801 from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 14:56:28.056: INFO: 	Container e2e ready: true, restart count 0
Feb 14 14:56:28.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 14 14:56:28.056: INFO: sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-wx8k7 from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 14:56:28.056: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb 14 14:56:28.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 14 14:56:28.056: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-node-2 before test
Feb 14 14:56:28.064: INFO: external-dns-d4cfd5855-fwcl4 from kube-system started at 2019-02-14 14:18:05 +0000 UTC (1 container statuses recorded)
Feb 14 14:56:28.064: INFO: 	Container external-dns ready: true, restart count 0
Feb 14 14:56:28.064: INFO: csi-linode-node-fwkhk from kube-system started at 2019-02-14 14:18:05 +0000 UTC (2 container statuses recorded)
Feb 14 14:56:28.064: INFO: 	Container csi-linode-plugin ready: true, restart count 0
Feb 14 14:56:28.064: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 14 14:56:28.064: INFO: metrics-server-68d85f76bb-tp9rx from kube-system started at 2019-02-14 14:18:05 +0000 UTC (1 container statuses recorded)
Feb 14 14:56:28.064: INFO: 	Container metrics-server ready: true, restart count 0
Feb 14 14:56:28.064: INFO: sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-4ljxn from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 14:56:28.064: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb 14 14:56:28.064: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 14 14:56:28.064: INFO: kube-proxy-wdgrm from kube-system started at 2019-02-14 14:17:45 +0000 UTC (1 container statuses recorded)
Feb 14 14:56:28.064: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 14:56:28.064: INFO: calico-node-kfmp7 from kube-system started at 2019-02-14 14:17:45 +0000 UTC (2 container statuses recorded)
Feb 14 14:56:28.064: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 14:56:28.064: INFO: 	Container install-cni ready: true, restart count 0
Feb 14 14:56:28.064: INFO: csi-linode-controller-0 from kube-system started at 2019-02-14 14:18:05 +0000 UTC (3 container statuses recorded)
Feb 14 14:56:28.064: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 14 14:56:28.064: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 14 14:56:28.064: INFO: 	Container linode-csi-plugin ready: true, restart count 0
Feb 14 14:56:28.064: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-node-3 before test
Feb 14 14:56:28.070: INFO: csi-linode-node-2kjrw from kube-system started at 2019-02-14 14:18:07 +0000 UTC (2 container statuses recorded)
Feb 14 14:56:28.070: INFO: 	Container csi-linode-plugin ready: true, restart count 0
Feb 14 14:56:28.070: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 14 14:56:28.070: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-14 14:19:44 +0000 UTC (1 container statuses recorded)
Feb 14 14:56:28.070: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 14 14:56:28.070: INFO: kube-proxy-hrh6d from kube-system started at 2019-02-14 14:17:47 +0000 UTC (1 container statuses recorded)
Feb 14 14:56:28.070: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 14:56:28.070: INFO: sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-jm8kn from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 14:56:28.070: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb 14 14:56:28.070: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 14 14:56:28.071: INFO: calico-node-rfldn from kube-system started at 2019-02-14 14:17:47 +0000 UTC (2 container statuses recorded)
Feb 14 14:56:28.071: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 14:56:28.071: INFO: 	Container install-cni ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node k8s-conformance-node-1
STEP: verifying the node has the label node k8s-conformance-node-2
STEP: verifying the node has the label node k8s-conformance-node-3
Feb 14 14:56:28.130: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-conformance-node-3
Feb 14 14:56:28.130: INFO: Pod sonobuoy-e2e-job-1123becdb7604801 requesting resource cpu=0m on Node k8s-conformance-node-1
Feb 14 14:56:28.130: INFO: Pod sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-4ljxn requesting resource cpu=0m on Node k8s-conformance-node-2
Feb 14 14:56:28.130: INFO: Pod sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-jm8kn requesting resource cpu=0m on Node k8s-conformance-node-3
Feb 14 14:56:28.130: INFO: Pod sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-wx8k7 requesting resource cpu=0m on Node k8s-conformance-node-1
Feb 14 14:56:28.130: INFO: Pod calico-node-kfmp7 requesting resource cpu=250m on Node k8s-conformance-node-2
Feb 14 14:56:28.130: INFO: Pod calico-node-rfldn requesting resource cpu=250m on Node k8s-conformance-node-3
Feb 14 14:56:28.130: INFO: Pod calico-node-x2wh4 requesting resource cpu=250m on Node k8s-conformance-node-1
Feb 14 14:56:28.130: INFO: Pod csi-linode-controller-0 requesting resource cpu=0m on Node k8s-conformance-node-2
Feb 14 14:56:28.130: INFO: Pod csi-linode-node-2kjrw requesting resource cpu=0m on Node k8s-conformance-node-3
Feb 14 14:56:28.130: INFO: Pod csi-linode-node-7zb7d requesting resource cpu=0m on Node k8s-conformance-node-1
Feb 14 14:56:28.130: INFO: Pod csi-linode-node-fwkhk requesting resource cpu=0m on Node k8s-conformance-node-2
Feb 14 14:56:28.130: INFO: Pod external-dns-d4cfd5855-fwcl4 requesting resource cpu=0m on Node k8s-conformance-node-2
Feb 14 14:56:28.130: INFO: Pod kube-proxy-4ls9n requesting resource cpu=0m on Node k8s-conformance-node-1
Feb 14 14:56:28.130: INFO: Pod kube-proxy-hrh6d requesting resource cpu=0m on Node k8s-conformance-node-3
Feb 14 14:56:28.130: INFO: Pod kube-proxy-wdgrm requesting resource cpu=0m on Node k8s-conformance-node-2
Feb 14 14:56:28.130: INFO: Pod metrics-server-68d85f76bb-tp9rx requesting resource cpu=0m on Node k8s-conformance-node-2
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e76f54-3068-11e9-a1dd-12216a2f1059.158342d70c8abc9f], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-gwcnn/filler-pod-b4e76f54-3068-11e9-a1dd-12216a2f1059 to k8s-conformance-node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e76f54-3068-11e9-a1dd-12216a2f1059.158342d73b371b11], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e76f54-3068-11e9-a1dd-12216a2f1059.158342d73d1ecac6], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e76f54-3068-11e9-a1dd-12216a2f1059.158342d746f37246], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e81d02-3068-11e9-a1dd-12216a2f1059.158342d70cfee90f], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-gwcnn/filler-pod-b4e81d02-3068-11e9-a1dd-12216a2f1059 to k8s-conformance-node-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e81d02-3068-11e9-a1dd-12216a2f1059.158342d744baf60f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e81d02-3068-11e9-a1dd-12216a2f1059.158342d74671f6e8], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e81d02-3068-11e9-a1dd-12216a2f1059.158342d7505b8cb8], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e9cd13-3068-11e9-a1dd-12216a2f1059.158342d70e7628d6], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-gwcnn/filler-pod-b4e9cd13-3068-11e9-a1dd-12216a2f1059 to k8s-conformance-node-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e9cd13-3068-11e9-a1dd-12216a2f1059.158342d7471c0552], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e9cd13-3068-11e9-a1dd-12216a2f1059.158342d7490da642], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4e9cd13-3068-11e9-a1dd-12216a2f1059.158342d756ab2fc8], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.158342d78689d2e0], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node k8s-conformance-node-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-conformance-node-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-conformance-node-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:56:31.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-gwcnn" for this suite.
Feb 14 14:56:37.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:56:37.312: INFO: namespace: e2e-tests-sched-pred-gwcnn, resource: bindings, ignored listing per whitelist
Feb 14 14:56:37.348: INFO: namespace e2e-tests-sched-pred-gwcnn deletion completed in 6.089526797s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.377 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:56:37.349: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-6dxkf
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-6dxkf to expose endpoints map[]
Feb 14 14:56:37.485: INFO: Get endpoints failed (7.345572ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb 14 14:56:38.488: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-6dxkf exposes endpoints map[] (1.010743122s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-6dxkf
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-6dxkf to expose endpoints map[pod1:[100]]
Feb 14 14:56:40.516: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-6dxkf exposes endpoints map[pod1:[100]] (2.019525468s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-6dxkf
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-6dxkf to expose endpoints map[pod1:[100] pod2:[101]]
Feb 14 14:56:42.559: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-6dxkf exposes endpoints map[pod1:[100] pod2:[101]] (2.039698288s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-6dxkf
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-6dxkf to expose endpoints map[pod2:[101]]
Feb 14 14:56:42.576: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-6dxkf exposes endpoints map[pod2:[101]] (12.275138ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-6dxkf
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-6dxkf to expose endpoints map[]
Feb 14 14:56:42.590: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-6dxkf exposes endpoints map[] (4.387795ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:56:42.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-6dxkf" for this suite.
Feb 14 14:57:04.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:57:04.667: INFO: namespace: e2e-tests-services-6dxkf, resource: bindings, ignored listing per whitelist
Feb 14 14:57:04.710: INFO: namespace e2e-tests-services-6dxkf deletion completed in 22.094058908s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:27.362 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:57:04.712: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 14 14:57:04.781: INFO: Waiting up to 5m0s for pod "pod-cabea212-3068-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-sdzxk" to be "success or failure"
Feb 14 14:57:04.787: INFO: Pod "pod-cabea212-3068-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 5.775885ms
Feb 14 14:57:06.790: INFO: Pod "pod-cabea212-3068-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009500152s
STEP: Saw pod success
Feb 14 14:57:06.791: INFO: Pod "pod-cabea212-3068-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:57:06.793: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-cabea212-3068-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 14:57:06.811: INFO: Waiting for pod pod-cabea212-3068-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:57:06.814: INFO: Pod pod-cabea212-3068-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:57:06.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sdzxk" for this suite.
Feb 14 14:57:12.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:57:12.888: INFO: namespace: e2e-tests-emptydir-sdzxk, resource: bindings, ignored listing per whitelist
Feb 14 14:57:12.900: INFO: namespace e2e-tests-emptydir-sdzxk deletion completed in 6.083354303s

• [SLOW TEST:8.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:57:12.901: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 14 14:57:12.967: INFO: Waiting up to 5m0s for pod "downward-api-cfa05b71-3068-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-vbppb" to be "success or failure"
Feb 14 14:57:12.971: INFO: Pod "downward-api-cfa05b71-3068-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 3.874587ms
Feb 14 14:57:14.975: INFO: Pod "downward-api-cfa05b71-3068-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007432765s
STEP: Saw pod success
Feb 14 14:57:14.975: INFO: Pod "downward-api-cfa05b71-3068-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:57:14.977: INFO: Trying to get logs from node k8s-conformance-node-1 pod downward-api-cfa05b71-3068-11e9-a1dd-12216a2f1059 container dapi-container: <nil>
STEP: delete the pod
Feb 14 14:57:15.008: INFO: Waiting for pod downward-api-cfa05b71-3068-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:57:15.025: INFO: Pod downward-api-cfa05b71-3068-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:57:15.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vbppb" for this suite.
Feb 14 14:57:21.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:57:21.100: INFO: namespace: e2e-tests-downward-api-vbppb, resource: bindings, ignored listing per whitelist
Feb 14 14:57:21.117: INFO: namespace e2e-tests-downward-api-vbppb deletion completed in 6.088178555s

• [SLOW TEST:8.217 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:57:21.117: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 14 14:57:21.193: INFO: Waiting up to 5m0s for pod "downward-api-d4871c69-3068-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-9dwsl" to be "success or failure"
Feb 14 14:57:21.201: INFO: Pod "downward-api-d4871c69-3068-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 7.936423ms
Feb 14 14:57:23.205: INFO: Pod "downward-api-d4871c69-3068-11e9-a1dd-12216a2f1059": Phase="Running", Reason="", readiness=true. Elapsed: 2.011710123s
Feb 14 14:57:25.209: INFO: Pod "downward-api-d4871c69-3068-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016098703s
STEP: Saw pod success
Feb 14 14:57:25.209: INFO: Pod "downward-api-d4871c69-3068-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:57:25.212: INFO: Trying to get logs from node k8s-conformance-node-3 pod downward-api-d4871c69-3068-11e9-a1dd-12216a2f1059 container dapi-container: <nil>
STEP: delete the pod
Feb 14 14:57:25.233: INFO: Waiting for pod downward-api-d4871c69-3068-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:57:25.235: INFO: Pod downward-api-d4871c69-3068-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:57:25.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-9dwsl" for this suite.
Feb 14 14:57:31.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:57:31.304: INFO: namespace: e2e-tests-downward-api-9dwsl, resource: bindings, ignored listing per whitelist
Feb 14 14:57:31.326: INFO: namespace e2e-tests-downward-api-9dwsl deletion completed in 6.087357414s

• [SLOW TEST:10.208 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:57:31.327: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 14:57:31.392: INFO: Creating deployment "test-recreate-deployment"
Feb 14 14:57:31.396: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 14 14:57:31.405: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Feb 14 14:57:33.412: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 14 14:57:33.414: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 14 14:57:33.419: INFO: Updating deployment test-recreate-deployment
Feb 14 14:57:33.420: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 14 14:57:33.512: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-csbp6,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-csbp6/deployments/test-recreate-deployment,UID:da9c8418-3068-11e9-aaef-f23c91bac0b7,ResourceVersion:11969,Generation:2,CreationTimestamp:2019-02-14 14:57:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-14 14:57:33 +0000 UTC 2019-02-14 14:57:33 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-14 14:57:33 +0000 UTC 2019-02-14 14:57:31 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 14 14:57:33.515: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-csbp6,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-csbp6/replicasets/test-recreate-deployment-697fbf54bf,UID:dbd81df4-3068-11e9-aaef-f23c91bac0b7,ResourceVersion:11968,Generation:1,CreationTimestamp:2019-02-14 14:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment da9c8418-3068-11e9-aaef-f23c91bac0b7 0xc00222c087 0xc00222c088}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 14 14:57:33.515: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 14 14:57:33.516: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-csbp6,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-csbp6/replicasets/test-recreate-deployment-5dfdcc846d,UID:da9da560-3068-11e9-aaef-f23c91bac0b7,ResourceVersion:11957,Generation:2,CreationTimestamp:2019-02-14 14:57:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment da9c8418-3068-11e9-aaef-f23c91bac0b7 0xc002471f67 0xc002471f68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 14 14:57:33.519: INFO: Pod "test-recreate-deployment-697fbf54bf-vzt89" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-vzt89,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-csbp6,SelfLink:/api/v1/namespaces/e2e-tests-deployment-csbp6/pods/test-recreate-deployment-697fbf54bf-vzt89,UID:dbd8e1a7-3068-11e9-aaef-f23c91bac0b7,ResourceVersion:11967,Generation:0,CreationTimestamp:2019-02-14 14:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf dbd81df4-3068-11e9-aaef-f23c91bac0b7 0xc00222cfc7 0xc00222cfc8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cvpd7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cvpd7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cvpd7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222d030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222d050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 14:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 14:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 14:57:33 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:,StartTime:2019-02-14 14:57:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:57:33.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-csbp6" for this suite.
Feb 14 14:57:39.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:57:39.556: INFO: namespace: e2e-tests-deployment-csbp6, resource: bindings, ignored listing per whitelist
Feb 14 14:57:39.612: INFO: namespace e2e-tests-deployment-csbp6 deletion completed in 6.089510419s

• [SLOW TEST:8.285 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:57:39.613: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-wps9
STEP: Creating a pod to test atomic-volume-subpath
Feb 14 14:57:39.685: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wps9" in namespace "e2e-tests-subpath-49bfv" to be "success or failure"
Feb 14 14:57:39.694: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.365832ms
Feb 14 14:57:41.698: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012937536s
Feb 14 14:57:43.701: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Running", Reason="", readiness=false. Elapsed: 4.016276133s
Feb 14 14:57:45.705: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Running", Reason="", readiness=false. Elapsed: 6.019717221s
Feb 14 14:57:47.708: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Running", Reason="", readiness=false. Elapsed: 8.023282432s
Feb 14 14:57:49.712: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Running", Reason="", readiness=false. Elapsed: 10.026856024s
Feb 14 14:57:51.715: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Running", Reason="", readiness=false. Elapsed: 12.030390668s
Feb 14 14:57:53.719: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Running", Reason="", readiness=false. Elapsed: 14.034128055s
Feb 14 14:57:55.723: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Running", Reason="", readiness=false. Elapsed: 16.038142723s
Feb 14 14:57:57.726: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Running", Reason="", readiness=false. Elapsed: 18.041360922s
Feb 14 14:57:59.730: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Running", Reason="", readiness=false. Elapsed: 20.044933784s
Feb 14 14:58:01.733: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Running", Reason="", readiness=false. Elapsed: 22.048409257s
Feb 14 14:58:03.737: INFO: Pod "pod-subpath-test-secret-wps9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.052400022s
STEP: Saw pod success
Feb 14 14:58:03.737: INFO: Pod "pod-subpath-test-secret-wps9" satisfied condition "success or failure"
Feb 14 14:58:03.740: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-subpath-test-secret-wps9 container test-container-subpath-secret-wps9: <nil>
STEP: delete the pod
Feb 14 14:58:03.768: INFO: Waiting for pod pod-subpath-test-secret-wps9 to disappear
Feb 14 14:58:03.774: INFO: Pod pod-subpath-test-secret-wps9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-wps9
Feb 14 14:58:03.774: INFO: Deleting pod "pod-subpath-test-secret-wps9" in namespace "e2e-tests-subpath-49bfv"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:58:03.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-49bfv" for this suite.
Feb 14 14:58:09.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:58:09.837: INFO: namespace: e2e-tests-subpath-49bfv, resource: bindings, ignored listing per whitelist
Feb 14 14:58:09.872: INFO: namespace e2e-tests-subpath-49bfv deletion completed in 6.090024332s

• [SLOW TEST:30.259 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:58:09.873: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 14 14:58:09.981: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 14:58:09.984: INFO: Number of nodes with available pods: 0
Feb 14 14:58:09.984: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 14:58:10.988: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 14:58:10.991: INFO: Number of nodes with available pods: 0
Feb 14 14:58:10.991: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 14:58:11.988: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 14:58:11.991: INFO: Number of nodes with available pods: 1
Feb 14 14:58:11.991: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 14:58:12.988: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 14:58:12.991: INFO: Number of nodes with available pods: 3
Feb 14 14:58:12.991: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 14 14:58:13.014: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 14:58:13.024: INFO: Number of nodes with available pods: 2
Feb 14 14:58:13.024: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 14:58:14.030: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 14:58:14.033: INFO: Number of nodes with available pods: 2
Feb 14 14:58:14.033: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 14:58:15.029: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 14:58:15.033: INFO: Number of nodes with available pods: 3
Feb 14 14:58:15.033: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-4gx4r, will wait for the garbage collector to delete the pods
Feb 14 14:58:15.096: INFO: Deleting DaemonSet.extensions daemon-set took: 6.097582ms
Feb 14 14:58:15.196: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.154227ms
Feb 14 14:58:57.500: INFO: Number of nodes with available pods: 0
Feb 14 14:58:57.500: INFO: Number of running nodes: 0, number of available pods: 0
Feb 14 14:58:57.503: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-4gx4r/daemonsets","resourceVersion":"12288"},"items":null}

Feb 14 14:58:57.506: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-4gx4r/pods","resourceVersion":"12288"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:58:57.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-4gx4r" for this suite.
Feb 14 14:59:03.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:59:03.551: INFO: namespace: e2e-tests-daemonsets-4gx4r, resource: bindings, ignored listing per whitelist
Feb 14 14:59:03.647: INFO: namespace e2e-tests-daemonsets-4gx4r deletion completed in 6.127321189s

• [SLOW TEST:53.775 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:59:03.648: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 14 14:59:03.742: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-9xd9z,SelfLink:/api/v1/namespaces/e2e-tests-watch-9xd9z/configmaps/e2e-watch-test-watch-closed,UID:11a660f8-3069-11e9-aaef-f23c91bac0b7,ResourceVersion:12339,Generation:0,CreationTimestamp:2019-02-14 14:59:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 14 14:59:03.743: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-9xd9z,SelfLink:/api/v1/namespaces/e2e-tests-watch-9xd9z/configmaps/e2e-watch-test-watch-closed,UID:11a660f8-3069-11e9-aaef-f23c91bac0b7,ResourceVersion:12340,Generation:0,CreationTimestamp:2019-02-14 14:59:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 14 14:59:03.758: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-9xd9z,SelfLink:/api/v1/namespaces/e2e-tests-watch-9xd9z/configmaps/e2e-watch-test-watch-closed,UID:11a660f8-3069-11e9-aaef-f23c91bac0b7,ResourceVersion:12341,Generation:0,CreationTimestamp:2019-02-14 14:59:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 14 14:59:03.758: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-9xd9z,SelfLink:/api/v1/namespaces/e2e-tests-watch-9xd9z/configmaps/e2e-watch-test-watch-closed,UID:11a660f8-3069-11e9-aaef-f23c91bac0b7,ResourceVersion:12342,Generation:0,CreationTimestamp:2019-02-14 14:59:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:59:03.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-9xd9z" for this suite.
Feb 14 14:59:09.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:59:09.849: INFO: namespace: e2e-tests-watch-9xd9z, resource: bindings, ignored listing per whitelist
Feb 14 14:59:09.881: INFO: namespace e2e-tests-watch-9xd9z deletion completed in 6.117361872s

• [SLOW TEST:6.234 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:59:09.883: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-155d276c-3069-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 14:59:09.975: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-155dbac9-3069-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-4nt6l" to be "success or failure"
Feb 14 14:59:09.979: INFO: Pod "pod-projected-secrets-155dbac9-3069-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 4.108813ms
Feb 14 14:59:11.983: INFO: Pod "pod-projected-secrets-155dbac9-3069-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00870396s
Feb 14 14:59:13.989: INFO: Pod "pod-projected-secrets-155dbac9-3069-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014082441s
STEP: Saw pod success
Feb 14 14:59:13.989: INFO: Pod "pod-projected-secrets-155dbac9-3069-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 14:59:13.993: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-projected-secrets-155dbac9-3069-11e9-a1dd-12216a2f1059 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 14 14:59:14.015: INFO: Waiting for pod pod-projected-secrets-155dbac9-3069-11e9-a1dd-12216a2f1059 to disappear
Feb 14 14:59:14.018: INFO: Pod pod-projected-secrets-155dbac9-3069-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 14:59:14.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4nt6l" for this suite.
Feb 14 14:59:20.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 14:59:20.057: INFO: namespace: e2e-tests-projected-4nt6l, resource: bindings, ignored listing per whitelist
Feb 14 14:59:20.137: INFO: namespace e2e-tests-projected-4nt6l deletion completed in 6.114737129s

• [SLOW TEST:10.254 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 14:59:20.137: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-1b7acdfe-3069-11e9-a1dd-12216a2f1059
STEP: Creating configMap with name cm-test-opt-upd-1b7ace50-3069-11e9-a1dd-12216a2f1059
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1b7acdfe-3069-11e9-a1dd-12216a2f1059
STEP: Updating configmap cm-test-opt-upd-1b7ace50-3069-11e9-a1dd-12216a2f1059
STEP: Creating configMap with name cm-test-opt-create-1b7ace71-3069-11e9-a1dd-12216a2f1059
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:00:30.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-c4wfd" for this suite.
Feb 14 15:00:52.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:00:52.832: INFO: namespace: e2e-tests-projected-c4wfd, resource: bindings, ignored listing per whitelist
Feb 14 15:00:52.854: INFO: namespace e2e-tests-projected-c4wfd deletion completed in 22.111592036s

• [SLOW TEST:92.717 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:00:52.856: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 14 15:00:53.437: INFO: Waiting up to 5m0s for pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-vndtz" in namespace "e2e-tests-svcaccounts-6vr84" to be "success or failure"
Feb 14 15:00:53.446: INFO: Pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-vndtz": Phase="Pending", Reason="", readiness=false. Elapsed: 9.067012ms
Feb 14 15:00:55.450: INFO: Pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-vndtz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013103291s
STEP: Saw pod success
Feb 14 15:00:55.450: INFO: Pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-vndtz" satisfied condition "success or failure"
Feb 14 15:00:55.452: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-vndtz container token-test: <nil>
STEP: delete the pod
Feb 14 15:00:55.478: INFO: Waiting for pod pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-vndtz to disappear
Feb 14 15:00:55.482: INFO: Pod pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-vndtz no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 14 15:00:55.488: INFO: Waiting up to 5m0s for pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-k486t" in namespace "e2e-tests-svcaccounts-6vr84" to be "success or failure"
Feb 14 15:00:55.494: INFO: Pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-k486t": Phase="Pending", Reason="", readiness=false. Elapsed: 5.907806ms
Feb 14 15:00:57.498: INFO: Pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-k486t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009552735s
Feb 14 15:00:59.502: INFO: Pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-k486t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013421864s
STEP: Saw pod success
Feb 14 15:00:59.502: INFO: Pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-k486t" satisfied condition "success or failure"
Feb 14 15:00:59.508: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-k486t container root-ca-test: <nil>
STEP: delete the pod
Feb 14 15:00:59.539: INFO: Waiting for pod pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-k486t to disappear
Feb 14 15:00:59.542: INFO: Pod pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-k486t no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 14 15:00:59.557: INFO: Waiting up to 5m0s for pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-lmgnj" in namespace "e2e-tests-svcaccounts-6vr84" to be "success or failure"
Feb 14 15:00:59.579: INFO: Pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-lmgnj": Phase="Pending", Reason="", readiness=false. Elapsed: 21.636661ms
Feb 14 15:01:01.584: INFO: Pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-lmgnj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026490092s
Feb 14 15:01:03.589: INFO: Pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-lmgnj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031362393s
STEP: Saw pod success
Feb 14 15:01:03.589: INFO: Pod "pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-lmgnj" satisfied condition "success or failure"
Feb 14 15:01:03.592: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-lmgnj container namespace-test: <nil>
STEP: delete the pod
Feb 14 15:01:03.613: INFO: Waiting for pod pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-lmgnj to disappear
Feb 14 15:01:03.616: INFO: Pod pod-service-account-5308bed8-3069-11e9-a1dd-12216a2f1059-lmgnj no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:01:03.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-6vr84" for this suite.
Feb 14 15:01:09.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:01:09.697: INFO: namespace: e2e-tests-svcaccounts-6vr84, resource: bindings, ignored listing per whitelist
Feb 14 15:01:09.755: INFO: namespace e2e-tests-svcaccounts-6vr84 deletion completed in 6.134843182s

• [SLOW TEST:16.899 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:01:09.755: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-5ccf5ea0-3069-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 15:01:09.847: INFO: Waiting up to 5m0s for pod "pod-configmaps-5cd00112-3069-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-configmap-lmxfl" to be "success or failure"
Feb 14 15:01:09.862: INFO: Pod "pod-configmaps-5cd00112-3069-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 15.204612ms
Feb 14 15:01:11.867: INFO: Pod "pod-configmaps-5cd00112-3069-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019567494s
Feb 14 15:01:13.878: INFO: Pod "pod-configmaps-5cd00112-3069-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031086807s
STEP: Saw pod success
Feb 14 15:01:13.878: INFO: Pod "pod-configmaps-5cd00112-3069-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:01:13.885: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-configmaps-5cd00112-3069-11e9-a1dd-12216a2f1059 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 15:01:13.903: INFO: Waiting for pod pod-configmaps-5cd00112-3069-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:01:13.911: INFO: Pod pod-configmaps-5cd00112-3069-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:01:13.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-lmxfl" for this suite.
Feb 14 15:01:19.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:01:20.037: INFO: namespace: e2e-tests-configmap-lmxfl, resource: bindings, ignored listing per whitelist
Feb 14 15:01:20.061: INFO: namespace e2e-tests-configmap-lmxfl deletion completed in 6.145539084s

• [SLOW TEST:10.307 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:01:20.063: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-62f388fd-3069-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 15:01:20.145: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62f404c1-3069-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-rs5rb" to be "success or failure"
Feb 14 15:01:20.150: INFO: Pod "pod-projected-secrets-62f404c1-3069-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 4.937905ms
Feb 14 15:01:22.154: INFO: Pod "pod-projected-secrets-62f404c1-3069-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00853863s
STEP: Saw pod success
Feb 14 15:01:22.154: INFO: Pod "pod-projected-secrets-62f404c1-3069-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:01:22.157: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-projected-secrets-62f404c1-3069-11e9-a1dd-12216a2f1059 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 14 15:01:22.184: INFO: Waiting for pod pod-projected-secrets-62f404c1-3069-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:01:22.187: INFO: Pod pod-projected-secrets-62f404c1-3069-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:01:22.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rs5rb" for this suite.
Feb 14 15:01:28.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:01:28.280: INFO: namespace: e2e-tests-projected-rs5rb, resource: bindings, ignored listing per whitelist
Feb 14 15:01:28.292: INFO: namespace e2e-tests-projected-rs5rb deletion completed in 6.101593119s

• [SLOW TEST:8.230 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:01:28.296: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-zlh9
STEP: Creating a pod to test atomic-volume-subpath
Feb 14 15:01:28.396: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zlh9" in namespace "e2e-tests-subpath-j9qb4" to be "success or failure"
Feb 14 15:01:28.406: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.96592ms
Feb 14 15:01:30.411: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014844754s
Feb 14 15:01:32.416: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Running", Reason="", readiness=false. Elapsed: 4.019775205s
Feb 14 15:01:34.420: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Running", Reason="", readiness=false. Elapsed: 6.024598514s
Feb 14 15:01:36.425: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Running", Reason="", readiness=false. Elapsed: 8.029147501s
Feb 14 15:01:38.429: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Running", Reason="", readiness=false. Elapsed: 10.033532864s
Feb 14 15:01:40.434: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Running", Reason="", readiness=false. Elapsed: 12.038295185s
Feb 14 15:01:42.440: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Running", Reason="", readiness=false. Elapsed: 14.044076369s
Feb 14 15:01:44.445: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Running", Reason="", readiness=false. Elapsed: 16.049234201s
Feb 14 15:01:46.450: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Running", Reason="", readiness=false. Elapsed: 18.054126631s
Feb 14 15:01:48.455: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Running", Reason="", readiness=false. Elapsed: 20.059255178s
Feb 14 15:01:50.460: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Running", Reason="", readiness=false. Elapsed: 22.063880842s
Feb 14 15:01:52.464: INFO: Pod "pod-subpath-test-configmap-zlh9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.068229492s
STEP: Saw pod success
Feb 14 15:01:52.464: INFO: Pod "pod-subpath-test-configmap-zlh9" satisfied condition "success or failure"
Feb 14 15:01:52.467: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-subpath-test-configmap-zlh9 container test-container-subpath-configmap-zlh9: <nil>
STEP: delete the pod
Feb 14 15:01:52.493: INFO: Waiting for pod pod-subpath-test-configmap-zlh9 to disappear
Feb 14 15:01:52.495: INFO: Pod pod-subpath-test-configmap-zlh9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zlh9
Feb 14 15:01:52.495: INFO: Deleting pod "pod-subpath-test-configmap-zlh9" in namespace "e2e-tests-subpath-j9qb4"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:01:52.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-j9qb4" for this suite.
Feb 14 15:01:58.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:01:58.533: INFO: namespace: e2e-tests-subpath-j9qb4, resource: bindings, ignored listing per whitelist
Feb 14 15:01:58.625: INFO: namespace e2e-tests-subpath-j9qb4 deletion completed in 6.122556596s

• [SLOW TEST:30.329 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:01:58.627: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 14 15:01:58.701: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:02:02.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-tppmb" for this suite.
Feb 14 15:02:24.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:02:25.066: INFO: namespace: e2e-tests-init-container-tppmb, resource: bindings, ignored listing per whitelist
Feb 14 15:02:25.087: INFO: namespace e2e-tests-init-container-tppmb deletion completed in 22.122338559s

• [SLOW TEST:26.461 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:02:25.089: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-89b7d2e6-3069-11e9-a1dd-12216a2f1059
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-89b7d2e6-3069-11e9-a1dd-12216a2f1059
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:02:29.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-xfn89" for this suite.
Feb 14 15:02:51.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:02:51.308: INFO: namespace: e2e-tests-configmap-xfn89, resource: bindings, ignored listing per whitelist
Feb 14 15:02:51.388: INFO: namespace e2e-tests-configmap-xfn89 deletion completed in 22.147945051s

• [SLOW TEST:26.299 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:02:51.388: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 14 15:02:51.525: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:02:51.529: INFO: Number of nodes with available pods: 0
Feb 14 15:02:51.529: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:02:52.537: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:02:52.542: INFO: Number of nodes with available pods: 0
Feb 14 15:02:52.542: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:02:53.535: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:02:53.539: INFO: Number of nodes with available pods: 2
Feb 14 15:02:53.540: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:02:54.542: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:02:54.556: INFO: Number of nodes with available pods: 3
Feb 14 15:02:54.556: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 14 15:02:54.584: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:02:54.588: INFO: Number of nodes with available pods: 2
Feb 14 15:02:54.588: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:02:55.596: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:02:55.600: INFO: Number of nodes with available pods: 2
Feb 14 15:02:55.600: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:02:56.593: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:02:56.598: INFO: Number of nodes with available pods: 2
Feb 14 15:02:56.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:02:57.593: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:02:57.596: INFO: Number of nodes with available pods: 2
Feb 14 15:02:57.596: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:02:58.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:02:58.597: INFO: Number of nodes with available pods: 2
Feb 14 15:02:58.597: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:02:59.593: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:02:59.598: INFO: Number of nodes with available pods: 2
Feb 14 15:02:59.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:00.595: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:00.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:00.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:01.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:01.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:01.599: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:02.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:02.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:02.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:03.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:03.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:03.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:04.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:04.597: INFO: Number of nodes with available pods: 2
Feb 14 15:03:04.597: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:05.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:05.599: INFO: Number of nodes with available pods: 2
Feb 14 15:03:05.599: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:06.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:06.597: INFO: Number of nodes with available pods: 2
Feb 14 15:03:06.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:07.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:07.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:07.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:08.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:08.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:08.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:09.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:09.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:09.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:10.595: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:10.599: INFO: Number of nodes with available pods: 2
Feb 14 15:03:10.599: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:11.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:11.599: INFO: Number of nodes with available pods: 2
Feb 14 15:03:11.599: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:12.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:12.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:12.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:13.595: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:13.599: INFO: Number of nodes with available pods: 2
Feb 14 15:03:13.599: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:14.595: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:14.603: INFO: Number of nodes with available pods: 2
Feb 14 15:03:14.603: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:15.595: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:15.600: INFO: Number of nodes with available pods: 2
Feb 14 15:03:15.600: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:16.595: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:16.600: INFO: Number of nodes with available pods: 2
Feb 14 15:03:16.600: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:17.593: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:17.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:17.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:18.595: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:18.600: INFO: Number of nodes with available pods: 2
Feb 14 15:03:18.600: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:19.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:19.601: INFO: Number of nodes with available pods: 2
Feb 14 15:03:19.601: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:20.595: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:20.599: INFO: Number of nodes with available pods: 2
Feb 14 15:03:20.599: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:21.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:21.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:21.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:22.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:22.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:22.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:23.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:23.599: INFO: Number of nodes with available pods: 2
Feb 14 15:03:23.599: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:24.595: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:24.599: INFO: Number of nodes with available pods: 2
Feb 14 15:03:24.599: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:25.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:25.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:25.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:26.593: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:26.602: INFO: Number of nodes with available pods: 2
Feb 14 15:03:26.602: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:27.593: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:27.601: INFO: Number of nodes with available pods: 2
Feb 14 15:03:27.601: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:28.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:28.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:28.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:29.595: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:29.599: INFO: Number of nodes with available pods: 2
Feb 14 15:03:29.600: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:30.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:30.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:30.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:31.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:31.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:31.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:32.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:32.599: INFO: Number of nodes with available pods: 2
Feb 14 15:03:32.599: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:33.593: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:33.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:33.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:34.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:34.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:34.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:35.593: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:35.597: INFO: Number of nodes with available pods: 2
Feb 14 15:03:35.597: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:36.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:36.597: INFO: Number of nodes with available pods: 2
Feb 14 15:03:36.597: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:37.593: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:37.598: INFO: Number of nodes with available pods: 2
Feb 14 15:03:37.598: INFO: Node k8s-conformance-node-3 is running more than one daemon pod
Feb 14 15:03:38.594: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:03:38.598: INFO: Number of nodes with available pods: 3
Feb 14 15:03:38.598: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-2pswh, will wait for the garbage collector to delete the pods
Feb 14 15:03:38.661: INFO: Deleting DaemonSet.extensions daemon-set took: 7.668038ms
Feb 14 15:03:38.761: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.393123ms
Feb 14 15:04:17.466: INFO: Number of nodes with available pods: 0
Feb 14 15:04:17.467: INFO: Number of running nodes: 0, number of available pods: 0
Feb 14 15:04:17.470: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-2pswh/daemonsets","resourceVersion":"13379"},"items":null}

Feb 14 15:04:17.474: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-2pswh/pods","resourceVersion":"13379"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:04:17.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-2pswh" for this suite.
Feb 14 15:04:23.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:04:23.525: INFO: namespace: e2e-tests-daemonsets-2pswh, resource: bindings, ignored listing per whitelist
Feb 14 15:04:23.596: INFO: namespace e2e-tests-daemonsets-2pswh deletion completed in 6.107166193s

• [SLOW TEST:92.208 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:04:23.597: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 14 15:04:26.211: INFO: Successfully updated pod "annotationupdated058b332-3069-11e9-a1dd-12216a2f1059"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:04:28.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lthng" for this suite.
Feb 14 15:04:50.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:04:50.292: INFO: namespace: e2e-tests-projected-lthng, resource: bindings, ignored listing per whitelist
Feb 14 15:04:50.431: INFO: namespace e2e-tests-projected-lthng deletion completed in 22.192722737s

• [SLOW TEST:26.835 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:04:50.433: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-5dx6s
Feb 14 15:04:54.544: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-5dx6s
STEP: checking the pod's current state and verifying that restartCount is present
Feb 14 15:04:54.552: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:08:55.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-5dx6s" for this suite.
Feb 14 15:09:01.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:09:01.161: INFO: namespace: e2e-tests-container-probe-5dx6s, resource: bindings, ignored listing per whitelist
Feb 14 15:09:01.164: INFO: namespace e2e-tests-container-probe-5dx6s deletion completed in 6.113157506s

• [SLOW TEST:250.731 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:09:01.164: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-75c91956-306a-11e9-a1dd-12216a2f1059
STEP: Creating secret with name secret-projected-all-test-volume-75c91933-306a-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 14 15:09:01.238: INFO: Waiting up to 5m0s for pod "projected-volume-75c918df-306a-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-4vj2m" to be "success or failure"
Feb 14 15:09:01.245: INFO: Pod "projected-volume-75c918df-306a-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 6.890614ms
Feb 14 15:09:03.249: INFO: Pod "projected-volume-75c918df-306a-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01034903s
STEP: Saw pod success
Feb 14 15:09:03.249: INFO: Pod "projected-volume-75c918df-306a-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:09:03.251: INFO: Trying to get logs from node k8s-conformance-node-3 pod projected-volume-75c918df-306a-11e9-a1dd-12216a2f1059 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 14 15:09:03.271: INFO: Waiting for pod projected-volume-75c918df-306a-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:09:03.274: INFO: Pod projected-volume-75c918df-306a-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:09:03.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4vj2m" for this suite.
Feb 14 15:09:09.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:09:09.320: INFO: namespace: e2e-tests-projected-4vj2m, resource: bindings, ignored listing per whitelist
Feb 14 15:09:09.372: INFO: namespace e2e-tests-projected-4vj2m deletion completed in 6.093409213s

• [SLOW TEST:8.208 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:09:09.374: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:09:09.435: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:09:11.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bq9z8" for this suite.
Feb 14 15:10:01.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:10:01.609: INFO: namespace: e2e-tests-pods-bq9z8, resource: bindings, ignored listing per whitelist
Feb 14 15:10:01.659: INFO: namespace e2e-tests-pods-bq9z8 deletion completed in 50.092792323s

• [SLOW TEST:52.286 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:10:01.659: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 14 15:10:01.726: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-284296654 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:10:01.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lc6sl" for this suite.
Feb 14 15:10:07.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:10:07.851: INFO: namespace: e2e-tests-kubectl-lc6sl, resource: bindings, ignored listing per whitelist
Feb 14 15:10:07.904: INFO: namespace e2e-tests-kubectl-lc6sl deletion completed in 6.088356742s

• [SLOW TEST:6.244 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:10:07.904: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 14 15:10:07.964: INFO: namespace e2e-tests-kubectl-pmg8x
Feb 14 15:10:07.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-pmg8x'
Feb 14 15:10:08.241: INFO: stderr: ""
Feb 14 15:10:08.241: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 14 15:10:09.244: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:10:09.244: INFO: Found 0 / 1
Feb 14 15:10:10.249: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:10:10.249: INFO: Found 0 / 1
Feb 14 15:10:11.245: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:10:11.245: INFO: Found 1 / 1
Feb 14 15:10:11.245: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 14 15:10:11.248: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:10:11.248: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 14 15:10:11.248: INFO: wait on redis-master startup in e2e-tests-kubectl-pmg8x 
Feb 14 15:10:11.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 logs redis-master-79vw5 redis-master --namespace=e2e-tests-kubectl-pmg8x'
Feb 14 15:10:11.353: INFO: stderr: ""
Feb 14 15:10:11.353: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Feb 15:10:09.979 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Feb 15:10:09.979 # Server started, Redis version 3.2.12\n1:M 14 Feb 15:10:09.980 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Feb 15:10:09.980 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 14 15:10:11.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-pmg8x'
Feb 14 15:10:11.493: INFO: stderr: ""
Feb 14 15:10:11.493: INFO: stdout: "service/rm2 exposed\n"
Feb 14 15:10:11.499: INFO: Service rm2 in namespace e2e-tests-kubectl-pmg8x found.
STEP: exposing service
Feb 14 15:10:13.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-pmg8x'
Feb 14 15:10:13.616: INFO: stderr: ""
Feb 14 15:10:13.616: INFO: stdout: "service/rm3 exposed\n"
Feb 14 15:10:13.620: INFO: Service rm3 in namespace e2e-tests-kubectl-pmg8x found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:10:15.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pmg8x" for this suite.
Feb 14 15:10:37.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:10:37.666: INFO: namespace: e2e-tests-kubectl-pmg8x, resource: bindings, ignored listing per whitelist
Feb 14 15:10:37.731: INFO: namespace e2e-tests-kubectl-pmg8x deletion completed in 22.101083239s

• [SLOW TEST:29.827 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:10:37.735: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 14 15:10:40.354: INFO: Successfully updated pod "annotationupdateaf59728e-306a-11e9-a1dd-12216a2f1059"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:10:44.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7ggsv" for this suite.
Feb 14 15:11:06.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:11:06.481: INFO: namespace: e2e-tests-downward-api-7ggsv, resource: bindings, ignored listing per whitelist
Feb 14 15:11:06.522: INFO: namespace e2e-tests-downward-api-7ggsv deletion completed in 22.109370563s

• [SLOW TEST:28.787 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:11:06.523: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:12:06.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-5zk4w" for this suite.
Feb 14 15:12:28.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:12:28.701: INFO: namespace: e2e-tests-container-probe-5zk4w, resource: bindings, ignored listing per whitelist
Feb 14 15:12:28.717: INFO: namespace e2e-tests-container-probe-5zk4w deletion completed in 22.106590718s

• [SLOW TEST:82.195 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:12:28.719: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 15:12:28.800: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f180dc0d-306a-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-sptd6" to be "success or failure"
Feb 14 15:12:28.806: INFO: Pod "downwardapi-volume-f180dc0d-306a-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 5.778027ms
Feb 14 15:12:30.810: INFO: Pod "downwardapi-volume-f180dc0d-306a-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010503289s
STEP: Saw pod success
Feb 14 15:12:30.810: INFO: Pod "downwardapi-volume-f180dc0d-306a-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:12:30.813: INFO: Trying to get logs from node k8s-conformance-node-1 pod downwardapi-volume-f180dc0d-306a-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 15:12:30.835: INFO: Waiting for pod downwardapi-volume-f180dc0d-306a-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:12:30.847: INFO: Pod downwardapi-volume-f180dc0d-306a-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:12:30.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sptd6" for this suite.
Feb 14 15:12:36.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:12:36.943: INFO: namespace: e2e-tests-downward-api-sptd6, resource: bindings, ignored listing per whitelist
Feb 14 15:12:36.970: INFO: namespace e2e-tests-downward-api-sptd6 deletion completed in 6.119564339s

• [SLOW TEST:8.252 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:12:36.971: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-f66bcf72-306a-11e9-a1dd-12216a2f1059
STEP: Creating secret with name s-test-opt-upd-f66bcfd3-306a-11e9-a1dd-12216a2f1059
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f66bcf72-306a-11e9-a1dd-12216a2f1059
STEP: Updating secret s-test-opt-upd-f66bcfd3-306a-11e9-a1dd-12216a2f1059
STEP: Creating secret with name s-test-opt-create-f66bd088-306a-11e9-a1dd-12216a2f1059
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:14:11.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-m5z4s" for this suite.
Feb 14 15:14:33.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:14:33.749: INFO: namespace: e2e-tests-projected-m5z4s, resource: bindings, ignored listing per whitelist
Feb 14 15:14:33.765: INFO: namespace e2e-tests-projected-m5z4s deletion completed in 22.114483819s

• [SLOW TEST:116.793 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:14:33.765: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 14 15:14:33.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 cluster-info'
Feb 14 15:14:33.955: INFO: stderr: ""
Feb 14 15:14:33.955: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:14:33.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hst84" for this suite.
Feb 14 15:14:39.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:14:40.035: INFO: namespace: e2e-tests-kubectl-hst84, resource: bindings, ignored listing per whitelist
Feb 14 15:14:40.071: INFO: namespace e2e-tests-kubectl-hst84 deletion completed in 6.110776769s

• [SLOW TEST:6.306 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:14:40.072: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-mfx54
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-mfx54
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-mfx54
Feb 14 15:14:40.155: INFO: Found 0 stateful pods, waiting for 1
Feb 14 15:14:50.160: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 14 15:14:50.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-mfx54 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 15:14:50.377: INFO: stderr: ""
Feb 14 15:14:50.377: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 15:14:50.377: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 15:14:50.385: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 14 15:15:00.390: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 15:15:00.390: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 15:15:00.407: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999982s
Feb 14 15:15:01.412: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997577165s
Feb 14 15:15:02.417: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992659012s
Feb 14 15:15:03.422: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988265489s
Feb 14 15:15:04.427: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983503329s
Feb 14 15:15:05.431: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978528111s
Feb 14 15:15:06.435: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.973960034s
Feb 14 15:15:07.439: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.969868028s
Feb 14 15:15:08.443: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.965796314s
Feb 14 15:15:09.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 962.041322ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-mfx54
Feb 14 15:15:10.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-mfx54 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:15:10.632: INFO: stderr: ""
Feb 14 15:15:10.632: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 15:15:10.632: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 15:15:10.648: INFO: Found 1 stateful pods, waiting for 3
Feb 14 15:15:20.652: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 15:15:20.652: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 15:15:20.652: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 14 15:15:20.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-mfx54 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 15:15:20.842: INFO: stderr: ""
Feb 14 15:15:20.842: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 15:15:20.842: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 15:15:20.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-mfx54 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 15:15:21.021: INFO: stderr: ""
Feb 14 15:15:21.021: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 15:15:21.021: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 15:15:21.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-mfx54 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 15:15:21.214: INFO: stderr: ""
Feb 14 15:15:21.214: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 15:15:21.214: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 15:15:21.214: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 15:15:21.217: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 14 15:15:31.222: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 15:15:31.222: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 15:15:31.222: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 15:15:31.234: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999964s
Feb 14 15:15:32.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992344906s
Feb 14 15:15:33.241: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988619192s
Feb 14 15:15:34.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985118059s
Feb 14 15:15:35.250: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980598972s
Feb 14 15:15:36.254: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975875548s
Feb 14 15:15:37.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972342814s
Feb 14 15:15:38.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965341011s
Feb 14 15:15:39.269: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961276163s
Feb 14 15:15:40.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.313037ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-mfx54
Feb 14 15:15:41.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-mfx54 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:15:41.482: INFO: stderr: ""
Feb 14 15:15:41.482: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 15:15:41.482: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 15:15:41.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-mfx54 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:15:41.703: INFO: stderr: ""
Feb 14 15:15:41.703: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 15:15:41.703: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 15:15:41.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-mfx54 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:15:41.896: INFO: stderr: ""
Feb 14 15:15:41.897: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 15:15:41.897: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 15:15:41.897: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 14 15:15:51.909: INFO: Deleting all statefulset in ns e2e-tests-statefulset-mfx54
Feb 14 15:15:51.911: INFO: Scaling statefulset ss to 0
Feb 14 15:15:51.918: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 15:15:51.920: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:15:51.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-mfx54" for this suite.
Feb 14 15:15:57.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:15:57.998: INFO: namespace: e2e-tests-statefulset-mfx54, resource: bindings, ignored listing per whitelist
Feb 14 15:15:58.027: INFO: namespace e2e-tests-statefulset-mfx54 deletion completed in 6.089367709s

• [SLOW TEST:77.955 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:15:58.028: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 15:15:58.094: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e40b694-306b-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-kqhvw" to be "success or failure"
Feb 14 15:15:58.102: INFO: Pod "downwardapi-volume-6e40b694-306b-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 7.739489ms
Feb 14 15:16:00.106: INFO: Pod "downwardapi-volume-6e40b694-306b-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011260309s
STEP: Saw pod success
Feb 14 15:16:00.106: INFO: Pod "downwardapi-volume-6e40b694-306b-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:16:00.108: INFO: Trying to get logs from node k8s-conformance-node-3 pod downwardapi-volume-6e40b694-306b-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 15:16:00.126: INFO: Waiting for pod downwardapi-volume-6e40b694-306b-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:16:00.130: INFO: Pod downwardapi-volume-6e40b694-306b-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:16:00.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kqhvw" for this suite.
Feb 14 15:16:06.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:16:06.181: INFO: namespace: e2e-tests-downward-api-kqhvw, resource: bindings, ignored listing per whitelist
Feb 14 15:16:06.224: INFO: namespace e2e-tests-downward-api-kqhvw deletion completed in 6.090473767s

• [SLOW TEST:8.196 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:16:06.225: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 14 15:16:06.289: INFO: Waiting up to 5m0s for pod "client-containers-73233905-306b-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-containers-c27ds" to be "success or failure"
Feb 14 15:16:06.301: INFO: Pod "client-containers-73233905-306b-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 12.245022ms
Feb 14 15:16:08.305: INFO: Pod "client-containers-73233905-306b-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016234025s
STEP: Saw pod success
Feb 14 15:16:08.305: INFO: Pod "client-containers-73233905-306b-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:16:08.307: INFO: Trying to get logs from node k8s-conformance-node-1 pod client-containers-73233905-306b-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 15:16:08.329: INFO: Waiting for pod client-containers-73233905-306b-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:16:08.332: INFO: Pod client-containers-73233905-306b-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:16:08.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-c27ds" for this suite.
Feb 14 15:16:14.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:16:14.374: INFO: namespace: e2e-tests-containers-c27ds, resource: bindings, ignored listing per whitelist
Feb 14 15:16:14.436: INFO: namespace e2e-tests-containers-c27ds deletion completed in 6.094517795s

• [SLOW TEST:8.212 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:16:14.438: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 14 15:16:14.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 api-versions'
Feb 14 15:16:14.637: INFO: stderr: ""
Feb 14 15:16:14.637: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ncsi.storage.k8s.io/v1alpha1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:16:14.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-krkvk" for this suite.
Feb 14 15:16:20.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:16:20.696: INFO: namespace: e2e-tests-kubectl-krkvk, resource: bindings, ignored listing per whitelist
Feb 14 15:16:20.756: INFO: namespace e2e-tests-kubectl-krkvk deletion completed in 6.115073321s

• [SLOW TEST:6.318 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:16:20.757: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-dgj7
STEP: Creating a pod to test atomic-volume-subpath
Feb 14 15:16:20.831: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dgj7" in namespace "e2e-tests-subpath-p6r8t" to be "success or failure"
Feb 14 15:16:20.837: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.770971ms
Feb 14 15:16:22.841: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010885323s
Feb 14 15:16:24.846: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Running", Reason="", readiness=false. Elapsed: 4.014968211s
Feb 14 15:16:26.850: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Running", Reason="", readiness=false. Elapsed: 6.019463591s
Feb 14 15:16:28.853: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Running", Reason="", readiness=false. Elapsed: 8.022826188s
Feb 14 15:16:30.857: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Running", Reason="", readiness=false. Elapsed: 10.02637443s
Feb 14 15:16:32.861: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Running", Reason="", readiness=false. Elapsed: 12.030204957s
Feb 14 15:16:34.867: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Running", Reason="", readiness=false. Elapsed: 14.035914815s
Feb 14 15:16:36.870: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Running", Reason="", readiness=false. Elapsed: 16.039562562s
Feb 14 15:16:38.873: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Running", Reason="", readiness=false. Elapsed: 18.042892052s
Feb 14 15:16:40.878: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Running", Reason="", readiness=false. Elapsed: 20.047237827s
Feb 14 15:16:42.882: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Running", Reason="", readiness=false. Elapsed: 22.050922477s
Feb 14 15:16:44.886: INFO: Pod "pod-subpath-test-downwardapi-dgj7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.05505302s
STEP: Saw pod success
Feb 14 15:16:44.886: INFO: Pod "pod-subpath-test-downwardapi-dgj7" satisfied condition "success or failure"
Feb 14 15:16:44.888: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-subpath-test-downwardapi-dgj7 container test-container-subpath-downwardapi-dgj7: <nil>
STEP: delete the pod
Feb 14 15:16:44.904: INFO: Waiting for pod pod-subpath-test-downwardapi-dgj7 to disappear
Feb 14 15:16:44.908: INFO: Pod pod-subpath-test-downwardapi-dgj7 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-dgj7
Feb 14 15:16:44.909: INFO: Deleting pod "pod-subpath-test-downwardapi-dgj7" in namespace "e2e-tests-subpath-p6r8t"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:16:44.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-p6r8t" for this suite.
Feb 14 15:16:50.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:16:50.966: INFO: namespace: e2e-tests-subpath-p6r8t, resource: bindings, ignored listing per whitelist
Feb 14 15:16:51.127: INFO: namespace e2e-tests-subpath-p6r8t deletion completed in 6.21325838s

• [SLOW TEST:30.371 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:16:51.131: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 14 15:16:53.221: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-8de9948b-306b-11e9-a1dd-12216a2f1059,GenerateName:,Namespace:e2e-tests-events-wh6vb,SelfLink:/api/v1/namespaces/e2e-tests-events-wh6vb/pods/send-events-8de9948b-306b-11e9-a1dd-12216a2f1059,UID:8dea1295-306b-11e9-aaef-f23c91bac0b7,ResourceVersion:15540,Generation:0,CreationTimestamp:2019-02-14 15:16:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 204016117,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.86/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kbkkb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kbkkb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-kbkkb true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00104ec40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00104ec60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:16:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:16:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:16:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:16:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:10.244.3.86,StartTime:2019-02-14 15:16:51 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-14 15:16:52 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://694ba40f3c3e4309573c73c74dcae385c9a1797a5dc83cf0ce15bd51d7ad9d7c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 14 15:16:55.226: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 14 15:16:57.230: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:16:57.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-wh6vb" for this suite.
Feb 14 15:17:39.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:17:39.329: INFO: namespace: e2e-tests-events-wh6vb, resource: bindings, ignored listing per whitelist
Feb 14 15:17:39.353: INFO: namespace e2e-tests-events-wh6vb deletion completed in 42.10782461s

• [SLOW TEST:48.223 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:17:39.353: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 14 15:17:39.435: INFO: Waiting up to 5m0s for pod "downward-api-aaa83e4d-306b-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-c75hp" to be "success or failure"
Feb 14 15:17:39.439: INFO: Pod "downward-api-aaa83e4d-306b-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 3.994279ms
Feb 14 15:17:41.443: INFO: Pod "downward-api-aaa83e4d-306b-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007742553s
STEP: Saw pod success
Feb 14 15:17:41.443: INFO: Pod "downward-api-aaa83e4d-306b-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:17:41.446: INFO: Trying to get logs from node k8s-conformance-node-3 pod downward-api-aaa83e4d-306b-11e9-a1dd-12216a2f1059 container dapi-container: <nil>
STEP: delete the pod
Feb 14 15:17:41.468: INFO: Waiting for pod downward-api-aaa83e4d-306b-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:17:41.470: INFO: Pod downward-api-aaa83e4d-306b-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:17:41.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-c75hp" for this suite.
Feb 14 15:17:47.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:17:47.552: INFO: namespace: e2e-tests-downward-api-c75hp, resource: bindings, ignored listing per whitelist
Feb 14 15:17:47.567: INFO: namespace e2e-tests-downward-api-c75hp deletion completed in 6.091988303s

• [SLOW TEST:8.214 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:17:47.568: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-af8bcdaf-306b-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 15:17:47.640: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af8c3f99-306b-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-djn22" to be "success or failure"
Feb 14 15:17:47.645: INFO: Pod "pod-projected-secrets-af8c3f99-306b-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 5.476364ms
Feb 14 15:17:49.649: INFO: Pod "pod-projected-secrets-af8c3f99-306b-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009563461s
STEP: Saw pod success
Feb 14 15:17:49.649: INFO: Pod "pod-projected-secrets-af8c3f99-306b-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:17:49.652: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-projected-secrets-af8c3f99-306b-11e9-a1dd-12216a2f1059 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 14 15:17:49.680: INFO: Waiting for pod pod-projected-secrets-af8c3f99-306b-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:17:49.684: INFO: Pod pod-projected-secrets-af8c3f99-306b-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:17:49.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-djn22" for this suite.
Feb 14 15:17:55.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:17:55.725: INFO: namespace: e2e-tests-projected-djn22, resource: bindings, ignored listing per whitelist
Feb 14 15:17:55.779: INFO: namespace e2e-tests-projected-djn22 deletion completed in 6.0915409s

• [SLOW TEST:8.211 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:17:55.781: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:17:55.846: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 14 15:17:55.853: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 14 15:18:00.858: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 14 15:18:00.858: INFO: Creating deployment "test-rolling-update-deployment"
Feb 14 15:18:00.861: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 14 15:18:00.869: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 14 15:18:02.876: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 14 15:18:02.878: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 14 15:18:02.885: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-6trr8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-6trr8/deployments/test-rolling-update-deployment,UID:b76e4ec6-306b-11e9-aaef-f23c91bac0b7,ResourceVersion:15792,Generation:1,CreationTimestamp:2019-02-14 15:18:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-14 15:18:00 +0000 UTC 2019-02-14 15:18:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-14 15:18:02 +0000 UTC 2019-02-14 15:18:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 14 15:18:02.888: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-6trr8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-6trr8/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:b770c61f-306b-11e9-aaef-f23c91bac0b7,ResourceVersion:15782,Generation:1,CreationTimestamp:2019-02-14 15:18:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b76e4ec6-306b-11e9-aaef-f23c91bac0b7 0xc002b54707 0xc002b54708}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 14 15:18:02.888: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 14 15:18:02.889: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-6trr8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-6trr8/replicasets/test-rolling-update-controller,UID:b471a399-306b-11e9-aaef-f23c91bac0b7,ResourceVersion:15791,Generation:2,CreationTimestamp:2019-02-14 15:17:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b76e4ec6-306b-11e9-aaef-f23c91bac0b7 0xc002b54637 0xc002b54638}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 14 15:18:02.891: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-mthp6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-mthp6,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-6trr8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6trr8/pods/test-rolling-update-deployment-68b55d7bc6-mthp6,UID:b7716fb7-306b-11e9-aaef-f23c91bac0b7,ResourceVersion:15781,Generation:0,CreationTimestamp:2019-02-14 15:18:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.88/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 b770c61f-306b-11e9-aaef-f23c91bac0b7 0xc0023fb7e7 0xc0023fb7e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-nk62w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nk62w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-nk62w true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023fb850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023fb870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:18:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:18:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:18:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:18:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:10.244.3.88,StartTime:2019-02-14 15:18:00 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-14 15:18:01 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://01719be8199b6109d8d754886d8ea29603389d900f62bddc772f627e545f60cb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:18:02.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-6trr8" for this suite.
Feb 14 15:18:08.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:18:08.991: INFO: namespace: e2e-tests-deployment-6trr8, resource: bindings, ignored listing per whitelist
Feb 14 15:18:08.995: INFO: namespace e2e-tests-deployment-6trr8 deletion completed in 6.099393257s

• [SLOW TEST:13.214 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:18:08.996: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:18:09.079: INFO: (0) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.114986ms)
Feb 14 15:18:09.083: INFO: (1) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.169792ms)
Feb 14 15:18:09.086: INFO: (2) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.100992ms)
Feb 14 15:18:09.089: INFO: (3) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.966283ms)
Feb 14 15:18:09.093: INFO: (4) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.67586ms)
Feb 14 15:18:09.096: INFO: (5) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.160392ms)
Feb 14 15:18:09.099: INFO: (6) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.988372ms)
Feb 14 15:18:09.111: INFO: (7) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 11.864499ms)
Feb 14 15:18:09.175: INFO: (8) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 63.907062ms)
Feb 14 15:18:09.194: INFO: (9) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 19.00546ms)
Feb 14 15:18:09.204: INFO: (10) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.149313ms)
Feb 14 15:18:09.210: INFO: (11) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.334666ms)
Feb 14 15:18:09.213: INFO: (12) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.318981ms)
Feb 14 15:18:09.217: INFO: (13) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.82608ms)
Feb 14 15:18:09.220: INFO: (14) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.064431ms)
Feb 14 15:18:09.223: INFO: (15) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.219751ms)
Feb 14 15:18:09.226: INFO: (16) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.900473ms)
Feb 14 15:18:09.230: INFO: (17) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.49299ms)
Feb 14 15:18:09.233: INFO: (18) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.133032ms)
Feb 14 15:18:09.236: INFO: (19) /api/v1/nodes/k8s-conformance-node-1:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.264162ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:18:09.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-vjr6z" for this suite.
Feb 14 15:18:15.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:18:15.275: INFO: namespace: e2e-tests-proxy-vjr6z, resource: bindings, ignored listing per whitelist
Feb 14 15:18:15.336: INFO: namespace e2e-tests-proxy-vjr6z deletion completed in 6.096493977s

• [SLOW TEST:6.341 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:18:15.337: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 15:18:15.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c0190083-306b-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-kqgf7" to be "success or failure"
Feb 14 15:18:15.416: INFO: Pod "downwardapi-volume-c0190083-306b-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 7.52893ms
Feb 14 15:18:17.419: INFO: Pod "downwardapi-volume-c0190083-306b-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010723188s
STEP: Saw pod success
Feb 14 15:18:17.419: INFO: Pod "downwardapi-volume-c0190083-306b-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:18:17.421: INFO: Trying to get logs from node k8s-conformance-node-3 pod downwardapi-volume-c0190083-306b-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 15:18:17.443: INFO: Waiting for pod downwardapi-volume-c0190083-306b-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:18:17.446: INFO: Pod downwardapi-volume-c0190083-306b-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:18:17.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kqgf7" for this suite.
Feb 14 15:18:23.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:18:23.507: INFO: namespace: e2e-tests-downward-api-kqgf7, resource: bindings, ignored listing per whitelist
Feb 14 15:18:23.540: INFO: namespace e2e-tests-downward-api-kqgf7 deletion completed in 6.089170283s

• [SLOW TEST:8.203 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:18:23.540: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-j9tdv.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-j9tdv.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-j9tdv.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-j9tdv.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-j9tdv.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-j9tdv.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 14 15:18:35.701: INFO: DNS probes using e2e-tests-dns-j9tdv/dns-test-c4fdc500-306b-11e9-a1dd-12216a2f1059 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:18:35.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-j9tdv" for this suite.
Feb 14 15:18:41.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:18:41.792: INFO: namespace: e2e-tests-dns-j9tdv, resource: bindings, ignored listing per whitelist
Feb 14 15:18:41.836: INFO: namespace e2e-tests-dns-j9tdv deletion completed in 6.099926721s

• [SLOW TEST:18.296 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:18:41.836: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Feb 14 15:18:43.949: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:19:05.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-kxtn6" for this suite.
Feb 14 15:19:12.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:19:12.055: INFO: namespace: e2e-tests-namespaces-kxtn6, resource: bindings, ignored listing per whitelist
Feb 14 15:19:12.108: INFO: namespace e2e-tests-namespaces-kxtn6 deletion completed in 6.111282385s
STEP: Destroying namespace "e2e-tests-nsdeletetest-zlkw2" for this suite.
Feb 14 15:19:12.111: INFO: Namespace e2e-tests-nsdeletetest-zlkw2 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-s8qk4" for this suite.
Feb 14 15:19:18.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:19:18.155: INFO: namespace: e2e-tests-nsdeletetest-s8qk4, resource: bindings, ignored listing per whitelist
Feb 14 15:19:18.216: INFO: namespace e2e-tests-nsdeletetest-s8qk4 deletion completed in 6.10564513s

• [SLOW TEST:36.380 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:19:18.219: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-lmfzt A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-lmfzt;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-lmfzt A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-lmfzt;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-lmfzt.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-lmfzt.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-lmfzt.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-lmfzt.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-lmfzt.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-lmfzt.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-lmfzt.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-lmfzt.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 206.161.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.161.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.161.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.161.206_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-lmfzt A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-lmfzt;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-lmfzt A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-lmfzt.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-lmfzt.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-lmfzt.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-lmfzt.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-lmfzt.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-lmfzt.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-lmfzt.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-lmfzt.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 206.161.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.161.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.161.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.161.206_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 14 15:19:28.371: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:28.393: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:28.395: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:28.398: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-lmfzt from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:28.401: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:28.404: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:28.407: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:28.409: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:28.413: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:28.433: INFO: Lookups using e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059 failed for: [wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-lmfzt jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt jessie_udp@dns-test-service.e2e-tests-dns-lmfzt.svc jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc]

Feb 14 15:19:33.461: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:33.489: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:33.492: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:33.495: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-lmfzt from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:33.498: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:33.501: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:33.504: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:33.507: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:33.510: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:33.530: INFO: Lookups using e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059 failed for: [wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-lmfzt jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt jessie_udp@dns-test-service.e2e-tests-dns-lmfzt.svc jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc]

Feb 14 15:19:38.460: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:38.492: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:38.495: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:38.499: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-lmfzt from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:38.502: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:38.505: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:38.508: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:38.511: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:38.515: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:38.534: INFO: Lookups using e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059 failed for: [wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-lmfzt jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt jessie_udp@dns-test-service.e2e-tests-dns-lmfzt.svc jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc]

Feb 14 15:19:43.458: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:43.482: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:43.484: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:43.488: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-lmfzt from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:43.491: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:43.494: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:43.498: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:43.501: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:43.505: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:43.523: INFO: Lookups using e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059 failed for: [wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-lmfzt jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt jessie_udp@dns-test-service.e2e-tests-dns-lmfzt.svc jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc]

Feb 14 15:19:48.460: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:48.486: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:48.489: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:48.492: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-lmfzt from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:48.495: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:48.499: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:48.502: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:48.505: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:48.507: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc from pod e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059: the server could not find the requested resource (get pods dns-test-e597b657-306b-11e9-a1dd-12216a2f1059)
Feb 14 15:19:48.526: INFO: Lookups using e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059 failed for: [wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-lmfzt jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt jessie_udp@dns-test-service.e2e-tests-dns-lmfzt.svc jessie_tcp@dns-test-service.e2e-tests-dns-lmfzt.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-lmfzt.svc]

Feb 14 15:19:53.530: INFO: DNS probes using e2e-tests-dns-lmfzt/dns-test-e597b657-306b-11e9-a1dd-12216a2f1059 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:19:53.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-lmfzt" for this suite.
Feb 14 15:19:59.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:19:59.625: INFO: namespace: e2e-tests-dns-lmfzt, resource: bindings, ignored listing per whitelist
Feb 14 15:19:59.692: INFO: namespace e2e-tests-dns-lmfzt deletion completed in 6.096626168s

• [SLOW TEST:41.474 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:19:59.693: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 14 15:20:05.802: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jk7qm PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:20:05.802: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:20:05.899: INFO: Exec stderr: ""
Feb 14 15:20:05.900: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jk7qm PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:20:05.900: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:20:06.002: INFO: Exec stderr: ""
Feb 14 15:20:06.002: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jk7qm PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:20:06.002: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:20:06.113: INFO: Exec stderr: ""
Feb 14 15:20:06.113: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jk7qm PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:20:06.113: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:20:06.221: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 14 15:20:06.222: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jk7qm PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:20:06.222: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:20:06.329: INFO: Exec stderr: ""
Feb 14 15:20:06.329: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jk7qm PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:20:06.329: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:20:06.440: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 14 15:20:06.440: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jk7qm PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:20:06.440: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:20:06.530: INFO: Exec stderr: ""
Feb 14 15:20:06.530: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jk7qm PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:20:06.530: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:20:06.613: INFO: Exec stderr: ""
Feb 14 15:20:06.613: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jk7qm PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:20:06.613: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:20:06.720: INFO: Exec stderr: ""
Feb 14 15:20:06.720: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jk7qm PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:20:06.720: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:20:06.825: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:20:06.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-jk7qm" for this suite.
Feb 14 15:20:48.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:20:48.852: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-jk7qm, resource: bindings, ignored listing per whitelist
Feb 14 15:20:48.914: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-jk7qm deletion completed in 42.085489613s

• [SLOW TEST:49.222 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:20:48.915: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 14 15:20:48.985: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-a,UID:1ba3ed05-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16467,Generation:0,CreationTimestamp:2019-02-14 15:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 14 15:20:48.985: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-a,UID:1ba3ed05-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16467,Generation:0,CreationTimestamp:2019-02-14 15:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 14 15:20:58.994: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-a,UID:1ba3ed05-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16488,Generation:0,CreationTimestamp:2019-02-14 15:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 14 15:20:58.994: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-a,UID:1ba3ed05-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16488,Generation:0,CreationTimestamp:2019-02-14 15:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 14 15:21:09.000: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-a,UID:1ba3ed05-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16509,Generation:0,CreationTimestamp:2019-02-14 15:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 14 15:21:09.000: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-a,UID:1ba3ed05-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16509,Generation:0,CreationTimestamp:2019-02-14 15:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 14 15:21:19.006: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-a,UID:1ba3ed05-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16530,Generation:0,CreationTimestamp:2019-02-14 15:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 14 15:21:19.006: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-a,UID:1ba3ed05-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16530,Generation:0,CreationTimestamp:2019-02-14 15:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 14 15:21:29.013: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-b,UID:337f7cef-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16551,Generation:0,CreationTimestamp:2019-02-14 15:21:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 14 15:21:29.013: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-b,UID:337f7cef-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16551,Generation:0,CreationTimestamp:2019-02-14 15:21:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 14 15:21:39.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-b,UID:337f7cef-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16572,Generation:0,CreationTimestamp:2019-02-14 15:21:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 14 15:21:39.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-r8ndc,SelfLink:/api/v1/namespaces/e2e-tests-watch-r8ndc/configmaps/e2e-watch-test-configmap-b,UID:337f7cef-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:16572,Generation:0,CreationTimestamp:2019-02-14 15:21:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:21:49.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-r8ndc" for this suite.
Feb 14 15:21:55.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:21:55.101: INFO: namespace: e2e-tests-watch-r8ndc, resource: bindings, ignored listing per whitelist
Feb 14 15:21:55.133: INFO: namespace e2e-tests-watch-r8ndc deletion completed in 6.109207163s

• [SLOW TEST:66.218 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:21:55.135: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 14 15:21:55.201: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 14 15:21:55.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:21:55.595: INFO: stderr: ""
Feb 14 15:21:55.595: INFO: stdout: "service/redis-slave created\n"
Feb 14 15:21:55.595: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 14 15:21:55.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:21:55.789: INFO: stderr: ""
Feb 14 15:21:55.789: INFO: stdout: "service/redis-master created\n"
Feb 14 15:21:55.789: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 14 15:21:55.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:21:55.985: INFO: stderr: ""
Feb 14 15:21:55.985: INFO: stdout: "service/frontend created\n"
Feb 14 15:21:55.985: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 14 15:21:55.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:21:56.174: INFO: stderr: ""
Feb 14 15:21:56.174: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 14 15:21:56.174: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 14 15:21:56.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:21:56.360: INFO: stderr: ""
Feb 14 15:21:56.360: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 14 15:21:56.360: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 14 15:21:56.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:21:56.568: INFO: stderr: ""
Feb 14 15:21:56.568: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 14 15:21:56.568: INFO: Waiting for all frontend pods to be Running.
Feb 14 15:22:16.619: INFO: Waiting for frontend to serve content.
Feb 14 15:22:16.634: INFO: Trying to add a new entry to the guestbook.
Feb 14 15:22:16.649: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 14 15:22:16.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:22:16.770: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 15:22:16.770: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 14 15:22:16.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:22:16.899: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 15:22:16.899: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 14 15:22:16.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:22:17.023: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 15:22:17.023: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 14 15:22:17.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:22:17.131: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 15:22:17.131: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 14 15:22:17.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:22:17.296: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 15:22:17.296: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 14 15:22:17.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cg4fd'
Feb 14 15:22:17.495: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 15:22:17.495: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:22:17.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cg4fd" for this suite.
Feb 14 15:22:57.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:22:57.559: INFO: namespace: e2e-tests-kubectl-cg4fd, resource: bindings, ignored listing per whitelist
Feb 14 15:22:57.592: INFO: namespace e2e-tests-kubectl-cg4fd deletion completed in 40.089621027s

• [SLOW TEST:62.456 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:22:57.592: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 14 15:22:57.677: INFO: Waiting up to 5m0s for pod "pod-6855bc8c-306c-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-q9kdx" to be "success or failure"
Feb 14 15:22:57.691: INFO: Pod "pod-6855bc8c-306c-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 14.135134ms
Feb 14 15:22:59.695: INFO: Pod "pod-6855bc8c-306c-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018119926s
STEP: Saw pod success
Feb 14 15:22:59.696: INFO: Pod "pod-6855bc8c-306c-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:22:59.698: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-6855bc8c-306c-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 15:22:59.731: INFO: Waiting for pod pod-6855bc8c-306c-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:22:59.735: INFO: Pod pod-6855bc8c-306c-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:22:59.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-q9kdx" for this suite.
Feb 14 15:23:05.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:23:05.800: INFO: namespace: e2e-tests-emptydir-q9kdx, resource: bindings, ignored listing per whitelist
Feb 14 15:23:05.828: INFO: namespace e2e-tests-emptydir-q9kdx deletion completed in 6.089933433s

• [SLOW TEST:8.236 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:23:05.828: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:23:05.897: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:23:06.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-nbkff" for this suite.
Feb 14 15:23:12.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:23:13.025: INFO: namespace: e2e-tests-custom-resource-definition-nbkff, resource: bindings, ignored listing per whitelist
Feb 14 15:23:13.054: INFO: namespace e2e-tests-custom-resource-definition-nbkff deletion completed in 6.103378917s

• [SLOW TEST:7.226 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:23:13.056: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 15:23:13.128: INFO: Waiting up to 5m0s for pod "downwardapi-volume-718df137-306c-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-6g25c" to be "success or failure"
Feb 14 15:23:13.136: INFO: Pod "downwardapi-volume-718df137-306c-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 7.665365ms
Feb 14 15:23:15.140: INFO: Pod "downwardapi-volume-718df137-306c-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011621223s
STEP: Saw pod success
Feb 14 15:23:15.140: INFO: Pod "downwardapi-volume-718df137-306c-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:23:15.142: INFO: Trying to get logs from node k8s-conformance-node-3 pod downwardapi-volume-718df137-306c-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 15:23:15.165: INFO: Waiting for pod downwardapi-volume-718df137-306c-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:23:15.167: INFO: Pod downwardapi-volume-718df137-306c-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:23:15.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6g25c" for this suite.
Feb 14 15:23:21.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:23:21.270: INFO: namespace: e2e-tests-projected-6g25c, resource: bindings, ignored listing per whitelist
Feb 14 15:23:21.272: INFO: namespace e2e-tests-projected-6g25c deletion completed in 6.100470871s

• [SLOW TEST:8.216 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:23:21.273: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 14 15:23:21.387: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:23:24.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-2ncm5" for this suite.
Feb 14 15:23:30.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:23:30.774: INFO: namespace: e2e-tests-init-container-2ncm5, resource: bindings, ignored listing per whitelist
Feb 14 15:23:30.785: INFO: namespace e2e-tests-init-container-2ncm5 deletion completed in 6.092604478s

• [SLOW TEST:9.512 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:23:30.785: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-7c1e3dfa-306c-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 15:23:30.863: INFO: Waiting up to 5m0s for pod "pod-secrets-7c1eeaba-306c-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-secrets-h7tnk" to be "success or failure"
Feb 14 15:23:30.871: INFO: Pod "pod-secrets-7c1eeaba-306c-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086067ms
Feb 14 15:23:32.875: INFO: Pod "pod-secrets-7c1eeaba-306c-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011766512s
STEP: Saw pod success
Feb 14 15:23:32.875: INFO: Pod "pod-secrets-7c1eeaba-306c-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:23:32.877: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-secrets-7c1eeaba-306c-11e9-a1dd-12216a2f1059 container secret-volume-test: <nil>
STEP: delete the pod
Feb 14 15:23:32.899: INFO: Waiting for pod pod-secrets-7c1eeaba-306c-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:23:32.902: INFO: Pod pod-secrets-7c1eeaba-306c-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:23:32.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-h7tnk" for this suite.
Feb 14 15:23:38.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:23:38.934: INFO: namespace: e2e-tests-secrets-h7tnk, resource: bindings, ignored listing per whitelist
Feb 14 15:23:38.994: INFO: namespace e2e-tests-secrets-h7tnk deletion completed in 6.088683051s

• [SLOW TEST:8.209 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:23:38.994: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-81027b0e-306c-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 15:23:39.062: INFO: Waiting up to 5m0s for pod "pod-secrets-8102e28a-306c-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-secrets-q422r" to be "success or failure"
Feb 14 15:23:39.067: INFO: Pod "pod-secrets-8102e28a-306c-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 5.111457ms
Feb 14 15:23:41.070: INFO: Pod "pod-secrets-8102e28a-306c-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008469526s
STEP: Saw pod success
Feb 14 15:23:41.070: INFO: Pod "pod-secrets-8102e28a-306c-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:23:41.073: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-secrets-8102e28a-306c-11e9-a1dd-12216a2f1059 container secret-volume-test: <nil>
STEP: delete the pod
Feb 14 15:23:41.108: INFO: Waiting for pod pod-secrets-8102e28a-306c-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:23:41.111: INFO: Pod pod-secrets-8102e28a-306c-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:23:41.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-q422r" for this suite.
Feb 14 15:23:47.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:23:47.172: INFO: namespace: e2e-tests-secrets-q422r, resource: bindings, ignored listing per whitelist
Feb 14 15:23:47.219: INFO: namespace e2e-tests-secrets-q422r deletion completed in 6.101951698s

• [SLOW TEST:8.225 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:23:47.221: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 14 15:23:47.297: INFO: Waiting up to 5m0s for pod "var-expansion-85eb3521-306c-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-var-expansion-fb7xr" to be "success or failure"
Feb 14 15:23:47.303: INFO: Pod "var-expansion-85eb3521-306c-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 5.921908ms
Feb 14 15:23:49.314: INFO: Pod "var-expansion-85eb3521-306c-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016933545s
Feb 14 15:23:51.318: INFO: Pod "var-expansion-85eb3521-306c-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020520018s
STEP: Saw pod success
Feb 14 15:23:51.318: INFO: Pod "var-expansion-85eb3521-306c-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:23:51.320: INFO: Trying to get logs from node k8s-conformance-node-3 pod var-expansion-85eb3521-306c-11e9-a1dd-12216a2f1059 container dapi-container: <nil>
STEP: delete the pod
Feb 14 15:23:51.344: INFO: Waiting for pod var-expansion-85eb3521-306c-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:23:51.347: INFO: Pod var-expansion-85eb3521-306c-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:23:51.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-fb7xr" for this suite.
Feb 14 15:23:57.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:23:57.392: INFO: namespace: e2e-tests-var-expansion-fb7xr, resource: bindings, ignored listing per whitelist
Feb 14 15:23:57.461: INFO: namespace e2e-tests-var-expansion-fb7xr deletion completed in 6.111305288s

• [SLOW TEST:10.240 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:23:57.462: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:23:57.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 version --client'
Feb 14 15:23:57.596: INFO: stderr: ""
Feb 14 15:23:57.596: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 14 15:23:57.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-9cm47'
Feb 14 15:23:57.801: INFO: stderr: ""
Feb 14 15:23:57.801: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 14 15:23:57.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-9cm47'
Feb 14 15:23:58.014: INFO: stderr: ""
Feb 14 15:23:58.014: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 14 15:23:59.018: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:23:59.018: INFO: Found 0 / 1
Feb 14 15:24:00.018: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:24:00.018: INFO: Found 1 / 1
Feb 14 15:24:00.018: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 14 15:24:00.020: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:24:00.020: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 14 15:24:00.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 describe pod redis-master-rj92f --namespace=e2e-tests-kubectl-9cm47'
Feb 14 15:24:00.132: INFO: stderr: ""
Feb 14 15:24:00.132: INFO: stdout: "Name:               redis-master-rj92f\nNamespace:          e2e-tests-kubectl-9cm47\nPriority:           0\nPriorityClassName:  <none>\nNode:               k8s-conformance-node-1/192.168.219.21\nStart Time:         Thu, 14 Feb 2019 15:23:57 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 10.244.3.96/32\nStatus:             Running\nIP:                 10.244.3.96\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://9a3d6523530968566ed360f34101d44249075d805939b60712dacba056ac003b\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 14 Feb 2019 15:23:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-67p4j (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-67p4j:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-67p4j\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                             Message\n  ----    ------     ----  ----                             -------\n  Normal  Scheduled  3s    default-scheduler                Successfully assigned e2e-tests-kubectl-9cm47/redis-master-rj92f to k8s-conformance-node-1\n  Normal  Pulled     2s    kubelet, k8s-conformance-node-1  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, k8s-conformance-node-1  Created container\n  Normal  Started    2s    kubelet, k8s-conformance-node-1  Started container\n"
Feb 14 15:24:00.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 describe rc redis-master --namespace=e2e-tests-kubectl-9cm47'
Feb 14 15:24:00.244: INFO: stderr: ""
Feb 14 15:24:00.244: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-9cm47\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-rj92f\n"
Feb 14 15:24:00.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 describe service redis-master --namespace=e2e-tests-kubectl-9cm47'
Feb 14 15:24:00.384: INFO: stderr: ""
Feb 14 15:24:00.384: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-9cm47\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.99.31.87\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.3.96:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 14 15:24:00.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 describe node k8s-conformance-master-1'
Feb 14 15:24:00.538: INFO: stderr: ""
Feb 14 15:24:00.538: INFO: stdout: "Name:               k8s-conformance-master-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=g6-standard-2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-central\n                    kubernetes.io/hostname=k8s-conformance-master-1\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.212.252/17\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 14 Feb 2019 14:15:53 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 14 Feb 2019 15:23:59 +0000   Thu, 14 Feb 2019 14:15:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 14 Feb 2019 15:23:59 +0000   Thu, 14 Feb 2019 14:15:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 14 Feb 2019 15:23:59 +0000   Thu, 14 Feb 2019 14:15:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 14 Feb 2019 15:23:59 +0000   Thu, 14 Feb 2019 14:16:33 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  Hostname:    k8s-conformance-master-1\n  ExternalIP:  45.33.21.127\n  InternalIP:  192.168.212.252\nCapacity:\n cpu:                2\n ephemeral-storage:  78853264Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             4042372Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  72671167983\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3939972Ki\n pods:               110\nSystem Info:\n Machine ID:                 8d10ad2103c346e386de484be5c033ad\n System UUID:                8d10ad2103c346e386de484be5c033ad\n Boot ID:                    0e0d85f2-3038-482b-b9c4-40d99e7d952c\n Kernel Version:             4.14.84-coreos\n OS Image:                   Container Linux by CoreOS 1911.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.13.0\n Kube-Proxy Version:         v1.13.0\nPodCIDR:                     10.244.0.0/24\nProviderID:                  linode://12832869\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-4q2rw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\n  kube-system                calico-node-mlrgm                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         67m\n  kube-system                ccm-linode-s8jtj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         67m\n  kube-system                coredns-86c58d9df4-g9g9m                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     67m\n  kube-system                coredns-86c58d9df4-lfjrm                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     67m\n  kube-system                etcd-k8s-conformance-master-1                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         67m\n  kube-system                kube-apiserver-k8s-conformance-master-1                    250m (12%)    0 (0%)      0 (0%)           0 (0%)         67m\n  kube-system                kube-controller-manager-k8s-conformance-master-1           200m (10%)    0 (0%)      0 (0%)           0 (0%)         67m\n  kube-system                kube-proxy-5hrks                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         67m\n  kube-system                kube-scheduler-k8s-conformance-master-1                    100m (5%)     0 (0%)      0 (0%)           0 (0%)         66m\n  kube-system                kubernetes-dashboard-57df4db6b-7mr9n                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         67m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1 (50%)     0 (0%)\n  memory             140Mi (3%)  340Mi (8%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Feb 14 15:24:00.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 describe namespace e2e-tests-kubectl-9cm47'
Feb 14 15:24:00.648: INFO: stderr: ""
Feb 14 15:24:00.648: INFO: stdout: "Name:         e2e-tests-kubectl-9cm47\nLabels:       e2e-framework=kubectl\n              e2e-run=a5e7314e-3063-11e9-a1dd-12216a2f1059\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:24:00.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9cm47" for this suite.
Feb 14 15:24:22.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:24:22.685: INFO: namespace: e2e-tests-kubectl-9cm47, resource: bindings, ignored listing per whitelist
Feb 14 15:24:22.756: INFO: namespace e2e-tests-kubectl-9cm47 deletion completed in 22.1051517s

• [SLOW TEST:25.295 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:24:22.757: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 14 15:24:22.828: INFO: Waiting up to 5m0s for pod "downward-api-9b193483-306c-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-z9pk5" to be "success or failure"
Feb 14 15:24:22.833: INFO: Pod "downward-api-9b193483-306c-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 5.19465ms
Feb 14 15:24:24.837: INFO: Pod "downward-api-9b193483-306c-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009046418s
STEP: Saw pod success
Feb 14 15:24:24.837: INFO: Pod "downward-api-9b193483-306c-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:24:24.840: INFO: Trying to get logs from node k8s-conformance-node-3 pod downward-api-9b193483-306c-11e9-a1dd-12216a2f1059 container dapi-container: <nil>
STEP: delete the pod
Feb 14 15:24:24.860: INFO: Waiting for pod downward-api-9b193483-306c-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:24:24.863: INFO: Pod downward-api-9b193483-306c-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:24:24.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-z9pk5" for this suite.
Feb 14 15:24:30.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:24:30.938: INFO: namespace: e2e-tests-downward-api-z9pk5, resource: bindings, ignored listing per whitelist
Feb 14 15:24:30.964: INFO: namespace e2e-tests-downward-api-z9pk5 deletion completed in 6.096934277s

• [SLOW TEST:8.207 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:24:30.968: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:24:31.038: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 14 15:24:36.042: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 14 15:24:36.042: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 14 15:24:38.045: INFO: Creating deployment "test-rollover-deployment"
Feb 14 15:24:38.052: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 14 15:24:40.060: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 14 15:24:40.064: INFO: Ensure that both replica sets have 1 created replica
Feb 14 15:24:40.068: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 14 15:24:40.076: INFO: Updating deployment test-rollover-deployment
Feb 14 15:24:40.076: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 14 15:24:42.086: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 14 15:24:42.092: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 14 15:24:42.096: INFO: all replica sets need to contain the pod-template-hash label
Feb 14 15:24:42.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754681, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 15:24:44.104: INFO: all replica sets need to contain the pod-template-hash label
Feb 14 15:24:44.104: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754681, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 15:24:46.103: INFO: all replica sets need to contain the pod-template-hash label
Feb 14 15:24:46.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754681, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 15:24:48.102: INFO: all replica sets need to contain the pod-template-hash label
Feb 14 15:24:48.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754681, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 15:24:50.103: INFO: all replica sets need to contain the pod-template-hash label
Feb 14 15:24:50.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754681, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754678, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 15:24:52.103: INFO: 
Feb 14 15:24:52.103: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 14 15:24:52.109: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-skj9l,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-skj9l/deployments/test-rollover-deployment,UID:a42c645e-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:17497,Generation:2,CreationTimestamp:2019-02-14 15:24:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-14 15:24:38 +0000 UTC 2019-02-14 15:24:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-14 15:24:51 +0000 UTC 2019-02-14 15:24:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 14 15:24:52.112: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-skj9l,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-skj9l/replicasets/test-rollover-deployment-6b7f9d6597,UID:a561d58f-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:17488,Generation:2,CreationTimestamp:2019-02-14 15:24:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a42c645e-306c-11e9-aaef-f23c91bac0b7 0xc000fb4627 0xc000fb4628}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 14 15:24:52.112: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 14 15:24:52.112: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-skj9l,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-skj9l/replicasets/test-rollover-controller,UID:9ffdffed-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:17496,Generation:2,CreationTimestamp:2019-02-14 15:24:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a42c645e-306c-11e9-aaef-f23c91bac0b7 0xc0022c4f47 0xc0022c4f48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 14 15:24:52.113: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-skj9l,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-skj9l/replicasets/test-rollover-deployment-6586df867b,UID:a42e9d2d-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:17456,Generation:2,CreationTimestamp:2019-02-14 15:24:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a42c645e-306c-11e9-aaef-f23c91bac0b7 0xc000fb41b7 0xc000fb41b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 14 15:24:52.116: INFO: Pod "test-rollover-deployment-6b7f9d6597-k8mns" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-k8mns,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-skj9l,SelfLink:/api/v1/namespaces/e2e-tests-deployment-skj9l/pods/test-rollover-deployment-6b7f9d6597-k8mns,UID:a5670883-306c-11e9-aaef-f23c91bac0b7,ResourceVersion:17465,Generation:0,CreationTimestamp:2019-02-14 15:24:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.2.105/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 a561d58f-306c-11e9-aaef-f23c91bac0b7 0xc000fb57e7 0xc000fb57e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-54p4v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-54p4v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-54p4v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000fb5850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000fb5870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:24:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:24:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:24:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:24:40 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:10.244.2.105,StartTime:2019-02-14 15:24:40 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-14 15:24:41 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://c807ea63a33b05a94f5bee761b947bfe40db08c8d7bc974006348d4812a1b5b3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:24:52.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-skj9l" for this suite.
Feb 14 15:24:58.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:24:58.190: INFO: namespace: e2e-tests-deployment-skj9l, resource: bindings, ignored listing per whitelist
Feb 14 15:24:58.219: INFO: namespace e2e-tests-deployment-skj9l deletion completed in 6.099164347s

• [SLOW TEST:27.251 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:24:58.219: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:25:03.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-45zn4" for this suite.
Feb 14 15:25:25.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:25:25.408: INFO: namespace: e2e-tests-replication-controller-45zn4, resource: bindings, ignored listing per whitelist
Feb 14 15:25:25.430: INFO: namespace e2e-tests-replication-controller-45zn4 deletion completed in 22.110432457s

• [SLOW TEST:27.211 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:25:25.430: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-c074df53-306c-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 15:25:25.532: INFO: Waiting up to 5m0s for pod "pod-secrets-c078c8da-306c-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-secrets-tt6bq" to be "success or failure"
Feb 14 15:25:25.538: INFO: Pod "pod-secrets-c078c8da-306c-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 5.12141ms
Feb 14 15:25:27.543: INFO: Pod "pod-secrets-c078c8da-306c-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010980066s
STEP: Saw pod success
Feb 14 15:25:27.543: INFO: Pod "pod-secrets-c078c8da-306c-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:25:27.548: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-secrets-c078c8da-306c-11e9-a1dd-12216a2f1059 container secret-volume-test: <nil>
STEP: delete the pod
Feb 14 15:25:27.569: INFO: Waiting for pod pod-secrets-c078c8da-306c-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:25:27.577: INFO: Pod pod-secrets-c078c8da-306c-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:25:27.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-tt6bq" for this suite.
Feb 14 15:25:33.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:25:33.619: INFO: namespace: e2e-tests-secrets-tt6bq, resource: bindings, ignored listing per whitelist
Feb 14 15:25:33.704: INFO: namespace e2e-tests-secrets-tt6bq deletion completed in 6.116455626s
STEP: Destroying namespace "e2e-tests-secret-namespace-7brsl" for this suite.
Feb 14 15:25:39.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:25:39.753: INFO: namespace: e2e-tests-secret-namespace-7brsl, resource: bindings, ignored listing per whitelist
Feb 14 15:25:39.794: INFO: namespace e2e-tests-secret-namespace-7brsl deletion completed in 6.090551722s

• [SLOW TEST:14.364 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:25:39.798: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 14 15:25:39.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-wgsrm'
Feb 14 15:25:40.020: INFO: stderr: ""
Feb 14 15:25:40.020: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 14 15:25:41.023: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:25:41.023: INFO: Found 0 / 1
Feb 14 15:25:42.024: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:25:42.024: INFO: Found 1 / 1
Feb 14 15:25:42.024: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 14 15:25:42.026: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:25:42.026: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 14 15:25:42.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 patch pod redis-master-5q6zb --namespace=e2e-tests-kubectl-wgsrm -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 14 15:25:42.122: INFO: stderr: ""
Feb 14 15:25:42.122: INFO: stdout: "pod/redis-master-5q6zb patched\n"
STEP: checking annotations
Feb 14 15:25:42.124: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:25:42.124: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:25:42.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wgsrm" for this suite.
Feb 14 15:26:04.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:26:04.216: INFO: namespace: e2e-tests-kubectl-wgsrm, resource: bindings, ignored listing per whitelist
Feb 14 15:26:04.236: INFO: namespace e2e-tests-kubectl-wgsrm deletion completed in 22.109538211s

• [SLOW TEST:24.439 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:26:04.237: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-rmdjl
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-rmdjl
STEP: Deleting pre-stop pod
Feb 14 15:26:15.436: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:26:15.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-rmdjl" for this suite.
Feb 14 15:26:53.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:26:53.536: INFO: namespace: e2e-tests-prestop-rmdjl, resource: bindings, ignored listing per whitelist
Feb 14 15:26:53.569: INFO: namespace e2e-tests-prestop-rmdjl deletion completed in 38.117852313s

• [SLOW TEST:49.332 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:26:53.572: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-jjzgh in namespace e2e-tests-proxy-2h9bw
I0214 15:26:53.678480      16 runners.go:184] Created replication controller with name: proxy-service-jjzgh, namespace: e2e-tests-proxy-2h9bw, replica count: 1
I0214 15:26:54.731211      16 runners.go:184] proxy-service-jjzgh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0214 15:26:55.731564      16 runners.go:184] proxy-service-jjzgh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0214 15:26:56.731802      16 runners.go:184] proxy-service-jjzgh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0214 15:26:57.732045      16 runners.go:184] proxy-service-jjzgh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0214 15:26:58.732266      16 runners.go:184] proxy-service-jjzgh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0214 15:26:59.732535      16 runners.go:184] proxy-service-jjzgh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0214 15:27:00.732825      16 runners.go:184] proxy-service-jjzgh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0214 15:27:01.733085      16 runners.go:184] proxy-service-jjzgh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0214 15:27:02.733413      16 runners.go:184] proxy-service-jjzgh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 14 15:27:02.737: INFO: setup took 9.077726617s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 14 15:27:02.747: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 9.649375ms)
Feb 14 15:27:02.747: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 9.689905ms)
Feb 14 15:27:02.747: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 9.939214ms)
Feb 14 15:27:02.747: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 9.506455ms)
Feb 14 15:27:02.747: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 10.065804ms)
Feb 14 15:27:02.750: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 13.01179ms)
Feb 14 15:27:02.759: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 22.264875ms)
Feb 14 15:27:02.759: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 21.504327ms)
Feb 14 15:27:02.760: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 21.690866ms)
Feb 14 15:27:02.760: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 22.250966ms)
Feb 14 15:27:02.760: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 22.538315ms)
Feb 14 15:27:02.761: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 23.700563ms)
Feb 14 15:27:02.761: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 24.101323ms)
Feb 14 15:27:02.762: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 24.712431ms)
Feb 14 15:27:02.762: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 24.460112ms)
Feb 14 15:27:02.766: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 29.023634ms)
Feb 14 15:27:02.781: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 14.051008ms)
Feb 14 15:27:02.781: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 13.888399ms)
Feb 14 15:27:02.781: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 14.138679ms)
Feb 14 15:27:02.781: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 14.284048ms)
Feb 14 15:27:02.783: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 16.652404ms)
Feb 14 15:27:02.784: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 16.840324ms)
Feb 14 15:27:02.784: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 17.684532ms)
Feb 14 15:27:02.785: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 17.711482ms)
Feb 14 15:27:02.785: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 18.115301ms)
Feb 14 15:27:02.785: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 18.109112ms)
Feb 14 15:27:02.786: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 18.878921ms)
Feb 14 15:27:02.786: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 19.37254ms)
Feb 14 15:27:02.788: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 20.307349ms)
Feb 14 15:27:02.788: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 20.891267ms)
Feb 14 15:27:02.788: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 21.174577ms)
Feb 14 15:27:02.788: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 21.286866ms)
Feb 14 15:27:02.794: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 5.878491ms)
Feb 14 15:27:02.795: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 6.337029ms)
Feb 14 15:27:02.795: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 6.26394ms)
Feb 14 15:27:02.795: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 6.79621ms)
Feb 14 15:27:02.795: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 7.018259ms)
Feb 14 15:27:02.796: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 6.870599ms)
Feb 14 15:27:02.797: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 8.168388ms)
Feb 14 15:27:02.798: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 8.631177ms)
Feb 14 15:27:02.798: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 9.130876ms)
Feb 14 15:27:02.799: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 9.664484ms)
Feb 14 15:27:02.801: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 12.69081ms)
Feb 14 15:27:02.802: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 12.73658ms)
Feb 14 15:27:02.802: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 13.56373ms)
Feb 14 15:27:02.802: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 13.421609ms)
Feb 14 15:27:02.803: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 13.314209ms)
Feb 14 15:27:02.803: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 14.004818ms)
Feb 14 15:27:02.807: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 3.836425ms)
Feb 14 15:27:02.809: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 6.41367ms)
Feb 14 15:27:02.814: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 10.851533ms)
Feb 14 15:27:02.816: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 12.92267ms)
Feb 14 15:27:02.817: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 13.55646ms)
Feb 14 15:27:02.817: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 12.982711ms)
Feb 14 15:27:02.817: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 13.828139ms)
Feb 14 15:27:02.819: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 15.345326ms)
Feb 14 15:27:02.819: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 15.554586ms)
Feb 14 15:27:02.819: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 15.943766ms)
Feb 14 15:27:02.819: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 15.581407ms)
Feb 14 15:27:02.820: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 16.632274ms)
Feb 14 15:27:02.820: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 16.351964ms)
Feb 14 15:27:02.820: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 16.858253ms)
Feb 14 15:27:02.820: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 16.467834ms)
Feb 14 15:27:02.820: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 16.562974ms)
Feb 14 15:27:02.835: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 14.484987ms)
Feb 14 15:27:02.836: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 13.963808ms)
Feb 14 15:27:02.836: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 15.386127ms)
Feb 14 15:27:02.836: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 15.425766ms)
Feb 14 15:27:02.836: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 15.996955ms)
Feb 14 15:27:02.837: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 15.402246ms)
Feb 14 15:27:02.838: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 17.358332ms)
Feb 14 15:27:02.839: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 17.865312ms)
Feb 14 15:27:02.839: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 18.231371ms)
Feb 14 15:27:02.839: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 17.987712ms)
Feb 14 15:27:02.840: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 18.414332ms)
Feb 14 15:27:02.840: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 18.270321ms)
Feb 14 15:27:02.840: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 19.34534ms)
Feb 14 15:27:02.840: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 18.984551ms)
Feb 14 15:27:02.840: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 19.199601ms)
Feb 14 15:27:02.841: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 19.950507ms)
Feb 14 15:27:02.853: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 10.842493ms)
Feb 14 15:27:02.853: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 11.524012ms)
Feb 14 15:27:02.853: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 11.961662ms)
Feb 14 15:27:02.856: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 13.888119ms)
Feb 14 15:27:02.856: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 15.404705ms)
Feb 14 15:27:02.857: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 15.652316ms)
Feb 14 15:27:02.857: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 15.488706ms)
Feb 14 15:27:02.857: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 15.922526ms)
Feb 14 15:27:02.857: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 16.443804ms)
Feb 14 15:27:02.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 16.654464ms)
Feb 14 15:27:02.858: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 16.361334ms)
Feb 14 15:27:02.859: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 17.027794ms)
Feb 14 15:27:02.859: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 17.976482ms)
Feb 14 15:27:02.859: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 18.045092ms)
Feb 14 15:27:02.860: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 17.602373ms)
Feb 14 15:27:02.860: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 17.856002ms)
Feb 14 15:27:02.865: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 4.526593ms)
Feb 14 15:27:02.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 5.942021ms)
Feb 14 15:27:02.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 5.851321ms)
Feb 14 15:27:02.866: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 5.890581ms)
Feb 14 15:27:02.871: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 10.267424ms)
Feb 14 15:27:02.872: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 10.981094ms)
Feb 14 15:27:02.872: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 11.398512ms)
Feb 14 15:27:02.872: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 11.706921ms)
Feb 14 15:27:02.872: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 11.599562ms)
Feb 14 15:27:02.872: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 12.055011ms)
Feb 14 15:27:02.876: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 16.151155ms)
Feb 14 15:27:02.876: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 15.605695ms)
Feb 14 15:27:02.877: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 15.958444ms)
Feb 14 15:27:02.878: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 18.270021ms)
Feb 14 15:27:02.879: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 17.903641ms)
Feb 14 15:27:02.879: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 18.60551ms)
Feb 14 15:27:02.885: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 6.103971ms)
Feb 14 15:27:02.890: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 10.591192ms)
Feb 14 15:27:02.890: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 10.725243ms)
Feb 14 15:27:02.890: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 11.015162ms)
Feb 14 15:27:02.894: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 14.171607ms)
Feb 14 15:27:02.895: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 15.791675ms)
Feb 14 15:27:02.896: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 16.431055ms)
Feb 14 15:27:02.896: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 16.756433ms)
Feb 14 15:27:02.897: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 16.880263ms)
Feb 14 15:27:02.897: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 16.909913ms)
Feb 14 15:27:02.897: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 17.106484ms)
Feb 14 15:27:02.897: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 17.911962ms)
Feb 14 15:27:02.897: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 17.480543ms)
Feb 14 15:27:02.897: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 17.634712ms)
Feb 14 15:27:02.897: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 17.971472ms)
Feb 14 15:27:02.897: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 17.902573ms)
Feb 14 15:27:02.911: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 12.502149ms)
Feb 14 15:27:02.911: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 12.99292ms)
Feb 14 15:27:02.911: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 13.617508ms)
Feb 14 15:27:02.912: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 13.635968ms)
Feb 14 15:27:02.913: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 14.989017ms)
Feb 14 15:27:02.913: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 15.409677ms)
Feb 14 15:27:02.915: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 16.479624ms)
Feb 14 15:27:02.915: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 16.806533ms)
Feb 14 15:27:02.915: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 16.861373ms)
Feb 14 15:27:02.915: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 17.199663ms)
Feb 14 15:27:02.915: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 17.083964ms)
Feb 14 15:27:02.916: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 17.799123ms)
Feb 14 15:27:02.916: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 18.075202ms)
Feb 14 15:27:02.916: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 17.697382ms)
Feb 14 15:27:02.916: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 18.260562ms)
Feb 14 15:27:02.916: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 18.333672ms)
Feb 14 15:27:02.924: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 8.039307ms)
Feb 14 15:27:02.925: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 8.336838ms)
Feb 14 15:27:02.930: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 12.52777ms)
Feb 14 15:27:02.930: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 13.222559ms)
Feb 14 15:27:02.930: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 12.85609ms)
Feb 14 15:27:02.930: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 13.124669ms)
Feb 14 15:27:02.932: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 15.182356ms)
Feb 14 15:27:02.932: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 15.578656ms)
Feb 14 15:27:02.932: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 15.767414ms)
Feb 14 15:27:02.933: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 16.202225ms)
Feb 14 15:27:02.934: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 16.652453ms)
Feb 14 15:27:02.934: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 17.346983ms)
Feb 14 15:27:02.935: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 18.580451ms)
Feb 14 15:27:02.936: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 18.64562ms)
Feb 14 15:27:02.936: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 18.62658ms)
Feb 14 15:27:02.936: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 19.01115ms)
Feb 14 15:27:02.953: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 17.189953ms)
Feb 14 15:27:02.957: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 20.877377ms)
Feb 14 15:27:02.962: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 25.46403ms)
Feb 14 15:27:02.962: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 25.544269ms)
Feb 14 15:27:02.962: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 25.77113ms)
Feb 14 15:27:02.962: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 26.198969ms)
Feb 14 15:27:02.963: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 26.372988ms)
Feb 14 15:27:02.963: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 26.613388ms)
Feb 14 15:27:02.963: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 26.587429ms)
Feb 14 15:27:02.963: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 26.572138ms)
Feb 14 15:27:02.965: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 28.327716ms)
Feb 14 15:27:02.965: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 28.923696ms)
Feb 14 15:27:02.965: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 28.707155ms)
Feb 14 15:27:02.965: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 29.085325ms)
Feb 14 15:27:02.965: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 29.358074ms)
Feb 14 15:27:02.969: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 32.545118ms)
Feb 14 15:27:02.975: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 5.67088ms)
Feb 14 15:27:02.981: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 11.379852ms)
Feb 14 15:27:02.984: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 14.938056ms)
Feb 14 15:27:02.984: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 14.707757ms)
Feb 14 15:27:02.985: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 15.160135ms)
Feb 14 15:27:02.986: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 16.276866ms)
Feb 14 15:27:02.986: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 16.255815ms)
Feb 14 15:27:02.986: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 16.196375ms)
Feb 14 15:27:02.987: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 17.145303ms)
Feb 14 15:27:02.987: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 17.004653ms)
Feb 14 15:27:02.987: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 17.604974ms)
Feb 14 15:27:02.987: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 17.478912ms)
Feb 14 15:27:02.987: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 17.674192ms)
Feb 14 15:27:02.987: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 17.301133ms)
Feb 14 15:27:02.988: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 18.288612ms)
Feb 14 15:27:02.990: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 20.019239ms)
Feb 14 15:27:03.000: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 9.843295ms)
Feb 14 15:27:03.003: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 11.832862ms)
Feb 14 15:27:03.003: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 12.536471ms)
Feb 14 15:27:03.004: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 13.842889ms)
Feb 14 15:27:03.005: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 13.746989ms)
Feb 14 15:27:03.005: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 14.500688ms)
Feb 14 15:27:03.006: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 15.196287ms)
Feb 14 15:27:03.006: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 15.329646ms)
Feb 14 15:27:03.008: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 17.328913ms)
Feb 14 15:27:03.008: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 17.032204ms)
Feb 14 15:27:03.008: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 17.589502ms)
Feb 14 15:27:03.008: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 17.911042ms)
Feb 14 15:27:03.009: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 17.571894ms)
Feb 14 15:27:03.009: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 18.016001ms)
Feb 14 15:27:03.009: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 18.647011ms)
Feb 14 15:27:03.011: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 20.652918ms)
Feb 14 15:27:03.022: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 10.876342ms)
Feb 14 15:27:03.022: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 10.354423ms)
Feb 14 15:27:03.022: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 10.865334ms)
Feb 14 15:27:03.022: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 10.900083ms)
Feb 14 15:27:03.023: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 10.937082ms)
Feb 14 15:27:03.023: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 10.757033ms)
Feb 14 15:27:03.023: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 10.916374ms)
Feb 14 15:27:03.023: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 11.575702ms)
Feb 14 15:27:03.023: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 11.618172ms)
Feb 14 15:27:03.023: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 11.414622ms)
Feb 14 15:27:03.028: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 16.488643ms)
Feb 14 15:27:03.030: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 18.071752ms)
Feb 14 15:27:03.030: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 18.036942ms)
Feb 14 15:27:03.030: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 17.757812ms)
Feb 14 15:27:03.031: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 18.576141ms)
Feb 14 15:27:03.032: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 20.673328ms)
Feb 14 15:27:03.039: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 6.448889ms)
Feb 14 15:27:03.042: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 9.611016ms)
Feb 14 15:27:03.043: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 10.006984ms)
Feb 14 15:27:03.046: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 12.979349ms)
Feb 14 15:27:03.046: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 12.822349ms)
Feb 14 15:27:03.046: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 13.295419ms)
Feb 14 15:27:03.047: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 14.106208ms)
Feb 14 15:27:03.047: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 13.914218ms)
Feb 14 15:27:03.047: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 14.541818ms)
Feb 14 15:27:03.047: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 14.562607ms)
Feb 14 15:27:03.047: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 14.430938ms)
Feb 14 15:27:03.048: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 14.421528ms)
Feb 14 15:27:03.048: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 14.225129ms)
Feb 14 15:27:03.048: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 15.164577ms)
Feb 14 15:27:03.049: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 16.814573ms)
Feb 14 15:27:03.049: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 16.237185ms)
Feb 14 15:27:03.061: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 11.761831ms)
Feb 14 15:27:03.062: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 12.053261ms)
Feb 14 15:27:03.075: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 24.315242ms)
Feb 14 15:27:03.076: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 25.349851ms)
Feb 14 15:27:03.189: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 138.577213ms)
Feb 14 15:27:03.189: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 138.182634ms)
Feb 14 15:27:03.189: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 138.765025ms)
Feb 14 15:27:03.194: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 144.943813ms)
Feb 14 15:27:03.195: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 144.363444ms)
Feb 14 15:27:03.195: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 143.949086ms)
Feb 14 15:27:03.195: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 144.775555ms)
Feb 14 15:27:03.195: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 145.115504ms)
Feb 14 15:27:03.195: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 145.550163ms)
Feb 14 15:27:03.195: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 145.114943ms)
Feb 14 15:27:03.197: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 147.106041ms)
Feb 14 15:27:03.198: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 148.01843ms)
Feb 14 15:27:03.213: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 14.419187ms)
Feb 14 15:27:03.215: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 16.411995ms)
Feb 14 15:27:03.215: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 16.059316ms)
Feb 14 15:27:03.221: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 22.861815ms)
Feb 14 15:27:03.222: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 22.665375ms)
Feb 14 15:27:03.223: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 23.964213ms)
Feb 14 15:27:03.233: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 33.562007ms)
Feb 14 15:27:03.236: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 36.977122ms)
Feb 14 15:27:03.237: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 37.377101ms)
Feb 14 15:27:03.257: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 57.510841ms)
Feb 14 15:27:03.258: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 58.093179ms)
Feb 14 15:27:03.258: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 58.22597ms)
Feb 14 15:27:03.258: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 58.478859ms)
Feb 14 15:27:03.258: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 58.605698ms)
Feb 14 15:27:03.258: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 58.466029ms)
Feb 14 15:27:03.259: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 59.859637ms)
Feb 14 15:27:03.279: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 19.688439ms)
Feb 14 15:27:03.279: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 19.917169ms)
Feb 14 15:27:03.282: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 20.948778ms)
Feb 14 15:27:03.284: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 24.166312ms)
Feb 14 15:27:03.285: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 25.27515ms)
Feb 14 15:27:03.285: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 25.38623ms)
Feb 14 15:27:03.285: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 25.173911ms)
Feb 14 15:27:03.285: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 24.92918ms)
Feb 14 15:27:03.286: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 25.170261ms)
Feb 14 15:27:03.286: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 25.50897ms)
Feb 14 15:27:03.286: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 25.858379ms)
Feb 14 15:27:03.286: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 26.598989ms)
Feb 14 15:27:03.287: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 26.879719ms)
Feb 14 15:27:03.288: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 28.008237ms)
Feb 14 15:27:03.288: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 27.728437ms)
Feb 14 15:27:03.288: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 28.305506ms)
Feb 14 15:27:03.301: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 12.283732ms)
Feb 14 15:27:03.301: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 12.644219ms)
Feb 14 15:27:03.302: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 12.520331ms)
Feb 14 15:27:03.302: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 12.7147ms)
Feb 14 15:27:03.304: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 14.461008ms)
Feb 14 15:27:03.305: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 15.364246ms)
Feb 14 15:27:03.305: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 15.685746ms)
Feb 14 15:27:03.305: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 15.860996ms)
Feb 14 15:27:03.305: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 15.571876ms)
Feb 14 15:27:03.306: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 18.103002ms)
Feb 14 15:27:03.307: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 17.912232ms)
Feb 14 15:27:03.308: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 19.17452ms)
Feb 14 15:27:03.309: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 20.957038ms)
Feb 14 15:27:03.310: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 20.555998ms)
Feb 14 15:27:03.310: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 20.807507ms)
Feb 14 15:27:03.310: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 20.772147ms)
Feb 14 15:27:03.320: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 9.863555ms)
Feb 14 15:27:03.321: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:462/proxy/: tls qux (200; 9.910225ms)
Feb 14 15:27:03.322: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc/proxy/rewriteme"... (200; 11.160432ms)
Feb 14 15:27:03.322: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:443/proxy/... (200; 11.733711ms)
Feb 14 15:27:03.323: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 11.802801ms)
Feb 14 15:27:03.323: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/https:proxy-service-jjzgh-xprnc:460/proxy/: tls baz (200; 12.244612ms)
Feb 14 15:27:03.325: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname2/proxy/: tls qux (200; 14.668077ms)
Feb 14 15:27:03.326: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:160/proxy/: foo (200; 15.132187ms)
Feb 14 15:27:03.326: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:1080/proxy/... (200; 15.451785ms)
Feb 14 15:27:03.326: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/proxy-service-jjzgh-xprnc:1080/proxy/rewri... (200; 15.929475ms)
Feb 14 15:27:03.330: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/pods/http:proxy-service-jjzgh-xprnc:162/proxy/: bar (200; 18.93088ms)
Feb 14 15:27:03.330: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname2/proxy/: bar (200; 19.38705ms)
Feb 14 15:27:03.330: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/https:proxy-service-jjzgh:tlsportname1/proxy/: tls baz (200; 19.432989ms)
Feb 14 15:27:03.331: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname2/proxy/: bar (200; 19.978489ms)
Feb 14 15:27:03.333: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/http:proxy-service-jjzgh:portname1/proxy/: foo (200; 21.916314ms)
Feb 14 15:27:03.333: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2h9bw/services/proxy-service-jjzgh:portname1/proxy/: foo (200; 22.477595ms)
STEP: deleting ReplicationController proxy-service-jjzgh in namespace e2e-tests-proxy-2h9bw, will wait for the garbage collector to delete the pods
Feb 14 15:27:03.396: INFO: Deleting ReplicationController proxy-service-jjzgh took: 10.103394ms
Feb 14 15:27:03.496: INFO: Terminating ReplicationController proxy-service-jjzgh pods took: 100.215594ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:27:05.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-2h9bw" for this suite.
Feb 14 15:27:12.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:27:12.056: INFO: namespace: e2e-tests-proxy-2h9bw, resource: bindings, ignored listing per whitelist
Feb 14 15:27:12.120: INFO: namespace e2e-tests-proxy-2h9bw deletion completed in 6.117423651s

• [SLOW TEST:18.549 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:27:12.121: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-000d86b7-306d-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 15:27:12.210: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-000e5f95-306d-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-2gbpn" to be "success or failure"
Feb 14 15:27:12.223: INFO: Pod "pod-projected-configmaps-000e5f95-306d-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 12.976453ms
Feb 14 15:27:14.230: INFO: Pod "pod-projected-configmaps-000e5f95-306d-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02052158s
STEP: Saw pod success
Feb 14 15:27:14.230: INFO: Pod "pod-projected-configmaps-000e5f95-306d-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:27:14.234: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-projected-configmaps-000e5f95-306d-11e9-a1dd-12216a2f1059 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 15:27:14.264: INFO: Waiting for pod pod-projected-configmaps-000e5f95-306d-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:27:14.267: INFO: Pod pod-projected-configmaps-000e5f95-306d-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:27:14.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2gbpn" for this suite.
Feb 14 15:27:20.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:27:20.389: INFO: namespace: e2e-tests-projected-2gbpn, resource: bindings, ignored listing per whitelist
Feb 14 15:27:20.451: INFO: namespace e2e-tests-projected-2gbpn deletion completed in 6.175163202s

• [SLOW TEST:8.330 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:27:20.451: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 14 15:27:20.514: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 14 15:27:20.520: INFO: Waiting for terminating namespaces to be deleted...
Feb 14 15:27:20.524: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-node-1 before test
Feb 14 15:27:20.531: INFO: kube-proxy-4ls9n from kube-system started at 2019-02-14 14:17:57 +0000 UTC (1 container statuses recorded)
Feb 14 15:27:20.531: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 15:27:20.531: INFO: csi-linode-node-7zb7d from kube-system started at 2019-02-14 14:18:26 +0000 UTC (2 container statuses recorded)
Feb 14 15:27:20.531: INFO: 	Container csi-linode-plugin ready: true, restart count 0
Feb 14 15:27:20.531: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 14 15:27:20.531: INFO: calico-node-x2wh4 from kube-system started at 2019-02-14 14:17:57 +0000 UTC (2 container statuses recorded)
Feb 14 15:27:20.531: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 15:27:20.531: INFO: 	Container install-cni ready: true, restart count 0
Feb 14 15:27:20.531: INFO: sonobuoy-e2e-job-1123becdb7604801 from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 15:27:20.531: INFO: 	Container e2e ready: true, restart count 0
Feb 14 15:27:20.531: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 14 15:27:20.531: INFO: sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-wx8k7 from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 15:27:20.531: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb 14 15:27:20.531: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 14 15:27:20.531: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-node-2 before test
Feb 14 15:27:20.540: INFO: calico-node-kfmp7 from kube-system started at 2019-02-14 14:17:45 +0000 UTC (2 container statuses recorded)
Feb 14 15:27:20.540: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 15:27:20.540: INFO: 	Container install-cni ready: true, restart count 0
Feb 14 15:27:20.540: INFO: csi-linode-controller-0 from kube-system started at 2019-02-14 14:18:05 +0000 UTC (3 container statuses recorded)
Feb 14 15:27:20.540: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 14 15:27:20.540: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 14 15:27:20.540: INFO: 	Container linode-csi-plugin ready: true, restart count 0
Feb 14 15:27:20.540: INFO: external-dns-d4cfd5855-fwcl4 from kube-system started at 2019-02-14 14:18:05 +0000 UTC (1 container statuses recorded)
Feb 14 15:27:20.540: INFO: 	Container external-dns ready: true, restart count 0
Feb 14 15:27:20.540: INFO: csi-linode-node-fwkhk from kube-system started at 2019-02-14 14:18:05 +0000 UTC (2 container statuses recorded)
Feb 14 15:27:20.540: INFO: 	Container csi-linode-plugin ready: true, restart count 0
Feb 14 15:27:20.540: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 14 15:27:20.540: INFO: metrics-server-68d85f76bb-tp9rx from kube-system started at 2019-02-14 14:18:05 +0000 UTC (1 container statuses recorded)
Feb 14 15:27:20.540: INFO: 	Container metrics-server ready: true, restart count 0
Feb 14 15:27:20.541: INFO: sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-4ljxn from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 15:27:20.541: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb 14 15:27:20.541: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 14 15:27:20.541: INFO: kube-proxy-wdgrm from kube-system started at 2019-02-14 14:17:45 +0000 UTC (1 container statuses recorded)
Feb 14 15:27:20.541: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 15:27:20.541: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-node-3 before test
Feb 14 15:27:20.547: INFO: calico-node-rfldn from kube-system started at 2019-02-14 14:17:47 +0000 UTC (2 container statuses recorded)
Feb 14 15:27:20.547: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 15:27:20.547: INFO: 	Container install-cni ready: true, restart count 0
Feb 14 15:27:20.547: INFO: sonobuoy-systemd-logs-daemon-set-8901159edc444a4a-jm8kn from heptio-sonobuoy started at 2019-02-14 14:19:47 +0000 UTC (2 container statuses recorded)
Feb 14 15:27:20.547: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb 14 15:27:20.547: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 14 15:27:20.548: INFO: kube-proxy-hrh6d from kube-system started at 2019-02-14 14:17:47 +0000 UTC (1 container statuses recorded)
Feb 14 15:27:20.548: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 15:27:20.548: INFO: csi-linode-node-2kjrw from kube-system started at 2019-02-14 14:18:07 +0000 UTC (2 container statuses recorded)
Feb 14 15:27:20.548: INFO: 	Container csi-linode-plugin ready: true, restart count 0
Feb 14 15:27:20.548: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 14 15:27:20.548: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-14 14:19:44 +0000 UTC (1 container statuses recorded)
Feb 14 15:27:20.548: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.158344865ae04b72], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:27:21.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-bxc5j" for this suite.
Feb 14 15:27:27.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:27:27.645: INFO: namespace: e2e-tests-sched-pred-bxc5j, resource: bindings, ignored listing per whitelist
Feb 14 15:27:27.701: INFO: namespace e2e-tests-sched-pred-bxc5j deletion completed in 6.105111575s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.251 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:27:27.702: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 14 15:27:27.789: INFO: Waiting up to 5m0s for pod "var-expansion-095783b8-306d-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-var-expansion-jsskx" to be "success or failure"
Feb 14 15:27:27.809: INFO: Pod "var-expansion-095783b8-306d-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 19.915665ms
Feb 14 15:27:29.813: INFO: Pod "var-expansion-095783b8-306d-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024276338s
STEP: Saw pod success
Feb 14 15:27:29.813: INFO: Pod "var-expansion-095783b8-306d-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:27:29.816: INFO: Trying to get logs from node k8s-conformance-node-1 pod var-expansion-095783b8-306d-11e9-a1dd-12216a2f1059 container dapi-container: <nil>
STEP: delete the pod
Feb 14 15:27:29.847: INFO: Waiting for pod var-expansion-095783b8-306d-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:27:29.850: INFO: Pod var-expansion-095783b8-306d-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:27:29.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-jsskx" for this suite.
Feb 14 15:27:35.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:27:35.952: INFO: namespace: e2e-tests-var-expansion-jsskx, resource: bindings, ignored listing per whitelist
Feb 14 15:27:35.975: INFO: namespace e2e-tests-var-expansion-jsskx deletion completed in 6.114591766s

• [SLOW TEST:8.273 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:27:35.975: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 15:27:36.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-zjkns'
Feb 14 15:27:36.176: INFO: stderr: ""
Feb 14 15:27:36.176: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Feb 14 15:27:36.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-zjkns'
Feb 14 15:27:46.806: INFO: stderr: ""
Feb 14 15:27:46.806: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:27:46.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zjkns" for this suite.
Feb 14 15:27:52.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:27:52.904: INFO: namespace: e2e-tests-kubectl-zjkns, resource: bindings, ignored listing per whitelist
Feb 14 15:27:52.934: INFO: namespace e2e-tests-kubectl-zjkns deletion completed in 6.122408228s

• [SLOW TEST:16.959 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:27:52.936: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 14 15:27:53.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-fmnwc'
Feb 14 15:27:53.198: INFO: stderr: ""
Feb 14 15:27:53.198: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 15:27:53.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fmnwc'
Feb 14 15:27:53.310: INFO: stderr: ""
Feb 14 15:27:53.310: INFO: stdout: "update-demo-nautilus-lx598 update-demo-nautilus-xjn89 "
Feb 14 15:27:53.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-lx598 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fmnwc'
Feb 14 15:27:53.428: INFO: stderr: ""
Feb 14 15:27:53.428: INFO: stdout: ""
Feb 14 15:27:53.428: INFO: update-demo-nautilus-lx598 is created but not running
Feb 14 15:27:58.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fmnwc'
Feb 14 15:27:58.540: INFO: stderr: ""
Feb 14 15:27:58.540: INFO: stdout: "update-demo-nautilus-lx598 update-demo-nautilus-xjn89 "
Feb 14 15:27:58.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-lx598 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fmnwc'
Feb 14 15:27:58.641: INFO: stderr: ""
Feb 14 15:27:58.641: INFO: stdout: "true"
Feb 14 15:27:58.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-lx598 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fmnwc'
Feb 14 15:27:58.734: INFO: stderr: ""
Feb 14 15:27:58.734: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 15:27:58.734: INFO: validating pod update-demo-nautilus-lx598
Feb 14 15:27:58.739: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 15:27:58.739: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 15:27:58.739: INFO: update-demo-nautilus-lx598 is verified up and running
Feb 14 15:27:58.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-xjn89 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fmnwc'
Feb 14 15:27:58.837: INFO: stderr: ""
Feb 14 15:27:58.837: INFO: stdout: "true"
Feb 14 15:27:58.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods update-demo-nautilus-xjn89 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fmnwc'
Feb 14 15:27:58.928: INFO: stderr: ""
Feb 14 15:27:58.928: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 15:27:58.928: INFO: validating pod update-demo-nautilus-xjn89
Feb 14 15:27:58.934: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 15:27:58.934: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 15:27:58.934: INFO: update-demo-nautilus-xjn89 is verified up and running
STEP: using delete to clean up resources
Feb 14 15:27:58.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-fmnwc'
Feb 14 15:27:59.037: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 15:27:59.037: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 14 15:27:59.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-fmnwc'
Feb 14 15:27:59.219: INFO: stderr: "No resources found.\n"
Feb 14 15:27:59.219: INFO: stdout: ""
Feb 14 15:27:59.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -l name=update-demo --namespace=e2e-tests-kubectl-fmnwc -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 14 15:27:59.397: INFO: stderr: ""
Feb 14 15:27:59.398: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:27:59.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fmnwc" for this suite.
Feb 14 15:28:21.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:28:21.491: INFO: namespace: e2e-tests-kubectl-fmnwc, resource: bindings, ignored listing per whitelist
Feb 14 15:28:21.508: INFO: namespace e2e-tests-kubectl-fmnwc deletion completed in 22.106366923s

• [SLOW TEST:28.573 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:28:21.510: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 14 15:28:25.653: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:25.658: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 14 15:28:27.659: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:27.662: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 14 15:28:29.659: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:29.663: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 14 15:28:31.659: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:31.663: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 14 15:28:33.659: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:33.663: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 14 15:28:35.659: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:35.663: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 14 15:28:37.659: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:37.662: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 14 15:28:39.659: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:39.663: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 14 15:28:41.659: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:41.663: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 14 15:28:43.659: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:43.664: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 14 15:28:45.659: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:45.663: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 14 15:28:47.659: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 14 15:28:47.662: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:28:47.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-5pvg9" for this suite.
Feb 14 15:29:09.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:29:09.737: INFO: namespace: e2e-tests-container-lifecycle-hook-5pvg9, resource: bindings, ignored listing per whitelist
Feb 14 15:29:09.761: INFO: namespace e2e-tests-container-lifecycle-hook-5pvg9 deletion completed in 22.095273195s

• [SLOW TEST:48.251 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:29:09.761: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 14 15:29:09.834: INFO: PodSpec: initContainers in spec.initContainers
Feb 14 15:29:54.213: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-462bb4d0-306d-11e9-a1dd-12216a2f1059", GenerateName:"", Namespace:"e2e-tests-init-container-gttkd", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-gttkd/pods/pod-init-462bb4d0-306d-11e9-a1dd-12216a2f1059", UID:"462c82a2-306d-11e9-aaef-f23c91bac0b7", ResourceVersion:"18560", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63685754949, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"834880841"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.3.104/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-58k5m", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00187c7c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-58k5m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-58k5m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-58k5m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00242a998), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-conformance-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001888120), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00242af80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00242afa0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00242afa8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00242afac)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754949, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754949, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754949, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685754949, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.219.21", PodIP:"10.244.3.104", StartTime:(*v1.Time)(0xc001a6c100), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001a4a310)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001a4a3f0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://89755a84bd5c0c342cdf855535d1763d26356f128eba17d8df0c9d4de4599010"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001a6c140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001a6c120), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:29:54.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-gttkd" for this suite.
Feb 14 15:30:16.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:30:16.284: INFO: namespace: e2e-tests-init-container-gttkd, resource: bindings, ignored listing per whitelist
Feb 14 15:30:16.326: INFO: namespace e2e-tests-init-container-gttkd deletion completed in 22.096665343s

• [SLOW TEST:66.565 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:30:16.327: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:30:16.429: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 14 15:30:16.437: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:16.442: INFO: Number of nodes with available pods: 0
Feb 14 15:30:16.443: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:30:17.463: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:17.476: INFO: Number of nodes with available pods: 0
Feb 14 15:30:17.476: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:30:18.448: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:18.452: INFO: Number of nodes with available pods: 2
Feb 14 15:30:18.452: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:30:19.449: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:19.452: INFO: Number of nodes with available pods: 3
Feb 14 15:30:19.452: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 14 15:30:19.483: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:19.483: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:19.483: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:19.487: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:20.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:20.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:20.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:20.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:21.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:21.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:21.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:21.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:22.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:22.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:22.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:22.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:23.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:23.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:23.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:23.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:24.497: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:24.497: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:24.497: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:24.501: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:25.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:25.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:25.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:25.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:26.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:26.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:26.493: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:26.498: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:27.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:27.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:27.491: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:27.498: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:28.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:28.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:28.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:28.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:29.493: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:29.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:29.493: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:29.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:30.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:30.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:30.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:30.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:31.493: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:31.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:31.493: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:31.498: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:32.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:32.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:32.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:32.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:33.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:33.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:33.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:33.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:34.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:34.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:34.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:34.498: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:35.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:35.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:35.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:35.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:36.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:36.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:36.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:36.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:37.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:37.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:37.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:37.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:38.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:38.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:38.491: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:38.494: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:39.493: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:39.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:39.493: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:39.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:40.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:40.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:40.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:40.498: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:41.493: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:41.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:41.493: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:41.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:42.493: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:42.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:42.493: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:42.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:43.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:43.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:43.493: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:43.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:44.494: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:44.494: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:44.494: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:44.499: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:45.493: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:45.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:45.493: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:45.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:46.493: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:46.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:46.493: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:46.498: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:47.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:47.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:47.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:47.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:48.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:48.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:48.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:48.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:49.493: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:49.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:49.493: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:49.498: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:50.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:50.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:50.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:50.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:51.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:51.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:51.492: INFO: Wrong image for pod: daemon-set-rh2p9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:51.492: INFO: Pod daemon-set-rh2p9 is not available
Feb 14 15:30:51.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:52.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:52.491: INFO: Pod daemon-set-lcslk is not available
Feb 14 15:30:52.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:52.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:53.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:53.493: INFO: Pod daemon-set-lcslk is not available
Feb 14 15:30:53.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:53.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:54.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:54.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:54.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:55.504: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:55.504: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:55.566: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:56.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:56.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:56.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:57.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:57.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:57.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:58.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:58.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:58.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:30:59.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:59.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:30:59.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:00.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:00.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:00.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:01.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:01.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:01.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:02.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:02.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:02.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:03.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:03.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:03.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:04.494: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:04.494: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:04.499: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:05.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:05.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:05.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:06.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:06.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:06.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:07.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:07.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:07.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:08.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:08.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:08.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:09.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:09.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:09.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:10.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:10.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:10.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:11.493: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:11.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:11.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:12.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:12.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:12.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:13.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:13.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:13.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:14.494: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:14.494: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:14.499: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:15.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:15.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:15.504: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:16.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:16.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:16.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:17.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:17.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:17.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:18.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:18.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:18.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:19.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:19.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:19.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:20.493: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:20.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:20.498: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:21.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:21.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:21.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:22.495: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:22.495: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:22.501: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:23.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:23.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:23.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:24.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:24.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:24.499: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:25.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:25.492: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:25.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:25.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:26.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:26.492: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:26.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:26.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:27.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:27.491: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:27.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:27.494: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:28.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:28.492: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:28.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:28.498: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:29.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:29.491: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:29.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:29.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:30.493: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:30.493: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:30.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:30.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:31.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:31.493: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:31.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:31.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:32.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:32.492: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:32.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:32.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:33.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:33.492: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:33.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:33.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:34.491: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:34.491: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:34.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:34.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:35.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:35.492: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:35.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:35.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:36.492: INFO: Wrong image for pod: daemon-set-cd78l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:36.492: INFO: Pod daemon-set-cd78l is not available
Feb 14 15:31:36.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:36.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:37.492: INFO: Pod daemon-set-mj27m is not available
Feb 14 15:31:37.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:37.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:38.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:38.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:39.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:39.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:40.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:40.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:41.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:41.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:42.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:42.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:43.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:43.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:44.495: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:44.499: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:45.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:45.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:46.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:46.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:47.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:47.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:48.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:48.502: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:49.491: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:49.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:50.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:50.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:51.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:51.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:52.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:52.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:53.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:53.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:54.500: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:54.505: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:55.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:55.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:56.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:56.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:57.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:57.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:58.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:58.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:31:59.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:31:59.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:00.493: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:32:00.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:01.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:32:01.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:02.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:32:02.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:03.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:32:03.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:04.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:32:04.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:05.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:32:05.497: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:06.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:32:06.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:07.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:32:07.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:08.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:32:08.495: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:09.492: INFO: Wrong image for pod: daemon-set-nkxzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 15:32:09.492: INFO: Pod daemon-set-nkxzg is not available
Feb 14 15:32:09.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:10.492: INFO: Pod daemon-set-r58ng is not available
Feb 14 15:32:10.496: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 14 15:32:10.499: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:10.501: INFO: Number of nodes with available pods: 2
Feb 14 15:32:10.501: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:32:11.508: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:11.511: INFO: Number of nodes with available pods: 2
Feb 14 15:32:11.511: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:32:12.508: INFO: DaemonSet pods can't tolerate node k8s-conformance-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 14 15:32:12.511: INFO: Number of nodes with available pods: 3
Feb 14 15:32:12.511: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-w84nb, will wait for the garbage collector to delete the pods
Feb 14 15:32:12.598: INFO: Deleting DaemonSet.extensions daemon-set took: 8.190998ms
Feb 14 15:32:12.698: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.361517ms
Feb 14 15:32:25.202: INFO: Number of nodes with available pods: 0
Feb 14 15:32:25.202: INFO: Number of running nodes: 0, number of available pods: 0
Feb 14 15:32:25.205: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-w84nb/daemonsets","resourceVersion":"18998"},"items":null}

Feb 14 15:32:25.208: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-w84nb/pods","resourceVersion":"18998"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:32:25.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-w84nb" for this suite.
Feb 14 15:32:31.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:32:31.308: INFO: namespace: e2e-tests-daemonsets-w84nb, resource: bindings, ignored listing per whitelist
Feb 14 15:32:31.323: INFO: namespace e2e-tests-daemonsets-w84nb deletion completed in 6.097103766s

• [SLOW TEST:134.997 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:32:31.324: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-bxksf
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-bxksf
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-bxksf
Feb 14 15:32:31.420: INFO: Found 0 stateful pods, waiting for 1
Feb 14 15:32:41.424: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 14 15:32:41.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 15:32:41.654: INFO: stderr: ""
Feb 14 15:32:41.654: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 15:32:41.654: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 15:32:41.658: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 14 15:32:51.663: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 15:32:51.663: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 15:32:51.675: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Feb 14 15:32:51.675: INFO: ss-0  k8s-conformance-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:31 +0000 UTC  }]
Feb 14 15:32:51.676: INFO: 
Feb 14 15:32:51.676: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 14 15:32:52.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994782037s
Feb 14 15:32:53.686: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989786593s
Feb 14 15:32:54.691: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985184005s
Feb 14 15:32:55.696: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979870927s
Feb 14 15:32:56.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975109203s
Feb 14 15:32:57.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969602062s
Feb 14 15:32:58.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963357865s
Feb 14 15:32:59.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954058964s
Feb 14 15:33:00.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.229857ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-bxksf
Feb 14 15:33:01.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:33:01.941: INFO: stderr: ""
Feb 14 15:33:01.941: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 15:33:01.941: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 15:33:01.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:33:02.164: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 14 15:33:02.164: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 15:33:02.164: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 15:33:02.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:33:02.375: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 14 15:33:02.375: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 15:33:02.375: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 15:33:02.380: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 14 15:33:12.385: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 15:33:12.385: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 15:33:12.385: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 14 15:33:12.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 15:33:12.577: INFO: stderr: ""
Feb 14 15:33:12.577: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 15:33:12.577: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 15:33:12.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 15:33:12.783: INFO: stderr: ""
Feb 14 15:33:12.783: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 15:33:12.783: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 15:33:12.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 15:33:13.142: INFO: stderr: ""
Feb 14 15:33:13.142: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 15:33:13.142: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 15:33:13.142: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 15:33:13.146: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 14 15:33:23.157: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 15:33:23.157: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 15:33:23.157: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 15:33:23.172: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Feb 14 15:33:23.172: INFO: ss-0  k8s-conformance-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:31 +0000 UTC  }]
Feb 14 15:33:23.172: INFO: ss-1  k8s-conformance-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:23.172: INFO: ss-2  k8s-conformance-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:23.172: INFO: 
Feb 14 15:33:23.172: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 14 15:33:24.178: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Feb 14 15:33:24.178: INFO: ss-0  k8s-conformance-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:31 +0000 UTC  }]
Feb 14 15:33:24.178: INFO: ss-1  k8s-conformance-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:24.178: INFO: ss-2  k8s-conformance-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:24.178: INFO: 
Feb 14 15:33:24.178: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 14 15:33:25.184: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Feb 14 15:33:25.184: INFO: ss-0  k8s-conformance-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:31 +0000 UTC  }]
Feb 14 15:33:25.184: INFO: ss-1  k8s-conformance-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:25.184: INFO: ss-2  k8s-conformance-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:25.184: INFO: 
Feb 14 15:33:25.184: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 14 15:33:26.188: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Feb 14 15:33:26.188: INFO: ss-1  k8s-conformance-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:26.188: INFO: ss-2  k8s-conformance-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:26.189: INFO: 
Feb 14 15:33:26.189: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 14 15:33:27.194: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Feb 14 15:33:27.194: INFO: ss-1  k8s-conformance-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:27.194: INFO: ss-2  k8s-conformance-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:27.194: INFO: 
Feb 14 15:33:27.194: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 14 15:33:28.199: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Feb 14 15:33:28.199: INFO: ss-2  k8s-conformance-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:28.199: INFO: 
Feb 14 15:33:28.199: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 14 15:33:29.203: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Feb 14 15:33:29.203: INFO: ss-2  k8s-conformance-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:29.203: INFO: 
Feb 14 15:33:29.203: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 14 15:33:30.208: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Feb 14 15:33:30.208: INFO: ss-2  k8s-conformance-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:30.208: INFO: 
Feb 14 15:33:30.208: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 14 15:33:31.213: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Feb 14 15:33:31.213: INFO: ss-2  k8s-conformance-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:31.213: INFO: 
Feb 14 15:33:31.213: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 14 15:33:32.218: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Feb 14 15:33:32.218: INFO: ss-2  k8s-conformance-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:33:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:32:51 +0000 UTC  }]
Feb 14 15:33:32.218: INFO: 
Feb 14 15:33:32.218: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-bxksf
Feb 14 15:33:33.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:33:33.349: INFO: rc: 1
Feb 14 15:33:33.350: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002f26f30 exit status 1 <nil> <nil> true [0xc0000c7b10 0xc0000c7c78 0xc0000c7d38] [0xc0000c7b10 0xc0000c7c78 0xc0000c7d38] [0xc0000c7bb8 0xc0000c7d20] [0x92f8e0 0x92f8e0] 0xc001888f00 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 14 15:33:43.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:33:43.440: INFO: rc: 1
Feb 14 15:33:43.440: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f272f0 exit status 1 <nil> <nil> true [0xc0000c7d48 0xc0000c7dd8 0xc0000c7eb0] [0xc0000c7d48 0xc0000c7dd8 0xc0000c7eb0] [0xc0000c7dd0 0xc0000c7e38] [0x92f8e0 0x92f8e0] 0xc001889200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:33:53.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:33:53.550: INFO: rc: 1
Feb 14 15:33:53.550: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f276e0 exit status 1 <nil> <nil> true [0xc0000c7ed8 0xc0000c7fe0 0xc000144210] [0xc0000c7ed8 0xc0000c7fe0 0xc000144210] [0xc0000c7f58 0xc000144098] [0x92f8e0 0x92f8e0] 0xc001889500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:34:03.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:34:03.639: INFO: rc: 1
Feb 14 15:34:03.639: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001075e00 exit status 1 <nil> <nil> true [0xc00000fed8 0xc00000ff00 0xc00000ffa0] [0xc00000fed8 0xc00000ff00 0xc00000ffa0] [0xc00000fee8 0xc00000ff80] [0x92f8e0 0x92f8e0] 0xc0021b2900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:34:13.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:34:13.743: INFO: rc: 1
Feb 14 15:34:13.743: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f27ad0 exit status 1 <nil> <nil> true [0xc000144338 0xc0001445e8 0xc000144938] [0xc000144338 0xc0001445e8 0xc000144938] [0xc000144540 0xc0001448c0] [0x92f8e0 0x92f8e0] 0xc001889800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:34:23.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:34:23.833: INFO: rc: 1
Feb 14 15:34:23.833: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f27ec0 exit status 1 <nil> <nil> true [0xc000144968 0xc0001449a0 0xc000144f68] [0xc000144968 0xc0001449a0 0xc000144f68] [0xc000144978 0xc0001449f0] [0x92f8e0 0x92f8e0] 0xc001889b00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:34:33.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:34:33.918: INFO: rc: 1
Feb 14 15:34:33.918: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00226e330 exit status 1 <nil> <nil> true [0xc00000ffc8 0xc002db8008 0xc002db8020] [0xc00000ffc8 0xc002db8008 0xc002db8020] [0xc002db8000 0xc002db8018] [0x92f8e0 0x92f8e0] 0xc0021b2c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:34:43.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:34:44.010: INFO: rc: 1
Feb 14 15:34:44.010: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020c6360 exit status 1 <nil> <nil> true [0xc000144f70 0xc000145090 0xc000d52038] [0xc000144f70 0xc000145090 0xc000d52038] [0xc000145048 0xc000d52028] [0x92f8e0 0x92f8e0] 0xc001889e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:34:54.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:34:54.102: INFO: rc: 1
Feb 14 15:34:54.102: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020c6750 exit status 1 <nil> <nil> true [0xc000d52050 0xc000d52170 0xc000d52240] [0xc000d52050 0xc000d52170 0xc000d52240] [0xc000d520e8 0xc000d521f8] [0x92f8e0 0x92f8e0] 0xc00209e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:35:04.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:35:04.187: INFO: rc: 1
Feb 14 15:35:04.187: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00226e720 exit status 1 <nil> <nil> true [0xc002db8028 0xc002db8040 0xc002db8058] [0xc002db8028 0xc002db8040 0xc002db8058] [0xc002db8038 0xc002db8050] [0x92f8e0 0x92f8e0] 0xc0021b2f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:35:14.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:35:14.279: INFO: rc: 1
Feb 14 15:35:14.279: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00226eb10 exit status 1 <nil> <nil> true [0xc002db8060 0xc002db8078 0xc002db8090] [0xc002db8060 0xc002db8078 0xc002db8090] [0xc002db8070 0xc002db8088] [0x92f8e0 0x92f8e0] 0xc0021b3200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:35:24.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:35:24.391: INFO: rc: 1
Feb 14 15:35:24.391: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00226ef60 exit status 1 <nil> <nil> true [0xc002db80a0 0xc002db8108 0xc002db8120] [0xc002db80a0 0xc002db8108 0xc002db8120] [0xc002db8100 0xc002db8118] [0x92f8e0 0x92f8e0] 0xc0021b34a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:35:34.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:35:34.488: INFO: rc: 1
Feb 14 15:35:34.488: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f267b0 exit status 1 <nil> <nil> true [0xc000144098 0xc0001443d0 0xc000144838] [0xc000144098 0xc0001443d0 0xc000144838] [0xc000144338 0xc0001445e8] [0x92f8e0 0x92f8e0] 0xc0018885a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:35:44.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:35:44.612: INFO: rc: 1
Feb 14 15:35:44.612: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001074420 exit status 1 <nil> <nil> true [0xc00000e140 0xc00000e1c8 0xc00000fd18] [0xc00000e140 0xc00000e1c8 0xc00000fd18] [0xc00000e1b8 0xc00000e318] [0x92f8e0 0x92f8e0] 0xc0021b2240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:35:54.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:35:54.693: INFO: rc: 1
Feb 14 15:35:54.693: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0010748d0 exit status 1 <nil> <nil> true [0xc00000fd28 0xc00000fd60 0xc00000fe18] [0xc00000fd28 0xc00000fd60 0xc00000fe18] [0xc00000fd50 0xc00000fdc8] [0x92f8e0 0x92f8e0] 0xc0021b2540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:36:04.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:36:04.789: INFO: rc: 1
Feb 14 15:36:04.789: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f26d80 exit status 1 <nil> <nil> true [0xc0001448c0 0xc000144970 0xc0001449e8] [0xc0001448c0 0xc000144970 0xc0001449e8] [0xc000144968 0xc0001449a0] [0x92f8e0 0x92f8e0] 0xc0018889c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:36:14.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:36:14.877: INFO: rc: 1
Feb 14 15:36:14.877: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001074f30 exit status 1 <nil> <nil> true [0xc00000fe30 0xc00000fe58 0xc00000fed8] [0xc00000fe30 0xc00000fe58 0xc00000fed8] [0xc00000fe50 0xc00000feb8] [0x92f8e0 0x92f8e0] 0xc0021b2840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:36:24.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:36:24.970: INFO: rc: 1
Feb 14 15:36:24.971: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0010753b0 exit status 1 <nil> <nil> true [0xc00000fee0 0xc00000ff50 0xc00000ffc8] [0xc00000fee0 0xc00000ff50 0xc00000ffc8] [0xc00000ff00 0xc00000ffa0] [0x92f8e0 0x92f8e0] 0xc0021b2b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:36:34.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:36:35.062: INFO: rc: 1
Feb 14 15:36:35.062: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001075800 exit status 1 <nil> <nil> true [0xc00000fff0 0xc0000c6380 0xc0000c6560] [0xc00000fff0 0xc0000c6380 0xc0000c6560] [0xc0000c6248 0xc0000c6440] [0x92f8e0 0x92f8e0] 0xc0021b2e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:36:45.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:36:45.154: INFO: rc: 1
Feb 14 15:36:45.154: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001075c20 exit status 1 <nil> <nil> true [0xc0000c6618 0xc0000c7030 0xc0000c71e0] [0xc0000c6618 0xc0000c7030 0xc0000c71e0] [0xc0000c6ed0 0xc0000c7190] [0x92f8e0 0x92f8e0] 0xc0021b3140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:36:55.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:36:55.237: INFO: rc: 1
Feb 14 15:36:55.237: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f271d0 exit status 1 <nil> <nil> true [0xc0001449f0 0xc000145008 0xc002db8000] [0xc0001449f0 0xc000145008 0xc002db8000] [0xc000144f70 0xc000145090] [0x92f8e0 0x92f8e0] 0xc001888d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:37:05.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:37:05.328: INFO: rc: 1
Feb 14 15:37:05.328: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f275c0 exit status 1 <nil> <nil> true [0xc002db8008 0xc002db8020 0xc002db8038] [0xc002db8008 0xc002db8020 0xc002db8038] [0xc002db8018 0xc002db8030] [0x92f8e0 0x92f8e0] 0xc001889020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:37:15.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:37:15.413: INFO: rc: 1
Feb 14 15:37:15.413: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f3c1e0 exit status 1 <nil> <nil> true [0xc0000c7210 0xc0000c7380 0xc0000c7520] [0xc0000c7210 0xc0000c7380 0xc0000c7520] [0xc0000c7298 0xc0000c73f8] [0x92f8e0 0x92f8e0] 0xc0021b36e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:37:25.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:37:25.530: INFO: rc: 1
Feb 14 15:37:25.530: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f3c5a0 exit status 1 <nil> <nil> true [0xc0000c7658 0xc0000c7958 0xc0000c7a18] [0xc0000c7658 0xc0000c7958 0xc0000c7a18] [0xc0000c7858 0xc0000c79d0] [0x92f8e0 0x92f8e0] 0xc0021b3a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:37:35.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:37:35.615: INFO: rc: 1
Feb 14 15:37:35.616: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001074450 exit status 1 <nil> <nil> true [0xc00000e140 0xc00000e1c8 0xc00000fd18] [0xc00000e140 0xc00000e1c8 0xc00000fd18] [0xc00000e1b8 0xc00000e318] [0x92f8e0 0x92f8e0] 0xc0018885a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:37:45.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:37:45.697: INFO: rc: 1
Feb 14 15:37:45.697: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f26810 exit status 1 <nil> <nil> true [0xc000144098 0xc0001443d0 0xc000144838] [0xc000144098 0xc0001443d0 0xc000144838] [0xc000144338 0xc0001445e8] [0x92f8e0 0x92f8e0] 0xc0021b2240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:37:55.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:37:55.795: INFO: rc: 1
Feb 14 15:37:55.796: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001074990 exit status 1 <nil> <nil> true [0xc00000fd28 0xc00000fd60 0xc00000fe18] [0xc00000fd28 0xc00000fd60 0xc00000fe18] [0xc00000fd50 0xc00000fdc8] [0x92f8e0 0x92f8e0] 0xc0018889c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:38:05.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:38:05.887: INFO: rc: 1
Feb 14 15:38:05.888: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f26db0 exit status 1 <nil> <nil> true [0xc0001448c0 0xc000144970 0xc0001449e8] [0xc0001448c0 0xc000144970 0xc0001449e8] [0xc000144968 0xc0001449a0] [0x92f8e0 0x92f8e0] 0xc0021b2540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:38:15.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:38:15.986: INFO: rc: 1
Feb 14 15:38:15.987: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f27170 exit status 1 <nil> <nil> true [0xc0001449f0 0xc000145008 0xc002db8000] [0xc0001449f0 0xc000145008 0xc002db8000] [0xc000144f70 0xc000145090] [0x92f8e0 0x92f8e0] 0xc0021b2840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:38:25.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:38:26.079: INFO: rc: 1
Feb 14 15:38:26.079: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001075050 exit status 1 <nil> <nil> true [0xc00000fe30 0xc00000fe58 0xc00000fed8] [0xc00000fe30 0xc00000fe58 0xc00000fed8] [0xc00000fe50 0xc00000feb8] [0x92f8e0 0x92f8e0] 0xc001888d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 15:38:36.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-bxksf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:38:36.167: INFO: rc: 1
Feb 14 15:38:36.167: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb 14 15:38:36.167: INFO: Scaling statefulset ss to 0
Feb 14 15:38:36.174: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 14 15:38:36.176: INFO: Deleting all statefulset in ns e2e-tests-statefulset-bxksf
Feb 14 15:38:36.178: INFO: Scaling statefulset ss to 0
Feb 14 15:38:36.186: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 15:38:36.189: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:38:36.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-bxksf" for this suite.
Feb 14 15:38:42.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:38:42.254: INFO: namespace: e2e-tests-statefulset-bxksf, resource: bindings, ignored listing per whitelist
Feb 14 15:38:42.307: INFO: namespace e2e-tests-statefulset-bxksf deletion completed in 6.102857054s

• [SLOW TEST:370.984 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:38:42.308: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 15:38:42.387: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b6f0f15-306e-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-xtw28" to be "success or failure"
Feb 14 15:38:42.393: INFO: Pod "downwardapi-volume-9b6f0f15-306e-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 6.749733ms
Feb 14 15:38:44.398: INFO: Pod "downwardapi-volume-9b6f0f15-306e-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011494476s
STEP: Saw pod success
Feb 14 15:38:44.398: INFO: Pod "downwardapi-volume-9b6f0f15-306e-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:38:44.402: INFO: Trying to get logs from node k8s-conformance-node-1 pod downwardapi-volume-9b6f0f15-306e-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 15:38:44.439: INFO: Waiting for pod downwardapi-volume-9b6f0f15-306e-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:38:44.446: INFO: Pod downwardapi-volume-9b6f0f15-306e-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:38:44.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xtw28" for this suite.
Feb 14 15:38:50.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:38:50.513: INFO: namespace: e2e-tests-downward-api-xtw28, resource: bindings, ignored listing per whitelist
Feb 14 15:38:50.574: INFO: namespace e2e-tests-downward-api-xtw28 deletion completed in 6.122421518s

• [SLOW TEST:8.267 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:38:50.576: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:38:50.661: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:38:52.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-n6zqs" for this suite.
Feb 14 15:39:38.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:39:38.786: INFO: namespace: e2e-tests-pods-n6zqs, resource: bindings, ignored listing per whitelist
Feb 14 15:39:38.807: INFO: namespace e2e-tests-pods-n6zqs deletion completed in 46.101171223s

• [SLOW TEST:48.231 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:39:38.809: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Feb 14 15:39:38.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 create -f - --namespace=e2e-tests-kubectl-hjkk5'
Feb 14 15:39:39.273: INFO: stderr: ""
Feb 14 15:39:39.273: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 14 15:39:40.284: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:39:40.284: INFO: Found 0 / 1
Feb 14 15:39:41.278: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:39:41.278: INFO: Found 1 / 1
Feb 14 15:39:41.278: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 14 15:39:41.281: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 15:39:41.281: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 14 15:39:41.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 logs redis-master-cfq8f redis-master --namespace=e2e-tests-kubectl-hjkk5'
Feb 14 15:39:41.387: INFO: stderr: ""
Feb 14 15:39:41.387: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Feb 15:39:40.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Feb 15:39:40.282 # Server started, Redis version 3.2.12\n1:M 14 Feb 15:39:40.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Feb 15:39:40.282 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 14 15:39:41.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 log redis-master-cfq8f redis-master --namespace=e2e-tests-kubectl-hjkk5 --tail=1'
Feb 14 15:39:41.518: INFO: stderr: ""
Feb 14 15:39:41.518: INFO: stdout: "1:M 14 Feb 15:39:40.282 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 14 15:39:41.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 log redis-master-cfq8f redis-master --namespace=e2e-tests-kubectl-hjkk5 --limit-bytes=1'
Feb 14 15:39:41.645: INFO: stderr: ""
Feb 14 15:39:41.645: INFO: stdout: " "
STEP: exposing timestamps
Feb 14 15:39:41.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 log redis-master-cfq8f redis-master --namespace=e2e-tests-kubectl-hjkk5 --tail=1 --timestamps'
Feb 14 15:39:41.763: INFO: stderr: ""
Feb 14 15:39:41.763: INFO: stdout: "2019-02-14T15:39:40.282922098Z 1:M 14 Feb 15:39:40.282 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 14 15:39:44.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 log redis-master-cfq8f redis-master --namespace=e2e-tests-kubectl-hjkk5 --since=1s'
Feb 14 15:39:44.371: INFO: stderr: ""
Feb 14 15:39:44.371: INFO: stdout: ""
Feb 14 15:39:44.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 log redis-master-cfq8f redis-master --namespace=e2e-tests-kubectl-hjkk5 --since=24h'
Feb 14 15:39:44.498: INFO: stderr: ""
Feb 14 15:39:44.498: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Feb 15:39:40.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Feb 15:39:40.282 # Server started, Redis version 3.2.12\n1:M 14 Feb 15:39:40.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Feb 15:39:40.282 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Feb 14 15:39:44.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-hjkk5'
Feb 14 15:39:44.678: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 15:39:44.678: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 14 15:39:44.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-hjkk5'
Feb 14 15:39:44.849: INFO: stderr: "No resources found.\n"
Feb 14 15:39:44.849: INFO: stdout: ""
Feb 14 15:39:44.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 get pods -l name=nginx --namespace=e2e-tests-kubectl-hjkk5 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 14 15:39:45.038: INFO: stderr: ""
Feb 14 15:39:45.038: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:39:45.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hjkk5" for this suite.
Feb 14 15:40:07.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:40:07.099: INFO: namespace: e2e-tests-kubectl-hjkk5, resource: bindings, ignored listing per whitelist
Feb 14 15:40:07.153: INFO: namespace e2e-tests-kubectl-hjkk5 deletion completed in 22.110034183s

• [SLOW TEST:28.344 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:40:07.153: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:40:07.225: INFO: Creating deployment "nginx-deployment"
Feb 14 15:40:07.233: INFO: Waiting for observed generation 1
Feb 14 15:40:09.240: INFO: Waiting for all required pods to come up
Feb 14 15:40:09.247: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 14 15:40:11.263: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 14 15:40:11.268: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 14 15:40:11.278: INFO: Updating deployment nginx-deployment
Feb 14 15:40:11.278: INFO: Waiting for observed generation 2
Feb 14 15:40:13.286: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 14 15:40:13.289: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 14 15:40:13.291: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 14 15:40:13.300: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 14 15:40:13.300: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 14 15:40:13.302: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 14 15:40:13.308: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 14 15:40:13.308: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 14 15:40:13.323: INFO: Updating deployment nginx-deployment
Feb 14 15:40:13.323: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 14 15:40:13.335: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 14 15:40:15.361: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 14 15:40:15.389: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tqfhw/deployments/nginx-deployment,UID:ce01fd72-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20536,Generation:3,CreationTimestamp:2019-02-14 15:40:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-02-14 15:40:13 +0000 UTC 2019-02-14 15:40:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-14 15:40:13 +0000 UTC 2019-02-14 15:40:07 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 14 15:40:15.399: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tqfhw/replicasets/nginx-deployment-65bbdb5f8,UID:d06bfa1f-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20525,Generation:3,CreationTimestamp:2019-02-14 15:40:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment ce01fd72-306e-11e9-aaef-f23c91bac0b7 0xc002090777 0xc002090778}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 14 15:40:15.399: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 14 15:40:15.399: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tqfhw/replicasets/nginx-deployment-555b55d965,UID:ce0378a0-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20535,Generation:3,CreationTimestamp:2019-02-14 15:40:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment ce01fd72-306e-11e9-aaef-f23c91bac0b7 0xc0020906b7 0xc0020906b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 14 15:40:15.408: INFO: Pod "nginx-deployment-555b55d965-2445m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2445m,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-2445m,UID:d1a5308b-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20598,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.2.123/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020651e7 0xc0020651e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002065250} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002065270}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.409: INFO: Pod "nginx-deployment-555b55d965-4mbsj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4mbsj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-4mbsj,UID:ce0ad626-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20360,Generation:0,CreationTimestamp:2019-02-14 15:40:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.2.116/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc002065337 0xc002065338}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020653a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020653c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:10.244.2.116,StartTime:2019-02-14 15:40:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 15:40:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://db341747cef2b3c827a3550f1584b256bab550753bd738f0fe86a5dc2a8b86bc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.409: INFO: Pod "nginx-deployment-555b55d965-4plwl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4plwl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-4plwl,UID:d1b20151-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20563,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc002065487 0xc002065488}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002065560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002065580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.413: INFO: Pod "nginx-deployment-555b55d965-6lzxl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6lzxl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-6lzxl,UID:ce0b0484-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20345,Generation:0,CreationTimestamp:2019-02-14 15:40:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.110/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc002065647 0xc002065648}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020656b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002065720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:10.244.3.110,StartTime:2019-02-14 15:40:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 15:40:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://aa00b43e8ab6fad6e26d343e1e08dbeafa34dcf52136beafd25654c718b3c4f9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.413: INFO: Pod "nginx-deployment-555b55d965-98fbq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-98fbq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-98fbq,UID:d1aa26cf-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20579,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.1.28/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020657f7 0xc0020657f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002065860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002065880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.185.104,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.422: INFO: Pod "nginx-deployment-555b55d965-9d98t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9d98t,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-9d98t,UID:d1a64650-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20578,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.2.122/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc002065947 0xc002065948}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002065a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002065a40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.427: INFO: Pod "nginx-deployment-555b55d965-9vjp6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9vjp6,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-9vjp6,UID:ce0f8a8f-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20357,Generation:0,CreationTimestamp:2019-02-14 15:40:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.2.119/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc002065b27 0xc002065b28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002065b90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002065bb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:10.244.2.119,StartTime:2019-02-14 15:40:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 15:40:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://860d60f6fe867e94d240feddf95cb65fd7f85349775841f15ce1d671e2bcbdd0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.427: INFO: Pod "nginx-deployment-555b55d965-btfzv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-btfzv,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-btfzv,UID:d1aa32d1-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20604,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.2.124/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc002065c87 0xc002065c88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002065cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002065d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.428: INFO: Pod "nginx-deployment-555b55d965-gvc9g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gvc9g,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-gvc9g,UID:d1aa1542-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20580,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.116/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc002065dd7 0xc002065dd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002065e40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002065ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.428: INFO: Pod "nginx-deployment-555b55d965-jw9hl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jw9hl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-jw9hl,UID:d1b39958-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20596,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.118/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc002065f87 0xc002065f88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002065ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020ca010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.428: INFO: Pod "nginx-deployment-555b55d965-mmld8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mmld8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-mmld8,UID:ce0fa563-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20338,Generation:0,CreationTimestamp:2019-02-14 15:40:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.1.25/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020ca0d7 0xc0020ca0d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020ca140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020ca160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.185.104,PodIP:10.244.1.25,StartTime:2019-02-14 15:40:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 15:40:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://8344185ef483403c01d0345c9e6ccc505bde3addf354219317385d46701ca971}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.428: INFO: Pod "nginx-deployment-555b55d965-nhnwd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-nhnwd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-nhnwd,UID:ce07a290-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20335,Generation:0,CreationTimestamp:2019-02-14 15:40:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.1.23/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020ca230 0xc0020ca231}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020ca290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020ca2b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.185.104,PodIP:10.244.1.23,StartTime:2019-02-14 15:40:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 15:40:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://5cdcd947baf956bd9676cfb8b2159ce591a4078ac6b75429c5f1bc188e99958a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.428: INFO: Pod "nginx-deployment-555b55d965-nsg2c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-nsg2c,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-nsg2c,UID:d1b1efb5-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20600,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.1.31/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020ca380 0xc0020ca381}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020ca3e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020ca400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.185.104,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.429: INFO: Pod "nginx-deployment-555b55d965-q788k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-q788k,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-q788k,UID:ce077c38-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20342,Generation:0,CreationTimestamp:2019-02-14 15:40:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.111/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020ca507 0xc0020ca508}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020ca590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020ca5b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:10.244.3.111,StartTime:2019-02-14 15:40:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 15:40:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://3ea003467f7b374dd47f07693846904a5aa26ff2b024feeff44697ba98d60969}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.429: INFO: Pod "nginx-deployment-555b55d965-qj2vx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qj2vx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-qj2vx,UID:d1aa29f9-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20527,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020ca687 0xc0020ca688}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020ca820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020ca8d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.429: INFO: Pod "nginx-deployment-555b55d965-sc8tq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sc8tq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-sc8tq,UID:d1a6acd0-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20573,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.115/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020ca9e7 0xc0020ca9e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020caa50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020caa70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.429: INFO: Pod "nginx-deployment-555b55d965-vc28h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-vc28h,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-vc28h,UID:ce0b1679-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20332,Generation:0,CreationTimestamp:2019-02-14 15:40:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.1.24/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020cab37 0xc0020cab38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020caba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cabc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.185.104,PodIP:10.244.1.24,StartTime:2019-02-14 15:40:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 15:40:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://535c707c0da34e333bcdacc63daacde7fee2c730b5504b0d287dee040fed3114}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.429: INFO: Pod "nginx-deployment-555b55d965-w7xks" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-w7xks,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-w7xks,UID:d1b316c2-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20624,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.120/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020cac90 0xc0020cac91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cacf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cad10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.430: INFO: Pod "nginx-deployment-555b55d965-z6zwc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-z6zwc,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-z6zwc,UID:ce0b1294-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20347,Generation:0,CreationTimestamp:2019-02-14 15:40:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.112/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020cadd7 0xc0020cadd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cae40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cae60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:10.244.3.112,StartTime:2019-02-14 15:40:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 15:40:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://c9ff029a7ebb1b54ba4d82fc0aa033de00ba15d440ee02f02c0bd85829735286}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.430: INFO: Pod "nginx-deployment-555b55d965-zv5pp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zv5pp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-555b55d965-zv5pp,UID:d1b3889d-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20594,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.1.30/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ce0378a0-306e-11e9-aaef-f23c91bac0b7 0xc0020caf77 0xc0020caf78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cb010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cb030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.185.104,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.430: INFO: Pod "nginx-deployment-65bbdb5f8-767zh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-767zh,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-767zh,UID:d1ac520d-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20588,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.117/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc0020cb0f7 0xc0020cb0f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cb160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cb180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.430: INFO: Pod "nginx-deployment-65bbdb5f8-9pdcz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-9pdcz,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-9pdcz,UID:d1a90c45-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20606,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.1.27/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc0020cb420 0xc0020cb421}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cb4d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cb4f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.185.104,PodIP:10.244.1.27,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.431: INFO: Pod "nginx-deployment-65bbdb5f8-bjh49" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-bjh49,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-bjh49,UID:d1b3055a-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20613,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.1.32/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc0020cb7c0 0xc0020cb7c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cb870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cb890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.185.104,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.431: INFO: Pod "nginx-deployment-65bbdb5f8-cdsmh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-cdsmh,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-cdsmh,UID:d06e48fa-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20565,Generation:0,CreationTimestamp:2019-02-14 15:40:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.1.26/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc0020cba90 0xc0020cba91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cbb20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cbb70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC  }],Message:,Reason:,HostIP:192.168.185.104,PodIP:10.244.1.26,StartTime:2019-02-14 15:40:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.431: INFO: Pod "nginx-deployment-65bbdb5f8-d2blj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-d2blj,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-d2blj,UID:d1b32fed-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20589,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.1.29/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc0020cbd80 0xc0020cbd81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cbdf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cbe10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.185.104,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.431: INFO: Pod "nginx-deployment-65bbdb5f8-fnh2c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-fnh2c,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-fnh2c,UID:d1b32885-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20605,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.119/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc0020cbf70 0xc0020cbf71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cbfe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025940f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.431: INFO: Pod "nginx-deployment-65bbdb5f8-gnqd7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-gnqd7,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-gnqd7,UID:d1b9ad56-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20577,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc002594580 0xc002594581}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025945f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002594610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.431: INFO: Pod "nginx-deployment-65bbdb5f8-p2kf5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-p2kf5,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-p2kf5,UID:d07ff5ec-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20595,Generation:0,CreationTimestamp:2019-02-14 15:40:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.2.121/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc0025946e0 0xc0025946e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002594750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002594770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:10.244.2.121,StartTime:2019-02-14 15:40:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.431: INFO: Pod "nginx-deployment-65bbdb5f8-qs92x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-qs92x,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-qs92x,UID:d06c8fb6-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20622,Generation:0,CreationTimestamp:2019-02-14 15:40:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.2.120/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc002594860 0xc002594861}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002594980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025949f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:10.244.2.120,StartTime:2019-02-14 15:40:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.432: INFO: Pod "nginx-deployment-65bbdb5f8-vxls7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-vxls7,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-vxls7,UID:d06e5f23-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20450,Generation:0,CreationTimestamp:2019-02-14 15:40:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.113/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc002594ae0 0xc002594ae1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002594b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002594b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:10.244.3.113,StartTime:2019-02-14 15:40:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.432: INFO: Pod "nginx-deployment-65bbdb5f8-wb8hb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-wb8hb,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-wb8hb,UID:d081cd56-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20446,Generation:0,CreationTimestamp:2019-02-14 15:40:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.114/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc002594c60 0xc002594c61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002594cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002594cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:11 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:10.244.3.114,StartTime:2019-02-14 15:40:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.432: INFO: Pod "nginx-deployment-65bbdb5f8-whrnl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-whrnl,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-whrnl,UID:d1b32049-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20552,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc002594dd0 0xc002594dd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002594ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002594ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:40:15.432: INFO: Pod "nginx-deployment-65bbdb5f8-x6mct" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-x6mct,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tqfhw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tqfhw/pods/nginx-deployment-65bbdb5f8-x6mct,UID:d1ac7650-306e-11e9-aaef-f23c91bac0b7,ResourceVersion:20538,Generation:0,CreationTimestamp:2019-02-14 15:40:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 d06bfa1f-306e-11e9-aaef-f23c91bac0b7 0xc002594fa0 0xc002594fa1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v84wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v84wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v84wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002595010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002595030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:40:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.184.46,PodIP:,StartTime:2019-02-14 15:40:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:40:15.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-tqfhw" for this suite.
Feb 14 15:40:23.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:40:23.530: INFO: namespace: e2e-tests-deployment-tqfhw, resource: bindings, ignored listing per whitelist
Feb 14 15:40:23.550: INFO: namespace e2e-tests-deployment-tqfhw deletion completed in 8.111642709s

• [SLOW TEST:16.397 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:40:23.551: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-d7d054a3-306e-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 15:40:23.800: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d7d57383-306e-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-xh9sc" to be "success or failure"
Feb 14 15:40:23.813: INFO: Pod "pod-projected-configmaps-d7d57383-306e-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 13.662489ms
Feb 14 15:40:25.818: INFO: Pod "pod-projected-configmaps-d7d57383-306e-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018028957s
STEP: Saw pod success
Feb 14 15:40:25.818: INFO: Pod "pod-projected-configmaps-d7d57383-306e-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:40:25.821: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-projected-configmaps-d7d57383-306e-11e9-a1dd-12216a2f1059 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 15:40:25.846: INFO: Waiting for pod pod-projected-configmaps-d7d57383-306e-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:40:25.855: INFO: Pod pod-projected-configmaps-d7d57383-306e-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:40:25.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xh9sc" for this suite.
Feb 14 15:40:31.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:40:31.932: INFO: namespace: e2e-tests-projected-xh9sc, resource: bindings, ignored listing per whitelist
Feb 14 15:40:31.975: INFO: namespace e2e-tests-projected-xh9sc deletion completed in 6.115287661s

• [SLOW TEST:8.425 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:40:31.976: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-dccdfbeb-306e-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 15:40:32.068: INFO: Waiting up to 5m0s for pod "pod-configmaps-dccec069-306e-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-configmap-sv2fw" to be "success or failure"
Feb 14 15:40:32.076: INFO: Pod "pod-configmaps-dccec069-306e-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 8.255157ms
Feb 14 15:40:34.081: INFO: Pod "pod-configmaps-dccec069-306e-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013743299s
STEP: Saw pod success
Feb 14 15:40:34.081: INFO: Pod "pod-configmaps-dccec069-306e-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:40:34.085: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-configmaps-dccec069-306e-11e9-a1dd-12216a2f1059 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 15:40:34.108: INFO: Waiting for pod pod-configmaps-dccec069-306e-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:40:34.112: INFO: Pod pod-configmaps-dccec069-306e-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:40:34.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sv2fw" for this suite.
Feb 14 15:40:40.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:40:40.189: INFO: namespace: e2e-tests-configmap-sv2fw, resource: bindings, ignored listing per whitelist
Feb 14 15:40:40.251: INFO: namespace e2e-tests-configmap-sv2fw deletion completed in 6.133881578s

• [SLOW TEST:8.275 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:40:40.251: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 14 15:40:40.357: INFO: Waiting up to 5m0s for pod "pod-e1be390a-306e-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-j5xbq" to be "success or failure"
Feb 14 15:40:40.366: INFO: Pod "pod-e1be390a-306e-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 9.349824ms
Feb 14 15:40:42.371: INFO: Pod "pod-e1be390a-306e-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014356168s
STEP: Saw pod success
Feb 14 15:40:42.371: INFO: Pod "pod-e1be390a-306e-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:40:42.375: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-e1be390a-306e-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 15:40:42.413: INFO: Waiting for pod pod-e1be390a-306e-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:40:42.418: INFO: Pod pod-e1be390a-306e-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:40:42.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-j5xbq" for this suite.
Feb 14 15:40:48.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:40:48.554: INFO: namespace: e2e-tests-emptydir-j5xbq, resource: bindings, ignored listing per whitelist
Feb 14 15:40:48.570: INFO: namespace e2e-tests-emptydir-j5xbq deletion completed in 6.145676937s

• [SLOW TEST:8.320 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:40:48.571: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 14 15:40:51.181: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e6b0a66d-306e-11e9-a1dd-12216a2f1059"
Feb 14 15:40:51.181: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e6b0a66d-306e-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-pods-ln55h" to be "terminated due to deadline exceeded"
Feb 14 15:40:51.184: INFO: Pod "pod-update-activedeadlineseconds-e6b0a66d-306e-11e9-a1dd-12216a2f1059": Phase="Running", Reason="", readiness=true. Elapsed: 2.987227ms
Feb 14 15:40:53.188: INFO: Pod "pod-update-activedeadlineseconds-e6b0a66d-306e-11e9-a1dd-12216a2f1059": Phase="Running", Reason="", readiness=true. Elapsed: 2.006394715s
Feb 14 15:40:55.275: INFO: Pod "pod-update-activedeadlineseconds-e6b0a66d-306e-11e9-a1dd-12216a2f1059": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.093279422s
Feb 14 15:40:55.275: INFO: Pod "pod-update-activedeadlineseconds-e6b0a66d-306e-11e9-a1dd-12216a2f1059" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:40:55.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-ln55h" for this suite.
Feb 14 15:41:01.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:41:01.383: INFO: namespace: e2e-tests-pods-ln55h, resource: bindings, ignored listing per whitelist
Feb 14 15:41:01.466: INFO: namespace e2e-tests-pods-ln55h deletion completed in 6.163219291s

• [SLOW TEST:12.895 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:41:01.466: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-ee607d36-306e-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 15:41:01.549: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee61cd50-306e-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-configmap-d559b" to be "success or failure"
Feb 14 15:41:01.554: INFO: Pod "pod-configmaps-ee61cd50-306e-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259387ms
Feb 14 15:41:03.558: INFO: Pod "pod-configmaps-ee61cd50-306e-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008814196s
STEP: Saw pod success
Feb 14 15:41:03.558: INFO: Pod "pod-configmaps-ee61cd50-306e-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:41:03.562: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-configmaps-ee61cd50-306e-11e9-a1dd-12216a2f1059 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 15:41:03.597: INFO: Waiting for pod pod-configmaps-ee61cd50-306e-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:41:03.601: INFO: Pod pod-configmaps-ee61cd50-306e-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:41:03.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-d559b" for this suite.
Feb 14 15:41:09.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:41:09.706: INFO: namespace: e2e-tests-configmap-d559b, resource: bindings, ignored listing per whitelist
Feb 14 15:41:09.720: INFO: namespace e2e-tests-configmap-d559b deletion completed in 6.11393552s

• [SLOW TEST:8.254 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:41:09.721: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-f352b3a7-306e-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 15:41:09.844: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f3534744-306e-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-2k2lp" to be "success or failure"
Feb 14 15:41:09.854: INFO: Pod "pod-projected-configmaps-f3534744-306e-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 9.894074ms
Feb 14 15:41:11.860: INFO: Pod "pod-projected-configmaps-f3534744-306e-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015995075s
Feb 14 15:41:13.866: INFO: Pod "pod-projected-configmaps-f3534744-306e-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021782153s
STEP: Saw pod success
Feb 14 15:41:13.866: INFO: Pod "pod-projected-configmaps-f3534744-306e-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:41:13.869: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-projected-configmaps-f3534744-306e-11e9-a1dd-12216a2f1059 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 15:41:13.901: INFO: Waiting for pod pod-projected-configmaps-f3534744-306e-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:41:13.915: INFO: Pod pod-projected-configmaps-f3534744-306e-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:41:13.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2k2lp" for this suite.
Feb 14 15:41:19.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:41:20.012: INFO: namespace: e2e-tests-projected-2k2lp, resource: bindings, ignored listing per whitelist
Feb 14 15:41:20.025: INFO: namespace e2e-tests-projected-2k2lp deletion completed in 6.10276502s

• [SLOW TEST:10.304 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:41:20.025: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-7k55
STEP: Creating a pod to test atomic-volume-subpath
Feb 14 15:41:20.105: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7k55" in namespace "e2e-tests-subpath-z7n5q" to be "success or failure"
Feb 14 15:41:20.111: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Pending", Reason="", readiness=false. Elapsed: 5.363768ms
Feb 14 15:41:22.115: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009329342s
Feb 14 15:41:24.119: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Running", Reason="", readiness=false. Elapsed: 4.013345117s
Feb 14 15:41:26.123: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Running", Reason="", readiness=false. Elapsed: 6.017217683s
Feb 14 15:41:28.126: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Running", Reason="", readiness=false. Elapsed: 8.020950011s
Feb 14 15:41:30.131: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Running", Reason="", readiness=false. Elapsed: 10.025442267s
Feb 14 15:41:32.134: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Running", Reason="", readiness=false. Elapsed: 12.028369409s
Feb 14 15:41:34.138: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Running", Reason="", readiness=false. Elapsed: 14.032090912s
Feb 14 15:41:36.141: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Running", Reason="", readiness=false. Elapsed: 16.035920973s
Feb 14 15:41:38.144: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Running", Reason="", readiness=false. Elapsed: 18.038961017s
Feb 14 15:41:40.148: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Running", Reason="", readiness=false. Elapsed: 20.042450122s
Feb 14 15:41:42.154: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Running", Reason="", readiness=false. Elapsed: 22.048238271s
Feb 14 15:41:44.159: INFO: Pod "pod-subpath-test-projected-7k55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.053481732s
STEP: Saw pod success
Feb 14 15:41:44.159: INFO: Pod "pod-subpath-test-projected-7k55" satisfied condition "success or failure"
Feb 14 15:41:44.164: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-subpath-test-projected-7k55 container test-container-subpath-projected-7k55: <nil>
STEP: delete the pod
Feb 14 15:41:44.188: INFO: Waiting for pod pod-subpath-test-projected-7k55 to disappear
Feb 14 15:41:44.191: INFO: Pod pod-subpath-test-projected-7k55 no longer exists
STEP: Deleting pod pod-subpath-test-projected-7k55
Feb 14 15:41:44.191: INFO: Deleting pod "pod-subpath-test-projected-7k55" in namespace "e2e-tests-subpath-z7n5q"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:41:44.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-z7n5q" for this suite.
Feb 14 15:41:50.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:41:50.406: INFO: namespace: e2e-tests-subpath-z7n5q, resource: bindings, ignored listing per whitelist
Feb 14 15:41:50.413: INFO: namespace e2e-tests-subpath-z7n5q deletion completed in 6.216439273s

• [SLOW TEST:30.388 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:41:50.413: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-0b8dac93-306f-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 15:41:50.494: INFO: Waiting up to 5m0s for pod "pod-secrets-0b8e2b54-306f-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-secrets-9k9zw" to be "success or failure"
Feb 14 15:41:50.497: INFO: Pod "pod-secrets-0b8e2b54-306f-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489525ms
Feb 14 15:41:52.502: INFO: Pod "pod-secrets-0b8e2b54-306f-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007457651s
STEP: Saw pod success
Feb 14 15:41:52.502: INFO: Pod "pod-secrets-0b8e2b54-306f-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:41:52.505: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-secrets-0b8e2b54-306f-11e9-a1dd-12216a2f1059 container secret-volume-test: <nil>
STEP: delete the pod
Feb 14 15:41:52.526: INFO: Waiting for pod pod-secrets-0b8e2b54-306f-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:41:52.530: INFO: Pod pod-secrets-0b8e2b54-306f-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:41:52.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-9k9zw" for this suite.
Feb 14 15:41:58.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:41:58.579: INFO: namespace: e2e-tests-secrets-9k9zw, resource: bindings, ignored listing per whitelist
Feb 14 15:41:58.637: INFO: namespace e2e-tests-secrets-9k9zw deletion completed in 6.102063992s

• [SLOW TEST:8.224 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:41:58.637: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 14 15:41:58.710: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 14 15:42:03.716: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:42:03.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-zjjwl" for this suite.
Feb 14 15:42:09.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:42:09.810: INFO: namespace: e2e-tests-replication-controller-zjjwl, resource: bindings, ignored listing per whitelist
Feb 14 15:42:09.868: INFO: namespace e2e-tests-replication-controller-zjjwl deletion completed in 6.120149143s

• [SLOW TEST:11.231 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:42:09.870: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-1729f7a4-306f-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 15:42:09.977: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-172a8f36-306f-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-mdfxz" to be "success or failure"
Feb 14 15:42:09.986: INFO: Pod "pod-projected-secrets-172a8f36-306f-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 9.474651ms
Feb 14 15:42:11.992: INFO: Pod "pod-projected-secrets-172a8f36-306f-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014905422s
STEP: Saw pod success
Feb 14 15:42:11.992: INFO: Pod "pod-projected-secrets-172a8f36-306f-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:42:11.996: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-projected-secrets-172a8f36-306f-11e9-a1dd-12216a2f1059 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 14 15:42:12.022: INFO: Waiting for pod pod-projected-secrets-172a8f36-306f-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:42:12.024: INFO: Pod pod-projected-secrets-172a8f36-306f-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:42:12.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mdfxz" for this suite.
Feb 14 15:42:18.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:42:18.114: INFO: namespace: e2e-tests-projected-mdfxz, resource: bindings, ignored listing per whitelist
Feb 14 15:42:18.142: INFO: namespace e2e-tests-projected-mdfxz deletion completed in 6.11330079s

• [SLOW TEST:8.272 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:42:18.143: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 14 15:42:18.238: INFO: Waiting up to 5m0s for pod "pod-1c1771f2-306f-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-lbdhf" to be "success or failure"
Feb 14 15:42:18.242: INFO: Pod "pod-1c1771f2-306f-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 3.705251ms
Feb 14 15:42:20.247: INFO: Pod "pod-1c1771f2-306f-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00891747s
STEP: Saw pod success
Feb 14 15:42:20.247: INFO: Pod "pod-1c1771f2-306f-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:42:20.252: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-1c1771f2-306f-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 15:42:20.308: INFO: Waiting for pod pod-1c1771f2-306f-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:42:20.311: INFO: Pod pod-1c1771f2-306f-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:42:20.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lbdhf" for this suite.
Feb 14 15:42:26.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:42:26.446: INFO: namespace: e2e-tests-emptydir-lbdhf, resource: bindings, ignored listing per whitelist
Feb 14 15:42:26.477: INFO: namespace e2e-tests-emptydir-lbdhf deletion completed in 6.160179148s

• [SLOW TEST:8.334 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:42:26.477: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:42:26.568: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 14 15:42:26.575: INFO: Number of nodes with available pods: 0
Feb 14 15:42:26.575: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 14 15:42:26.604: INFO: Number of nodes with available pods: 0
Feb 14 15:42:26.604: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:27.609: INFO: Number of nodes with available pods: 0
Feb 14 15:42:27.609: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:28.609: INFO: Number of nodes with available pods: 1
Feb 14 15:42:28.610: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 14 15:42:28.630: INFO: Number of nodes with available pods: 1
Feb 14 15:42:28.630: INFO: Number of running nodes: 0, number of available pods: 1
Feb 14 15:42:29.635: INFO: Number of nodes with available pods: 0
Feb 14 15:42:29.635: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 14 15:42:29.648: INFO: Number of nodes with available pods: 0
Feb 14 15:42:29.648: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:30.651: INFO: Number of nodes with available pods: 0
Feb 14 15:42:30.651: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:31.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:31.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:32.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:32.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:33.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:33.653: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:34.675: INFO: Number of nodes with available pods: 0
Feb 14 15:42:34.675: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:35.651: INFO: Number of nodes with available pods: 0
Feb 14 15:42:35.651: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:36.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:36.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:37.653: INFO: Number of nodes with available pods: 0
Feb 14 15:42:37.653: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:38.651: INFO: Number of nodes with available pods: 0
Feb 14 15:42:38.651: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:39.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:39.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:40.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:40.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:41.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:41.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:42.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:42.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:43.657: INFO: Number of nodes with available pods: 0
Feb 14 15:42:43.657: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:44.651: INFO: Number of nodes with available pods: 0
Feb 14 15:42:44.651: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:45.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:45.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:46.651: INFO: Number of nodes with available pods: 0
Feb 14 15:42:46.651: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:47.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:47.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:48.651: INFO: Number of nodes with available pods: 0
Feb 14 15:42:48.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:49.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:49.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:50.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:50.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:51.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:51.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:52.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:52.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:53.653: INFO: Number of nodes with available pods: 0
Feb 14 15:42:53.653: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:54.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:54.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:55.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:55.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:56.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:56.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:57.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:57.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:58.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:58.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:42:59.652: INFO: Number of nodes with available pods: 0
Feb 14 15:42:59.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:43:00.656: INFO: Number of nodes with available pods: 0
Feb 14 15:43:00.656: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:43:01.653: INFO: Number of nodes with available pods: 0
Feb 14 15:43:01.653: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:43:02.652: INFO: Number of nodes with available pods: 0
Feb 14 15:43:02.652: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:43:03.653: INFO: Number of nodes with available pods: 0
Feb 14 15:43:03.653: INFO: Node k8s-conformance-node-1 is running more than one daemon pod
Feb 14 15:43:04.652: INFO: Number of nodes with available pods: 1
Feb 14 15:43:04.652: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-8wgh6, will wait for the garbage collector to delete the pods
Feb 14 15:43:04.717: INFO: Deleting DaemonSet.extensions daemon-set took: 7.205595ms
Feb 14 15:43:04.818: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.419032ms
Feb 14 15:43:47.521: INFO: Number of nodes with available pods: 0
Feb 14 15:43:47.521: INFO: Number of running nodes: 0, number of available pods: 0
Feb 14 15:43:47.523: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-8wgh6/daemonsets","resourceVersion":"21760"},"items":null}

Feb 14 15:43:47.526: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-8wgh6/pods","resourceVersion":"21760"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:43:47.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-8wgh6" for this suite.
Feb 14 15:43:53.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:43:53.639: INFO: namespace: e2e-tests-daemonsets-8wgh6, resource: bindings, ignored listing per whitelist
Feb 14 15:43:53.666: INFO: namespace e2e-tests-daemonsets-8wgh6 deletion completed in 6.112371972s

• [SLOW TEST:87.189 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:43:53.669: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:43:53.795: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"55093922-306f-11e9-aaef-f23c91bac0b7", Controller:(*bool)(0xc001588ee2), BlockOwnerDeletion:(*bool)(0xc001588ee3)}}
Feb 14 15:43:53.799: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5506a8ba-306f-11e9-aaef-f23c91bac0b7", Controller:(*bool)(0xc0015672c6), BlockOwnerDeletion:(*bool)(0xc0015672c7)}}
Feb 14 15:43:53.807: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"55077465-306f-11e9-aaef-f23c91bac0b7", Controller:(*bool)(0xc00158948e), BlockOwnerDeletion:(*bool)(0xc00158948f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:43:58.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-6bwdh" for this suite.
Feb 14 15:44:04.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:44:04.973: INFO: namespace: e2e-tests-gc-6bwdh, resource: bindings, ignored listing per whitelist
Feb 14 15:44:04.978: INFO: namespace e2e-tests-gc-6bwdh deletion completed in 6.156598615s

• [SLOW TEST:11.310 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:44:04.979: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-q5vjw
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 14 15:44:05.072: INFO: Found 0 stateful pods, waiting for 3
Feb 14 15:44:15.077: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 15:44:15.077: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 15:44:15.077: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 14 15:44:15.105: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 14 15:44:25.136: INFO: Updating stateful set ss2
Feb 14 15:44:25.144: INFO: Waiting for Pod e2e-tests-statefulset-q5vjw/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 14 15:44:35.240: INFO: Found 2 stateful pods, waiting for 3
Feb 14 15:44:45.246: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 15:44:45.246: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 15:44:45.246: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 14 15:44:45.272: INFO: Updating stateful set ss2
Feb 14 15:44:45.283: INFO: Waiting for Pod e2e-tests-statefulset-q5vjw/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 14 15:44:55.290: INFO: Waiting for Pod e2e-tests-statefulset-q5vjw/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 14 15:45:05.312: INFO: Updating stateful set ss2
Feb 14 15:45:05.335: INFO: Waiting for StatefulSet e2e-tests-statefulset-q5vjw/ss2 to complete update
Feb 14 15:45:05.335: INFO: Waiting for Pod e2e-tests-statefulset-q5vjw/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 14 15:45:15.345: INFO: Deleting all statefulset in ns e2e-tests-statefulset-q5vjw
Feb 14 15:45:15.349: INFO: Scaling statefulset ss2 to 0
Feb 14 15:45:25.367: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 15:45:25.385: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:45:25.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-q5vjw" for this suite.
Feb 14 15:45:31.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:45:31.492: INFO: namespace: e2e-tests-statefulset-q5vjw, resource: bindings, ignored listing per whitelist
Feb 14 15:45:31.523: INFO: namespace e2e-tests-statefulset-q5vjw deletion completed in 6.118288503s

• [SLOW TEST:86.543 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:45:31.523: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:45:31.605: INFO: Creating ReplicaSet my-hostname-basic-8f5a0647-306f-11e9-a1dd-12216a2f1059
Feb 14 15:45:31.613: INFO: Pod name my-hostname-basic-8f5a0647-306f-11e9-a1dd-12216a2f1059: Found 0 pods out of 1
Feb 14 15:45:36.617: INFO: Pod name my-hostname-basic-8f5a0647-306f-11e9-a1dd-12216a2f1059: Found 1 pods out of 1
Feb 14 15:45:36.617: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8f5a0647-306f-11e9-a1dd-12216a2f1059" is running
Feb 14 15:45:36.620: INFO: Pod "my-hostname-basic-8f5a0647-306f-11e9-a1dd-12216a2f1059-sx6bd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 15:45:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 15:45:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 15:45:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 15:45:31 +0000 UTC Reason: Message:}])
Feb 14 15:45:36.620: INFO: Trying to dial the pod
Feb 14 15:45:41.631: INFO: Controller my-hostname-basic-8f5a0647-306f-11e9-a1dd-12216a2f1059: Got expected result from replica 1 [my-hostname-basic-8f5a0647-306f-11e9-a1dd-12216a2f1059-sx6bd]: "my-hostname-basic-8f5a0647-306f-11e9-a1dd-12216a2f1059-sx6bd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:45:41.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-w54k6" for this suite.
Feb 14 15:45:47.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:45:47.682: INFO: namespace: e2e-tests-replicaset-w54k6, resource: bindings, ignored listing per whitelist
Feb 14 15:45:47.745: INFO: namespace e2e-tests-replicaset-w54k6 deletion completed in 6.109077893s

• [SLOW TEST:16.222 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:45:47.746: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 15:45:47.819: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9903026a-306f-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-69lrs" to be "success or failure"
Feb 14 15:45:47.825: INFO: Pod "downwardapi-volume-9903026a-306f-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 5.955251ms
Feb 14 15:45:49.829: INFO: Pod "downwardapi-volume-9903026a-306f-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009829071s
STEP: Saw pod success
Feb 14 15:45:49.829: INFO: Pod "downwardapi-volume-9903026a-306f-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:45:49.832: INFO: Trying to get logs from node k8s-conformance-node-1 pod downwardapi-volume-9903026a-306f-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 15:45:49.864: INFO: Waiting for pod downwardapi-volume-9903026a-306f-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:45:49.868: INFO: Pod downwardapi-volume-9903026a-306f-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:45:49.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-69lrs" for this suite.
Feb 14 15:45:55.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:45:55.978: INFO: namespace: e2e-tests-projected-69lrs, resource: bindings, ignored listing per whitelist
Feb 14 15:45:56.042: INFO: namespace e2e-tests-projected-69lrs deletion completed in 6.168631554s

• [SLOW TEST:8.296 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:45:56.044: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9e1e5fae-306f-11e9-a1dd-12216a2f1059
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-9e1e5fae-306f-11e9-a1dd-12216a2f1059
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:46:00.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k5j2s" for this suite.
Feb 14 15:46:18.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:46:18.504: INFO: namespace: e2e-tests-projected-k5j2s, resource: bindings, ignored listing per whitelist
Feb 14 15:46:18.536: INFO: namespace e2e-tests-projected-k5j2s deletion completed in 18.096946466s

• [SLOW TEST:22.493 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:46:18.538: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-k7g7z
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 14 15:46:18.604: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 14 15:46:40.717: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.143:8080/dial?request=hostName&protocol=udp&host=10.244.1.35&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-k7g7z PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:46:40.717: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:46:40.849: INFO: Waiting for endpoints: map[]
Feb 14 15:46:40.852: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.143:8080/dial?request=hostName&protocol=udp&host=10.244.2.142&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-k7g7z PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:46:40.852: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:46:40.967: INFO: Waiting for endpoints: map[]
Feb 14 15:46:40.971: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.143:8080/dial?request=hostName&protocol=udp&host=10.244.3.131&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-k7g7z PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 15:46:40.971: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
Feb 14 15:46:41.113: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:46:41.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-k7g7z" for this suite.
Feb 14 15:47:03.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:47:03.219: INFO: namespace: e2e-tests-pod-network-test-k7g7z, resource: bindings, ignored listing per whitelist
Feb 14 15:47:03.223: INFO: namespace e2e-tests-pod-network-test-k7g7z deletion completed in 22.103812468s

• [SLOW TEST:44.685 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:47:03.225: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 14 15:47:03.832: INFO: created pod pod-service-account-defaultsa
Feb 14 15:47:03.832: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 14 15:47:03.863: INFO: created pod pod-service-account-mountsa
Feb 14 15:47:03.863: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 14 15:47:03.888: INFO: created pod pod-service-account-nomountsa
Feb 14 15:47:03.888: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 14 15:47:03.900: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 14 15:47:03.900: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 14 15:47:03.909: INFO: created pod pod-service-account-mountsa-mountspec
Feb 14 15:47:03.909: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 14 15:47:03.916: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 14 15:47:03.916: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 14 15:47:03.949: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 14 15:47:03.949: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 14 15:47:03.963: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 14 15:47:03.963: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 14 15:47:03.975: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 14 15:47:03.975: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:47:03.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-tvldq" for this suite.
Feb 14 15:47:10.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:47:10.035: INFO: namespace: e2e-tests-svcaccounts-tvldq, resource: bindings, ignored listing per whitelist
Feb 14 15:47:10.104: INFO: namespace e2e-tests-svcaccounts-tvldq deletion completed in 6.117987124s

• [SLOW TEST:6.879 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:47:10.104: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-lrjpw
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lrjpw to expose endpoints map[]
Feb 14 15:47:10.193: INFO: Get endpoints failed (3.8728ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb 14 15:47:11.197: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lrjpw exposes endpoints map[] (1.00801272s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-lrjpw
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lrjpw to expose endpoints map[pod1:[80]]
Feb 14 15:47:13.231: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lrjpw exposes endpoints map[pod1:[80]] (2.025582874s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-lrjpw
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lrjpw to expose endpoints map[pod1:[80] pod2:[80]]
Feb 14 15:47:15.279: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lrjpw exposes endpoints map[pod1:[80] pod2:[80]] (2.037549333s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-lrjpw
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lrjpw to expose endpoints map[pod2:[80]]
Feb 14 15:47:15.296: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lrjpw exposes endpoints map[pod2:[80]] (8.225465ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-lrjpw
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lrjpw to expose endpoints map[]
Feb 14 15:47:15.313: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lrjpw exposes endpoints map[] (3.28495ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:47:15.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-lrjpw" for this suite.
Feb 14 15:47:37.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:47:37.447: INFO: namespace: e2e-tests-services-lrjpw, resource: bindings, ignored listing per whitelist
Feb 14 15:47:37.456: INFO: namespace e2e-tests-services-lrjpw deletion completed in 22.108318335s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:27.352 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:47:37.456: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 15:47:37.531: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da67c925-306f-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-rpb4l" to be "success or failure"
Feb 14 15:47:37.534: INFO: Pod "downwardapi-volume-da67c925-306f-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.672706ms
Feb 14 15:47:39.538: INFO: Pod "downwardapi-volume-da67c925-306f-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006739035s
STEP: Saw pod success
Feb 14 15:47:39.538: INFO: Pod "downwardapi-volume-da67c925-306f-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:47:39.541: INFO: Trying to get logs from node k8s-conformance-node-3 pod downwardapi-volume-da67c925-306f-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 15:47:39.568: INFO: Waiting for pod downwardapi-volume-da67c925-306f-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:47:39.571: INFO: Pod downwardapi-volume-da67c925-306f-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:47:39.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rpb4l" for this suite.
Feb 14 15:47:45.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:47:45.673: INFO: namespace: e2e-tests-downward-api-rpb4l, resource: bindings, ignored listing per whitelist
Feb 14 15:47:45.691: INFO: namespace e2e-tests-downward-api-rpb4l deletion completed in 6.117005682s

• [SLOW TEST:8.236 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:47:45.693: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0214 15:48:16.305881      16 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 14 15:48:16.306: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:48:16.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-kl6n9" for this suite.
Feb 14 15:48:22.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:48:22.385: INFO: namespace: e2e-tests-gc-kl6n9, resource: bindings, ignored listing per whitelist
Feb 14 15:48:22.464: INFO: namespace e2e-tests-gc-kl6n9 deletion completed in 6.152101967s

• [SLOW TEST:36.772 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:48:22.467: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:48:22.560: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 14 15:48:27.565: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 14 15:48:27.565: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 14 15:48:27.593: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-brtmc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-brtmc/deployments/test-cleanup-deployment,UID:f83c6eb8-306f-11e9-aaef-f23c91bac0b7,ResourceVersion:23058,Generation:1,CreationTimestamp:2019-02-14 15:48:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Feb 14 15:48:27.603: INFO: New ReplicaSet "test-cleanup-deployment-7dbbfcf846" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846,GenerateName:,Namespace:e2e-tests-deployment-brtmc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-brtmc/replicasets/test-cleanup-deployment-7dbbfcf846,UID:f840830f-306f-11e9-aaef-f23c91bac0b7,ResourceVersion:23060,Generation:1,CreationTimestamp:2019-02-14 15:48:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment f83c6eb8-306f-11e9-aaef-f23c91bac0b7 0xc0019a8187 0xc0019a8188}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 14 15:48:27.603: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 14 15:48:27.604: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-brtmc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-brtmc/replicasets/test-cleanup-controller,UID:f53ede00-306f-11e9-aaef-f23c91bac0b7,ResourceVersion:23059,Generation:1,CreationTimestamp:2019-02-14 15:48:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment f83c6eb8-306f-11e9-aaef-f23c91bac0b7 0xc0027f5ff7 0xc0027f5ff8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 14 15:48:27.612: INFO: Pod "test-cleanup-controller-qn5v6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-qn5v6,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-brtmc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-brtmc/pods/test-cleanup-controller-qn5v6,UID:f53fc854-306f-11e9-aaef-f23c91bac0b7,ResourceVersion:23049,Generation:0,CreationTimestamp:2019-02-14 15:48:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.138/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller f53ede00-306f-11e9-aaef-f23c91bac0b7 0xc001bc98b7 0xc001bc98b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-69xf7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-69xf7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-69xf7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bc99a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bc99c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:48:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:48:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:48:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 15:48:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.219.21,PodIP:10.244.3.138,StartTime:2019-02-14 15:48:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 15:48:23 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://f9e52a80c76c4caf6b3b802341bcb9673b93612e451b8b3888d2453e70dd57ed}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 15:48:27.612: INFO: Pod "test-cleanup-deployment-7dbbfcf846-6zfrw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846-6zfrw,GenerateName:test-cleanup-deployment-7dbbfcf846-,Namespace:e2e-tests-deployment-brtmc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-brtmc/pods/test-cleanup-deployment-7dbbfcf846-6zfrw,UID:f8419c51-306f-11e9-aaef-f23c91bac0b7,ResourceVersion:23063,Generation:0,CreationTimestamp:2019-02-14 15:48:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-7dbbfcf846 f840830f-306f-11e9-aaef-f23c91bac0b7 0xc001bc9b07 0xc001bc9b08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-69xf7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-69xf7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-69xf7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bc9f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bc9f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:48:27.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-brtmc" for this suite.
Feb 14 15:48:33.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:48:33.726: INFO: namespace: e2e-tests-deployment-brtmc, resource: bindings, ignored listing per whitelist
Feb 14 15:48:33.759: INFO: namespace e2e-tests-deployment-brtmc deletion completed in 6.127927493s

• [SLOW TEST:11.292 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:48:33.759: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 14 15:48:33.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 --namespace=e2e-tests-kubectl-s2hmd run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 14 15:48:35.902: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 14 15:48:35.902: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:48:37.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-s2hmd" for this suite.
Feb 14 15:48:43.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:48:44.032: INFO: namespace: e2e-tests-kubectl-s2hmd, resource: bindings, ignored listing per whitelist
Feb 14 15:48:44.043: INFO: namespace e2e-tests-kubectl-s2hmd deletion completed in 6.131018366s

• [SLOW TEST:10.284 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:48:44.044: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-v45rk
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 14 15:48:44.142: INFO: Found 0 stateful pods, waiting for 3
Feb 14 15:48:54.146: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 15:48:54.146: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 15:48:54.146: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 15:48:54.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-v45rk ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 15:48:54.355: INFO: stderr: ""
Feb 14 15:48:54.355: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 15:48:54.355: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 14 15:49:04.391: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 14 15:49:14.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-v45rk ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:49:14.650: INFO: stderr: ""
Feb 14 15:49:14.650: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 15:49:14.650: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 15:49:24.674: INFO: Waiting for StatefulSet e2e-tests-statefulset-v45rk/ss2 to complete update
Feb 14 15:49:24.674: INFO: Waiting for Pod e2e-tests-statefulset-v45rk/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Feb 14 15:49:34.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-v45rk ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 15:49:34.860: INFO: stderr: ""
Feb 14 15:49:34.860: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 15:49:34.860: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 15:49:44.899: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 14 15:49:54.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 exec --namespace=e2e-tests-statefulset-v45rk ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 15:49:55.114: INFO: stderr: ""
Feb 14 15:49:55.114: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 15:49:55.114: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 14 15:50:15.137: INFO: Deleting all statefulset in ns e2e-tests-statefulset-v45rk
Feb 14 15:50:15.140: INFO: Scaling statefulset ss2 to 0
Feb 14 15:50:45.172: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 15:50:45.175: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:50:45.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-v45rk" for this suite.
Feb 14 15:50:51.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:50:51.239: INFO: namespace: e2e-tests-statefulset-v45rk, resource: bindings, ignored listing per whitelist
Feb 14 15:50:51.304: INFO: namespace e2e-tests-statefulset-v45rk deletion completed in 6.10254262s

• [SLOW TEST:127.260 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:50:51.305: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:50:57.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-kg6nc" for this suite.
Feb 14 15:51:03.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:51:03.561: INFO: namespace: e2e-tests-namespaces-kg6nc, resource: bindings, ignored listing per whitelist
Feb 14 15:51:03.646: INFO: namespace e2e-tests-namespaces-kg6nc deletion completed in 6.111386002s
STEP: Destroying namespace "e2e-tests-nsdeletetest-pscvw" for this suite.
Feb 14 15:51:03.649: INFO: Namespace e2e-tests-nsdeletetest-pscvw was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-p9zp4" for this suite.
Feb 14 15:51:09.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:51:09.673: INFO: namespace: e2e-tests-nsdeletetest-p9zp4, resource: bindings, ignored listing per whitelist
Feb 14 15:51:09.743: INFO: namespace e2e-tests-nsdeletetest-p9zp4 deletion completed in 6.094101369s

• [SLOW TEST:18.439 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:51:09.744: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 14 15:51:09.820: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-6dtn8" to be "success or failure"
Feb 14 15:51:09.825: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.842672ms
Feb 14 15:51:11.828: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008374966s
STEP: Saw pod success
Feb 14 15:51:11.828: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 14 15:51:11.831: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 14 15:51:11.862: INFO: Waiting for pod pod-host-path-test to disappear
Feb 14 15:51:11.867: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:51:11.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-6dtn8" for this suite.
Feb 14 15:51:17.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:51:17.899: INFO: namespace: e2e-tests-hostpath-6dtn8, resource: bindings, ignored listing per whitelist
Feb 14 15:51:17.976: INFO: namespace e2e-tests-hostpath-6dtn8 deletion completed in 6.105423155s

• [SLOW TEST:8.233 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:51:17.978: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-5lwqn/secret-test-5dd84720-3070-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 15:51:18.065: INFO: Waiting up to 5m0s for pod "pod-configmaps-5dd8ed69-3070-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-secrets-5lwqn" to be "success or failure"
Feb 14 15:51:18.074: INFO: Pod "pod-configmaps-5dd8ed69-3070-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 9.094496ms
Feb 14 15:51:20.079: INFO: Pod "pod-configmaps-5dd8ed69-3070-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013768648s
STEP: Saw pod success
Feb 14 15:51:20.079: INFO: Pod "pod-configmaps-5dd8ed69-3070-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:51:20.082: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-configmaps-5dd8ed69-3070-11e9-a1dd-12216a2f1059 container env-test: <nil>
STEP: delete the pod
Feb 14 15:51:20.113: INFO: Waiting for pod pod-configmaps-5dd8ed69-3070-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:51:20.116: INFO: Pod pod-configmaps-5dd8ed69-3070-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:51:20.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-5lwqn" for this suite.
Feb 14 15:51:26.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:51:26.205: INFO: namespace: e2e-tests-secrets-5lwqn, resource: bindings, ignored listing per whitelist
Feb 14 15:51:26.227: INFO: namespace e2e-tests-secrets-5lwqn deletion completed in 6.107292294s

• [SLOW TEST:8.249 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:51:26.228: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 14 15:51:30.396: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 15:51:30.399: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 15:51:32.400: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 15:51:32.403: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 15:51:34.400: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 15:51:34.403: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 15:51:36.400: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 15:51:36.403: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 15:51:38.400: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 15:51:38.403: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:51:38.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-vgrh9" for this suite.
Feb 14 15:52:00.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:52:00.457: INFO: namespace: e2e-tests-container-lifecycle-hook-vgrh9, resource: bindings, ignored listing per whitelist
Feb 14 15:52:00.511: INFO: namespace e2e-tests-container-lifecycle-hook-vgrh9 deletion completed in 22.103595685s

• [SLOW TEST:34.283 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:52:00.511: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 14 15:52:00.588: INFO: Waiting up to 5m0s for pod "pod-77335a64-3070-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-52l6s" to be "success or failure"
Feb 14 15:52:00.596: INFO: Pod "pod-77335a64-3070-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 7.116986ms
Feb 14 15:52:02.599: INFO: Pod "pod-77335a64-3070-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010703578s
STEP: Saw pod success
Feb 14 15:52:02.599: INFO: Pod "pod-77335a64-3070-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:52:02.601: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-77335a64-3070-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 15:52:02.627: INFO: Waiting for pod pod-77335a64-3070-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:52:02.636: INFO: Pod pod-77335a64-3070-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:52:02.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-52l6s" for this suite.
Feb 14 15:52:08.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:52:08.724: INFO: namespace: e2e-tests-emptydir-52l6s, resource: bindings, ignored listing per whitelist
Feb 14 15:52:08.739: INFO: namespace e2e-tests-emptydir-52l6s deletion completed in 6.097342027s

• [SLOW TEST:8.228 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:52:08.739: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-7c19f98f-3070-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume configMaps
Feb 14 15:52:08.819: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7c1a9bc3-3070-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-l77zv" to be "success or failure"
Feb 14 15:52:08.824: INFO: Pod "pod-projected-configmaps-7c1a9bc3-3070-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 5.613842ms
Feb 14 15:52:10.828: INFO: Pod "pod-projected-configmaps-7c1a9bc3-3070-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009521175s
STEP: Saw pod success
Feb 14 15:52:10.828: INFO: Pod "pod-projected-configmaps-7c1a9bc3-3070-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:52:10.830: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-projected-configmaps-7c1a9bc3-3070-11e9-a1dd-12216a2f1059 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 15:52:10.847: INFO: Waiting for pod pod-projected-configmaps-7c1a9bc3-3070-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:52:10.850: INFO: Pod pod-projected-configmaps-7c1a9bc3-3070-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:52:10.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l77zv" for this suite.
Feb 14 15:52:16.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:52:16.961: INFO: namespace: e2e-tests-projected-l77zv, resource: bindings, ignored listing per whitelist
Feb 14 15:52:16.961: INFO: namespace e2e-tests-projected-l77zv deletion completed in 6.107136978s

• [SLOW TEST:8.222 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:52:16.962: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:52:19.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-8wflx" for this suite.
Feb 14 15:53:09.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:53:09.166: INFO: namespace: e2e-tests-kubelet-test-8wflx, resource: bindings, ignored listing per whitelist
Feb 14 15:53:09.173: INFO: namespace e2e-tests-kubelet-test-8wflx deletion completed in 50.097942592s

• [SLOW TEST:52.212 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:53:09.175: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-a01fb293-3070-11e9-a1dd-12216a2f1059
STEP: Creating a pod to test consume secrets
Feb 14 15:53:09.250: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a020382e-3070-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-prxgj" to be "success or failure"
Feb 14 15:53:09.253: INFO: Pod "pod-projected-secrets-a020382e-3070-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 3.366752ms
Feb 14 15:53:11.258: INFO: Pod "pod-projected-secrets-a020382e-3070-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008105055s
Feb 14 15:53:13.262: INFO: Pod "pod-projected-secrets-a020382e-3070-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012403791s
STEP: Saw pod success
Feb 14 15:53:13.263: INFO: Pod "pod-projected-secrets-a020382e-3070-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:53:13.266: INFO: Trying to get logs from node k8s-conformance-node-3 pod pod-projected-secrets-a020382e-3070-11e9-a1dd-12216a2f1059 container secret-volume-test: <nil>
STEP: delete the pod
Feb 14 15:53:13.304: INFO: Waiting for pod pod-projected-secrets-a020382e-3070-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:53:13.307: INFO: Pod pod-projected-secrets-a020382e-3070-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:53:13.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-prxgj" for this suite.
Feb 14 15:53:19.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:53:19.415: INFO: namespace: e2e-tests-projected-prxgj, resource: bindings, ignored listing per whitelist
Feb 14 15:53:19.418: INFO: namespace e2e-tests-projected-prxgj deletion completed in 6.106710786s

• [SLOW TEST:10.244 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:53:19.418: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 14 15:53:19.502: INFO: Waiting up to 5m0s for pod "pod-a63c3c39-3070-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-emptydir-ljs46" to be "success or failure"
Feb 14 15:53:19.505: INFO: Pod "pod-a63c3c39-3070-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 3.143342ms
Feb 14 15:53:21.508: INFO: Pod "pod-a63c3c39-3070-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006554301s
STEP: Saw pod success
Feb 14 15:53:21.508: INFO: Pod "pod-a63c3c39-3070-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:53:21.510: INFO: Trying to get logs from node k8s-conformance-node-1 pod pod-a63c3c39-3070-11e9-a1dd-12216a2f1059 container test-container: <nil>
STEP: delete the pod
Feb 14 15:53:21.535: INFO: Waiting for pod pod-a63c3c39-3070-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:53:21.543: INFO: Pod pod-a63c3c39-3070-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:53:21.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-ljs46" for this suite.
Feb 14 15:53:27.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:53:27.618: INFO: namespace: e2e-tests-emptydir-ljs46, resource: bindings, ignored listing per whitelist
Feb 14 15:53:27.639: INFO: namespace e2e-tests-emptydir-ljs46 deletion completed in 6.083248284s

• [SLOW TEST:8.221 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:53:27.639: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 15:53:27.762: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab2827ca-3070-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-projected-4kdqg" to be "success or failure"
Feb 14 15:53:27.771: INFO: Pod "downwardapi-volume-ab2827ca-3070-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 9.458521ms
Feb 14 15:53:29.775: INFO: Pod "downwardapi-volume-ab2827ca-3070-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013514288s
STEP: Saw pod success
Feb 14 15:53:29.775: INFO: Pod "downwardapi-volume-ab2827ca-3070-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:53:29.777: INFO: Trying to get logs from node k8s-conformance-node-3 pod downwardapi-volume-ab2827ca-3070-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 15:53:29.795: INFO: Waiting for pod downwardapi-volume-ab2827ca-3070-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:53:29.800: INFO: Pod downwardapi-volume-ab2827ca-3070-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:53:29.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4kdqg" for this suite.
Feb 14 15:53:35.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:53:35.853: INFO: namespace: e2e-tests-projected-4kdqg, resource: bindings, ignored listing per whitelist
Feb 14 15:53:35.907: INFO: namespace e2e-tests-projected-4kdqg deletion completed in 6.102818733s

• [SLOW TEST:8.268 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:53:35.908: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 15:53:35.983: INFO: (0) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.686155ms)
Feb 14 15:53:35.986: INFO: (1) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.191346ms)
Feb 14 15:53:35.989: INFO: (2) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.763828ms)
Feb 14 15:53:35.992: INFO: (3) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.776497ms)
Feb 14 15:53:35.995: INFO: (4) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.435156ms)
Feb 14 15:53:36.000: INFO: (5) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.473516ms)
Feb 14 15:53:36.003: INFO: (6) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.899797ms)
Feb 14 15:53:36.006: INFO: (7) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.921458ms)
Feb 14 15:53:36.009: INFO: (8) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.950396ms)
Feb 14 15:53:36.012: INFO: (9) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.036398ms)
Feb 14 15:53:36.015: INFO: (10) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.889017ms)
Feb 14 15:53:36.018: INFO: (11) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.053617ms)
Feb 14 15:53:36.021: INFO: (12) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.786377ms)
Feb 14 15:53:36.024: INFO: (13) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.417806ms)
Feb 14 15:53:36.028: INFO: (14) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.233717ms)
Feb 14 15:53:36.031: INFO: (15) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.111126ms)
Feb 14 15:53:36.034: INFO: (16) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.015448ms)
Feb 14 15:53:36.037: INFO: (17) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.832107ms)
Feb 14 15:53:36.040: INFO: (18) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.890527ms)
Feb 14 15:53:36.043: INFO: (19) /api/v1/nodes/k8s-conformance-node-1/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.927567ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:53:36.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-748qk" for this suite.
Feb 14 15:53:42.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:53:42.128: INFO: namespace: e2e-tests-proxy-748qk, resource: bindings, ignored listing per whitelist
Feb 14 15:53:42.138: INFO: namespace e2e-tests-proxy-748qk deletion completed in 6.090713199s

• [SLOW TEST:6.229 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:53:42.138: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 15:53:42.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-hgrv2'
Feb 14 15:53:42.411: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 14 15:53:42.411: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Feb 14 15:53:42.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-hgrv2'
Feb 14 15:53:42.529: INFO: stderr: ""
Feb 14 15:53:42.529: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:53:42.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hgrv2" for this suite.
Feb 14 15:53:48.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:53:48.560: INFO: namespace: e2e-tests-kubectl-hgrv2, resource: bindings, ignored listing per whitelist
Feb 14 15:53:48.625: INFO: namespace e2e-tests-kubectl-hgrv2 deletion completed in 6.091904602s

• [SLOW TEST:6.487 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:53:48.625: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 15:53:48.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-qkfmg'
Feb 14 15:53:48.798: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 14 15:53:48.798: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Feb 14 15:53:50.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-qkfmg'
Feb 14 15:53:50.906: INFO: stderr: ""
Feb 14 15:53:50.906: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:53:50.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qkfmg" for this suite.
Feb 14 15:53:56.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:53:57.087: INFO: namespace: e2e-tests-kubectl-qkfmg, resource: bindings, ignored listing per whitelist
Feb 14 15:53:57.098: INFO: namespace e2e-tests-kubectl-qkfmg deletion completed in 6.181286761s

• [SLOW TEST:8.473 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:53:57.098: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 15:53:57.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-glc99'
Feb 14 15:53:57.259: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 14 15:53:57.259: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Feb 14 15:53:59.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-284296654 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-glc99'
Feb 14 15:53:59.379: INFO: stderr: ""
Feb 14 15:53:59.379: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:53:59.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-glc99" for this suite.
Feb 14 15:54:21.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:54:21.472: INFO: namespace: e2e-tests-kubectl-glc99, resource: bindings, ignored listing per whitelist
Feb 14 15:54:21.489: INFO: namespace e2e-tests-kubectl-glc99 deletion completed in 22.105186023s

• [SLOW TEST:24.391 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:54:21.489: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 15:54:21.568: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb3a6361-3070-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-68p5d" to be "success or failure"
Feb 14 15:54:21.575: INFO: Pod "downwardapi-volume-cb3a6361-3070-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 6.829815ms
Feb 14 15:54:23.579: INFO: Pod "downwardapi-volume-cb3a6361-3070-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010999841s
STEP: Saw pod success
Feb 14 15:54:23.579: INFO: Pod "downwardapi-volume-cb3a6361-3070-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:54:23.582: INFO: Trying to get logs from node k8s-conformance-node-1 pod downwardapi-volume-cb3a6361-3070-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 15:54:23.605: INFO: Waiting for pod downwardapi-volume-cb3a6361-3070-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:54:23.608: INFO: Pod downwardapi-volume-cb3a6361-3070-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:54:23.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-68p5d" for this suite.
Feb 14 15:54:29.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:54:29.711: INFO: namespace: e2e-tests-downward-api-68p5d, resource: bindings, ignored listing per whitelist
Feb 14 15:54:29.715: INFO: namespace e2e-tests-downward-api-68p5d deletion completed in 6.088899438s

• [SLOW TEST:8.226 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:54:29.716: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 15:54:29.786: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d020e770-3070-11e9-a1dd-12216a2f1059" in namespace "e2e-tests-downward-api-t5hwx" to be "success or failure"
Feb 14 15:54:29.790: INFO: Pod "downwardapi-volume-d020e770-3070-11e9-a1dd-12216a2f1059": Phase="Pending", Reason="", readiness=false. Elapsed: 3.883956ms
Feb 14 15:54:31.793: INFO: Pod "downwardapi-volume-d020e770-3070-11e9-a1dd-12216a2f1059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007075454s
STEP: Saw pod success
Feb 14 15:54:31.793: INFO: Pod "downwardapi-volume-d020e770-3070-11e9-a1dd-12216a2f1059" satisfied condition "success or failure"
Feb 14 15:54:31.796: INFO: Trying to get logs from node k8s-conformance-node-1 pod downwardapi-volume-d020e770-3070-11e9-a1dd-12216a2f1059 container client-container: <nil>
STEP: delete the pod
Feb 14 15:54:31.830: INFO: Waiting for pod downwardapi-volume-d020e770-3070-11e9-a1dd-12216a2f1059 to disappear
Feb 14 15:54:31.833: INFO: Pod downwardapi-volume-d020e770-3070-11e9-a1dd-12216a2f1059 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:54:31.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-t5hwx" for this suite.
Feb 14 15:54:37.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:54:37.865: INFO: namespace: e2e-tests-downward-api-t5hwx, resource: bindings, ignored listing per whitelist
Feb 14 15:54:37.942: INFO: namespace e2e-tests-downward-api-t5hwx deletion completed in 6.10576699s

• [SLOW TEST:8.226 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:54:37.943: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 14 15:54:38.012: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:54:41.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-f2vxn" for this suite.
Feb 14 15:54:47.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:54:47.406: INFO: namespace: e2e-tests-init-container-f2vxn, resource: bindings, ignored listing per whitelist
Feb 14 15:54:47.438: INFO: namespace e2e-tests-init-container-f2vxn deletion completed in 6.094366833s

• [SLOW TEST:9.495 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 15:54:47.438: INFO: >>> kubeConfig: /tmp/kubeconfig-284296654
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 15:54:49.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-28djh" for this suite.
Feb 14 15:54:55.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 15:54:55.655: INFO: namespace: e2e-tests-emptydir-wrapper-28djh, resource: bindings, ignored listing per whitelist
Feb 14 15:54:55.695: INFO: namespace e2e-tests-emptydir-wrapper-28djh deletion completed in 6.09474176s

• [SLOW TEST:8.257 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSFeb 14 15:54:55.695: INFO: Running AfterSuite actions on all nodes
Feb 14 15:54:55.696: INFO: Running AfterSuite actions on node 1
Feb 14 15:54:55.696: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5678.559 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h34m40.42672454s
Test Suite Passed
