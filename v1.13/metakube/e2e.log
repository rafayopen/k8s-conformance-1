I0204 16:20:41.650069      13 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-296264527
I0204 16:20:41.650442      13 e2e.go:224] Starting e2e run "cee1d542-2898-11e9-9eab-8e57f341003b" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1549297237 - Will randomize all specs
Will run 201 of 1946 specs

Feb  4 16:20:42.342: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 16:20:42.347: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb  4 16:20:42.396: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb  4 16:20:42.480: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb  4 16:20:42.481: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb  4 16:20:42.481: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb  4 16:20:42.503: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Feb  4 16:20:42.503: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb  4 16:20:42.503: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb  4 16:20:42.504: INFO: e2e test version: v1.13.0
Feb  4 16:20:42.506: INFO: kube-apiserver version: v1.13.2
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:20:42.512: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
Feb  4 16:20:42.718: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  4 16:20:49.474: INFO: Successfully updated pod "labelsupdated18b5ce3-2898-11e9-9eab-8e57f341003b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:20:51.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mpkxm" for this suite.
Feb  4 16:21:15.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:21:15.899: INFO: namespace: e2e-tests-downward-api-mpkxm, resource: bindings, ignored listing per whitelist
Feb  4 16:21:16.051: INFO: namespace e2e-tests-downward-api-mpkxm deletion completed in 24.524743207s

• [SLOW TEST:33.539 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:21:16.057: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb  4 16:21:26.369: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  4 16:21:26.375: INFO: Pod pod-with-prestop-http-hook still exists
Feb  4 16:21:28.375: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  4 16:21:28.432: INFO: Pod pod-with-prestop-http-hook still exists
Feb  4 16:21:30.375: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  4 16:21:30.382: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:21:30.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-vnn6t" for this suite.
Feb  4 16:21:54.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:21:54.874: INFO: namespace: e2e-tests-container-lifecycle-hook-vnn6t, resource: bindings, ignored listing per whitelist
Feb  4 16:21:55.028: INFO: namespace e2e-tests-container-lifecycle-hook-vnn6t deletion completed in 24.413864606s

• [SLOW TEST:38.971 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:21:55.035: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb  4 16:21:55.347: INFO: Number of nodes with available pods: 0
Feb  4 16:21:55.347: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:21:56.363: INFO: Number of nodes with available pods: 0
Feb  4 16:21:56.363: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:21:57.361: INFO: Number of nodes with available pods: 0
Feb  4 16:21:57.361: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:21:58.361: INFO: Number of nodes with available pods: 0
Feb  4 16:21:58.361: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:21:59.379: INFO: Number of nodes with available pods: 1
Feb  4 16:21:59.379: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 is running more than one daemon pod
Feb  4 16:22:00.359: INFO: Number of nodes with available pods: 2
Feb  4 16:22:00.359: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb  4 16:22:00.404: INFO: Number of nodes with available pods: 1
Feb  4 16:22:00.404: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:01.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:01.417: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:02.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:02.417: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:03.415: INFO: Number of nodes with available pods: 1
Feb  4 16:22:03.415: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:04.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:04.417: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:05.424: INFO: Number of nodes with available pods: 1
Feb  4 16:22:05.424: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:06.420: INFO: Number of nodes with available pods: 1
Feb  4 16:22:06.420: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:07.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:07.417: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:08.416: INFO: Number of nodes with available pods: 1
Feb  4 16:22:08.416: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:09.432: INFO: Number of nodes with available pods: 1
Feb  4 16:22:09.432: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:10.424: INFO: Number of nodes with available pods: 1
Feb  4 16:22:10.424: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:11.421: INFO: Number of nodes with available pods: 1
Feb  4 16:22:11.421: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:12.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:12.417: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:13.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:13.417: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:14.415: INFO: Number of nodes with available pods: 1
Feb  4 16:22:14.416: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:15.424: INFO: Number of nodes with available pods: 1
Feb  4 16:22:15.424: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:16.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:16.418: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:17.416: INFO: Number of nodes with available pods: 1
Feb  4 16:22:17.416: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:18.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:18.417: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:19.414: INFO: Number of nodes with available pods: 1
Feb  4 16:22:19.414: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:20.430: INFO: Number of nodes with available pods: 1
Feb  4 16:22:20.430: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:21.430: INFO: Number of nodes with available pods: 1
Feb  4 16:22:21.430: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:22.415: INFO: Number of nodes with available pods: 1
Feb  4 16:22:22.415: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:23.418: INFO: Number of nodes with available pods: 1
Feb  4 16:22:23.418: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:24.421: INFO: Number of nodes with available pods: 1
Feb  4 16:22:24.421: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:25.416: INFO: Number of nodes with available pods: 1
Feb  4 16:22:25.416: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:26.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:26.417: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:27.419: INFO: Number of nodes with available pods: 1
Feb  4 16:22:27.419: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:28.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:28.417: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:29.415: INFO: Number of nodes with available pods: 1
Feb  4 16:22:29.415: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:30.422: INFO: Number of nodes with available pods: 1
Feb  4 16:22:30.422: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:31.425: INFO: Number of nodes with available pods: 1
Feb  4 16:22:31.425: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:32.416: INFO: Number of nodes with available pods: 1
Feb  4 16:22:32.416: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:33.423: INFO: Number of nodes with available pods: 1
Feb  4 16:22:33.423: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:34.418: INFO: Number of nodes with available pods: 1
Feb  4 16:22:34.418: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:35.426: INFO: Number of nodes with available pods: 1
Feb  4 16:22:35.426: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:36.424: INFO: Number of nodes with available pods: 1
Feb  4 16:22:36.424: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:37.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:37.417: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:38.416: INFO: Number of nodes with available pods: 1
Feb  4 16:22:38.416: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:39.415: INFO: Number of nodes with available pods: 1
Feb  4 16:22:39.415: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:40.433: INFO: Number of nodes with available pods: 1
Feb  4 16:22:40.434: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:41.434: INFO: Number of nodes with available pods: 1
Feb  4 16:22:41.435: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:42.417: INFO: Number of nodes with available pods: 1
Feb  4 16:22:42.417: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:22:43.424: INFO: Number of nodes with available pods: 2
Feb  4 16:22:43.424: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-4rmcd, will wait for the garbage collector to delete the pods
Feb  4 16:22:43.506: INFO: Deleting DaemonSet.extensions daemon-set took: 18.587849ms
Feb  4 16:22:43.606: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.219567ms
Feb  4 16:23:22.766: INFO: Number of nodes with available pods: 0
Feb  4 16:23:22.766: INFO: Number of running nodes: 0, number of available pods: 0
Feb  4 16:23:22.777: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-4rmcd/daemonsets","resourceVersion":"2864"},"items":null}

Feb  4 16:23:22.782: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-4rmcd/pods","resourceVersion":"2864"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:23:22.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-4rmcd" for this suite.
Feb  4 16:23:30.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:23:30.919: INFO: namespace: e2e-tests-daemonsets-4rmcd, resource: bindings, ignored listing per whitelist
Feb  4 16:23:31.224: INFO: namespace e2e-tests-daemonsets-4rmcd deletion completed in 8.418104537s

• [SLOW TEST:96.190 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:23:31.228: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0204 16:23:41.549857      13 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  4 16:23:41.550: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:23:41.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-h74qh" for this suite.
Feb  4 16:23:47.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:23:47.729: INFO: namespace: e2e-tests-gc-h74qh, resource: bindings, ignored listing per whitelist
Feb  4 16:23:47.925: INFO: namespace e2e-tests-gc-h74qh deletion completed in 6.368964897s

• [SLOW TEST:16.698 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:23:47.931: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb  4 16:23:48.167: INFO: Number of nodes with available pods: 0
Feb  4 16:23:48.167: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:23:49.214: INFO: Number of nodes with available pods: 0
Feb  4 16:23:49.214: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:23:50.243: INFO: Number of nodes with available pods: 0
Feb  4 16:23:50.244: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:23:51.181: INFO: Number of nodes with available pods: 0
Feb  4 16:23:51.181: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:23:52.179: INFO: Number of nodes with available pods: 2
Feb  4 16:23:52.179: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb  4 16:23:52.231: INFO: Number of nodes with available pods: 2
Feb  4 16:23:52.231: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-kjsfs, will wait for the garbage collector to delete the pods
Feb  4 16:23:53.324: INFO: Deleting DaemonSet.extensions daemon-set took: 15.598941ms
Feb  4 16:23:53.424: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.160047ms
Feb  4 16:24:31.242: INFO: Number of nodes with available pods: 0
Feb  4 16:24:31.242: INFO: Number of running nodes: 0, number of available pods: 0
Feb  4 16:24:31.248: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-kjsfs/daemonsets","resourceVersion":"3154"},"items":null}

Feb  4 16:24:31.254: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-kjsfs/pods","resourceVersion":"3154"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:24:31.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-kjsfs" for this suite.
Feb  4 16:24:37.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:24:37.367: INFO: namespace: e2e-tests-daemonsets-kjsfs, resource: bindings, ignored listing per whitelist
Feb  4 16:24:37.676: INFO: namespace e2e-tests-daemonsets-kjsfs deletion completed in 6.388425564s

• [SLOW TEST:49.746 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:24:37.680: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb  4 16:24:37.904: INFO: namespace e2e-tests-kubectl-45tgp
Feb  4 16:24:37.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-45tgp'
Feb  4 16:24:39.724: INFO: stderr: ""
Feb  4 16:24:39.724: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb  4 16:24:40.731: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 16:24:40.731: INFO: Found 0 / 1
Feb  4 16:24:41.740: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 16:24:41.740: INFO: Found 0 / 1
Feb  4 16:24:42.731: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 16:24:42.731: INFO: Found 0 / 1
Feb  4 16:24:43.733: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 16:24:43.733: INFO: Found 0 / 1
Feb  4 16:24:44.733: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 16:24:44.734: INFO: Found 1 / 1
Feb  4 16:24:44.734: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  4 16:24:44.739: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 16:24:44.740: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  4 16:24:44.740: INFO: wait on redis-master startup in e2e-tests-kubectl-45tgp 
Feb  4 16:24:44.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 logs redis-master-sp86x redis-master --namespace=e2e-tests-kubectl-45tgp'
Feb  4 16:24:45.020: INFO: stderr: ""
Feb  4 16:24:45.020: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 04 Feb 16:24:43.560 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 04 Feb 16:24:43.560 # Server started, Redis version 3.2.12\n1:M 04 Feb 16:24:43.560 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 04 Feb 16:24:43.560 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb  4 16:24:45.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-45tgp'
Feb  4 16:24:45.358: INFO: stderr: ""
Feb  4 16:24:45.358: INFO: stdout: "service/rm2 exposed\n"
Feb  4 16:24:45.367: INFO: Service rm2 in namespace e2e-tests-kubectl-45tgp found.
STEP: exposing service
Feb  4 16:24:47.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-45tgp'
Feb  4 16:24:47.534: INFO: stderr: ""
Feb  4 16:24:47.534: INFO: stdout: "service/rm3 exposed\n"
Feb  4 16:24:47.553: INFO: Service rm3 in namespace e2e-tests-kubectl-45tgp found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:24:49.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-45tgp" for this suite.
Feb  4 16:25:13.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:25:13.967: INFO: namespace: e2e-tests-kubectl-45tgp, resource: bindings, ignored listing per whitelist
Feb  4 16:25:14.082: INFO: namespace e2e-tests-kubectl-45tgp deletion completed in 24.465794086s

• [SLOW TEST:36.402 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:25:14.083: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-5t8x
STEP: Creating a pod to test atomic-volume-subpath
Feb  4 16:25:14.302: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5t8x" in namespace "e2e-tests-subpath-z2flh" to be "success or failure"
Feb  4 16:25:14.307: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.361959ms
Feb  4 16:25:16.323: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020485897s
Feb  4 16:25:18.329: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02676834s
Feb  4 16:25:20.347: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044729574s
Feb  4 16:25:22.356: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Pending", Reason="", readiness=false. Elapsed: 8.053981127s
Feb  4 16:25:24.363: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Running", Reason="", readiness=false. Elapsed: 10.060539567s
Feb  4 16:25:26.379: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Running", Reason="", readiness=false. Elapsed: 12.076739006s
Feb  4 16:25:28.386: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Running", Reason="", readiness=false. Elapsed: 14.084078197s
Feb  4 16:25:30.393: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Running", Reason="", readiness=false. Elapsed: 16.090547905s
Feb  4 16:25:32.399: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Running", Reason="", readiness=false. Elapsed: 18.097145537s
Feb  4 16:25:34.406: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Running", Reason="", readiness=false. Elapsed: 20.103495779s
Feb  4 16:25:36.419: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Running", Reason="", readiness=false. Elapsed: 22.116821878s
Feb  4 16:25:38.425: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Running", Reason="", readiness=false. Elapsed: 24.122268468s
Feb  4 16:25:40.431: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Running", Reason="", readiness=false. Elapsed: 26.128762074s
Feb  4 16:25:42.437: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Running", Reason="", readiness=false. Elapsed: 28.135245168s
Feb  4 16:25:44.450: INFO: Pod "pod-subpath-test-projected-5t8x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.147738912s
STEP: Saw pod success
Feb  4 16:25:44.451: INFO: Pod "pod-subpath-test-projected-5t8x" satisfied condition "success or failure"
Feb  4 16:25:44.467: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-subpath-test-projected-5t8x container test-container-subpath-projected-5t8x: <nil>
STEP: delete the pod
Feb  4 16:25:44.627: INFO: Waiting for pod pod-subpath-test-projected-5t8x to disappear
Feb  4 16:25:44.663: INFO: Pod pod-subpath-test-projected-5t8x no longer exists
STEP: Deleting pod pod-subpath-test-projected-5t8x
Feb  4 16:25:44.663: INFO: Deleting pod "pod-subpath-test-projected-5t8x" in namespace "e2e-tests-subpath-z2flh"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:25:44.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-z2flh" for this suite.
Feb  4 16:25:52.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:25:53.098: INFO: namespace: e2e-tests-subpath-z2flh, resource: bindings, ignored listing per whitelist
Feb  4 16:25:53.124: INFO: namespace e2e-tests-subpath-z2flh deletion completed in 8.438056738s

• [SLOW TEST:39.042 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:25:53.130: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-8ac01b19-2899-11e9-9eab-8e57f341003b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:26:01.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-spwpb" for this suite.
Feb  4 16:26:25.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:26:25.822: INFO: namespace: e2e-tests-configmap-spwpb, resource: bindings, ignored listing per whitelist
Feb  4 16:26:26.164: INFO: namespace e2e-tests-configmap-spwpb deletion completed in 24.465062209s

• [SLOW TEST:33.034 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:26:26.167: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 16:26:26.374: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e5d7e9b-2899-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-t7vbq" to be "success or failure"
Feb  4 16:26:26.378: INFO: Pod "downwardapi-volume-9e5d7e9b-2899-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.547862ms
Feb  4 16:26:28.406: INFO: Pod "downwardapi-volume-9e5d7e9b-2899-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03221484s
Feb  4 16:26:30.420: INFO: Pod "downwardapi-volume-9e5d7e9b-2899-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046363927s
STEP: Saw pod success
Feb  4 16:26:30.420: INFO: Pod "downwardapi-volume-9e5d7e9b-2899-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:26:30.424: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downwardapi-volume-9e5d7e9b-2899-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 16:26:30.584: INFO: Waiting for pod downwardapi-volume-9e5d7e9b-2899-11e9-9eab-8e57f341003b to disappear
Feb  4 16:26:30.590: INFO: Pod downwardapi-volume-9e5d7e9b-2899-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:26:30.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t7vbq" for this suite.
Feb  4 16:26:36.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:26:36.718: INFO: namespace: e2e-tests-projected-t7vbq, resource: bindings, ignored listing per whitelist
Feb  4 16:26:37.140: INFO: namespace e2e-tests-projected-t7vbq deletion completed in 6.541188766s

• [SLOW TEST:10.973 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:26:37.143: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb  4 16:26:37.390: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-pgdf9,SelfLink:/api/v1/namespaces/e2e-tests-watch-pgdf9/configmaps/e2e-watch-test-resource-version,UID:a4e6d35c-2899-11e9-9959-0a580af41676,ResourceVersion:3653,Generation:0,CreationTimestamp:2019-02-04 16:26:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  4 16:26:37.391: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-pgdf9,SelfLink:/api/v1/namespaces/e2e-tests-watch-pgdf9/configmaps/e2e-watch-test-resource-version,UID:a4e6d35c-2899-11e9-9959-0a580af41676,ResourceVersion:3654,Generation:0,CreationTimestamp:2019-02-04 16:26:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:26:37.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-pgdf9" for this suite.
Feb  4 16:26:43.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:26:43.554: INFO: namespace: e2e-tests-watch-pgdf9, resource: bindings, ignored listing per whitelist
Feb  4 16:26:43.750: INFO: namespace e2e-tests-watch-pgdf9 deletion completed in 6.351053131s

• [SLOW TEST:6.608 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:26:43.754: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb  4 16:26:43.995: INFO: Waiting up to 5m0s for pod "pod-a8df0a8b-2899-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-pvd27" to be "success or failure"
Feb  4 16:26:43.999: INFO: Pod "pod-a8df0a8b-2899-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.608425ms
Feb  4 16:26:46.005: INFO: Pod "pod-a8df0a8b-2899-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009603752s
Feb  4 16:26:48.012: INFO: Pod "pod-a8df0a8b-2899-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016502954s
STEP: Saw pod success
Feb  4 16:26:48.012: INFO: Pod "pod-a8df0a8b-2899-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:26:48.017: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-a8df0a8b-2899-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 16:26:48.163: INFO: Waiting for pod pod-a8df0a8b-2899-11e9-9eab-8e57f341003b to disappear
Feb  4 16:26:48.168: INFO: Pod pod-a8df0a8b-2899-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:26:48.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pvd27" for this suite.
Feb  4 16:26:54.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:26:54.239: INFO: namespace: e2e-tests-emptydir-pvd27, resource: bindings, ignored listing per whitelist
Feb  4 16:26:54.532: INFO: namespace e2e-tests-emptydir-pvd27 deletion completed in 6.358412012s

• [SLOW TEST:10.778 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:26:54.532: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  4 16:26:54.687: INFO: Waiting up to 5m0s for pod "downward-api-af3e7918-2899-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-jb7rx" to be "success or failure"
Feb  4 16:26:54.692: INFO: Pod "downward-api-af3e7918-2899-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.454338ms
Feb  4 16:26:56.717: INFO: Pod "downward-api-af3e7918-2899-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03024064s
Feb  4 16:26:58.723: INFO: Pod "downward-api-af3e7918-2899-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036728027s
STEP: Saw pod success
Feb  4 16:26:58.723: INFO: Pod "downward-api-af3e7918-2899-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:26:58.730: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downward-api-af3e7918-2899-11e9-9eab-8e57f341003b container dapi-container: <nil>
STEP: delete the pod
Feb  4 16:26:58.903: INFO: Waiting for pod downward-api-af3e7918-2899-11e9-9eab-8e57f341003b to disappear
Feb  4 16:26:58.917: INFO: Pod downward-api-af3e7918-2899-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:26:58.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jb7rx" for this suite.
Feb  4 16:27:04.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:27:05.141: INFO: namespace: e2e-tests-downward-api-jb7rx, resource: bindings, ignored listing per whitelist
Feb  4 16:27:05.341: INFO: namespace e2e-tests-downward-api-jb7rx deletion completed in 6.408112054s

• [SLOW TEST:10.809 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:27:05.347: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-6ltb2
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb  4 16:27:05.607: INFO: Found 0 stateful pods, waiting for 3
Feb  4 16:27:15.625: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  4 16:27:15.625: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  4 16:27:15.625: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb  4 16:27:15.675: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb  4 16:27:25.736: INFO: Updating stateful set ss2
Feb  4 16:27:25.745: INFO: Waiting for Pod e2e-tests-statefulset-6ltb2/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb  4 16:27:35.903: INFO: Found 2 stateful pods, waiting for 3
Feb  4 16:27:45.925: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  4 16:27:45.925: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  4 16:27:45.925: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb  4 16:27:45.961: INFO: Updating stateful set ss2
Feb  4 16:27:45.971: INFO: Waiting for Pod e2e-tests-statefulset-6ltb2/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  4 16:27:56.034: INFO: Updating stateful set ss2
Feb  4 16:27:56.050: INFO: Waiting for StatefulSet e2e-tests-statefulset-6ltb2/ss2 to complete update
Feb  4 16:27:56.050: INFO: Waiting for Pod e2e-tests-statefulset-6ltb2/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  4 16:28:06.099: INFO: Waiting for StatefulSet e2e-tests-statefulset-6ltb2/ss2 to complete update
Feb  4 16:28:06.099: INFO: Waiting for Pod e2e-tests-statefulset-6ltb2/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  4 16:28:16.062: INFO: Deleting all statefulset in ns e2e-tests-statefulset-6ltb2
Feb  4 16:28:16.066: INFO: Scaling statefulset ss2 to 0
Feb  4 16:28:46.135: INFO: Waiting for statefulset status.replicas updated to 0
Feb  4 16:28:46.140: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:28:46.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-6ltb2" for this suite.
Feb  4 16:28:54.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:28:54.836: INFO: namespace: e2e-tests-statefulset-6ltb2, resource: bindings, ignored listing per whitelist
Feb  4 16:28:54.949: INFO: namespace e2e-tests-statefulset-6ltb2 deletion completed in 8.619057019s

• [SLOW TEST:109.603 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:28:54.950: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb  4 16:28:55.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:28:56.010: INFO: stderr: ""
Feb  4 16:28:56.010: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  4 16:28:56.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:28:56.379: INFO: stderr: ""
Feb  4 16:28:56.379: INFO: stdout: "update-demo-nautilus-bp2nv update-demo-nautilus-qhc64 "
Feb  4 16:28:56.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-bp2nv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:28:56.732: INFO: stderr: ""
Feb  4 16:28:56.732: INFO: stdout: ""
Feb  4 16:28:56.732: INFO: update-demo-nautilus-bp2nv is created but not running
Feb  4 16:29:01.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:02.021: INFO: stderr: ""
Feb  4 16:29:02.021: INFO: stdout: "update-demo-nautilus-bp2nv update-demo-nautilus-qhc64 "
Feb  4 16:29:02.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-bp2nv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:02.277: INFO: stderr: ""
Feb  4 16:29:02.277: INFO: stdout: ""
Feb  4 16:29:02.277: INFO: update-demo-nautilus-bp2nv is created but not running
Feb  4 16:29:07.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:07.537: INFO: stderr: ""
Feb  4 16:29:07.537: INFO: stdout: "update-demo-nautilus-bp2nv update-demo-nautilus-qhc64 "
Feb  4 16:29:07.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-bp2nv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:07.824: INFO: stderr: ""
Feb  4 16:29:07.824: INFO: stdout: "true"
Feb  4 16:29:07.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-bp2nv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:07.927: INFO: stderr: ""
Feb  4 16:29:07.928: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  4 16:29:07.928: INFO: validating pod update-demo-nautilus-bp2nv
Feb  4 16:29:08.017: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  4 16:29:08.017: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  4 16:29:08.017: INFO: update-demo-nautilus-bp2nv is verified up and running
Feb  4 16:29:08.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-qhc64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:08.270: INFO: stderr: ""
Feb  4 16:29:08.270: INFO: stdout: "true"
Feb  4 16:29:08.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-qhc64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:08.549: INFO: stderr: ""
Feb  4 16:29:08.549: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  4 16:29:08.549: INFO: validating pod update-demo-nautilus-qhc64
Feb  4 16:29:08.649: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  4 16:29:08.649: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  4 16:29:08.649: INFO: update-demo-nautilus-qhc64 is verified up and running
STEP: rolling-update to new replication controller
Feb  4 16:29:08.656: INFO: scanned /root for discovery docs: <nil>
Feb  4 16:29:08.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:33.936: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb  4 16:29:33.936: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  4 16:29:33.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:34.235: INFO: stderr: ""
Feb  4 16:29:34.235: INFO: stdout: "update-demo-kitten-7wlt6 update-demo-kitten-x4vr4 "
Feb  4 16:29:34.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-kitten-7wlt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:34.488: INFO: stderr: ""
Feb  4 16:29:34.488: INFO: stdout: "true"
Feb  4 16:29:34.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-kitten-7wlt6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:34.825: INFO: stderr: ""
Feb  4 16:29:34.825: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb  4 16:29:34.825: INFO: validating pod update-demo-kitten-7wlt6
Feb  4 16:29:34.953: INFO: got data: {
  "image": "kitten.jpg"
}

Feb  4 16:29:34.953: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb  4 16:29:34.953: INFO: update-demo-kitten-7wlt6 is verified up and running
Feb  4 16:29:34.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-kitten-x4vr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:35.225: INFO: stderr: ""
Feb  4 16:29:35.225: INFO: stdout: "true"
Feb  4 16:29:35.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-kitten-x4vr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lqf82'
Feb  4 16:29:35.545: INFO: stderr: ""
Feb  4 16:29:35.545: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb  4 16:29:35.545: INFO: validating pod update-demo-kitten-x4vr4
Feb  4 16:29:35.647: INFO: got data: {
  "image": "kitten.jpg"
}

Feb  4 16:29:35.647: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb  4 16:29:35.647: INFO: update-demo-kitten-x4vr4 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:29:35.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lqf82" for this suite.
Feb  4 16:29:59.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:30:00.097: INFO: namespace: e2e-tests-kubectl-lqf82, resource: bindings, ignored listing per whitelist
Feb  4 16:30:00.221: INFO: namespace e2e-tests-kubectl-lqf82 deletion completed in 24.565785305s

• [SLOW TEST:65.275 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:30:00.229: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-1e016604-289a-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 16:30:00.544: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1e043e28-289a-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-vpdhn" to be "success or failure"
Feb  4 16:30:00.548: INFO: Pod "pod-projected-configmaps-1e043e28-289a-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.444855ms
Feb  4 16:30:02.555: INFO: Pod "pod-projected-configmaps-1e043e28-289a-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011467722s
Feb  4 16:30:04.630: INFO: Pod "pod-projected-configmaps-1e043e28-289a-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085900773s
STEP: Saw pod success
Feb  4 16:30:04.630: INFO: Pod "pod-projected-configmaps-1e043e28-289a-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:30:04.635: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-projected-configmaps-1e043e28-289a-11e9-9eab-8e57f341003b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 16:30:04.876: INFO: Waiting for pod pod-projected-configmaps-1e043e28-289a-11e9-9eab-8e57f341003b to disappear
Feb  4 16:30:04.912: INFO: Pod pod-projected-configmaps-1e043e28-289a-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:30:04.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vpdhn" for this suite.
Feb  4 16:30:11.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:30:11.220: INFO: namespace: e2e-tests-projected-vpdhn, resource: bindings, ignored listing per whitelist
Feb  4 16:30:11.344: INFO: namespace e2e-tests-projected-vpdhn deletion completed in 6.425387052s

• [SLOW TEST:11.115 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:30:11.350: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 16:30:11.553: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2492c860-289a-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-g5rfm" to be "success or failure"
Feb  4 16:30:11.558: INFO: Pod "downwardapi-volume-2492c860-289a-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.049983ms
Feb  4 16:30:13.567: INFO: Pod "downwardapi-volume-2492c860-289a-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013644406s
Feb  4 16:30:15.573: INFO: Pod "downwardapi-volume-2492c860-289a-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019648808s
STEP: Saw pod success
Feb  4 16:30:15.573: INFO: Pod "downwardapi-volume-2492c860-289a-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:30:15.577: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downwardapi-volume-2492c860-289a-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 16:30:15.703: INFO: Waiting for pod downwardapi-volume-2492c860-289a-11e9-9eab-8e57f341003b to disappear
Feb  4 16:30:15.707: INFO: Pod downwardapi-volume-2492c860-289a-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:30:15.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-g5rfm" for this suite.
Feb  4 16:30:21.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:30:21.894: INFO: namespace: e2e-tests-downward-api-g5rfm, resource: bindings, ignored listing per whitelist
Feb  4 16:30:22.028: INFO: namespace e2e-tests-downward-api-g5rfm deletion completed in 6.310216617s

• [SLOW TEST:10.678 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:30:22.029: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb  4 16:30:22.775: INFO: Pod name wrapped-volume-race-2b437ea3-289a-11e9-9eab-8e57f341003b: Found 0 pods out of 5
Feb  4 16:30:27.787: INFO: Pod name wrapped-volume-race-2b437ea3-289a-11e9-9eab-8e57f341003b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2b437ea3-289a-11e9-9eab-8e57f341003b in namespace e2e-tests-emptydir-wrapper-d7rdk, will wait for the garbage collector to delete the pods
Feb  4 16:30:41.900: INFO: Deleting ReplicationController wrapped-volume-race-2b437ea3-289a-11e9-9eab-8e57f341003b took: 13.348814ms
Feb  4 16:30:42.001: INFO: Terminating ReplicationController wrapped-volume-race-2b437ea3-289a-11e9-9eab-8e57f341003b pods took: 100.602295ms
STEP: Creating RC which spawns configmap-volume pods
Feb  4 16:31:22.252: INFO: Pod name wrapped-volume-race-4eb41fa9-289a-11e9-9eab-8e57f341003b: Found 0 pods out of 5
Feb  4 16:31:27.264: INFO: Pod name wrapped-volume-race-4eb41fa9-289a-11e9-9eab-8e57f341003b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4eb41fa9-289a-11e9-9eab-8e57f341003b in namespace e2e-tests-emptydir-wrapper-d7rdk, will wait for the garbage collector to delete the pods
Feb  4 16:31:41.384: INFO: Deleting ReplicationController wrapped-volume-race-4eb41fa9-289a-11e9-9eab-8e57f341003b took: 14.402564ms
Feb  4 16:31:41.488: INFO: Terminating ReplicationController wrapped-volume-race-4eb41fa9-289a-11e9-9eab-8e57f341003b pods took: 103.38951ms
STEP: Creating RC which spawns configmap-volume pods
Feb  4 16:32:20.138: INFO: Pod name wrapped-volume-race-7135093d-289a-11e9-9eab-8e57f341003b: Found 0 pods out of 5
Feb  4 16:32:25.150: INFO: Pod name wrapped-volume-race-7135093d-289a-11e9-9eab-8e57f341003b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7135093d-289a-11e9-9eab-8e57f341003b in namespace e2e-tests-emptydir-wrapper-d7rdk, will wait for the garbage collector to delete the pods
Feb  4 16:32:39.275: INFO: Deleting ReplicationController wrapped-volume-race-7135093d-289a-11e9-9eab-8e57f341003b took: 22.358887ms
Feb  4 16:32:39.375: INFO: Terminating ReplicationController wrapped-volume-race-7135093d-289a-11e9-9eab-8e57f341003b pods took: 100.864626ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:33:16.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-d7rdk" for this suite.
Feb  4 16:33:26.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:33:26.764: INFO: namespace: e2e-tests-emptydir-wrapper-d7rdk, resource: bindings, ignored listing per whitelist
Feb  4 16:33:27.033: INFO: namespace e2e-tests-emptydir-wrapper-d7rdk deletion completed in 10.466970604s

• [SLOW TEST:185.005 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:33:27.038: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9938ade5-289a-11e9-9eab-8e57f341003b
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-9938ade5-289a-11e9-9eab-8e57f341003b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:34:53.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-f6l8b" for this suite.
Feb  4 16:35:17.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:35:17.366: INFO: namespace: e2e-tests-projected-f6l8b, resource: bindings, ignored listing per whitelist
Feb  4 16:35:17.458: INFO: namespace e2e-tests-projected-f6l8b deletion completed in 24.341588354s

• [SLOW TEST:110.421 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:35:17.468: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-db089e40-289a-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 16:35:17.658: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-db0a58ad-289a-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-crl2p" to be "success or failure"
Feb  4 16:35:17.675: INFO: Pod "pod-projected-secrets-db0a58ad-289a-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.195261ms
Feb  4 16:35:19.681: INFO: Pod "pod-projected-secrets-db0a58ad-289a-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022431038s
Feb  4 16:35:21.686: INFO: Pod "pod-projected-secrets-db0a58ad-289a-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027788318s
STEP: Saw pod success
Feb  4 16:35:21.687: INFO: Pod "pod-projected-secrets-db0a58ad-289a-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:35:21.692: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-projected-secrets-db0a58ad-289a-11e9-9eab-8e57f341003b container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  4 16:35:21.871: INFO: Waiting for pod pod-projected-secrets-db0a58ad-289a-11e9-9eab-8e57f341003b to disappear
Feb  4 16:35:21.877: INFO: Pod pod-projected-secrets-db0a58ad-289a-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:35:21.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-crl2p" for this suite.
Feb  4 16:35:27.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:35:27.968: INFO: namespace: e2e-tests-projected-crl2p, resource: bindings, ignored listing per whitelist
Feb  4 16:35:28.292: INFO: namespace e2e-tests-projected-crl2p deletion completed in 6.4077861s

• [SLOW TEST:10.824 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:35:28.295: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-e17aa12e-289a-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 16:35:28.484: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e17c1fa9-289a-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-pbvml" to be "success or failure"
Feb  4 16:35:28.488: INFO: Pod "pod-projected-secrets-e17c1fa9-289a-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.766191ms
Feb  4 16:35:30.496: INFO: Pod "pod-projected-secrets-e17c1fa9-289a-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012441477s
Feb  4 16:35:32.515: INFO: Pod "pod-projected-secrets-e17c1fa9-289a-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031806528s
STEP: Saw pod success
Feb  4 16:35:32.516: INFO: Pod "pod-projected-secrets-e17c1fa9-289a-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:35:32.522: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-projected-secrets-e17c1fa9-289a-11e9-9eab-8e57f341003b container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  4 16:35:32.589: INFO: Waiting for pod pod-projected-secrets-e17c1fa9-289a-11e9-9eab-8e57f341003b to disappear
Feb  4 16:35:32.594: INFO: Pod pod-projected-secrets-e17c1fa9-289a-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:35:32.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pbvml" for this suite.
Feb  4 16:35:38.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:35:38.810: INFO: namespace: e2e-tests-projected-pbvml, resource: bindings, ignored listing per whitelist
Feb  4 16:35:38.951: INFO: namespace e2e-tests-projected-pbvml deletion completed in 6.352239977s

• [SLOW TEST:10.656 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:35:38.955: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb  4 16:35:39.127: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  4 16:35:39.139: INFO: Waiting for terminating namespaces to be deleted...
Feb  4 16:35:39.143: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf before test
Feb  4 16:35:39.268: INFO: ark-5f9c9897c5-gmcbs from heptio-ark started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.268: INFO: 	Container ark ready: true, restart count 3
Feb  4 16:35:39.268: INFO: node-exporter-wzptg from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.269: INFO: 	Container node-exporter ready: true, restart count 0
Feb  4 16:35:39.269: INFO: sonobuoy-systemd-logs-daemon-set-fde55ff1e0804945-6mf79 from heptio-sonobuoy started at 2019-02-04 16:18:54 +0000 UTC (2 container statuses recorded)
Feb  4 16:35:39.269: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  4 16:35:39.269: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  4 16:35:39.269: INFO: cluster-autoscaler-5f8478765d-8rjp2 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.269: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Feb  4 16:35:39.269: INFO: openvpn-client-7c9496f697-sq426 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (2 container statuses recorded)
Feb  4 16:35:39.270: INFO: 	Container dnat-controller ready: true, restart count 0
Feb  4 16:35:39.270: INFO: 	Container openvpn-client ready: true, restart count 0
Feb  4 16:35:39.270: INFO: kube-dns-6fcf8486c9-fp64s from kube-system started at 2019-02-04 16:17:59 +0000 UTC (3 container statuses recorded)
Feb  4 16:35:39.270: INFO: 	Container dnsmasq ready: true, restart count 0
Feb  4 16:35:39.270: INFO: 	Container kubedns ready: true, restart count 0
Feb  4 16:35:39.270: INFO: 	Container sidecar ready: true, restart count 0
Feb  4 16:35:39.272: INFO: kube-dns-6fcf8486c9-cfvbk from kube-system started at 2019-02-04 16:16:04 +0000 UTC (3 container statuses recorded)
Feb  4 16:35:39.272: INFO: 	Container dnsmasq ready: true, restart count 0
Feb  4 16:35:39.272: INFO: 	Container kubedns ready: true, restart count 0
Feb  4 16:35:39.272: INFO: 	Container sidecar ready: true, restart count 0
Feb  4 16:35:39.272: INFO: kubernetes-dashboard-657fd84c97-4t2v8 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.272: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb  4 16:35:39.272: INFO: kube-dns-autoscaler-6686fffcd4-q7xr8 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.272: INFO: 	Container autoscaler ready: true, restart count 0
Feb  4 16:35:39.272: INFO: canal-7fvph from kube-system started at 2019-02-04 16:16:04 +0000 UTC (3 container statuses recorded)
Feb  4 16:35:39.272: INFO: 	Container calico-node ready: true, restart count 0
Feb  4 16:35:39.272: INFO: 	Container install-cni ready: true, restart count 0
Feb  4 16:35:39.272: INFO: 	Container kube-flannel ready: true, restart count 0
Feb  4 16:35:39.272: INFO: restic-9gtnw from heptio-ark started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.272: INFO: 	Container ark ready: true, restart count 0
Feb  4 16:35:39.272: INFO: tiller-deploy-7cf4c86f8-ntzqt from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.272: INFO: 	Container tiller ready: true, restart count 0
Feb  4 16:35:39.273: INFO: metrics-server-6b98b49585-cwwl9 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.273: INFO: 	Container metrics-server ready: true, restart count 0
Feb  4 16:35:39.273: INFO: kube-proxy-lwvdf from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.273: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  4 16:35:39.273: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 before test
Feb  4 16:35:39.315: INFO: sonobuoy-systemd-logs-daemon-set-fde55ff1e0804945-k8lln from heptio-sonobuoy started at 2019-02-04 16:18:54 +0000 UTC (2 container statuses recorded)
Feb  4 16:35:39.315: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  4 16:35:39.315: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  4 16:35:39.315: INFO: restic-2rnhq from heptio-ark started at 2019-02-04 16:17:55 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.315: INFO: 	Container ark ready: true, restart count 0
Feb  4 16:35:39.315: INFO: canal-9jx2t from kube-system started at 2019-02-04 16:17:55 +0000 UTC (3 container statuses recorded)
Feb  4 16:35:39.316: INFO: 	Container calico-node ready: true, restart count 0
Feb  4 16:35:39.316: INFO: 	Container install-cni ready: true, restart count 0
Feb  4 16:35:39.316: INFO: 	Container kube-flannel ready: true, restart count 0
Feb  4 16:35:39.316: INFO: node-exporter-xtlhp from kube-system started at 2019-02-04 16:17:55 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.316: INFO: 	Container node-exporter ready: true, restart count 0
Feb  4 16:35:39.316: INFO: sonobuoy-e2e-job-45efe79f680344de from heptio-sonobuoy started at 2019-02-04 16:18:54 +0000 UTC (2 container statuses recorded)
Feb  4 16:35:39.317: INFO: 	Container e2e ready: true, restart count 0
Feb  4 16:35:39.317: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  4 16:35:39.317: INFO: kube-proxy-7bbjk from kube-system started at 2019-02-04 16:17:55 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.317: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  4 16:35:39.317: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-04 16:18:44 +0000 UTC (1 container statuses recorded)
Feb  4 16:35:39.317: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ea6cde99-289a-11e9-9eab-8e57f341003b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-ea6cde99-289a-11e9-9eab-8e57f341003b off the node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ea6cde99-289a-11e9-9eab-8e57f341003b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:35:47.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-rxr6n" for this suite.
Feb  4 16:36:03.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:36:03.789: INFO: namespace: e2e-tests-sched-pred-rxr6n, resource: bindings, ignored listing per whitelist
Feb  4 16:36:03.913: INFO: namespace e2e-tests-sched-pred-rxr6n deletion completed in 16.333047717s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:24.960 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:36:03.917: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb  4 16:36:04.210: INFO: Waiting up to 5m0s for pod "pod-f6c87cc3-289a-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-hzcfz" to be "success or failure"
Feb  4 16:36:04.218: INFO: Pod "pod-f6c87cc3-289a-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.823997ms
Feb  4 16:36:06.231: INFO: Pod "pod-f6c87cc3-289a-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020458253s
STEP: Saw pod success
Feb  4 16:36:06.231: INFO: Pod "pod-f6c87cc3-289a-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:36:06.234: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-f6c87cc3-289a-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 16:36:06.331: INFO: Waiting for pod pod-f6c87cc3-289a-11e9-9eab-8e57f341003b to disappear
Feb  4 16:36:06.336: INFO: Pod pod-f6c87cc3-289a-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:36:06.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hzcfz" for this suite.
Feb  4 16:36:12.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:36:12.727: INFO: namespace: e2e-tests-emptydir-hzcfz, resource: bindings, ignored listing per whitelist
Feb  4 16:36:12.913: INFO: namespace e2e-tests-emptydir-hzcfz deletion completed in 6.569033061s

• [SLOW TEST:8.997 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:36:12.916: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  4 16:36:17.682: INFO: Successfully updated pod "labelsupdatefc1228e8-289a-11e9-9eab-8e57f341003b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:36:19.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-z9tzk" for this suite.
Feb  4 16:36:43.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:36:43.944: INFO: namespace: e2e-tests-projected-z9tzk, resource: bindings, ignored listing per whitelist
Feb  4 16:36:44.158: INFO: namespace e2e-tests-projected-z9tzk deletion completed in 24.339536316s

• [SLOW TEST:31.242 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:36:44.162: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-0eb5a2da-289b-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 16:36:44.384: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ebaac6f-289b-11e9-9eab-8e57f341003b" in namespace "e2e-tests-configmap-48275" to be "success or failure"
Feb  4 16:36:44.399: INFO: Pod "pod-configmaps-0ebaac6f-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.125778ms
Feb  4 16:36:46.405: INFO: Pod "pod-configmaps-0ebaac6f-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020978336s
Feb  4 16:36:48.512: INFO: Pod "pod-configmaps-0ebaac6f-289b-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.127314225s
STEP: Saw pod success
Feb  4 16:36:48.512: INFO: Pod "pod-configmaps-0ebaac6f-289b-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:36:48.517: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-configmaps-0ebaac6f-289b-11e9-9eab-8e57f341003b container configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 16:36:48.573: INFO: Waiting for pod pod-configmaps-0ebaac6f-289b-11e9-9eab-8e57f341003b to disappear
Feb  4 16:36:48.583: INFO: Pod pod-configmaps-0ebaac6f-289b-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:36:48.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-48275" for this suite.
Feb  4 16:36:54.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:36:54.943: INFO: namespace: e2e-tests-configmap-48275, resource: bindings, ignored listing per whitelist
Feb  4 16:36:55.040: INFO: namespace e2e-tests-configmap-48275 deletion completed in 6.449714101s

• [SLOW TEST:10.878 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:36:55.044: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  4 16:36:55.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-zsllj'
Feb  4 16:36:56.433: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  4 16:36:56.433: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb  4 16:36:56.445: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb  4 16:36:56.461: INFO: scanned /root for discovery docs: <nil>
Feb  4 16:36:56.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-zsllj'
Feb  4 16:37:09.823: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb  4 16:37:09.823: INFO: stdout: "Created e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7\nScaling up e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb  4 16:37:09.823: INFO: stdout: "Created e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7\nScaling up e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb  4 16:37:09.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-zsllj'
Feb  4 16:37:10.121: INFO: stderr: ""
Feb  4 16:37:10.121: INFO: stdout: "e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7-jxtn5 "
Feb  4 16:37:10.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7-jxtn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zsllj'
Feb  4 16:37:10.379: INFO: stderr: ""
Feb  4 16:37:10.379: INFO: stdout: "true"
Feb  4 16:37:10.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7-jxtn5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zsllj'
Feb  4 16:37:10.796: INFO: stderr: ""
Feb  4 16:37:10.796: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb  4 16:37:10.796: INFO: e2e-test-nginx-rc-c99745b14260e1d75c413a45061ecca7-jxtn5 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Feb  4 16:37:10.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-zsllj'
Feb  4 16:37:11.327: INFO: stderr: ""
Feb  4 16:37:11.327: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:37:11.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zsllj" for this suite.
Feb  4 16:37:35.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:37:35.489: INFO: namespace: e2e-tests-kubectl-zsllj, resource: bindings, ignored listing per whitelist
Feb  4 16:37:35.865: INFO: namespace e2e-tests-kubectl-zsllj deletion completed in 24.470343325s

• [SLOW TEST:40.822 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:37:35.869: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb  4 16:37:36.014: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  4 16:37:36.025: INFO: Waiting for terminating namespaces to be deleted...
Feb  4 16:37:36.031: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf before test
Feb  4 16:37:36.129: INFO: node-exporter-wzptg from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.129: INFO: 	Container node-exporter ready: true, restart count 0
Feb  4 16:37:36.129: INFO: sonobuoy-systemd-logs-daemon-set-fde55ff1e0804945-6mf79 from heptio-sonobuoy started at 2019-02-04 16:18:54 +0000 UTC (2 container statuses recorded)
Feb  4 16:37:36.129: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  4 16:37:36.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  4 16:37:36.129: INFO: ark-5f9c9897c5-gmcbs from heptio-ark started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.130: INFO: 	Container ark ready: true, restart count 3
Feb  4 16:37:36.130: INFO: openvpn-client-7c9496f697-sq426 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (2 container statuses recorded)
Feb  4 16:37:36.130: INFO: 	Container dnat-controller ready: true, restart count 0
Feb  4 16:37:36.130: INFO: 	Container openvpn-client ready: true, restart count 0
Feb  4 16:37:36.130: INFO: kube-dns-6fcf8486c9-fp64s from kube-system started at 2019-02-04 16:17:59 +0000 UTC (3 container statuses recorded)
Feb  4 16:37:36.130: INFO: 	Container dnsmasq ready: true, restart count 0
Feb  4 16:37:36.131: INFO: 	Container kubedns ready: true, restart count 0
Feb  4 16:37:36.131: INFO: 	Container sidecar ready: true, restart count 0
Feb  4 16:37:36.131: INFO: cluster-autoscaler-5f8478765d-8rjp2 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.131: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Feb  4 16:37:36.131: INFO: kubernetes-dashboard-657fd84c97-4t2v8 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.131: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb  4 16:37:36.132: INFO: kube-dns-autoscaler-6686fffcd4-q7xr8 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.132: INFO: 	Container autoscaler ready: true, restart count 0
Feb  4 16:37:36.132: INFO: canal-7fvph from kube-system started at 2019-02-04 16:16:04 +0000 UTC (3 container statuses recorded)
Feb  4 16:37:36.132: INFO: 	Container calico-node ready: true, restart count 0
Feb  4 16:37:36.132: INFO: 	Container install-cni ready: true, restart count 0
Feb  4 16:37:36.132: INFO: 	Container kube-flannel ready: true, restart count 0
Feb  4 16:37:36.133: INFO: restic-9gtnw from heptio-ark started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.133: INFO: 	Container ark ready: true, restart count 0
Feb  4 16:37:36.133: INFO: kube-dns-6fcf8486c9-cfvbk from kube-system started at 2019-02-04 16:16:04 +0000 UTC (3 container statuses recorded)
Feb  4 16:37:36.133: INFO: 	Container dnsmasq ready: true, restart count 0
Feb  4 16:37:36.133: INFO: 	Container kubedns ready: true, restart count 0
Feb  4 16:37:36.134: INFO: 	Container sidecar ready: true, restart count 0
Feb  4 16:37:36.134: INFO: metrics-server-6b98b49585-cwwl9 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.134: INFO: 	Container metrics-server ready: true, restart count 0
Feb  4 16:37:36.134: INFO: kube-proxy-lwvdf from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.134: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  4 16:37:36.135: INFO: tiller-deploy-7cf4c86f8-ntzqt from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.135: INFO: 	Container tiller ready: true, restart count 0
Feb  4 16:37:36.135: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 before test
Feb  4 16:37:36.177: INFO: restic-2rnhq from heptio-ark started at 2019-02-04 16:17:55 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.178: INFO: 	Container ark ready: true, restart count 0
Feb  4 16:37:36.178: INFO: canal-9jx2t from kube-system started at 2019-02-04 16:17:55 +0000 UTC (3 container statuses recorded)
Feb  4 16:37:36.178: INFO: 	Container calico-node ready: true, restart count 0
Feb  4 16:37:36.178: INFO: 	Container install-cni ready: true, restart count 0
Feb  4 16:37:36.178: INFO: 	Container kube-flannel ready: true, restart count 0
Feb  4 16:37:36.178: INFO: node-exporter-xtlhp from kube-system started at 2019-02-04 16:17:55 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.179: INFO: 	Container node-exporter ready: true, restart count 0
Feb  4 16:37:36.179: INFO: sonobuoy-e2e-job-45efe79f680344de from heptio-sonobuoy started at 2019-02-04 16:18:54 +0000 UTC (2 container statuses recorded)
Feb  4 16:37:36.179: INFO: 	Container e2e ready: true, restart count 0
Feb  4 16:37:36.179: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  4 16:37:36.179: INFO: sonobuoy-systemd-logs-daemon-set-fde55ff1e0804945-k8lln from heptio-sonobuoy started at 2019-02-04 16:18:54 +0000 UTC (2 container statuses recorded)
Feb  4 16:37:36.179: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  4 16:37:36.179: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  4 16:37:36.180: INFO: kube-proxy-7bbjk from kube-system started at 2019-02-04 16:17:55 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.180: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  4 16:37:36.180: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-04 16:18:44 +0000 UTC (1 container statuses recorded)
Feb  4 16:37:36.180: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
STEP: verifying the node has the label node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54
Feb  4 16:37:36.263: INFO: Pod ark-5f9c9897c5-gmcbs requesting resource cpu=10m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.263: INFO: Pod restic-2rnhq requesting resource cpu=5m on Node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54
Feb  4 16:37:36.263: INFO: Pod restic-9gtnw requesting resource cpu=5m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.263: INFO: Pod sonobuoy requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54
Feb  4 16:37:36.263: INFO: Pod sonobuoy-e2e-job-45efe79f680344de requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54
Feb  4 16:37:36.263: INFO: Pod sonobuoy-systemd-logs-daemon-set-fde55ff1e0804945-6mf79 requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.263: INFO: Pod sonobuoy-systemd-logs-daemon-set-fde55ff1e0804945-k8lln requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54
Feb  4 16:37:36.264: INFO: Pod canal-7fvph requesting resource cpu=250m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.264: INFO: Pod canal-9jx2t requesting resource cpu=250m on Node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54
Feb  4 16:37:36.264: INFO: Pod cluster-autoscaler-5f8478765d-8rjp2 requesting resource cpu=10m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.264: INFO: Pod kube-dns-6fcf8486c9-cfvbk requesting resource cpu=260m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.264: INFO: Pod kube-dns-6fcf8486c9-fp64s requesting resource cpu=260m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.264: INFO: Pod kube-dns-autoscaler-6686fffcd4-q7xr8 requesting resource cpu=20m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.264: INFO: Pod kube-proxy-7bbjk requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54
Feb  4 16:37:36.264: INFO: Pod kube-proxy-lwvdf requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.264: INFO: Pod kubernetes-dashboard-657fd84c97-4t2v8 requesting resource cpu=50m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.264: INFO: Pod metrics-server-6b98b49585-cwwl9 requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.264: INFO: Pod node-exporter-wzptg requesting resource cpu=3m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.264: INFO: Pod node-exporter-xtlhp requesting resource cpu=3m on Node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54
Feb  4 16:37:36.264: INFO: Pod openvpn-client-7c9496f697-sq426 requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
Feb  4 16:37:36.264: INFO: Pod tiller-deploy-7cf4c86f8-ntzqt requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2da9a61e-289b-11e9-9eab-8e57f341003b.1580368e3a08334c], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-8xqxh/filler-pod-2da9a61e-289b-11e9-9eab-8e57f341003b to machine-kubermatic-conformancecluster-61wiwtyxah-6rthf]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2da9a61e-289b-11e9-9eab-8e57f341003b.1580368e886893c3], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2da9a61e-289b-11e9-9eab-8e57f341003b.1580368e925368ce], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2da9a61e-289b-11e9-9eab-8e57f341003b.1580368ea5efbb71], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2dabe1d2-289b-11e9-9eab-8e57f341003b.1580368e3a9d7def], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-8xqxh/filler-pod-2dabe1d2-289b-11e9-9eab-8e57f341003b to machine-kubermatic-conformancecluster-61wiwtyxah-t5p54]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2dabe1d2-289b-11e9-9eab-8e57f341003b.1580368eda9d6c59], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2dabe1d2-289b-11e9-9eab-8e57f341003b.1580368f18df4548], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2dabe1d2-289b-11e9-9eab-8e57f341003b.1580368f3e696a7d], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1580368fa6bd0c84], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:37:43.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-8xqxh" for this suite.
Feb  4 16:37:49.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:37:49.777: INFO: namespace: e2e-tests-sched-pred-8xqxh, resource: bindings, ignored listing per whitelist
Feb  4 16:37:49.912: INFO: namespace e2e-tests-sched-pred-8xqxh deletion completed in 6.4041955s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:14.044 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:37:49.913: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  4 16:37:50.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-clv5q'
Feb  4 16:37:50.720: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  4 16:37:50.720: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Feb  4 16:37:50.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-clv5q'
Feb  4 16:37:51.100: INFO: stderr: ""
Feb  4 16:37:51.100: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:37:51.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-clv5q" for this suite.
Feb  4 16:37:57.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:37:57.596: INFO: namespace: e2e-tests-kubectl-clv5q, resource: bindings, ignored listing per whitelist
Feb  4 16:37:58.588: INFO: namespace e2e-tests-kubectl-clv5q deletion completed in 7.48296139s

• [SLOW TEST:8.675 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:37:58.590: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:38:02.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-nnbdn" for this suite.
Feb  4 16:39:11.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:39:11.186: INFO: namespace: e2e-tests-kubelet-test-nnbdn, resource: bindings, ignored listing per whitelist
Feb  4 16:39:11.362: INFO: namespace e2e-tests-kubelet-test-nnbdn deletion completed in 1m8.449857914s

• [SLOW TEST:72.772 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:39:11.367: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb  4 16:39:11.562: INFO: Waiting up to 5m0s for pod "pod-6674994f-289b-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-pfrmv" to be "success or failure"
Feb  4 16:39:11.567: INFO: Pod "pod-6674994f-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.330565ms
Feb  4 16:39:13.587: INFO: Pod "pod-6674994f-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024870155s
Feb  4 16:39:15.597: INFO: Pod "pod-6674994f-289b-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033916188s
STEP: Saw pod success
Feb  4 16:39:15.597: INFO: Pod "pod-6674994f-289b-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:39:15.603: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-6674994f-289b-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 16:39:15.678: INFO: Waiting for pod pod-6674994f-289b-11e9-9eab-8e57f341003b to disappear
Feb  4 16:39:15.682: INFO: Pod pod-6674994f-289b-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:39:15.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pfrmv" for this suite.
Feb  4 16:39:21.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:39:22.078: INFO: namespace: e2e-tests-emptydir-pfrmv, resource: bindings, ignored listing per whitelist
Feb  4 16:39:22.112: INFO: namespace e2e-tests-emptydir-pfrmv deletion completed in 6.424244808s

• [SLOW TEST:10.745 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:39:22.116: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 16:39:22.337: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb  4 16:39:22.360: INFO: Number of nodes with available pods: 0
Feb  4 16:39:22.360: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb  4 16:39:22.393: INFO: Number of nodes with available pods: 0
Feb  4 16:39:22.394: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:23.411: INFO: Number of nodes with available pods: 0
Feb  4 16:39:23.411: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:24.404: INFO: Number of nodes with available pods: 0
Feb  4 16:39:24.404: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:25.399: INFO: Number of nodes with available pods: 1
Feb  4 16:39:25.399: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb  4 16:39:25.437: INFO: Number of nodes with available pods: 0
Feb  4 16:39:25.437: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb  4 16:39:25.476: INFO: Number of nodes with available pods: 0
Feb  4 16:39:25.476: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:26.484: INFO: Number of nodes with available pods: 0
Feb  4 16:39:26.484: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:27.485: INFO: Number of nodes with available pods: 0
Feb  4 16:39:27.485: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:28.483: INFO: Number of nodes with available pods: 0
Feb  4 16:39:28.483: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:29.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:29.482: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:30.486: INFO: Number of nodes with available pods: 0
Feb  4 16:39:30.486: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:31.483: INFO: Number of nodes with available pods: 0
Feb  4 16:39:31.483: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:32.492: INFO: Number of nodes with available pods: 0
Feb  4 16:39:32.492: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:33.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:33.483: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:34.513: INFO: Number of nodes with available pods: 0
Feb  4 16:39:34.514: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:35.484: INFO: Number of nodes with available pods: 0
Feb  4 16:39:35.485: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:36.483: INFO: Number of nodes with available pods: 0
Feb  4 16:39:36.483: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:37.492: INFO: Number of nodes with available pods: 0
Feb  4 16:39:37.492: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:38.489: INFO: Number of nodes with available pods: 0
Feb  4 16:39:38.489: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:39.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:39.482: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:40.488: INFO: Number of nodes with available pods: 0
Feb  4 16:39:40.488: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:41.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:41.482: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:42.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:42.482: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:43.490: INFO: Number of nodes with available pods: 0
Feb  4 16:39:43.491: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:44.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:44.482: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:45.483: INFO: Number of nodes with available pods: 0
Feb  4 16:39:45.483: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:46.484: INFO: Number of nodes with available pods: 0
Feb  4 16:39:46.485: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:47.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:47.483: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:48.484: INFO: Number of nodes with available pods: 0
Feb  4 16:39:48.484: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:49.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:49.482: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:50.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:50.482: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:51.484: INFO: Number of nodes with available pods: 0
Feb  4 16:39:51.484: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:52.510: INFO: Number of nodes with available pods: 0
Feb  4 16:39:52.510: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:53.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:53.482: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:54.558: INFO: Number of nodes with available pods: 0
Feb  4 16:39:54.558: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:55.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:55.482: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:56.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:56.482: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:57.482: INFO: Number of nodes with available pods: 0
Feb  4 16:39:57.482: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:58.483: INFO: Number of nodes with available pods: 0
Feb  4 16:39:58.483: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:39:59.484: INFO: Number of nodes with available pods: 0
Feb  4 16:39:59.484: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:40:00.536: INFO: Number of nodes with available pods: 0
Feb  4 16:40:00.536: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 16:40:01.484: INFO: Number of nodes with available pods: 1
Feb  4 16:40:01.484: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-td4th, will wait for the garbage collector to delete the pods
Feb  4 16:40:01.572: INFO: Deleting DaemonSet.extensions daemon-set took: 11.847139ms
Feb  4 16:40:01.672: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.238182ms
Feb  4 16:40:35.540: INFO: Number of nodes with available pods: 0
Feb  4 16:40:35.540: INFO: Number of running nodes: 0, number of available pods: 0
Feb  4 16:40:35.546: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-td4th/daemonsets","resourceVersion":"7566"},"items":null}

Feb  4 16:40:35.550: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-td4th/pods","resourceVersion":"7566"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:40:35.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-td4th" for this suite.
Feb  4 16:40:41.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:40:41.792: INFO: namespace: e2e-tests-daemonsets-td4th, resource: bindings, ignored listing per whitelist
Feb  4 16:40:42.007: INFO: namespace e2e-tests-daemonsets-td4th deletion completed in 6.415230755s

• [SLOW TEST:79.891 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:40:42.009: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9c7ed1b9-289b-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 16:40:42.234: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c805142-289b-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-7qzqh" to be "success or failure"
Feb  4 16:40:42.252: INFO: Pod "pod-projected-configmaps-9c805142-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.609145ms
Feb  4 16:40:44.258: INFO: Pod "pod-projected-configmaps-9c805142-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02364105s
Feb  4 16:40:46.273: INFO: Pod "pod-projected-configmaps-9c805142-289b-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038736081s
STEP: Saw pod success
Feb  4 16:40:46.273: INFO: Pod "pod-projected-configmaps-9c805142-289b-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:40:46.277: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-projected-configmaps-9c805142-289b-11e9-9eab-8e57f341003b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 16:40:46.333: INFO: Waiting for pod pod-projected-configmaps-9c805142-289b-11e9-9eab-8e57f341003b to disappear
Feb  4 16:40:46.339: INFO: Pod pod-projected-configmaps-9c805142-289b-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:40:46.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7qzqh" for this suite.
Feb  4 16:40:54.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:40:54.900: INFO: namespace: e2e-tests-projected-7qzqh, resource: bindings, ignored listing per whitelist
Feb  4 16:40:55.031: INFO: namespace e2e-tests-projected-7qzqh deletion completed in 8.685680197s

• [SLOW TEST:13.022 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:40:55.038: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 16:40:55.445: INFO: Creating deployment "nginx-deployment"
Feb  4 16:40:55.456: INFO: Waiting for observed generation 1
Feb  4 16:40:57.493: INFO: Waiting for all required pods to come up
Feb  4 16:40:57.503: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb  4 16:41:07.536: INFO: Waiting for deployment "nginx-deployment" to complete
Feb  4 16:41:07.553: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb  4 16:41:07.572: INFO: Updating deployment nginx-deployment
Feb  4 16:41:07.573: INFO: Waiting for observed generation 2
Feb  4 16:41:09.649: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb  4 16:41:09.664: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb  4 16:41:09.670: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb  4 16:41:09.704: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb  4 16:41:09.704: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb  4 16:41:09.712: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb  4 16:41:09.721: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb  4 16:41:09.721: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb  4 16:41:09.740: INFO: Updating deployment nginx-deployment
Feb  4 16:41:09.741: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb  4 16:41:09.768: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb  4 16:41:09.773: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  4 16:41:12.000: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-krk88,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-krk88/deployments/nginx-deployment,UID:a463d442-289b-11e9-9959-0a580af41676,ResourceVersion:7954,Generation:3,CreationTimestamp:2019-02-04 16:40:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-02-04 16:41:09 +0000 UTC 2019-02-04 16:41:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-04 16:41:10 +0000 UTC 2019-02-04 16:40:55 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb  4 16:41:12.009: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-krk88,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-krk88/replicasets/nginx-deployment-65bbdb5f8,UID:ab9dce94-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7951,Generation:3,CreationTimestamp:2019-02-04 16:41:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a463d442-289b-11e9-9959-0a580af41676 0xc0019c0017 0xc0019c0018}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  4 16:41:12.009: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb  4 16:41:12.010: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-krk88,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-krk88/replicasets/nginx-deployment-555b55d965,UID:a4680cd9-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7944,Generation:3,CreationTimestamp:2019-02-04 16:40:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a463d442-289b-11e9-9959-0a580af41676 0xc001229f57 0xc001229f58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb  4 16:41:12.023: INFO: Pod "nginx-deployment-555b55d965-26pks" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-26pks,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-26pks,UID:a471097d-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7812,Generation:0,CreationTimestamp:2019-02-04 16:40:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc0019c1460 0xc0019c1461}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019c1550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019c1570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:172.25.1.33,StartTime:2019-02-04 16:40:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-04 16:41:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://0a172611c4956730db01269d415bacd3f451a8c77f10ec50864a926f38b92ec2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.024: INFO: Pod "nginx-deployment-555b55d965-2r8dw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2r8dw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-2r8dw,UID:a470f2db-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7761,Generation:0,CreationTimestamp:2019-02-04 16:40:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc0019c1630 0xc0019c1631}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019c1750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019c1770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.0.52,StartTime:2019-02-04 16:40:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-04 16:40:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://bbaef1d3361c4d41a97c8a28706e9322e901df47321e3896e6754fe34ea38fd4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.024: INFO: Pod "nginx-deployment-555b55d965-5j2hr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5j2hr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-5j2hr,UID:acf51a23-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7996,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc0019c1830 0xc0019c1831}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019c1890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019c18b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.025: INFO: Pod "nginx-deployment-555b55d965-72qnd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-72qnd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-72qnd,UID:acf5184f-289b-11e9-9a5d-0a580af41e66,ResourceVersion:8001,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc0019c1a37 0xc0019c1a38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019c1aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019c1ac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.027: INFO: Pod "nginx-deployment-555b55d965-8lmbl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-8lmbl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-8lmbl,UID:acfc7544-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7987,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc0019c1bf7 0xc0019c1bf8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ff6cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ff6ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.028: INFO: Pod "nginx-deployment-555b55d965-9fkp9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9fkp9,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-9fkp9,UID:acfc8f42-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7993,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000ff6fb0 0xc000ff6fb1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ff7010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ff7030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.028: INFO: Pod "nginx-deployment-555b55d965-9htvd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9htvd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-9htvd,UID:acfc80d2-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7995,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000ff7300 0xc000ff7301}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ff78c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ff78e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.028: INFO: Pod "nginx-deployment-555b55d965-9sbq7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9sbq7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-9sbq7,UID:acf5159e-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7978,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000ff7cc7 0xc000ff7cc8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ff7d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ff7d50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.029: INFO: Pod "nginx-deployment-555b55d965-cjnq7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-cjnq7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-cjnq7,UID:a46ee33b-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7755,Generation:0,CreationTimestamp:2019-02-04 16:40:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000924170 0xc000924171}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000924240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0009244c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.0.55,StartTime:2019-02-04 16:40:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-04 16:40:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://746c72a98d5a91b7dcded21ea8e7d0f6af93bd67f91f02c297e5b678de7bb7cf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.029: INFO: Pod "nginx-deployment-555b55d965-dhjhw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dhjhw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-dhjhw,UID:a46bd331-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7801,Generation:0,CreationTimestamp:2019-02-04 16:40:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc0009250a0 0xc0009250a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000925100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000925120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:172.25.1.31,StartTime:2019-02-04 16:40:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-04 16:41:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://cdd641a1fef0ff87b8529747bd487b1ba89fa7320ca86faf19aec2dd9b947007}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.029: INFO: Pod "nginx-deployment-555b55d965-g6vwt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-g6vwt,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-g6vwt,UID:acfca31f-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7939,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000925890 0xc000925891}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0009258f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000925920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.029: INFO: Pod "nginx-deployment-555b55d965-gg7rr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gg7rr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-gg7rr,UID:a46f016b-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7796,Generation:0,CreationTimestamp:2019-02-04 16:40:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000bae040 0xc000bae041}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000bae1d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000bae390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:172.25.1.32,StartTime:2019-02-04 16:40:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-04 16:41:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://92472237b31d5e6724bb84549816699da82d9041ca18339bda2e9244c02024cf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.030: INFO: Pod "nginx-deployment-555b55d965-gjcj5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gjcj5,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-gjcj5,UID:a4751d45-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7769,Generation:0,CreationTimestamp:2019-02-04 16:40:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000bae450 0xc000bae451}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000bae4c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000bae4e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.0.53,StartTime:2019-02-04 16:40:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-04 16:40:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://1cf767c050afa2d075aa1c30b075fefb3d5cda14ec0dc13e0cc4e81ce617de97}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.030: INFO: Pod "nginx-deployment-555b55d965-kx4dj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-kx4dj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-kx4dj,UID:acfc8e9e-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7938,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000bae9a0 0xc000bae9a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000baea00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000baea20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.030: INFO: Pod "nginx-deployment-555b55d965-l2xtt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-l2xtt,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-l2xtt,UID:aced250c-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7946,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000baeb50 0xc000baeb51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000baf5a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000baf5c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.033: INFO: Pod "nginx-deployment-555b55d965-qr2c4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qr2c4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-qr2c4,UID:acf4de3a-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7991,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000baf677 0xc000baf678}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000baf860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000baf880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.054: INFO: Pod "nginx-deployment-555b55d965-r5tlk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-r5tlk,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-r5tlk,UID:acefb79a-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7975,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000baf9a7 0xc000baf9a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000bafa10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000bafab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.055: INFO: Pod "nginx-deployment-555b55d965-ssjf6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ssjf6,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-ssjf6,UID:a47100e4-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7764,Generation:0,CreationTimestamp:2019-02-04 16:40:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc000bafc57 0xc000bafc58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000bafcd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007cc0c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.0.54,StartTime:2019-02-04 16:40:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-04 16:40:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://7b3a418c3b6c3601f152d115783bc3954cbe5d1b9b22eeb94fa289e38b99b67a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.055: INFO: Pod "nginx-deployment-555b55d965-twpbp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-twpbp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-twpbp,UID:acefda2a-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7988,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc0007cc1f0 0xc0007cc1f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007cc390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007cc3b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.057: INFO: Pod "nginx-deployment-555b55d965-xz8dj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-xz8dj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-555b55d965-xz8dj,UID:a47505f5-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7758,Generation:0,CreationTimestamp:2019-02-04 16:40:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a4680cd9-289b-11e9-9a5d-0a580af41e66 0xc0007cc467 0xc0007cc468}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007cc4d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007cc660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:40:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.0.56,StartTime:2019-02-04 16:40:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-04 16:40:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://bf4df753e66ec2e645e80cd6b10e9dbdd881c60c0ec5ce81bb0b1da4a7ded22e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.057: INFO: Pod "nginx-deployment-65bbdb5f8-46f77" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-46f77,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-46f77,UID:aba1381b-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7868,Generation:0,CreationTimestamp:2019-02-04 16:41:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc0007cc720 0xc0007cc721}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007cc850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007cc870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.057: INFO: Pod "nginx-deployment-65bbdb5f8-dq5pc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-dq5pc,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-dq5pc,UID:acfc6437-289b-11e9-9a5d-0a580af41e66,ResourceVersion:8000,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc0007cca70 0xc0007cca71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007ccae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007ccb00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.058: INFO: Pod "nginx-deployment-65bbdb5f8-kbdtd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-kbdtd,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-kbdtd,UID:acf4c7d5-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7970,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc0007ccc80 0xc0007ccc81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007cccf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007ccd10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-02-04 16:41:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.058: INFO: Pod "nginx-deployment-65bbdb5f8-mz8kg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-mz8kg,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-mz8kg,UID:ab9ee775-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7857,Generation:0,CreationTimestamp:2019-02-04 16:41:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc0007ccec0 0xc0007ccec1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007ccf30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007ccf50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.059: INFO: Pod "nginx-deployment-65bbdb5f8-pctq9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-pctq9,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-pctq9,UID:abbabebe-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7871,Generation:0,CreationTimestamp:2019-02-04 16:41:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc0007cd080 0xc0007cd081}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007cd1a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007cd1c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.063: INFO: Pod "nginx-deployment-65bbdb5f8-pgh4x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-pgh4x,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-pgh4x,UID:acf50ebf-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7986,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc0007cd2e0 0xc0007cd2e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007cd350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007cd840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.063: INFO: Pod "nginx-deployment-65bbdb5f8-pj8sd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-pj8sd,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-pj8sd,UID:acfc575f-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7973,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc0007cda50 0xc0007cda51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007cdad0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007cdb00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-02-04 16:41:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.064: INFO: Pod "nginx-deployment-65bbdb5f8-rr6nz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rr6nz,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-rr6nz,UID:abb45732-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7882,Generation:0,CreationTimestamp:2019-02-04 16:41:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc0007cdef0 0xc0007cdef1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007cdfc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007cdfe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.0.58,StartTime:2019-02-04 16:41:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.064: INFO: Pod "nginx-deployment-65bbdb5f8-t2p99" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-t2p99,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-t2p99,UID:acefc9c5-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7943,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc001496310 0xc001496311}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001496390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0014963b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-02-04 16:41:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.064: INFO: Pod "nginx-deployment-65bbdb5f8-vhb2d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-vhb2d,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-vhb2d,UID:acfc48f0-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7999,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc001496630 0xc001496631}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0014966b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0014966d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.065: INFO: Pod "nginx-deployment-65bbdb5f8-wtqtp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-wtqtp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-wtqtp,UID:ad004cc6-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7940,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc0014967e0 0xc0014967e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001496860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001496880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.065: INFO: Pod "nginx-deployment-65bbdb5f8-xcgvc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-xcgvc,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-xcgvc,UID:aba1284a-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7998,Generation:0,CreationTimestamp:2019-02-04 16:41:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc0014968f0 0xc0014968f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001496960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001496980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-02-04 16:41:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 16:41:12.065: INFO: Pod "nginx-deployment-65bbdb5f8-zqvbv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-zqvbv,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-krk88,SelfLink:/api/v1/namespaces/e2e-tests-deployment-krk88/pods/nginx-deployment-65bbdb5f8-zqvbv,UID:acfbf836-289b-11e9-9a5d-0a580af41e66,ResourceVersion:7985,Generation:0,CreationTimestamp:2019-02-04 16:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ab9dce94-289b-11e9-9a5d-0a580af41e66 0xc001496a80 0xc001496a81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r6mvc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6mvc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r6mvc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001496af0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001496b10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 16:41:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-02-04 16:41:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:41:12.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-krk88" for this suite.
Feb  4 16:41:22.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:41:22.222: INFO: namespace: e2e-tests-deployment-krk88, resource: bindings, ignored listing per whitelist
Feb  4 16:41:22.456: INFO: namespace e2e-tests-deployment-krk88 deletion completed in 10.383819387s

• [SLOW TEST:27.419 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:41:22.459: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 16:41:22.690: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-hxztd" to be "success or failure"
Feb  4 16:41:22.698: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.147456ms
Feb  4 16:41:24.705: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015295172s
Feb  4 16:41:26.712: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021650762s
Feb  4 16:41:28.725: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035172553s
Feb  4 16:41:30.731: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04088728s
Feb  4 16:41:32.738: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047951922s
Feb  4 16:41:34.744: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.054190468s
Feb  4 16:41:36.755: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.064742832s
Feb  4 16:41:38.781: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.091085191s
Feb  4 16:41:40.789: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.099200462s
Feb  4 16:41:42.797: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.106702304s
STEP: Saw pod success
Feb  4 16:41:42.797: INFO: Pod "downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:41:42.802: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 16:41:42.912: INFO: Waiting for pod downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b to disappear
Feb  4 16:41:42.918: INFO: Pod downwardapi-volume-b49c40e3-289b-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:41:42.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hxztd" for this suite.
Feb  4 16:41:49.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:41:49.459: INFO: namespace: e2e-tests-downward-api-hxztd, resource: bindings, ignored listing per whitelist
Feb  4 16:41:49.555: INFO: namespace e2e-tests-downward-api-hxztd deletion completed in 6.630217019s

• [SLOW TEST:27.097 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:41:49.559: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-c4bf3a57-289b-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 16:41:49.775: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c4c14cef-289b-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-hn9s9" to be "success or failure"
Feb  4 16:41:49.780: INFO: Pod "pod-projected-secrets-c4c14cef-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.47576ms
Feb  4 16:41:51.787: INFO: Pod "pod-projected-secrets-c4c14cef-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011375631s
Feb  4 16:41:53.794: INFO: Pod "pod-projected-secrets-c4c14cef-289b-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018181654s
STEP: Saw pod success
Feb  4 16:41:53.794: INFO: Pod "pod-projected-secrets-c4c14cef-289b-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:41:53.798: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-projected-secrets-c4c14cef-289b-11e9-9eab-8e57f341003b container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  4 16:41:53.846: INFO: Waiting for pod pod-projected-secrets-c4c14cef-289b-11e9-9eab-8e57f341003b to disappear
Feb  4 16:41:53.850: INFO: Pod pod-projected-secrets-c4c14cef-289b-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:41:53.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hn9s9" for this suite.
Feb  4 16:41:59.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:41:59.950: INFO: namespace: e2e-tests-projected-hn9s9, resource: bindings, ignored listing per whitelist
Feb  4 16:42:00.233: INFO: namespace e2e-tests-projected-hn9s9 deletion completed in 6.376696066s

• [SLOW TEST:10.674 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:42:00.234: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-74r8
STEP: Creating a pod to test atomic-volume-subpath
Feb  4 16:42:00.498: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-74r8" in namespace "e2e-tests-subpath-44nrn" to be "success or failure"
Feb  4 16:42:00.510: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.238812ms
Feb  4 16:42:02.626: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.127566706s
Feb  4 16:42:04.663: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.163962364s
Feb  4 16:42:06.801: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Running", Reason="", readiness=false. Elapsed: 6.302393962s
Feb  4 16:42:08.808: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Running", Reason="", readiness=false. Elapsed: 8.309441114s
Feb  4 16:42:10.829: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Running", Reason="", readiness=false. Elapsed: 10.33041704s
Feb  4 16:42:12.835: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Running", Reason="", readiness=false. Elapsed: 12.336592211s
Feb  4 16:42:14.842: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Running", Reason="", readiness=false. Elapsed: 14.343211043s
Feb  4 16:42:16.849: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Running", Reason="", readiness=false. Elapsed: 16.350390535s
Feb  4 16:42:18.854: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Running", Reason="", readiness=false. Elapsed: 18.355941997s
Feb  4 16:42:20.874: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Running", Reason="", readiness=false. Elapsed: 20.375388664s
Feb  4 16:42:22.893: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Running", Reason="", readiness=false. Elapsed: 22.394137278s
Feb  4 16:42:24.900: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Running", Reason="", readiness=false. Elapsed: 24.401013346s
Feb  4 16:42:26.920: INFO: Pod "pod-subpath-test-configmap-74r8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.421475699s
STEP: Saw pod success
Feb  4 16:42:26.920: INFO: Pod "pod-subpath-test-configmap-74r8" satisfied condition "success or failure"
Feb  4 16:42:26.925: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-subpath-test-configmap-74r8 container test-container-subpath-configmap-74r8: <nil>
STEP: delete the pod
Feb  4 16:42:27.138: INFO: Waiting for pod pod-subpath-test-configmap-74r8 to disappear
Feb  4 16:42:27.173: INFO: Pod pod-subpath-test-configmap-74r8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-74r8
Feb  4 16:42:27.173: INFO: Deleting pod "pod-subpath-test-configmap-74r8" in namespace "e2e-tests-subpath-44nrn"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:42:27.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-44nrn" for this suite.
Feb  4 16:42:33.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:42:33.274: INFO: namespace: e2e-tests-subpath-44nrn, resource: bindings, ignored listing per whitelist
Feb  4 16:42:33.578: INFO: namespace e2e-tests-subpath-44nrn deletion completed in 6.391375071s

• [SLOW TEST:33.344 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:42:33.583: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 16:42:33.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-defe17fe-289b-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-vzpvk" to be "success or failure"
Feb  4 16:42:33.812: INFO: Pod "downwardapi-volume-defe17fe-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.369214ms
Feb  4 16:42:35.817: INFO: Pod "downwardapi-volume-defe17fe-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017739095s
Feb  4 16:42:37.825: INFO: Pod "downwardapi-volume-defe17fe-289b-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025512497s
STEP: Saw pod success
Feb  4 16:42:37.825: INFO: Pod "downwardapi-volume-defe17fe-289b-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:42:37.829: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod downwardapi-volume-defe17fe-289b-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 16:42:37.873: INFO: Waiting for pod downwardapi-volume-defe17fe-289b-11e9-9eab-8e57f341003b to disappear
Feb  4 16:42:37.880: INFO: Pod downwardapi-volume-defe17fe-289b-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:42:37.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vzpvk" for this suite.
Feb  4 16:42:43.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:42:44.212: INFO: namespace: e2e-tests-projected-vzpvk, resource: bindings, ignored listing per whitelist
Feb  4 16:42:44.277: INFO: namespace e2e-tests-projected-vzpvk deletion completed in 6.391489994s

• [SLOW TEST:10.695 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:42:44.282: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 16:42:44.483: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e55d4bd9-289b-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-xm8mv" to be "success or failure"
Feb  4 16:42:44.488: INFO: Pod "downwardapi-volume-e55d4bd9-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.977577ms
Feb  4 16:42:46.495: INFO: Pod "downwardapi-volume-e55d4bd9-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012222847s
Feb  4 16:42:48.502: INFO: Pod "downwardapi-volume-e55d4bd9-289b-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019507717s
STEP: Saw pod success
Feb  4 16:42:48.503: INFO: Pod "downwardapi-volume-e55d4bd9-289b-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:42:48.509: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downwardapi-volume-e55d4bd9-289b-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 16:42:48.709: INFO: Waiting for pod downwardapi-volume-e55d4bd9-289b-11e9-9eab-8e57f341003b to disappear
Feb  4 16:42:48.721: INFO: Pod downwardapi-volume-e55d4bd9-289b-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:42:48.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xm8mv" for this suite.
Feb  4 16:42:54.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:42:55.133: INFO: namespace: e2e-tests-downward-api-xm8mv, resource: bindings, ignored listing per whitelist
Feb  4 16:42:55.149: INFO: namespace e2e-tests-downward-api-xm8mv deletion completed in 6.396962177s

• [SLOW TEST:10.868 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:42:55.151: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb  4 16:42:55.417: INFO: Waiting up to 5m0s for pod "pod-ebdf260e-289b-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-8lmjl" to be "success or failure"
Feb  4 16:42:55.425: INFO: Pod "pod-ebdf260e-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.986087ms
Feb  4 16:42:57.431: INFO: Pod "pod-ebdf260e-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014063151s
Feb  4 16:42:59.437: INFO: Pod "pod-ebdf260e-289b-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019472436s
Feb  4 16:43:01.442: INFO: Pod "pod-ebdf260e-289b-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024986222s
STEP: Saw pod success
Feb  4 16:43:01.442: INFO: Pod "pod-ebdf260e-289b-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:43:01.447: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-ebdf260e-289b-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 16:43:01.527: INFO: Waiting for pod pod-ebdf260e-289b-11e9-9eab-8e57f341003b to disappear
Feb  4 16:43:01.533: INFO: Pod pod-ebdf260e-289b-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:43:01.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8lmjl" for this suite.
Feb  4 16:43:07.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:43:07.644: INFO: namespace: e2e-tests-emptydir-8lmjl, resource: bindings, ignored listing per whitelist
Feb  4 16:43:07.934: INFO: namespace e2e-tests-emptydir-8lmjl deletion completed in 6.395383294s

• [SLOW TEST:12.784 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:43:07.941: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  4 16:43:08.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-fvw2k'
Feb  4 16:43:08.479: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  4 16:43:08.479: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Feb  4 16:43:12.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-fvw2k'
Feb  4 16:43:12.963: INFO: stderr: ""
Feb  4 16:43:12.963: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:43:12.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fvw2k" for this suite.
Feb  4 16:43:19.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:43:19.109: INFO: namespace: e2e-tests-kubectl-fvw2k, resource: bindings, ignored listing per whitelist
Feb  4 16:43:19.402: INFO: namespace e2e-tests-kubectl-fvw2k deletion completed in 6.42422117s

• [SLOW TEST:11.461 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:43:19.409: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-kcnrb
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-kcnrb
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-kcnrb
Feb  4 16:43:19.721: INFO: Found 0 stateful pods, waiting for 1
Feb  4 16:43:29.818: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb  4 16:43:29.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  4 16:43:30.728: INFO: stderr: ""
Feb  4 16:43:30.728: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  4 16:43:30.728: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  4 16:43:30.734: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb  4 16:43:40.776: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  4 16:43:40.776: INFO: Waiting for statefulset status.replicas updated to 0
Feb  4 16:43:40.803: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999357s
Feb  4 16:43:41.811: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994732824s
Feb  4 16:43:42.817: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987276431s
Feb  4 16:43:43.835: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.98128569s
Feb  4 16:43:44.843: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.963132613s
Feb  4 16:43:45.851: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.955065038s
Feb  4 16:43:46.866: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.947107134s
Feb  4 16:43:47.878: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.932662594s
Feb  4 16:43:48.888: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.920607241s
Feb  4 16:43:49.895: INFO: Verifying statefulset ss doesn't scale past 1 for another 910.324951ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-kcnrb
Feb  4 16:43:50.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:43:51.711: INFO: stderr: ""
Feb  4 16:43:51.711: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  4 16:43:51.711: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  4 16:43:51.719: INFO: Found 1 stateful pods, waiting for 3
Feb  4 16:44:01.738: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  4 16:44:01.738: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  4 16:44:01.738: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb  4 16:44:01.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  4 16:44:02.541: INFO: stderr: ""
Feb  4 16:44:02.541: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  4 16:44:02.541: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  4 16:44:02.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  4 16:44:03.463: INFO: stderr: ""
Feb  4 16:44:03.463: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  4 16:44:03.463: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  4 16:44:03.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  4 16:44:04.466: INFO: stderr: ""
Feb  4 16:44:04.466: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  4 16:44:04.466: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  4 16:44:04.466: INFO: Waiting for statefulset status.replicas updated to 0
Feb  4 16:44:04.474: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb  4 16:44:14.505: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  4 16:44:14.505: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb  4 16:44:14.505: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb  4 16:44:14.528: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999535s
Feb  4 16:44:15.534: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993769151s
Feb  4 16:44:16.543: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987479593s
Feb  4 16:44:17.552: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978174003s
Feb  4 16:44:18.561: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970039127s
Feb  4 16:44:19.569: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.960370842s
Feb  4 16:44:20.576: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.952558116s
Feb  4 16:44:21.586: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.945509544s
Feb  4 16:44:22.593: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.935532285s
Feb  4 16:44:23.601: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.407764ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-kcnrb
Feb  4 16:44:24.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:44:25.454: INFO: stderr: ""
Feb  4 16:44:25.454: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  4 16:44:25.454: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  4 16:44:25.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:44:26.305: INFO: stderr: ""
Feb  4 16:44:26.305: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  4 16:44:26.305: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  4 16:44:26.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:44:26.889: INFO: rc: 126
Feb  4 16:44:26.889: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil> cannot exec in a stopped state: unknown
 command terminated with exit code 126
 [] <nil> 0xc0014fde30 exit status 126 <nil> <nil> true [0xc00000f0b8 0xc00000f110 0xc00000f168] [0xc00000f0b8 0xc00000f110 0xc00000f168] [0xc00000f0f0 0xc00000f150] [0x92f8e0 0x92f8e0] 0xc0017bdc20 <nil>}:
Command stdout:
cannot exec in a stopped state: unknown

stderr:
command terminated with exit code 126

error:
exit status 126

Feb  4 16:44:36.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:44:37.136: INFO: rc: 1
Feb  4 16:44:37.136: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0009ca300 exit status 1 <nil> <nil> true [0xc00000f188 0xc00000f1b8 0xc00000f1f8] [0xc00000f188 0xc00000f1b8 0xc00000f1f8] [0xc00000f1a0 0xc00000f1e0] [0x92f8e0 0x92f8e0] 0xc0017bdf20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:44:47.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:44:47.359: INFO: rc: 1
Feb  4 16:44:47.359: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0009ca6f0 exit status 1 <nil> <nil> true [0xc00000f200 0xc00000f240 0xc00000f270] [0xc00000f200 0xc00000f240 0xc00000f270] [0xc00000f238 0xc00000f260] [0x92f8e0 0x92f8e0] 0xc0018a02a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:44:57.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:44:57.571: INFO: rc: 1
Feb  4 16:44:57.571: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fc570 exit status 1 <nil> <nil> true [0xc000b92010 0xc000b92068 0xc000b92080] [0xc000b92010 0xc000b92068 0xc000b92080] [0xc000b92050 0xc000b92078] [0x92f8e0 0x92f8e0] 0xc0017bc300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:45:07.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:45:07.799: INFO: rc: 1
Feb  4 16:45:07.799: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fd140 exit status 1 <nil> <nil> true [0xc000b920a0 0xc000b920d8 0xc000b92118] [0xc000b920a0 0xc000b920d8 0xc000b92118] [0xc000b920c0 0xc000b92110] [0x92f8e0 0x92f8e0] 0xc0017bc900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:45:17.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:45:18.031: INFO: rc: 1
Feb  4 16:45:18.031: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fd560 exit status 1 <nil> <nil> true [0xc000b92120 0xc000b92138 0xc000b92190] [0xc000b92120 0xc000b92138 0xc000b92190] [0xc000b92130 0xc000b92170] [0x92f8e0 0x92f8e0] 0xc0017bce40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:45:28.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:45:28.252: INFO: rc: 1
Feb  4 16:45:28.252: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fd980 exit status 1 <nil> <nil> true [0xc000b921a8 0xc000b921c0 0xc000b921d8] [0xc000b921a8 0xc000b921c0 0xc000b921d8] [0xc000b921b8 0xc000b921d0] [0x92f8e0 0x92f8e0] 0xc0017bd1a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:45:38.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:45:38.474: INFO: rc: 1
Feb  4 16:45:38.474: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fddd0 exit status 1 <nil> <nil> true [0xc000b921e0 0xc000b921f8 0xc000b92230] [0xc000b921e0 0xc000b921f8 0xc000b92230] [0xc000b921f0 0xc000b92220] [0x92f8e0 0x92f8e0] 0xc0017bd5c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:45:48.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:45:48.784: INFO: rc: 1
Feb  4 16:45:48.784: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002076c60 exit status 1 <nil> <nil> true [0xc000b92250 0xc000b92290 0xc000b922a8] [0xc000b92250 0xc000b92290 0xc000b922a8] [0xc000b92288 0xc000b922a0] [0x92f8e0 0x92f8e0] 0xc0017bd8c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:45:58.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:45:59.010: INFO: rc: 1
Feb  4 16:45:59.010: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002077050 exit status 1 <nil> <nil> true [0xc000b922b0 0xc000b922c8 0xc000b922e8] [0xc000b922b0 0xc000b922c8 0xc000b922e8] [0xc000b922c0 0xc000b922e0] [0x92f8e0 0x92f8e0] 0xc0017bdbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:46:09.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:46:09.246: INFO: rc: 1
Feb  4 16:46:09.246: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002077470 exit status 1 <nil> <nil> true [0xc000b922f0 0xc000b92320 0xc000b92338] [0xc000b922f0 0xc000b92320 0xc000b92338] [0xc000b92300 0xc000b92330] [0x92f8e0 0x92f8e0] 0xc0017bdec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:46:19.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:46:19.483: INFO: rc: 1
Feb  4 16:46:19.484: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002077860 exit status 1 <nil> <nil> true [0xc000b92340 0xc000b92368 0xc000b923a8] [0xc000b92340 0xc000b92368 0xc000b923a8] [0xc000b92350 0xc000b92390] [0x92f8e0 0x92f8e0] 0xc0019a0240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:46:29.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:46:29.712: INFO: rc: 1
Feb  4 16:46:29.712: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002077c80 exit status 1 <nil> <nil> true [0xc000b923b0 0xc000b923c8 0xc000b923e0] [0xc000b923b0 0xc000b923c8 0xc000b923e0] [0xc000b923c0 0xc000b923d8] [0x92f8e0 0x92f8e0] 0xc0019a05a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:46:39.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:46:39.955: INFO: rc: 1
Feb  4 16:46:39.955: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013c4060 exit status 1 <nil> <nil> true [0xc000b923e8 0xc000b92400 0xc000b92418] [0xc000b923e8 0xc000b92400 0xc000b92418] [0xc000b923f8 0xc000b92410] [0x92f8e0 0x92f8e0] 0xc0019a0e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:46:49.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:46:50.171: INFO: rc: 1
Feb  4 16:46:50.172: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013c4450 exit status 1 <nil> <nil> true [0xc000b92420 0xc000b92438 0xc000b92450] [0xc000b92420 0xc000b92438 0xc000b92450] [0xc000b92430 0xc000b92448] [0x92f8e0 0x92f8e0] 0xc0019a15c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:47:00.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:47:00.453: INFO: rc: 1
Feb  4 16:47:00.453: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013c4870 exit status 1 <nil> <nil> true [0xc000b92458 0xc000b92470 0xc000b92488] [0xc000b92458 0xc000b92470 0xc000b92488] [0xc000b92468 0xc000b92480] [0x92f8e0 0x92f8e0] 0xc0019a1bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:47:10.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:47:10.743: INFO: rc: 1
Feb  4 16:47:10.743: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002076e10 exit status 1 <nil> <nil> true [0xc000b92010 0xc000b92068 0xc000b92080] [0xc000b92010 0xc000b92068 0xc000b92080] [0xc000b92050 0xc000b92078] [0x92f8e0 0x92f8e0] 0xc0017bc300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:47:20.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:47:20.963: INFO: rc: 1
Feb  4 16:47:20.963: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002077260 exit status 1 <nil> <nil> true [0xc000b920a0 0xc000b920d8 0xc000b92118] [0xc000b920a0 0xc000b920d8 0xc000b92118] [0xc000b920c0 0xc000b92110] [0x92f8e0 0x92f8e0] 0xc0017bc900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:47:30.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:47:31.186: INFO: rc: 1
Feb  4 16:47:31.186: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002077680 exit status 1 <nil> <nil> true [0xc000b92120 0xc000b92138 0xc000b92190] [0xc000b92120 0xc000b92138 0xc000b92190] [0xc000b92130 0xc000b92170] [0x92f8e0 0x92f8e0] 0xc0017bce40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:47:41.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:47:41.295: INFO: rc: 1
Feb  4 16:47:41.295: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002077aa0 exit status 1 <nil> <nil> true [0xc000b921a8 0xc000b921c0 0xc000b921d8] [0xc000b921a8 0xc000b921c0 0xc000b921d8] [0xc000b921b8 0xc000b921d0] [0x92f8e0 0x92f8e0] 0xc0017bd1a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:47:51.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:47:51.562: INFO: rc: 1
Feb  4 16:47:51.562: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002077ef0 exit status 1 <nil> <nil> true [0xc000b921e0 0xc000b921f8 0xc000b92230] [0xc000b921e0 0xc000b921f8 0xc000b92230] [0xc000b921f0 0xc000b92220] [0x92f8e0 0x92f8e0] 0xc0017bd5c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:48:01.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:48:01.819: INFO: rc: 1
Feb  4 16:48:01.819: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fc480 exit status 1 <nil> <nil> true [0xc000b92250 0xc000b92290 0xc000b922a8] [0xc000b92250 0xc000b92290 0xc000b922a8] [0xc000b92288 0xc000b922a0] [0x92f8e0 0x92f8e0] 0xc0017bd8c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:48:11.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:48:12.091: INFO: rc: 1
Feb  4 16:48:12.091: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fd020 exit status 1 <nil> <nil> true [0xc000b922b0 0xc000b922c8 0xc000b922e8] [0xc000b922b0 0xc000b922c8 0xc000b922e8] [0xc000b922c0 0xc000b922e0] [0x92f8e0 0x92f8e0] 0xc0017bdbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:48:22.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:48:22.355: INFO: rc: 1
Feb  4 16:48:22.355: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fd4d0 exit status 1 <nil> <nil> true [0xc000b922f0 0xc000b92320 0xc000b92338] [0xc000b922f0 0xc000b92320 0xc000b92338] [0xc000b92300 0xc000b92330] [0x92f8e0 0x92f8e0] 0xc0017bdec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:48:32.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:48:32.612: INFO: rc: 1
Feb  4 16:48:32.612: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fd920 exit status 1 <nil> <nil> true [0xc000b92340 0xc000b92368 0xc000b923a8] [0xc000b92340 0xc000b92368 0xc000b923a8] [0xc000b92350 0xc000b92390] [0x92f8e0 0x92f8e0] 0xc0019a0240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:48:42.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:48:42.914: INFO: rc: 1
Feb  4 16:48:42.914: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fdd70 exit status 1 <nil> <nil> true [0xc000b923b0 0xc000b923c8 0xc000b923e0] [0xc000b923b0 0xc000b923c8 0xc000b923e0] [0xc000b923c0 0xc000b923d8] [0x92f8e0 0x92f8e0] 0xc0019a05a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:48:52.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:48:53.170: INFO: rc: 1
Feb  4 16:48:53.170: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013c41e0 exit status 1 <nil> <nil> true [0xc000b923e8 0xc000b92400 0xc000b92418] [0xc000b923e8 0xc000b92400 0xc000b92418] [0xc000b923f8 0xc000b92410] [0x92f8e0 0x92f8e0] 0xc0019a0e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:49:03.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:49:03.408: INFO: rc: 1
Feb  4 16:49:03.408: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fc5a0 exit status 1 <nil> <nil> true [0xc000b92010 0xc000b92068 0xc000b92080] [0xc000b92010 0xc000b92068 0xc000b92080] [0xc000b92050 0xc000b92078] [0x92f8e0 0x92f8e0] 0xc0017bc300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:49:13.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:49:13.656: INFO: rc: 1
Feb  4 16:49:13.656: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fd170 exit status 1 <nil> <nil> true [0xc000b920a0 0xc000b920d8 0xc000b92118] [0xc000b920a0 0xc000b920d8 0xc000b92118] [0xc000b920c0 0xc000b92110] [0x92f8e0 0x92f8e0] 0xc0017bc900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:49:23.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:49:24.060: INFO: rc: 1
Feb  4 16:49:24.060: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014fd5c0 exit status 1 <nil> <nil> true [0xc000b92120 0xc000b92138 0xc000b92190] [0xc000b92120 0xc000b92138 0xc000b92190] [0xc000b92130 0xc000b92170] [0x92f8e0 0x92f8e0] 0xc0017bce40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb  4 16:49:34.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-kcnrb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:49:34.151: INFO: rc: 1
Feb  4 16:49:34.151: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb  4 16:49:34.151: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  4 16:49:34.190: INFO: Deleting all statefulset in ns e2e-tests-statefulset-kcnrb
Feb  4 16:49:34.196: INFO: Scaling statefulset ss to 0
Feb  4 16:49:34.219: INFO: Waiting for statefulset status.replicas updated to 0
Feb  4 16:49:34.226: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:49:34.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-kcnrb" for this suite.
Feb  4 16:49:40.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:49:40.576: INFO: namespace: e2e-tests-statefulset-kcnrb, resource: bindings, ignored listing per whitelist
Feb  4 16:49:40.596: INFO: namespace e2e-tests-statefulset-kcnrb deletion completed in 6.329735555s

• [SLOW TEST:381.188 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:49:40.601: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:49:45.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-tttqw" for this suite.
Feb  4 16:50:35.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:50:35.312: INFO: namespace: e2e-tests-kubelet-test-tttqw, resource: bindings, ignored listing per whitelist
Feb  4 16:50:35.469: INFO: namespace e2e-tests-kubelet-test-tttqw deletion completed in 50.451089184s

• [SLOW TEST:54.869 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:50:35.475: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb  4 16:50:35.670: INFO: Waiting up to 5m0s for pod "pod-fe352313-289c-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-szfc7" to be "success or failure"
Feb  4 16:50:35.681: INFO: Pod "pod-fe352313-289c-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.125326ms
Feb  4 16:50:37.686: INFO: Pod "pod-fe352313-289c-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016508092s
Feb  4 16:50:39.692: INFO: Pod "pod-fe352313-289c-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022631016s
STEP: Saw pod success
Feb  4 16:50:39.692: INFO: Pod "pod-fe352313-289c-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:50:39.697: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-fe352313-289c-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 16:50:39.872: INFO: Waiting for pod pod-fe352313-289c-11e9-9eab-8e57f341003b to disappear
Feb  4 16:50:39.903: INFO: Pod pod-fe352313-289c-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:50:39.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-szfc7" for this suite.
Feb  4 16:50:46.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:50:46.345: INFO: namespace: e2e-tests-emptydir-szfc7, resource: bindings, ignored listing per whitelist
Feb  4 16:50:46.402: INFO: namespace e2e-tests-emptydir-szfc7 deletion completed in 6.492608672s

• [SLOW TEST:10.927 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:50:46.406: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-fjgnj
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb  4 16:50:46.665: INFO: Found 0 stateful pods, waiting for 3
Feb  4 16:50:56.698: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  4 16:50:56.698: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  4 16:50:56.698: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb  4 16:50:56.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-fjgnj ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  4 16:50:57.382: INFO: stderr: ""
Feb  4 16:50:57.382: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  4 16:50:57.382: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb  4 16:51:07.506: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb  4 16:51:17.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-fjgnj ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:51:18.370: INFO: stderr: ""
Feb  4 16:51:18.370: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  4 16:51:18.370: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  4 16:51:28.561: INFO: Waiting for StatefulSet e2e-tests-statefulset-fjgnj/ss2 to complete update
Feb  4 16:51:28.561: INFO: Waiting for Pod e2e-tests-statefulset-fjgnj/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  4 16:51:28.561: INFO: Waiting for Pod e2e-tests-statefulset-fjgnj/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  4 16:51:28.561: INFO: Waiting for Pod e2e-tests-statefulset-fjgnj/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  4 16:51:38.584: INFO: Waiting for StatefulSet e2e-tests-statefulset-fjgnj/ss2 to complete update
Feb  4 16:51:38.585: INFO: Waiting for Pod e2e-tests-statefulset-fjgnj/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  4 16:51:38.585: INFO: Waiting for Pod e2e-tests-statefulset-fjgnj/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  4 16:51:48.576: INFO: Waiting for StatefulSet e2e-tests-statefulset-fjgnj/ss2 to complete update
Feb  4 16:51:48.576: INFO: Waiting for Pod e2e-tests-statefulset-fjgnj/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Feb  4 16:51:58.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-fjgnj ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  4 16:51:59.463: INFO: stderr: ""
Feb  4 16:51:59.463: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  4 16:51:59.463: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  4 16:51:59.517: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb  4 16:52:09.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-fjgnj ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 16:52:10.342: INFO: stderr: ""
Feb  4 16:52:10.342: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  4 16:52:10.342: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  4 16:52:30.399: INFO: Waiting for StatefulSet e2e-tests-statefulset-fjgnj/ss2 to complete update
Feb  4 16:52:30.400: INFO: Waiting for Pod e2e-tests-statefulset-fjgnj/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  4 16:52:40.463: INFO: Deleting all statefulset in ns e2e-tests-statefulset-fjgnj
Feb  4 16:52:40.473: INFO: Scaling statefulset ss2 to 0
Feb  4 16:53:20.503: INFO: Waiting for statefulset status.replicas updated to 0
Feb  4 16:53:20.510: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:53:20.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-fjgnj" for this suite.
Feb  4 16:53:28.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:53:28.934: INFO: namespace: e2e-tests-statefulset-fjgnj, resource: bindings, ignored listing per whitelist
Feb  4 16:53:28.993: INFO: namespace e2e-tests-statefulset-fjgnj deletion completed in 8.418271898s

• [SLOW TEST:162.587 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:53:28.994: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  4 16:53:29.158: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:53:33.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-cd4kw" for this suite.
Feb  4 16:53:55.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:53:55.879: INFO: namespace: e2e-tests-init-container-cd4kw, resource: bindings, ignored listing per whitelist
Feb  4 16:53:56.040: INFO: namespace e2e-tests-init-container-cd4kw deletion completed in 22.525841438s

• [SLOW TEST:27.047 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:53:56.045: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-wwlpk
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  4 16:53:56.251: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  4 16:54:24.432: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 172.25.1.51 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-wwlpk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 16:54:24.433: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 16:54:25.986: INFO: Found all expected endpoints: [netserver-0]
Feb  4 16:54:25.992: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 172.25.0.78 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-wwlpk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 16:54:25.992: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 16:54:27.428: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:54:27.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-wwlpk" for this suite.
Feb  4 16:54:51.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:54:51.523: INFO: namespace: e2e-tests-pod-network-test-wwlpk, resource: bindings, ignored listing per whitelist
Feb  4 16:54:51.811: INFO: namespace e2e-tests-pod-network-test-wwlpk deletion completed in 24.376262576s

• [SLOW TEST:55.767 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:54:51.814: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-96ff66ca-289d-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 16:54:52.023: INFO: Waiting up to 5m0s for pod "pod-secrets-97035939-289d-11e9-9eab-8e57f341003b" in namespace "e2e-tests-secrets-qbn56" to be "success or failure"
Feb  4 16:54:52.029: INFO: Pod "pod-secrets-97035939-289d-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.98897ms
Feb  4 16:54:54.106: INFO: Pod "pod-secrets-97035939-289d-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083154043s
Feb  4 16:54:56.113: INFO: Pod "pod-secrets-97035939-289d-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089697215s
STEP: Saw pod success
Feb  4 16:54:56.113: INFO: Pod "pod-secrets-97035939-289d-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:54:56.118: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-secrets-97035939-289d-11e9-9eab-8e57f341003b container secret-volume-test: <nil>
STEP: delete the pod
Feb  4 16:54:56.228: INFO: Waiting for pod pod-secrets-97035939-289d-11e9-9eab-8e57f341003b to disappear
Feb  4 16:54:56.233: INFO: Pod pod-secrets-97035939-289d-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:54:56.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-qbn56" for this suite.
Feb  4 16:55:02.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:55:02.428: INFO: namespace: e2e-tests-secrets-qbn56, resource: bindings, ignored listing per whitelist
Feb  4 16:55:02.671: INFO: namespace e2e-tests-secrets-qbn56 deletion completed in 6.431104116s

• [SLOW TEST:10.858 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:55:02.672: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-pf9rl
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  4 16:55:02.832: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  4 16:55:24.966: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.53:8080/dial?request=hostName&protocol=http&host=172.25.1.52&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-pf9rl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 16:55:24.966: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 16:55:25.457: INFO: Waiting for endpoints: map[]
Feb  4 16:55:25.463: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.53:8080/dial?request=hostName&protocol=http&host=172.25.0.81&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-pf9rl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 16:55:25.463: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 16:55:26.014: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:55:26.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-pf9rl" for this suite.
Feb  4 16:55:50.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:55:50.199: INFO: namespace: e2e-tests-pod-network-test-pf9rl, resource: bindings, ignored listing per whitelist
Feb  4 16:55:50.390: INFO: namespace e2e-tests-pod-network-test-pf9rl deletion completed in 24.368246722s

• [SLOW TEST:47.718 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:55:50.396: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-slvxm/secret-test-b9ef2318-289d-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 16:55:50.637: INFO: Waiting up to 5m0s for pod "pod-configmaps-b9f27d4e-289d-11e9-9eab-8e57f341003b" in namespace "e2e-tests-secrets-slvxm" to be "success or failure"
Feb  4 16:55:50.642: INFO: Pod "pod-configmaps-b9f27d4e-289d-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.740738ms
Feb  4 16:55:52.650: INFO: Pod "pod-configmaps-b9f27d4e-289d-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012051031s
Feb  4 16:55:54.656: INFO: Pod "pod-configmaps-b9f27d4e-289d-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018281592s
STEP: Saw pod success
Feb  4 16:55:54.656: INFO: Pod "pod-configmaps-b9f27d4e-289d-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:55:54.660: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-configmaps-b9f27d4e-289d-11e9-9eab-8e57f341003b container env-test: <nil>
STEP: delete the pod
Feb  4 16:55:54.793: INFO: Waiting for pod pod-configmaps-b9f27d4e-289d-11e9-9eab-8e57f341003b to disappear
Feb  4 16:55:54.799: INFO: Pod pod-configmaps-b9f27d4e-289d-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:55:54.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-slvxm" for this suite.
Feb  4 16:56:52.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:56:53.112: INFO: namespace: e2e-tests-secrets-slvxm, resource: bindings, ignored listing per whitelist
Feb  4 16:56:53.269: INFO: namespace e2e-tests-secrets-slvxm deletion completed in 58.445602615s

• [SLOW TEST:62.874 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:56:53.274: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0204 16:57:03.895663      13 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  4 16:57:03.895: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:57:03.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-x6ls4" for this suite.
Feb  4 16:57:11.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:57:12.221: INFO: namespace: e2e-tests-gc-x6ls4, resource: bindings, ignored listing per whitelist
Feb  4 16:57:12.575: INFO: namespace e2e-tests-gc-x6ls4 deletion completed in 8.674642643s

• [SLOW TEST:19.302 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:57:12.576: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-eafb2bfc-289d-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 16:57:12.919: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eafd6819-289d-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-xljvd" to be "success or failure"
Feb  4 16:57:12.934: INFO: Pod "pod-projected-secrets-eafd6819-289d-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.084594ms
Feb  4 16:57:15.097: INFO: Pod "pod-projected-secrets-eafd6819-289d-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178192666s
STEP: Saw pod success
Feb  4 16:57:15.097: INFO: Pod "pod-projected-secrets-eafd6819-289d-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 16:57:15.102: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-projected-secrets-eafd6819-289d-11e9-9eab-8e57f341003b container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  4 16:57:15.277: INFO: Waiting for pod pod-projected-secrets-eafd6819-289d-11e9-9eab-8e57f341003b to disappear
Feb  4 16:57:15.282: INFO: Pod pod-projected-secrets-eafd6819-289d-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:57:15.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xljvd" for this suite.
Feb  4 16:57:23.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:57:23.395: INFO: namespace: e2e-tests-projected-xljvd, resource: bindings, ignored listing per whitelist
Feb  4 16:57:23.712: INFO: namespace e2e-tests-projected-xljvd deletion completed in 8.421920567s

• [SLOW TEST:11.136 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:57:23.716: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb  4 16:57:36.314: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  4 16:57:36.333: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  4 16:57:38.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  4 16:57:38.342: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  4 16:57:40.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  4 16:57:40.342: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  4 16:57:42.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  4 16:57:42.341: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  4 16:57:44.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  4 16:57:44.341: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  4 16:57:46.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  4 16:57:46.352: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  4 16:57:48.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  4 16:57:48.341: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  4 16:57:50.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  4 16:57:50.342: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  4 16:57:52.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  4 16:57:52.345: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  4 16:57:54.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  4 16:57:54.342: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  4 16:57:56.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  4 16:57:56.348: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:57:56.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-k5nq9" for this suite.
Feb  4 16:58:30.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:58:30.641: INFO: namespace: e2e-tests-container-lifecycle-hook-k5nq9, resource: bindings, ignored listing per whitelist
Feb  4 16:58:30.839: INFO: namespace e2e-tests-container-lifecycle-hook-k5nq9 deletion completed in 34.468633986s

• [SLOW TEST:67.123 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:58:30.846: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Feb  4 16:58:42.497: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:59:00.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-xlrlv" for this suite.
Feb  4 16:59:06.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:59:06.849: INFO: namespace: e2e-tests-namespaces-xlrlv, resource: bindings, ignored listing per whitelist
Feb  4 16:59:06.885: INFO: namespace e2e-tests-namespaces-xlrlv deletion completed in 6.392523129s
STEP: Destroying namespace "e2e-tests-nsdeletetest-zngjv" for this suite.
Feb  4 16:59:06.891: INFO: Namespace e2e-tests-nsdeletetest-zngjv was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-b62ph" for this suite.
Feb  4 16:59:12.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:59:13.241: INFO: namespace: e2e-tests-nsdeletetest-b62ph, resource: bindings, ignored listing per whitelist
Feb  4 16:59:13.274: INFO: namespace e2e-tests-nsdeletetest-b62ph deletion completed in 6.382489196s

• [SLOW TEST:42.428 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:59:13.278: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-fzs2
STEP: Creating a pod to test atomic-volume-subpath
Feb  4 16:59:13.581: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fzs2" in namespace "e2e-tests-subpath-jcvwd" to be "success or failure"
Feb  4 16:59:13.590: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.472993ms
Feb  4 16:59:15.600: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019024301s
Feb  4 16:59:17.618: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Running", Reason="", readiness=false. Elapsed: 4.036423712s
Feb  4 16:59:19.624: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Running", Reason="", readiness=false. Elapsed: 6.0422062s
Feb  4 16:59:21.631: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Running", Reason="", readiness=false. Elapsed: 8.04952679s
Feb  4 16:59:23.647: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Running", Reason="", readiness=false. Elapsed: 10.065787398s
Feb  4 16:59:25.658: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Running", Reason="", readiness=false. Elapsed: 12.07621344s
Feb  4 16:59:27.707: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Running", Reason="", readiness=false. Elapsed: 14.125377241s
Feb  4 16:59:29.716: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Running", Reason="", readiness=false. Elapsed: 16.135003805s
Feb  4 16:59:31.726: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Running", Reason="", readiness=false. Elapsed: 18.144857966s
Feb  4 16:59:33.733: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Running", Reason="", readiness=false. Elapsed: 20.151340375s
Feb  4 16:59:35.740: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Running", Reason="", readiness=false. Elapsed: 22.158459573s
Feb  4 16:59:37.765: INFO: Pod "pod-subpath-test-configmap-fzs2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.183694247s
STEP: Saw pod success
Feb  4 16:59:37.765: INFO: Pod "pod-subpath-test-configmap-fzs2" satisfied condition "success or failure"
Feb  4 16:59:37.772: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-subpath-test-configmap-fzs2 container test-container-subpath-configmap-fzs2: <nil>
STEP: delete the pod
Feb  4 16:59:37.941: INFO: Waiting for pod pod-subpath-test-configmap-fzs2 to disappear
Feb  4 16:59:37.948: INFO: Pod pod-subpath-test-configmap-fzs2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fzs2
Feb  4 16:59:37.949: INFO: Deleting pod "pod-subpath-test-configmap-fzs2" in namespace "e2e-tests-subpath-jcvwd"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:59:37.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-jcvwd" for this suite.
Feb  4 16:59:44.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:59:44.199: INFO: namespace: e2e-tests-subpath-jcvwd, resource: bindings, ignored listing per whitelist
Feb  4 16:59:44.469: INFO: namespace e2e-tests-subpath-jcvwd deletion completed in 6.508316675s

• [SLOW TEST:31.191 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:59:44.473: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  4 16:59:44.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-v25hx'
Feb  4 16:59:45.776: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  4 16:59:45.776: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Feb  4 16:59:45.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-v25hx'
Feb  4 16:59:46.306: INFO: stderr: ""
Feb  4 16:59:46.306: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 16:59:46.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-v25hx" for this suite.
Feb  4 16:59:52.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 16:59:52.438: INFO: namespace: e2e-tests-kubectl-v25hx, resource: bindings, ignored listing per whitelist
Feb  4 16:59:52.708: INFO: namespace e2e-tests-kubectl-v25hx deletion completed in 6.389181s

• [SLOW TEST:8.236 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 16:59:52.710: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 16:59:52.950: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb  4 16:59:57.956: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  4 16:59:57.957: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb  4 17:00:00.056: INFO: Creating deployment "test-rollover-deployment"
Feb  4 17:00:00.114: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb  4 17:00:02.137: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb  4 17:00:02.151: INFO: Ensure that both replica sets have 1 created replica
Feb  4 17:00:02.162: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb  4 17:00:02.185: INFO: Updating deployment test-rollover-deployment
Feb  4 17:00:02.185: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb  4 17:00:04.209: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb  4 17:00:04.219: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb  4 17:00:04.233: INFO: all replica sets need to contain the pod-template-hash label
Feb  4 17:00:04.233: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896402, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  4 17:00:06.249: INFO: all replica sets need to contain the pod-template-hash label
Feb  4 17:00:06.250: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896404, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  4 17:00:08.247: INFO: all replica sets need to contain the pod-template-hash label
Feb  4 17:00:08.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896404, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  4 17:00:10.277: INFO: all replica sets need to contain the pod-template-hash label
Feb  4 17:00:10.277: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896404, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  4 17:00:12.362: INFO: all replica sets need to contain the pod-template-hash label
Feb  4 17:00:12.362: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896404, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  4 17:00:14.351: INFO: all replica sets need to contain the pod-template-hash label
Feb  4 17:00:14.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896404, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684896400, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  4 17:00:16.256: INFO: 
Feb  4 17:00:16.256: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  4 17:00:16.281: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-pfzrf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pfzrf/deployments/test-rollover-deployment,UID:4ea1f988-289e-11e9-9959-0a580af41676,ResourceVersion:12507,Generation:2,CreationTimestamp:2019-02-04 17:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-04 17:00:00 +0000 UTC 2019-02-04 17:00:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-04 17:00:14 +0000 UTC 2019-02-04 17:00:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb  4 17:00:16.287: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-pfzrf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pfzrf/replicasets/test-rollover-deployment-6b7f9d6597,UID:4fe69f68-289e-11e9-9a5d-0a580af41e66,ResourceVersion:12496,Generation:2,CreationTimestamp:2019-02-04 17:00:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4ea1f988-289e-11e9-9959-0a580af41676 0xc0024fad67 0xc0024fad68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb  4 17:00:16.288: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb  4 17:00:16.288: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-pfzrf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pfzrf/replicasets/test-rollover-controller,UID:4a62c746-289e-11e9-9959-0a580af41676,ResourceVersion:12506,Generation:2,CreationTimestamp:2019-02-04 16:59:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4ea1f988-289e-11e9-9959-0a580af41676 0xc0024fabd7 0xc0024fabd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  4 17:00:16.289: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-pfzrf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pfzrf/replicasets/test-rollover-deployment-6586df867b,UID:4eafb89a-289e-11e9-9a5d-0a580af41e66,ResourceVersion:12454,Generation:2,CreationTimestamp:2019-02-04 17:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4ea1f988-289e-11e9-9959-0a580af41676 0xc0024fac97 0xc0024fac98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  4 17:00:16.296: INFO: Pod "test-rollover-deployment-6b7f9d6597-q8zx6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-q8zx6,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-pfzrf,SelfLink:/api/v1/namespaces/e2e-tests-deployment-pfzrf/pods/test-rollover-deployment-6b7f9d6597-q8zx6,UID:4ff44b76-289e-11e9-9a5d-0a580af41e66,ResourceVersion:12469,Generation:0,CreationTimestamp:2019-02-04 17:00:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 4fe69f68-289e-11e9-9a5d-0a580af41e66 0xc0024fb8a7 0xc0024fb8a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gsdhb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gsdhb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-gsdhb true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024fb910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024fb930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:00:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:00:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:00:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:00:02 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.0.90,StartTime:2019-02-04 17:00:02 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-04 17:00:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://2d1a56cde5c5784c9520e4784c6bd07fb9db621042883e3bda5032536e85ef1b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:00:16.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-pfzrf" for this suite.
Feb  4 17:00:24.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:00:24.717: INFO: namespace: e2e-tests-deployment-pfzrf, resource: bindings, ignored listing per whitelist
Feb  4 17:00:24.782: INFO: namespace e2e-tests-deployment-pfzrf deletion completed in 8.47818056s

• [SLOW TEST:32.073 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:00:24.787: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:00:25.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-hvqmm" for this suite.
Feb  4 17:00:49.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:00:49.554: INFO: namespace: e2e-tests-kubelet-test-hvqmm, resource: bindings, ignored listing per whitelist
Feb  4 17:00:49.792: INFO: namespace e2e-tests-kubelet-test-hvqmm deletion completed in 24.539724571s

• [SLOW TEST:25.006 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:00:49.796: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb  4 17:00:56.961: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6c7a5ccc-289e-11e9-9eab-8e57f341003b"
Feb  4 17:00:56.961: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6c7a5ccc-289e-11e9-9eab-8e57f341003b" in namespace "e2e-tests-pods-dp2xd" to be "terminated due to deadline exceeded"
Feb  4 17:00:56.968: INFO: Pod "pod-update-activedeadlineseconds-6c7a5ccc-289e-11e9-9eab-8e57f341003b": Phase="Running", Reason="", readiness=true. Elapsed: 6.367522ms
Feb  4 17:00:59.005: INFO: Pod "pod-update-activedeadlineseconds-6c7a5ccc-289e-11e9-9eab-8e57f341003b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.04366542s
Feb  4 17:00:59.005: INFO: Pod "pod-update-activedeadlineseconds-6c7a5ccc-289e-11e9-9eab-8e57f341003b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:00:59.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-dp2xd" for this suite.
Feb  4 17:01:05.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:01:05.174: INFO: namespace: e2e-tests-pods-dp2xd, resource: bindings, ignored listing per whitelist
Feb  4 17:01:05.376: INFO: namespace e2e-tests-pods-dp2xd deletion completed in 6.34639297s

• [SLOW TEST:15.581 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:01:05.377: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-75afdac2-289e-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 17:01:05.620: INFO: Waiting up to 5m0s for pod "pod-secrets-75b18da4-289e-11e9-9eab-8e57f341003b" in namespace "e2e-tests-secrets-gkwzh" to be "success or failure"
Feb  4 17:01:05.631: INFO: Pod "pod-secrets-75b18da4-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.809901ms
Feb  4 17:01:07.637: INFO: Pod "pod-secrets-75b18da4-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017188875s
Feb  4 17:01:09.644: INFO: Pod "pod-secrets-75b18da4-289e-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024404303s
STEP: Saw pod success
Feb  4 17:01:09.645: INFO: Pod "pod-secrets-75b18da4-289e-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:01:09.655: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-secrets-75b18da4-289e-11e9-9eab-8e57f341003b container secret-env-test: <nil>
STEP: delete the pod
Feb  4 17:01:09.790: INFO: Waiting for pod pod-secrets-75b18da4-289e-11e9-9eab-8e57f341003b to disappear
Feb  4 17:01:09.796: INFO: Pod pod-secrets-75b18da4-289e-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:01:09.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-gkwzh" for this suite.
Feb  4 17:01:15.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:01:16.105: INFO: namespace: e2e-tests-secrets-gkwzh, resource: bindings, ignored listing per whitelist
Feb  4 17:01:16.166: INFO: namespace e2e-tests-secrets-gkwzh deletion completed in 6.361520366s

• [SLOW TEST:10.789 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:01:16.172: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-7c20a47d-289e-11e9-9eab-8e57f341003b
STEP: Creating secret with name secret-projected-all-test-volume-7c20a467-289e-11e9-9eab-8e57f341003b
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb  4 17:01:16.451: INFO: Waiting up to 5m0s for pod "projected-volume-7c20a42f-289e-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-fc4r2" to be "success or failure"
Feb  4 17:01:16.461: INFO: Pod "projected-volume-7c20a42f-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.548977ms
Feb  4 17:01:18.470: INFO: Pod "projected-volume-7c20a42f-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018160642s
Feb  4 17:01:20.477: INFO: Pod "projected-volume-7c20a42f-289e-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024958664s
STEP: Saw pod success
Feb  4 17:01:20.477: INFO: Pod "projected-volume-7c20a42f-289e-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:01:20.482: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod projected-volume-7c20a42f-289e-11e9-9eab-8e57f341003b container projected-all-volume-test: <nil>
STEP: delete the pod
Feb  4 17:01:20.542: INFO: Waiting for pod projected-volume-7c20a42f-289e-11e9-9eab-8e57f341003b to disappear
Feb  4 17:01:20.548: INFO: Pod projected-volume-7c20a42f-289e-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:01:20.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fc4r2" for this suite.
Feb  4 17:01:26.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:01:26.622: INFO: namespace: e2e-tests-projected-fc4r2, resource: bindings, ignored listing per whitelist
Feb  4 17:01:27.004: INFO: namespace e2e-tests-projected-fc4r2 deletion completed in 6.449930113s

• [SLOW TEST:10.833 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:01:27.008: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-829200a8-289e-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 17:01:27.261: INFO: Waiting up to 5m0s for pod "pod-secrets-82953a09-289e-11e9-9eab-8e57f341003b" in namespace "e2e-tests-secrets-qrzcx" to be "success or failure"
Feb  4 17:01:27.269: INFO: Pod "pod-secrets-82953a09-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.946056ms
Feb  4 17:01:29.276: INFO: Pod "pod-secrets-82953a09-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015249512s
Feb  4 17:01:31.284: INFO: Pod "pod-secrets-82953a09-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02320626s
Feb  4 17:01:33.293: INFO: Pod "pod-secrets-82953a09-289e-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0316966s
STEP: Saw pod success
Feb  4 17:01:33.293: INFO: Pod "pod-secrets-82953a09-289e-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:01:33.297: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-secrets-82953a09-289e-11e9-9eab-8e57f341003b container secret-volume-test: <nil>
STEP: delete the pod
Feb  4 17:01:33.507: INFO: Waiting for pod pod-secrets-82953a09-289e-11e9-9eab-8e57f341003b to disappear
Feb  4 17:01:33.514: INFO: Pod pod-secrets-82953a09-289e-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:01:33.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-qrzcx" for this suite.
Feb  4 17:01:39.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:01:39.802: INFO: namespace: e2e-tests-secrets-qrzcx, resource: bindings, ignored listing per whitelist
Feb  4 17:01:39.983: INFO: namespace e2e-tests-secrets-qrzcx deletion completed in 6.460990835s

• [SLOW TEST:12.976 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:01:39.989: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:01:40.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 version --client'
Feb  4 17:01:40.540: INFO: stderr: ""
Feb  4 17:01:40.540: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb  4 17:01:40.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-tzls4'
Feb  4 17:01:41.319: INFO: stderr: ""
Feb  4 17:01:41.319: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb  4 17:01:41.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-tzls4'
Feb  4 17:01:42.243: INFO: stderr: ""
Feb  4 17:01:42.244: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb  4 17:01:43.310: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 17:01:43.310: INFO: Found 0 / 1
Feb  4 17:01:44.250: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 17:01:44.250: INFO: Found 0 / 1
Feb  4 17:01:45.262: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 17:01:45.262: INFO: Found 0 / 1
Feb  4 17:01:46.253: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 17:01:46.253: INFO: Found 1 / 1
Feb  4 17:01:46.253: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  4 17:01:46.259: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 17:01:46.259: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  4 17:01:46.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 describe pod redis-master-mcwpm --namespace=e2e-tests-kubectl-tzls4'
Feb  4 17:01:46.570: INFO: stderr: ""
Feb  4 17:01:46.570: INFO: stdout: "Name:               redis-master-mcwpm\nNamespace:          e2e-tests-kubectl-tzls4\nPriority:           0\nPriorityClassName:  <none>\nNode:               machine-kubermatic-conformancecluster-61wiwtyxah-t5p54/192.168.1.10\nStart Time:         Mon, 04 Feb 2019 17:01:41 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 172.25.1.65\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://14fc9039c4dfcbf3f726510a6dd127ead51b2bb12468ce5333fd9052cff570d0\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 04 Feb 2019 17:01:45 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xnn5z (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-xnn5z:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-xnn5z\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                             Message\n  ----    ------     ----  ----                                                             -------\n  Normal  Scheduled  5s    default-scheduler                                                Successfully assigned e2e-tests-kubectl-tzls4/redis-master-mcwpm to machine-kubermatic-conformancecluster-61wiwtyxah-t5p54\n  Normal  Pulling    3s    kubelet, machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     1s    kubelet, machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    1s    kubelet, machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Created container\n  Normal  Started    1s    kubelet, machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Started container\n"
Feb  4 17:01:46.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 describe rc redis-master --namespace=e2e-tests-kubectl-tzls4'
Feb  4 17:01:46.916: INFO: stderr: ""
Feb  4 17:01:46.916: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-tzls4\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: redis-master-mcwpm\n"
Feb  4 17:01:46.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 describe service redis-master --namespace=e2e-tests-kubectl-tzls4'
Feb  4 17:01:47.245: INFO: stderr: ""
Feb  4 17:01:47.245: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-tzls4\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.10.10.236\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.25.1.65:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb  4 17:01:47.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 describe node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf'
Feb  4 17:01:47.612: INFO: stderr: ""
Feb  4 17:01:47.613: INFO: stdout: "Name:               machine-kubermatic-conformancecluster-61wiwtyxah-6rthf\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=72c6b6e7-de6f-4da4-a457-6dd477c99aa3\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=dbl\n                    failure-domain.beta.kubernetes.io/zone=dbl1\n                    kubernetes.io/hostname=machine-kubermatic-conformancecluster-61wiwtyxah-6rthf\n                    machine-controller/owned-by=8280d54b-2897-11e9-950f-0a580af41327\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"4a:54:cb:8f:f2:d0\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.3\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 04 Feb 2019 16:16:02 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 04 Feb 2019 17:01:45 +0000   Mon, 04 Feb 2019 16:16:02 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 04 Feb 2019 17:01:45 +0000   Mon, 04 Feb 2019 16:16:02 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 04 Feb 2019 17:01:45 +0000   Mon, 04 Feb 2019 16:16:02 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 04 Feb 2019 17:01:45 +0000   Mon, 04 Feb 2019 16:16:32 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.1.3\n  ExternalIP:  195.192.132.78\n  Hostname:    machine-kubermatic-conformancecluster-61wiwtyxah-6rthf\nCapacity:\n cpu:                1\n ephemeral-storage:  51473000Ki\n hugepages-2Mi:      0\n memory:             2041240Ki\n pods:               110\nAllocatable:\n cpu:                1\n ephemeral-storage:  47437516722\n hugepages-2Mi:      0\n memory:             1938840Ki\n pods:               110\nSystem Info:\n Machine ID:                 1e808eccff864574b25e377eda9fbdd1\n System UUID:                21C9E5B0-6BB2-491D-BA7F-F63814F9D822\n Boot ID:                    ee485898-4252-45b1-98c6-b039e9c03273\n Kernel Version:             4.15.0-30-generic\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.0\n Kubelet Version:            v1.13.2\n Kube-Proxy Version:         v1.13.2\nPodCIDR:                     172.25.0.0/24\nProviderID:                  openstack:///21c9e5b0-6bb2-491d-ba7f-f63814f9d822\nNon-terminated Pods:         (14 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-ark                 ark-5f9c9897c5-gmcbs                                       10m (1%)      100m (10%)  64Mi (3%)        300Mi (15%)    50m\n  heptio-ark                 restic-9gtnw                                               5m (0%)       100m (10%)  32Mi (1%)        300Mi (15%)    45m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-fde55ff1e0804945-6mf79    0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  kube-system                canal-7fvph                                                250m (25%)    0 (0%)      0 (0%)           0 (0%)         45m\n  kube-system                cluster-autoscaler-5f8478765d-8rjp2                        10m (1%)      100m (10%)  40Mi (2%)        300Mi (15%)    50m\n  kube-system                kube-dns-6fcf8486c9-cfvbk                                  260m (26%)    0 (0%)      110Mi (5%)       170Mi (8%)     50m\n  kube-system                kube-dns-6fcf8486c9-fp64s                                  260m (26%)    0 (0%)      110Mi (5%)       170Mi (8%)     43m\n  kube-system                kube-dns-autoscaler-6686fffcd4-q7xr8                       20m (2%)      0 (0%)      10Mi (0%)        0 (0%)         50m\n  kube-system                kube-proxy-lwvdf                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\n  kube-system                kubernetes-dashboard-657fd84c97-4t2v8                      50m (5%)      100m (10%)  100Mi (5%)       300Mi (15%)    50m\n  kube-system                metrics-server-6b98b49585-cwwl9                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                node-exporter-wzptg                                        3m (0%)       200m (20%)  16Mi (0%)        50Mi (2%)      45m\n  kube-system                openvpn-client-7c9496f697-sq426                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                tiller-deploy-7cf4c86f8-ntzqt                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                868m (86%)   600m (60%)\n  memory             482Mi (25%)  1590Mi (83%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                From                                                                Message\n  ----    ------                   ----               ----                                                                -------\n  Normal  Starting                 45m                kubelet, machine-kubermatic-conformancecluster-61wiwtyxah-6rthf     Starting kubelet.\n  Normal  NodeHasSufficientMemory  45m (x2 over 45m)  kubelet, machine-kubermatic-conformancecluster-61wiwtyxah-6rthf     Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    45m (x2 over 45m)  kubelet, machine-kubermatic-conformancecluster-61wiwtyxah-6rthf     Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     45m (x2 over 45m)  kubelet, machine-kubermatic-conformancecluster-61wiwtyxah-6rthf     Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  45m                kubelet, machine-kubermatic-conformancecluster-61wiwtyxah-6rthf     Updated Node Allocatable limit across pods\n  Normal  Starting                 45m                kube-proxy, machine-kubermatic-conformancecluster-61wiwtyxah-6rthf  Starting kube-proxy.\n  Normal  NodeReady                45m                kubelet, machine-kubermatic-conformancecluster-61wiwtyxah-6rthf     Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf status is now: NodeReady\n"
Feb  4 17:01:47.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 describe namespace e2e-tests-kubectl-tzls4'
Feb  4 17:01:47.916: INFO: stderr: ""
Feb  4 17:01:47.916: INFO: stdout: "Name:         e2e-tests-kubectl-tzls4\nLabels:       e2e-framework=kubectl\n              e2e-run=cee1d542-2898-11e9-9eab-8e57f341003b\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:01:47.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tzls4" for this suite.
Feb  4 17:02:12.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:02:12.044: INFO: namespace: e2e-tests-kubectl-tzls4, resource: bindings, ignored listing per whitelist
Feb  4 17:02:12.399: INFO: namespace e2e-tests-kubectl-tzls4 deletion completed in 24.465596973s

• [SLOW TEST:32.411 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:02:12.403: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Feb  4 17:02:12.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-v79t8'
Feb  4 17:02:13.400: INFO: stderr: ""
Feb  4 17:02:13.400: INFO: stdout: "pod/pause created\n"
Feb  4 17:02:13.400: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb  4 17:02:13.400: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-v79t8" to be "running and ready"
Feb  4 17:02:13.414: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 13.974392ms
Feb  4 17:02:15.422: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022801938s
Feb  4 17:02:17.429: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.029036033s
Feb  4 17:02:17.429: INFO: Pod "pause" satisfied condition "running and ready"
Feb  4 17:02:17.429: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb  4 17:02:17.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-v79t8'
Feb  4 17:02:17.709: INFO: stderr: ""
Feb  4 17:02:17.709: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb  4 17:02:17.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pod pause -L testing-label --namespace=e2e-tests-kubectl-v79t8'
Feb  4 17:02:18.010: INFO: stderr: ""
Feb  4 17:02:18.010: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb  4 17:02:18.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 label pods pause testing-label- --namespace=e2e-tests-kubectl-v79t8'
Feb  4 17:02:18.297: INFO: stderr: ""
Feb  4 17:02:18.297: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb  4 17:02:18.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pod pause -L testing-label --namespace=e2e-tests-kubectl-v79t8'
Feb  4 17:02:18.556: INFO: stderr: ""
Feb  4 17:02:18.556: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Feb  4 17:02:18.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-v79t8'
Feb  4 17:02:19.085: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  4 17:02:19.085: INFO: stdout: "pod \"pause\" force deleted\n"
Feb  4 17:02:19.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-v79t8'
Feb  4 17:02:19.531: INFO: stderr: "No resources found.\n"
Feb  4 17:02:19.531: INFO: stdout: ""
Feb  4 17:02:19.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -l name=pause --namespace=e2e-tests-kubectl-v79t8 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  4 17:02:19.990: INFO: stderr: ""
Feb  4 17:02:19.990: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:02:19.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-v79t8" for this suite.
Feb  4 17:02:26.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:02:26.164: INFO: namespace: e2e-tests-kubectl-v79t8, resource: bindings, ignored listing per whitelist
Feb  4 17:02:26.421: INFO: namespace e2e-tests-kubectl-v79t8 deletion completed in 6.422684966s

• [SLOW TEST:14.018 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:02:26.422: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 17:02:26.623: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5fa8990-289e-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-jmt4n" to be "success or failure"
Feb  4 17:02:26.633: INFO: Pod "downwardapi-volume-a5fa8990-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.542845ms
Feb  4 17:02:28.640: INFO: Pod "downwardapi-volume-a5fa8990-289e-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017441742s
STEP: Saw pod success
Feb  4 17:02:28.641: INFO: Pod "downwardapi-volume-a5fa8990-289e-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:02:28.656: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod downwardapi-volume-a5fa8990-289e-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 17:02:28.701: INFO: Waiting for pod downwardapi-volume-a5fa8990-289e-11e9-9eab-8e57f341003b to disappear
Feb  4 17:02:28.707: INFO: Pod downwardapi-volume-a5fa8990-289e-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:02:28.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jmt4n" for this suite.
Feb  4 17:02:34.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:02:35.098: INFO: namespace: e2e-tests-downward-api-jmt4n, resource: bindings, ignored listing per whitelist
Feb  4 17:02:35.145: INFO: namespace e2e-tests-downward-api-jmt4n deletion completed in 6.426760542s

• [SLOW TEST:8.723 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:02:35.148: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0204 17:02:41.536011      13 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  4 17:02:41.536: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:02:41.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-srjpw" for this suite.
Feb  4 17:02:49.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:02:49.815: INFO: namespace: e2e-tests-gc-srjpw, resource: bindings, ignored listing per whitelist
Feb  4 17:02:49.989: INFO: namespace e2e-tests-gc-srjpw deletion completed in 8.447224673s

• [SLOW TEST:14.842 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:02:49.993: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:02:50.337: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb  4 17:02:55.467: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  4 17:02:55.467: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  4 17:02:55.516: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-hz45q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hz45q/deployments/test-cleanup-deployment,UID:b731338b-289e-11e9-9959-0a580af41676,ResourceVersion:13359,Generation:1,CreationTimestamp:2019-02-04 17:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Feb  4 17:02:55.555: INFO: New ReplicaSet "test-cleanup-deployment-7dbbfcf846" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846,GenerateName:,Namespace:e2e-tests-deployment-hz45q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hz45q/replicasets/test-cleanup-deployment-7dbbfcf846,UID:b7368be0-289e-11e9-9a5d-0a580af41e66,ResourceVersion:13361,Generation:1,CreationTimestamp:2019-02-04 17:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment b731338b-289e-11e9-9959-0a580af41676 0xc0007cd557 0xc0007cd558}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  4 17:02:55.555: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb  4 17:02:55.556: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-hz45q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hz45q/replicasets/test-cleanup-controller,UID:b4151671-289e-11e9-9959-0a580af41676,ResourceVersion:13360,Generation:1,CreationTimestamp:2019-02-04 17:02:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment b731338b-289e-11e9-9959-0a580af41676 0xc0007cd497 0xc0007cd498}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb  4 17:02:55.564: INFO: Pod "test-cleanup-controller-l9cgt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-l9cgt,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-hz45q,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hz45q/pods/test-cleanup-controller-l9cgt,UID:b421256d-289e-11e9-9a5d-0a580af41e66,ResourceVersion:13349,Generation:0,CreationTimestamp:2019-02-04 17:02:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller b4151671-289e-11e9-9959-0a580af41676 0xc0007cdf37 0xc0007cdf38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-sz8hg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sz8hg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sz8hg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007cdfa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007cdfd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:02:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:02:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:02:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:02:50 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:172.25.1.69,StartTime:2019-02-04 17:02:50 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-04 17:02:52 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://111f3323855fda37aa9245f59edc19849b41c8f8b1a3a0bf3c157fb0d6380064}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  4 17:02:55.565: INFO: Pod "test-cleanup-deployment-7dbbfcf846-jlljw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846-jlljw,GenerateName:test-cleanup-deployment-7dbbfcf846-,Namespace:e2e-tests-deployment-hz45q,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hz45q/pods/test-cleanup-deployment-7dbbfcf846-jlljw,UID:b7382bcf-289e-11e9-9a5d-0a580af41e66,ResourceVersion:13364,Generation:0,CreationTimestamp:2019-02-04 17:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-7dbbfcf846 b7368be0-289e-11e9-9a5d-0a580af41e66 0xc001496097 0xc001496098}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-sz8hg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sz8hg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-sz8hg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001496100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001496120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:02:55.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-hz45q" for this suite.
Feb  4 17:03:01.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:03:01.739: INFO: namespace: e2e-tests-deployment-hz45q, resource: bindings, ignored listing per whitelist
Feb  4 17:03:01.988: INFO: namespace e2e-tests-deployment-hz45q deletion completed in 6.390551315s

• [SLOW TEST:11.995 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:03:01.994: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-bb331058-289e-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 17:03:02.243: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb345a57-289e-11e9-9eab-8e57f341003b" in namespace "e2e-tests-configmap-k46fh" to be "success or failure"
Feb  4 17:03:02.278: INFO: Pod "pod-configmaps-bb345a57-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 34.969453ms
Feb  4 17:03:04.292: INFO: Pod "pod-configmaps-bb345a57-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049129363s
Feb  4 17:03:06.312: INFO: Pod "pod-configmaps-bb345a57-289e-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068789354s
STEP: Saw pod success
Feb  4 17:03:06.312: INFO: Pod "pod-configmaps-bb345a57-289e-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:03:06.317: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-configmaps-bb345a57-289e-11e9-9eab-8e57f341003b container configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 17:03:06.487: INFO: Waiting for pod pod-configmaps-bb345a57-289e-11e9-9eab-8e57f341003b to disappear
Feb  4 17:03:06.501: INFO: Pod pod-configmaps-bb345a57-289e-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:03:06.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-k46fh" for this suite.
Feb  4 17:03:12.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:03:12.841: INFO: namespace: e2e-tests-configmap-k46fh, resource: bindings, ignored listing per whitelist
Feb  4 17:03:12.905: INFO: namespace e2e-tests-configmap-k46fh deletion completed in 6.396714865s

• [SLOW TEST:10.912 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:03:12.909: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  4 17:03:13.207: INFO: Waiting up to 5m0s for pod "downward-api-c1be1237-289e-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-wqwg7" to be "success or failure"
Feb  4 17:03:13.214: INFO: Pod "downward-api-c1be1237-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.599042ms
Feb  4 17:03:15.229: INFO: Pod "downward-api-c1be1237-289e-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021178148s
Feb  4 17:03:17.244: INFO: Pod "downward-api-c1be1237-289e-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03673553s
STEP: Saw pod success
Feb  4 17:03:17.244: INFO: Pod "downward-api-c1be1237-289e-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:03:17.254: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downward-api-c1be1237-289e-11e9-9eab-8e57f341003b container dapi-container: <nil>
STEP: delete the pod
Feb  4 17:03:17.402: INFO: Waiting for pod downward-api-c1be1237-289e-11e9-9eab-8e57f341003b to disappear
Feb  4 17:03:17.410: INFO: Pod downward-api-c1be1237-289e-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:03:17.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wqwg7" for this suite.
Feb  4 17:03:23.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:03:23.772: INFO: namespace: e2e-tests-downward-api-wqwg7, resource: bindings, ignored listing per whitelist
Feb  4 17:03:23.850: INFO: namespace e2e-tests-downward-api-wqwg7 deletion completed in 6.419727929s

• [SLOW TEST:10.941 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:03:23.854: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-2dt2d
Feb  4 17:03:28.096: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-2dt2d
STEP: checking the pod's current state and verifying that restartCount is present
Feb  4 17:03:28.103: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:07:28.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-2dt2d" for this suite.
Feb  4 17:07:34.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:07:34.516: INFO: namespace: e2e-tests-container-probe-2dt2d, resource: bindings, ignored listing per whitelist
Feb  4 17:07:34.626: INFO: namespace e2e-tests-container-probe-2dt2d deletion completed in 6.459130814s

• [SLOW TEST:250.773 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:07:34.627: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb  4 17:07:34.995: INFO: Waiting up to 5m0s for pod "pod-5dc71fa3-289f-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-5gj8c" to be "success or failure"
Feb  4 17:07:35.002: INFO: Pod "pod-5dc71fa3-289f-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.493749ms
Feb  4 17:07:37.010: INFO: Pod "pod-5dc71fa3-289f-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015626449s
Feb  4 17:07:39.016: INFO: Pod "pod-5dc71fa3-289f-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021727493s
STEP: Saw pod success
Feb  4 17:07:39.017: INFO: Pod "pod-5dc71fa3-289f-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:07:39.021: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-5dc71fa3-289f-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 17:07:39.176: INFO: Waiting for pod pod-5dc71fa3-289f-11e9-9eab-8e57f341003b to disappear
Feb  4 17:07:39.192: INFO: Pod pod-5dc71fa3-289f-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:07:39.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5gj8c" for this suite.
Feb  4 17:07:45.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:07:45.580: INFO: namespace: e2e-tests-emptydir-5gj8c, resource: bindings, ignored listing per whitelist
Feb  4 17:07:45.683: INFO: namespace e2e-tests-emptydir-5gj8c deletion completed in 6.481791456s

• [SLOW TEST:11.057 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:07:45.688: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-6468f91e-289f-11e9-9eab-8e57f341003b
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-6468f91e-289f-11e9-9eab-8e57f341003b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:09:10.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-xtx5k" for this suite.
Feb  4 17:09:34.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:09:34.181: INFO: namespace: e2e-tests-configmap-xtx5k, resource: bindings, ignored listing per whitelist
Feb  4 17:09:34.509: INFO: namespace e2e-tests-configmap-xtx5k deletion completed in 24.390620139s

• [SLOW TEST:108.822 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:09:34.511: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  4 17:09:34.732: INFO: Waiting up to 5m0s for pod "downward-api-a526e267-289f-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-6hnls" to be "success or failure"
Feb  4 17:09:34.739: INFO: Pod "downward-api-a526e267-289f-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.990147ms
Feb  4 17:09:36.777: INFO: Pod "downward-api-a526e267-289f-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04544909s
Feb  4 17:09:38.796: INFO: Pod "downward-api-a526e267-289f-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06350015s
STEP: Saw pod success
Feb  4 17:09:38.796: INFO: Pod "downward-api-a526e267-289f-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:09:38.800: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downward-api-a526e267-289f-11e9-9eab-8e57f341003b container dapi-container: <nil>
STEP: delete the pod
Feb  4 17:09:38.994: INFO: Waiting for pod downward-api-a526e267-289f-11e9-9eab-8e57f341003b to disappear
Feb  4 17:09:39.000: INFO: Pod downward-api-a526e267-289f-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:09:39.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6hnls" for this suite.
Feb  4 17:09:45.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:09:45.401: INFO: namespace: e2e-tests-downward-api-6hnls, resource: bindings, ignored listing per whitelist
Feb  4 17:09:45.480: INFO: namespace e2e-tests-downward-api-6hnls deletion completed in 6.472390116s

• [SLOW TEST:10.969 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:09:45.484: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:10:15.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-s989d" for this suite.
Feb  4 17:10:21.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:10:21.882: INFO: namespace: e2e-tests-container-runtime-s989d, resource: bindings, ignored listing per whitelist
Feb  4 17:10:22.157: INFO: namespace e2e-tests-container-runtime-s989d deletion completed in 6.414436933s

• [SLOW TEST:36.673 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:10:22.162: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-4r6kz
Feb  4 17:10:28.433: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-4r6kz
STEP: checking the pod's current state and verifying that restartCount is present
Feb  4 17:10:28.439: INFO: Initial restart count of pod liveness-http is 0
Feb  4 17:10:44.705: INFO: Restart count of pod e2e-tests-container-probe-4r6kz/liveness-http is now 1 (16.265916925s elapsed)
Feb  4 17:11:04.844: INFO: Restart count of pod e2e-tests-container-probe-4r6kz/liveness-http is now 2 (36.405434579s elapsed)
Feb  4 17:11:25.025: INFO: Restart count of pod e2e-tests-container-probe-4r6kz/liveness-http is now 3 (56.586185635s elapsed)
Feb  4 17:11:45.280: INFO: Restart count of pod e2e-tests-container-probe-4r6kz/liveness-http is now 4 (1m16.841108214s elapsed)
Feb  4 17:12:48.238: INFO: Restart count of pod e2e-tests-container-probe-4r6kz/liveness-http is now 5 (2m19.799218815s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:12:48.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-4r6kz" for this suite.
Feb  4 17:12:54.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:12:54.649: INFO: namespace: e2e-tests-container-probe-4r6kz, resource: bindings, ignored listing per whitelist
Feb  4 17:12:54.714: INFO: namespace e2e-tests-container-probe-4r6kz deletion completed in 6.440453171s

• [SLOW TEST:152.552 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:12:54.714: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 17:12:54.949: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c7b1eca-28a0-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-h4dkr" to be "success or failure"
Feb  4 17:12:54.956: INFO: Pod "downwardapi-volume-1c7b1eca-28a0-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.726348ms
Feb  4 17:12:56.962: INFO: Pod "downwardapi-volume-1c7b1eca-28a0-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01254583s
Feb  4 17:12:58.972: INFO: Pod "downwardapi-volume-1c7b1eca-28a0-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021937922s
STEP: Saw pod success
Feb  4 17:12:58.972: INFO: Pod "downwardapi-volume-1c7b1eca-28a0-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:12:58.977: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod downwardapi-volume-1c7b1eca-28a0-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 17:12:59.040: INFO: Waiting for pod downwardapi-volume-1c7b1eca-28a0-11e9-9eab-8e57f341003b to disappear
Feb  4 17:12:59.045: INFO: Pod downwardapi-volume-1c7b1eca-28a0-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:12:59.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h4dkr" for this suite.
Feb  4 17:13:05.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:13:05.134: INFO: namespace: e2e-tests-projected-h4dkr, resource: bindings, ignored listing per whitelist
Feb  4 17:13:05.377: INFO: namespace e2e-tests-projected-h4dkr deletion completed in 6.325784633s

• [SLOW TEST:10.663 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:13:05.381: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb  4 17:13:13.691: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:13.707: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:15.707: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:15.713: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:17.707: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:17.804: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:19.708: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:19.718: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:21.708: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:21.809: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:23.707: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:23.724: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:25.708: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:25.716: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:27.707: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:27.714: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:29.708: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:29.720: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:31.707: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:31.714: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:33.707: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:33.714: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:35.707: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:35.761: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:37.707: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:37.719: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:39.707: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:39.714: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  4 17:13:41.707: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  4 17:13:41.713: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:13:41.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-vjcsj" for this suite.
Feb  4 17:14:05.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:14:06.051: INFO: namespace: e2e-tests-container-lifecycle-hook-vjcsj, resource: bindings, ignored listing per whitelist
Feb  4 17:14:06.205: INFO: namespace e2e-tests-container-lifecycle-hook-vjcsj deletion completed in 24.396355716s

• [SLOW TEST:60.825 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:14:06.209: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb  4 17:14:10.459: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-4716cdf8-28a0-11e9-9eab-8e57f341003b", GenerateName:"", Namespace:"e2e-tests-pods-tbf6x", SelfLink:"/api/v1/namespaces/e2e-tests-pods-tbf6x/pods/pod-submit-remove-4716cdf8-28a0-11e9-9eab-8e57f341003b", UID:"471b9fd5-28a0-11e9-9959-0a580af41676", ResourceVersion:"15436", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63684897246, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"406622702"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-5krgm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001b36740), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5krgm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0014972f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"machine-kubermatic-conformancecluster-61wiwtyxah-6rthf", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0008673e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001497470)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001497490)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001497498), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00149749c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684897246, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684897248, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684897248, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684897246, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.3", PodIP:"172.25.0.106", StartTime:(*v1.Time)(0xc00190fd60), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc00190fd80), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://02e68924a0eb6e4516b0cb80c4c1581ac9164c7d2200944b0635370a970aee01"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:14:21.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-tbf6x" for this suite.
Feb  4 17:14:27.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:14:27.531: INFO: namespace: e2e-tests-pods-tbf6x, resource: bindings, ignored listing per whitelist
Feb  4 17:14:27.772: INFO: namespace e2e-tests-pods-tbf6x deletion completed in 6.433167464s

• [SLOW TEST:21.563 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:14:27.773: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-53f239d0-28a0-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 17:14:28.009: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-53f35bde-28a0-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-896vk" to be "success or failure"
Feb  4 17:14:28.024: INFO: Pod "pod-projected-configmaps-53f35bde-28a0-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.435155ms
Feb  4 17:14:30.031: INFO: Pod "pod-projected-configmaps-53f35bde-28a0-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021990627s
Feb  4 17:14:32.049: INFO: Pod "pod-projected-configmaps-53f35bde-28a0-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040144386s
STEP: Saw pod success
Feb  4 17:14:32.049: INFO: Pod "pod-projected-configmaps-53f35bde-28a0-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:14:32.056: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-projected-configmaps-53f35bde-28a0-11e9-9eab-8e57f341003b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 17:14:32.190: INFO: Waiting for pod pod-projected-configmaps-53f35bde-28a0-11e9-9eab-8e57f341003b to disappear
Feb  4 17:14:32.199: INFO: Pod pod-projected-configmaps-53f35bde-28a0-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:14:32.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-896vk" for this suite.
Feb  4 17:14:38.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:14:38.295: INFO: namespace: e2e-tests-projected-896vk, resource: bindings, ignored listing per whitelist
Feb  4 17:14:38.675: INFO: namespace e2e-tests-projected-896vk deletion completed in 6.468403981s

• [SLOW TEST:10.902 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:14:38.676: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:14:38.994: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:14:45.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4qptg" for this suite.
Feb  4 17:15:35.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:15:35.605: INFO: namespace: e2e-tests-pods-4qptg, resource: bindings, ignored listing per whitelist
Feb  4 17:15:35.625: INFO: namespace e2e-tests-pods-4qptg deletion completed in 50.341973155s

• [SLOW TEST:56.949 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:15:35.628: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb  4 17:15:35.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 --namespace=e2e-tests-kubectl-xffr8 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb  4 17:15:44.516: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb  4 17:15:44.516: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:15:46.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xffr8" for this suite.
Feb  4 17:15:52.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:15:52.747: INFO: namespace: e2e-tests-kubectl-xffr8, resource: bindings, ignored listing per whitelist
Feb  4 17:15:52.953: INFO: namespace e2e-tests-kubectl-xffr8 deletion completed in 6.40824273s

• [SLOW TEST:17.326 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:15:52.954: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb  4 17:15:53.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 api-versions'
Feb  4 17:15:53.458: INFO: stderr: ""
Feb  4 17:15:53.458: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nark.heptio.com/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:15:53.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hxstp" for this suite.
Feb  4 17:16:01.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:16:02.129: INFO: namespace: e2e-tests-kubectl-hxstp, resource: bindings, ignored listing per whitelist
Feb  4 17:16:02.134: INFO: namespace e2e-tests-kubectl-hxstp deletion completed in 8.667209458s

• [SLOW TEST:9.180 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:16:02.141: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb  4 17:16:04.682: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-8c38f73b-28a0-11e9-9eab-8e57f341003b,GenerateName:,Namespace:e2e-tests-events-8mjjr,SelfLink:/api/v1/namespaces/e2e-tests-events-8mjjr/pods/send-events-8c38f73b-28a0-11e9-9eab-8e57f341003b,UID:8c3bba97-28a0-11e9-9959-0a580af41676,ResourceVersion:15846,Generation:0,CreationTimestamp:2019-02-04 17:16:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 393285657,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-9ccf5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ccf5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-9ccf5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001814060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001814160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:16:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:16:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:16:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:16:02 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.0.108,StartTime:2019-02-04 17:16:02 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-04 17:16:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://901be1822158f7b6d11e770f33e784730849606feb1c3b206351051d71757e74}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb  4 17:16:06.689: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb  4 17:16:08.697: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:16:08.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-8mjjr" for this suite.
Feb  4 17:16:52.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:16:53.080: INFO: namespace: e2e-tests-events-8mjjr, resource: bindings, ignored listing per whitelist
Feb  4 17:16:53.146: INFO: namespace e2e-tests-events-8mjjr deletion completed in 44.428656579s

• [SLOW TEST:51.005 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:16:53.150: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb  4 17:16:53.383: INFO: Waiting up to 5m0s for pod "client-containers-aa9a5a40-28a0-11e9-9eab-8e57f341003b" in namespace "e2e-tests-containers-4f4pv" to be "success or failure"
Feb  4 17:16:53.394: INFO: Pod "client-containers-aa9a5a40-28a0-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.13969ms
Feb  4 17:16:55.414: INFO: Pod "client-containers-aa9a5a40-28a0-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03041517s
Feb  4 17:16:57.527: INFO: Pod "client-containers-aa9a5a40-28a0-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.143146731s
Feb  4 17:16:59.556: INFO: Pod "client-containers-aa9a5a40-28a0-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.172553969s
STEP: Saw pod success
Feb  4 17:16:59.556: INFO: Pod "client-containers-aa9a5a40-28a0-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:16:59.562: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod client-containers-aa9a5a40-28a0-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 17:16:59.749: INFO: Waiting for pod client-containers-aa9a5a40-28a0-11e9-9eab-8e57f341003b to disappear
Feb  4 17:16:59.810: INFO: Pod client-containers-aa9a5a40-28a0-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:16:59.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-4f4pv" for this suite.
Feb  4 17:17:05.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:17:06.220: INFO: namespace: e2e-tests-containers-4f4pv, resource: bindings, ignored listing per whitelist
Feb  4 17:17:06.239: INFO: namespace e2e-tests-containers-4f4pv deletion completed in 6.417583599s

• [SLOW TEST:13.089 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:17:06.240: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:18:06.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-fc588" for this suite.
Feb  4 17:18:30.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:18:30.686: INFO: namespace: e2e-tests-container-probe-fc588, resource: bindings, ignored listing per whitelist
Feb  4 17:18:31.099: INFO: namespace e2e-tests-container-probe-fc588 deletion completed in 24.662127339s

• [SLOW TEST:84.859 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:18:31.105: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-txnm
STEP: Creating a pod to test atomic-volume-subpath
Feb  4 17:18:31.972: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-txnm" in namespace "e2e-tests-subpath-8xjnq" to be "success or failure"
Feb  4 17:18:31.978: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.282372ms
Feb  4 17:18:33.987: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01485025s
Feb  4 17:18:36.009: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037011542s
Feb  4 17:18:38.016: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Running", Reason="", readiness=false. Elapsed: 6.043137554s
Feb  4 17:18:40.023: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Running", Reason="", readiness=false. Elapsed: 8.050542286s
Feb  4 17:18:42.030: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Running", Reason="", readiness=false. Elapsed: 10.057623877s
Feb  4 17:18:44.038: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Running", Reason="", readiness=false. Elapsed: 12.065260254s
Feb  4 17:18:46.106: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Running", Reason="", readiness=false. Elapsed: 14.133967417s
Feb  4 17:18:48.112: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Running", Reason="", readiness=false. Elapsed: 16.139906206s
Feb  4 17:18:50.121: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Running", Reason="", readiness=false. Elapsed: 18.148690723s
Feb  4 17:18:52.131: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Running", Reason="", readiness=false. Elapsed: 20.158423982s
Feb  4 17:18:54.137: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Running", Reason="", readiness=false. Elapsed: 22.164936598s
Feb  4 17:18:56.156: INFO: Pod "pod-subpath-test-downwardapi-txnm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.183177692s
STEP: Saw pod success
Feb  4 17:18:56.156: INFO: Pod "pod-subpath-test-downwardapi-txnm" satisfied condition "success or failure"
Feb  4 17:18:56.160: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-subpath-test-downwardapi-txnm container test-container-subpath-downwardapi-txnm: <nil>
STEP: delete the pod
Feb  4 17:18:56.363: INFO: Waiting for pod pod-subpath-test-downwardapi-txnm to disappear
Feb  4 17:18:56.369: INFO: Pod pod-subpath-test-downwardapi-txnm no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-txnm
Feb  4 17:18:56.369: INFO: Deleting pod "pod-subpath-test-downwardapi-txnm" in namespace "e2e-tests-subpath-8xjnq"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:18:56.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-8xjnq" for this suite.
Feb  4 17:19:02.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:19:02.652: INFO: namespace: e2e-tests-subpath-8xjnq, resource: bindings, ignored listing per whitelist
Feb  4 17:19:02.793: INFO: namespace e2e-tests-subpath-8xjnq deletion completed in 6.403065695s

• [SLOW TEST:31.689 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:19:02.800: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 17:19:03.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7e2431d-28a0-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-jsgxn" to be "success or failure"
Feb  4 17:19:03.055: INFO: Pod "downwardapi-volume-f7e2431d-28a0-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.540145ms
Feb  4 17:19:05.064: INFO: Pod "downwardapi-volume-f7e2431d-28a0-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025409387s
Feb  4 17:19:07.081: INFO: Pod "downwardapi-volume-f7e2431d-28a0-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042595589s
STEP: Saw pod success
Feb  4 17:19:07.081: INFO: Pod "downwardapi-volume-f7e2431d-28a0-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:19:07.086: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod downwardapi-volume-f7e2431d-28a0-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 17:19:07.141: INFO: Waiting for pod downwardapi-volume-f7e2431d-28a0-11e9-9eab-8e57f341003b to disappear
Feb  4 17:19:07.147: INFO: Pod downwardapi-volume-f7e2431d-28a0-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:19:07.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jsgxn" for this suite.
Feb  4 17:19:13.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:19:13.282: INFO: namespace: e2e-tests-downward-api-jsgxn, resource: bindings, ignored listing per whitelist
Feb  4 17:19:13.571: INFO: namespace e2e-tests-downward-api-jsgxn deletion completed in 6.417346586s

• [SLOW TEST:10.773 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:19:13.575: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:19:13.869: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:19:15.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-trwqr" for this suite.
Feb  4 17:19:21.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:19:21.497: INFO: namespace: e2e-tests-custom-resource-definition-trwqr, resource: bindings, ignored listing per whitelist
Feb  4 17:19:21.578: INFO: namespace e2e-tests-custom-resource-definition-trwqr deletion completed in 6.502007585s

• [SLOW TEST:8.003 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:19:21.581: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb  4 17:19:26.428: INFO: Successfully updated pod "pod-update-03159bbe-28a1-11e9-9eab-8e57f341003b"
STEP: verifying the updated pod is in kubernetes
Feb  4 17:19:26.448: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:19:26.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-f7vz2" for this suite.
Feb  4 17:19:50.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:19:50.828: INFO: namespace: e2e-tests-pods-f7vz2, resource: bindings, ignored listing per whitelist
Feb  4 17:19:51.089: INFO: namespace e2e-tests-pods-f7vz2 deletion completed in 24.634016973s

• [SLOW TEST:29.508 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:19:51.090: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-ksnwl.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-ksnwl.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-ksnwl.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-ksnwl.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-ksnwl.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-ksnwl.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  4 17:20:23.138: INFO: DNS probes using e2e-tests-dns-ksnwl/dns-test-14aa2740-28a1-11e9-9eab-8e57f341003b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:20:23.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-ksnwl" for this suite.
Feb  4 17:20:29.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:20:29.340: INFO: namespace: e2e-tests-dns-ksnwl, resource: bindings, ignored listing per whitelist
Feb  4 17:20:29.613: INFO: namespace e2e-tests-dns-ksnwl deletion completed in 6.422733579s

• [SLOW TEST:38.524 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:20:29.617: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb  4 17:20:54.894: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:20:56.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-t2csx" for this suite.
Feb  4 17:21:20.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:21:20.138: INFO: namespace: e2e-tests-replicaset-t2csx, resource: bindings, ignored listing per whitelist
Feb  4 17:21:20.367: INFO: namespace e2e-tests-replicaset-t2csx deletion completed in 24.350415326s

• [SLOW TEST:50.750 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:21:20.375: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-49e908e3-28a1-11e9-9eab-8e57f341003b
Feb  4 17:21:20.654: INFO: Pod name my-hostname-basic-49e908e3-28a1-11e9-9eab-8e57f341003b: Found 0 pods out of 1
Feb  4 17:21:25.834: INFO: Pod name my-hostname-basic-49e908e3-28a1-11e9-9eab-8e57f341003b: Found 1 pods out of 1
Feb  4 17:21:25.834: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-49e908e3-28a1-11e9-9eab-8e57f341003b" are running
Feb  4 17:21:25.841: INFO: Pod "my-hostname-basic-49e908e3-28a1-11e9-9eab-8e57f341003b-c6tq7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-04 17:21:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-04 17:21:23 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-04 17:21:23 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-04 17:21:20 +0000 UTC Reason: Message:}])
Feb  4 17:21:25.842: INFO: Trying to dial the pod
Feb  4 17:21:31.042: INFO: Controller my-hostname-basic-49e908e3-28a1-11e9-9eab-8e57f341003b: Got expected result from replica 1 [my-hostname-basic-49e908e3-28a1-11e9-9eab-8e57f341003b-c6tq7]: "my-hostname-basic-49e908e3-28a1-11e9-9eab-8e57f341003b-c6tq7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:21:31.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-6jfsk" for this suite.
Feb  4 17:21:39.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:21:39.294: INFO: namespace: e2e-tests-replication-controller-6jfsk, resource: bindings, ignored listing per whitelist
Feb  4 17:21:39.868: INFO: namespace e2e-tests-replication-controller-6jfsk deletion completed in 8.818003276s

• [SLOW TEST:19.493 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:21:39.869: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  4 17:21:40.531: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:21:45.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-58q5t" for this suite.
Feb  4 17:21:52.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:21:52.356: INFO: namespace: e2e-tests-init-container-58q5t, resource: bindings, ignored listing per whitelist
Feb  4 17:21:52.437: INFO: namespace e2e-tests-init-container-58q5t deletion completed in 6.505599298s

• [SLOW TEST:12.567 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:21:52.439: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb  4 17:21:52.803: INFO: Waiting up to 5m0s for pod "pod-5d11c907-28a1-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-w6dqk" to be "success or failure"
Feb  4 17:21:52.819: INFO: Pod "pod-5d11c907-28a1-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.404844ms
Feb  4 17:21:54.830: INFO: Pod "pod-5d11c907-28a1-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027510303s
Feb  4 17:21:56.845: INFO: Pod "pod-5d11c907-28a1-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042458069s
STEP: Saw pod success
Feb  4 17:21:56.845: INFO: Pod "pod-5d11c907-28a1-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:21:56.850: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-5d11c907-28a1-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 17:21:56.970: INFO: Waiting for pod pod-5d11c907-28a1-11e9-9eab-8e57f341003b to disappear
Feb  4 17:21:56.995: INFO: Pod pod-5d11c907-28a1-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:21:57.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-w6dqk" for this suite.
Feb  4 17:22:03.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:22:03.256: INFO: namespace: e2e-tests-emptydir-w6dqk, resource: bindings, ignored listing per whitelist
Feb  4 17:22:03.427: INFO: namespace e2e-tests-emptydir-w6dqk deletion completed in 6.411394023s

• [SLOW TEST:10.988 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:22:03.428: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-63943caf-28a1-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 17:22:03.759: INFO: Waiting up to 5m0s for pod "pod-secrets-6396ea14-28a1-11e9-9eab-8e57f341003b" in namespace "e2e-tests-secrets-4znx2" to be "success or failure"
Feb  4 17:22:03.765: INFO: Pod "pod-secrets-6396ea14-28a1-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.383989ms
Feb  4 17:22:05.773: INFO: Pod "pod-secrets-6396ea14-28a1-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014174022s
Feb  4 17:22:07.798: INFO: Pod "pod-secrets-6396ea14-28a1-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038872268s
STEP: Saw pod success
Feb  4 17:22:07.798: INFO: Pod "pod-secrets-6396ea14-28a1-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:22:07.805: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-secrets-6396ea14-28a1-11e9-9eab-8e57f341003b container secret-volume-test: <nil>
STEP: delete the pod
Feb  4 17:22:07.938: INFO: Waiting for pod pod-secrets-6396ea14-28a1-11e9-9eab-8e57f341003b to disappear
Feb  4 17:22:07.944: INFO: Pod pod-secrets-6396ea14-28a1-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:22:07.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4znx2" for this suite.
Feb  4 17:22:13.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:22:14.117: INFO: namespace: e2e-tests-secrets-4znx2, resource: bindings, ignored listing per whitelist
Feb  4 17:22:14.294: INFO: namespace e2e-tests-secrets-4znx2 deletion completed in 6.339982065s

• [SLOW TEST:10.866 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:22:14.295: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 17:22:14.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a053d15-28a1-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-p6xx6" to be "success or failure"
Feb  4 17:22:14.536: INFO: Pod "downwardapi-volume-6a053d15-28a1-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.78959ms
Feb  4 17:22:16.546: INFO: Pod "downwardapi-volume-6a053d15-28a1-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016611829s
Feb  4 17:22:18.568: INFO: Pod "downwardapi-volume-6a053d15-28a1-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039304709s
STEP: Saw pod success
Feb  4 17:22:18.568: INFO: Pod "downwardapi-volume-6a053d15-28a1-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:22:18.573: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod downwardapi-volume-6a053d15-28a1-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 17:22:18.620: INFO: Waiting for pod downwardapi-volume-6a053d15-28a1-11e9-9eab-8e57f341003b to disappear
Feb  4 17:22:18.651: INFO: Pod downwardapi-volume-6a053d15-28a1-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:22:18.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-p6xx6" for this suite.
Feb  4 17:22:24.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:22:24.802: INFO: namespace: e2e-tests-downward-api-p6xx6, resource: bindings, ignored listing per whitelist
Feb  4 17:22:25.067: INFO: namespace e2e-tests-downward-api-p6xx6 deletion completed in 6.409754517s

• [SLOW TEST:10.773 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:22:25.078: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:22:27.419: INFO: Waiting up to 5m0s for pod "client-envvars-71b52a8e-28a1-11e9-9eab-8e57f341003b" in namespace "e2e-tests-pods-nlfv9" to be "success or failure"
Feb  4 17:22:27.456: INFO: Pod "client-envvars-71b52a8e-28a1-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 36.700786ms
Feb  4 17:22:29.509: INFO: Pod "client-envvars-71b52a8e-28a1-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089483085s
Feb  4 17:22:31.517: INFO: Pod "client-envvars-71b52a8e-28a1-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.097232857s
STEP: Saw pod success
Feb  4 17:22:31.517: INFO: Pod "client-envvars-71b52a8e-28a1-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:22:31.522: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod client-envvars-71b52a8e-28a1-11e9-9eab-8e57f341003b container env3cont: <nil>
STEP: delete the pod
Feb  4 17:22:31.690: INFO: Waiting for pod client-envvars-71b52a8e-28a1-11e9-9eab-8e57f341003b to disappear
Feb  4 17:22:31.704: INFO: Pod client-envvars-71b52a8e-28a1-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:22:31.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-nlfv9" for this suite.
Feb  4 17:23:13.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:23:14.174: INFO: namespace: e2e-tests-pods-nlfv9, resource: bindings, ignored listing per whitelist
Feb  4 17:23:14.274: INFO: namespace e2e-tests-pods-nlfv9 deletion completed in 42.563122347s

• [SLOW TEST:49.197 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:23:14.275: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-rx2x2
Feb  4 17:23:20.589: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-rx2x2
STEP: checking the pod's current state and verifying that restartCount is present
Feb  4 17:23:20.594: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:27:22.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-rx2x2" for this suite.
Feb  4 17:27:28.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:27:28.768: INFO: namespace: e2e-tests-container-probe-rx2x2, resource: bindings, ignored listing per whitelist
Feb  4 17:27:28.850: INFO: namespace e2e-tests-container-probe-rx2x2 deletion completed in 6.524148574s

• [SLOW TEST:254.575 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:27:28.855: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  4 17:27:29.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-68m6d'
Feb  4 17:27:30.626: INFO: stderr: ""
Feb  4 17:27:30.626: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb  4 17:27:35.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-68m6d -o json'
Feb  4 17:27:35.935: INFO: stderr: ""
Feb  4 17:27:35.935: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-02-04T17:27:30Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-68m6d\",\n        \"resourceVersion\": \"18087\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-68m6d/pods/e2e-test-nginx-pod\",\n        \"uid\": \"266e0782-28a2-11e9-9959-0a580af41676\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-bqcmb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"machine-kubermatic-conformancecluster-61wiwtyxah-t5p54\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-bqcmb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-bqcmb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-04T17:27:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-04T17:27:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-04T17:27:32Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-04T17:27:30Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://e64c9c6587930099e440aa7a1a9f78204043772a6ff4f44871237a928e8cba54\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-04T17:27:32Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.10\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.1.90\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-04T17:27:30Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb  4 17:27:35.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 replace -f - --namespace=e2e-tests-kubectl-68m6d'
Feb  4 17:27:36.558: INFO: stderr: ""
Feb  4 17:27:36.558: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Feb  4 17:27:36.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-68m6d'
Feb  4 17:27:39.387: INFO: stderr: ""
Feb  4 17:27:39.387: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:27:39.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-68m6d" for this suite.
Feb  4 17:27:45.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:27:45.593: INFO: namespace: e2e-tests-kubectl-68m6d, resource: bindings, ignored listing per whitelist
Feb  4 17:27:45.813: INFO: namespace e2e-tests-kubectl-68m6d deletion completed in 6.418032174s

• [SLOW TEST:16.958 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:27:45.817: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  4 17:27:46.074: INFO: PodSpec: initContainers in spec.initContainers
Feb  4 17:28:34.861: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2fa62feb-28a2-11e9-9eab-8e57f341003b", GenerateName:"", Namespace:"e2e-tests-init-container-nzwmz", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-nzwmz/pods/pod-init-2fa62feb-28a2-11e9-9eab-8e57f341003b", UID:"2fa9a0e1-28a2-11e9-9959-0a580af41676", ResourceVersion:"18293", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63684898066, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"74452074"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-pk2rg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001d5a3c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pk2rg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pk2rg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pk2rg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00143c9f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"machine-kubermatic-conformancecluster-61wiwtyxah-6rthf", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001b186c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00143ca70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00143ca90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00143ca98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00143ca9c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684898066, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684898066, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684898066, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684898066, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.3", PodIP:"172.25.0.114", StartTime:(*v1.Time)(0xc0012ee040), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00248bdc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00248be30)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://165570051ccc7acb08b67fe7ecfbb59171615a26d62ba3fa2dc9a14dcd568476"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0012ee080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0012ee060), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:28:34.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-nzwmz" for this suite.
Feb  4 17:28:58.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:28:59.102: INFO: namespace: e2e-tests-init-container-nzwmz, resource: bindings, ignored listing per whitelist
Feb  4 17:28:59.429: INFO: namespace e2e-tests-init-container-nzwmz deletion completed in 24.548327241s

• [SLOW TEST:73.612 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:28:59.435: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb  4 17:28:59.759: INFO: Pod name pod-release: Found 0 pods out of 1
Feb  4 17:29:04.766: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:29:04.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-cvqqz" for this suite.
Feb  4 17:29:10.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:29:11.051: INFO: namespace: e2e-tests-replication-controller-cvqqz, resource: bindings, ignored listing per whitelist
Feb  4 17:29:11.287: INFO: namespace e2e-tests-replication-controller-cvqqz deletion completed in 6.391011499s

• [SLOW TEST:11.851 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:29:11.288: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-632007cd-28a2-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 17:29:12.477: INFO: Waiting up to 5m0s for pod "pod-configmaps-63211548-28a2-11e9-9eab-8e57f341003b" in namespace "e2e-tests-configmap-bwgjl" to be "success or failure"
Feb  4 17:29:12.492: INFO: Pod "pod-configmaps-63211548-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.454609ms
Feb  4 17:29:14.498: INFO: Pod "pod-configmaps-63211548-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020040081s
Feb  4 17:29:16.505: INFO: Pod "pod-configmaps-63211548-28a2-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027411178s
STEP: Saw pod success
Feb  4 17:29:16.505: INFO: Pod "pod-configmaps-63211548-28a2-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:29:16.512: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-configmaps-63211548-28a2-11e9-9eab-8e57f341003b container configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 17:29:16.630: INFO: Waiting for pod pod-configmaps-63211548-28a2-11e9-9eab-8e57f341003b to disappear
Feb  4 17:29:16.637: INFO: Pod pod-configmaps-63211548-28a2-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:29:16.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bwgjl" for this suite.
Feb  4 17:29:22.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:29:22.953: INFO: namespace: e2e-tests-configmap-bwgjl, resource: bindings, ignored listing per whitelist
Feb  4 17:29:22.998: INFO: namespace e2e-tests-configmap-bwgjl deletion completed in 6.353674999s

• [SLOW TEST:11.711 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:29:23.002: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-mwdbr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-mwdbr to expose endpoints map[]
Feb  4 17:29:23.313: INFO: Get endpoints failed (6.359273ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb  4 17:29:24.321: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mwdbr exposes endpoints map[] (1.013425913s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-mwdbr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-mwdbr to expose endpoints map[pod1:[80]]
Feb  4 17:29:27.397: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mwdbr exposes endpoints map[pod1:[80]] (3.052867747s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-mwdbr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-mwdbr to expose endpoints map[pod1:[80] pod2:[80]]
Feb  4 17:29:31.519: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mwdbr exposes endpoints map[pod2:[80] pod1:[80]] (4.104770285s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-mwdbr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-mwdbr to expose endpoints map[pod2:[80]]
Feb  4 17:29:32.568: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mwdbr exposes endpoints map[pod2:[80]] (1.034932046s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-mwdbr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-mwdbr to expose endpoints map[]
Feb  4 17:29:32.611: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-mwdbr exposes endpoints map[] (7.57728ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:29:32.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-mwdbr" for this suite.
Feb  4 17:29:38.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:29:38.791: INFO: namespace: e2e-tests-services-mwdbr, resource: bindings, ignored listing per whitelist
Feb  4 17:29:39.162: INFO: namespace e2e-tests-services-mwdbr deletion completed in 6.508170997s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:16.161 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:29:39.163: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 17:29:39.558: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7346bbe8-28a2-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-c6rzr" to be "success or failure"
Feb  4 17:29:39.570: INFO: Pod "downwardapi-volume-7346bbe8-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.085137ms
Feb  4 17:29:41.577: INFO: Pod "downwardapi-volume-7346bbe8-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018896664s
Feb  4 17:29:43.583: INFO: Pod "downwardapi-volume-7346bbe8-28a2-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025349081s
STEP: Saw pod success
Feb  4 17:29:43.583: INFO: Pod "downwardapi-volume-7346bbe8-28a2-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:29:43.652: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downwardapi-volume-7346bbe8-28a2-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 17:29:43.781: INFO: Waiting for pod downwardapi-volume-7346bbe8-28a2-11e9-9eab-8e57f341003b to disappear
Feb  4 17:29:43.790: INFO: Pod downwardapi-volume-7346bbe8-28a2-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:29:43.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-c6rzr" for this suite.
Feb  4 17:29:49.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:29:50.138: INFO: namespace: e2e-tests-downward-api-c6rzr, resource: bindings, ignored listing per whitelist
Feb  4 17:29:50.267: INFO: namespace e2e-tests-downward-api-c6rzr deletion completed in 6.469849835s

• [SLOW TEST:11.104 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:29:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:29:50.578: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"79d6bf17-28a2-11e9-9959-0a580af41676", Controller:(*bool)(0xc00122c6ee), BlockOwnerDeletion:(*bool)(0xc00122c6ef)}}
Feb  4 17:29:50.591: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"79d16f21-28a2-11e9-9959-0a580af41676", Controller:(*bool)(0xc00122c9ce), BlockOwnerDeletion:(*bool)(0xc00122c9cf)}}
Feb  4 17:29:50.606: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"79d39a75-28a2-11e9-9959-0a580af41676", Controller:(*bool)(0xc00122cd26), BlockOwnerDeletion:(*bool)(0xc00122cd27)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:29:55.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-2ql7k" for this suite.
Feb  4 17:30:01.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:30:01.999: INFO: namespace: e2e-tests-gc-2ql7k, resource: bindings, ignored listing per whitelist
Feb  4 17:30:02.080: INFO: namespace e2e-tests-gc-2ql7k deletion completed in 6.423369606s

• [SLOW TEST:11.809 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:30:02.085: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-d7qh
STEP: Creating a pod to test atomic-volume-subpath
Feb  4 17:30:02.337: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-d7qh" in namespace "e2e-tests-subpath-7t9cj" to be "success or failure"
Feb  4 17:30:02.342: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430062ms
Feb  4 17:30:04.349: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011282134s
Feb  4 17:30:06.355: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Running", Reason="", readiness=false. Elapsed: 4.018129834s
Feb  4 17:30:08.363: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Running", Reason="", readiness=false. Elapsed: 6.026080458s
Feb  4 17:30:10.370: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Running", Reason="", readiness=false. Elapsed: 8.032757144s
Feb  4 17:30:12.404: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Running", Reason="", readiness=false. Elapsed: 10.066418645s
Feb  4 17:30:14.410: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Running", Reason="", readiness=false. Elapsed: 12.072670405s
Feb  4 17:30:16.507: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Running", Reason="", readiness=false. Elapsed: 14.169605234s
Feb  4 17:30:18.514: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Running", Reason="", readiness=false. Elapsed: 16.176536022s
Feb  4 17:30:20.523: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Running", Reason="", readiness=false. Elapsed: 18.185536211s
Feb  4 17:30:22.542: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Running", Reason="", readiness=false. Elapsed: 20.204226759s
Feb  4 17:30:24.549: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Running", Reason="", readiness=false. Elapsed: 22.21151861s
Feb  4 17:30:26.557: INFO: Pod "pod-subpath-test-secret-d7qh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.219932561s
STEP: Saw pod success
Feb  4 17:30:26.558: INFO: Pod "pod-subpath-test-secret-d7qh" satisfied condition "success or failure"
Feb  4 17:30:26.563: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-subpath-test-secret-d7qh container test-container-subpath-secret-d7qh: <nil>
STEP: delete the pod
Feb  4 17:30:26.734: INFO: Waiting for pod pod-subpath-test-secret-d7qh to disappear
Feb  4 17:30:26.753: INFO: Pod pod-subpath-test-secret-d7qh no longer exists
STEP: Deleting pod pod-subpath-test-secret-d7qh
Feb  4 17:30:26.753: INFO: Deleting pod "pod-subpath-test-secret-d7qh" in namespace "e2e-tests-subpath-7t9cj"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:30:26.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-7t9cj" for this suite.
Feb  4 17:30:32.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:30:32.917: INFO: namespace: e2e-tests-subpath-7t9cj, resource: bindings, ignored listing per whitelist
Feb  4 17:30:33.221: INFO: namespace e2e-tests-subpath-7t9cj deletion completed in 6.456672123s

• [SLOW TEST:31.137 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:30:33.225: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb  4 17:30:33.511: INFO: Waiting up to 5m0s for pod "client-containers-93715466-28a2-11e9-9eab-8e57f341003b" in namespace "e2e-tests-containers-b6sjk" to be "success or failure"
Feb  4 17:30:33.516: INFO: Pod "client-containers-93715466-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914013ms
Feb  4 17:30:35.632: INFO: Pod "client-containers-93715466-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120298989s
Feb  4 17:30:37.730: INFO: Pod "client-containers-93715466-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.218391078s
Feb  4 17:30:39.736: INFO: Pod "client-containers-93715466-28a2-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.224067642s
STEP: Saw pod success
Feb  4 17:30:39.736: INFO: Pod "client-containers-93715466-28a2-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:30:39.740: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod client-containers-93715466-28a2-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 17:30:39.894: INFO: Waiting for pod client-containers-93715466-28a2-11e9-9eab-8e57f341003b to disappear
Feb  4 17:30:39.946: INFO: Pod client-containers-93715466-28a2-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:30:39.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-b6sjk" for this suite.
Feb  4 17:30:47.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:30:48.140: INFO: namespace: e2e-tests-containers-b6sjk, resource: bindings, ignored listing per whitelist
Feb  4 17:30:48.278: INFO: namespace e2e-tests-containers-b6sjk deletion completed in 8.324584999s

• [SLOW TEST:15.054 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:30:48.283: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:30:48.843: INFO: Creating ReplicaSet my-hostname-basic-9c967d39-28a2-11e9-9eab-8e57f341003b
Feb  4 17:30:48.957: INFO: Pod name my-hostname-basic-9c967d39-28a2-11e9-9eab-8e57f341003b: Found 0 pods out of 1
Feb  4 17:30:53.964: INFO: Pod name my-hostname-basic-9c967d39-28a2-11e9-9eab-8e57f341003b: Found 1 pods out of 1
Feb  4 17:30:53.964: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9c967d39-28a2-11e9-9eab-8e57f341003b" is running
Feb  4 17:30:53.969: INFO: Pod "my-hostname-basic-9c967d39-28a2-11e9-9eab-8e57f341003b-dvclk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-04 17:30:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-04 17:30:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-04 17:30:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-04 17:30:49 +0000 UTC Reason: Message:}])
Feb  4 17:30:53.969: INFO: Trying to dial the pod
Feb  4 17:30:59.114: INFO: Controller my-hostname-basic-9c967d39-28a2-11e9-9eab-8e57f341003b: Got expected result from replica 1 [my-hostname-basic-9c967d39-28a2-11e9-9eab-8e57f341003b-dvclk]: "my-hostname-basic-9c967d39-28a2-11e9-9eab-8e57f341003b-dvclk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:30:59.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-jbp4s" for this suite.
Feb  4 17:31:05.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:31:05.257: INFO: namespace: e2e-tests-replicaset-jbp4s, resource: bindings, ignored listing per whitelist
Feb  4 17:31:05.574: INFO: namespace e2e-tests-replicaset-jbp4s deletion completed in 6.451029017s

• [SLOW TEST:17.291 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:31:05.581: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-a6b8d970-28a2-11e9-9eab-8e57f341003b
STEP: Creating configMap with name cm-test-opt-upd-a6b8d9bf-28a2-11e9-9eab-8e57f341003b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a6b8d970-28a2-11e9-9eab-8e57f341003b
STEP: Updating configmap cm-test-opt-upd-a6b8d9bf-28a2-11e9-9eab-8e57f341003b
STEP: Creating configMap with name cm-test-opt-create-a6b8d9f6-28a2-11e9-9eab-8e57f341003b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:31:12.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ckrgv" for this suite.
Feb  4 17:31:36.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:31:36.681: INFO: namespace: e2e-tests-configmap-ckrgv, resource: bindings, ignored listing per whitelist
Feb  4 17:31:36.946: INFO: namespace e2e-tests-configmap-ckrgv deletion completed in 24.470838878s

• [SLOW TEST:31.365 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:31:36.950: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb  4 17:31:37.260: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-99bj2,SelfLink:/api/v1/namespaces/e2e-tests-watch-99bj2/configmaps/e2e-watch-test-label-changed,UID:b96d7d9c-28a2-11e9-9959-0a580af41676,ResourceVersion:19109,Generation:0,CreationTimestamp:2019-02-04 17:31:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  4 17:31:37.260: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-99bj2,SelfLink:/api/v1/namespaces/e2e-tests-watch-99bj2/configmaps/e2e-watch-test-label-changed,UID:b96d7d9c-28a2-11e9-9959-0a580af41676,ResourceVersion:19110,Generation:0,CreationTimestamp:2019-02-04 17:31:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb  4 17:31:37.260: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-99bj2,SelfLink:/api/v1/namespaces/e2e-tests-watch-99bj2/configmaps/e2e-watch-test-label-changed,UID:b96d7d9c-28a2-11e9-9959-0a580af41676,ResourceVersion:19111,Generation:0,CreationTimestamp:2019-02-04 17:31:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb  4 17:31:47.323: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-99bj2,SelfLink:/api/v1/namespaces/e2e-tests-watch-99bj2/configmaps/e2e-watch-test-label-changed,UID:b96d7d9c-28a2-11e9-9959-0a580af41676,ResourceVersion:19138,Generation:0,CreationTimestamp:2019-02-04 17:31:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  4 17:31:47.323: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-99bj2,SelfLink:/api/v1/namespaces/e2e-tests-watch-99bj2/configmaps/e2e-watch-test-label-changed,UID:b96d7d9c-28a2-11e9-9959-0a580af41676,ResourceVersion:19139,Generation:0,CreationTimestamp:2019-02-04 17:31:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb  4 17:31:47.323: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-99bj2,SelfLink:/api/v1/namespaces/e2e-tests-watch-99bj2/configmaps/e2e-watch-test-label-changed,UID:b96d7d9c-28a2-11e9-9959-0a580af41676,ResourceVersion:19140,Generation:0,CreationTimestamp:2019-02-04 17:31:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:31:47.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-99bj2" for this suite.
Feb  4 17:31:53.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:31:53.761: INFO: namespace: e2e-tests-watch-99bj2, resource: bindings, ignored listing per whitelist
Feb  4 17:31:53.930: INFO: namespace e2e-tests-watch-99bj2 deletion completed in 6.591086701s

• [SLOW TEST:16.981 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:31:53.933: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-c383e36f-28a2-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 17:31:54.305: INFO: Waiting up to 5m0s for pod "pod-secrets-c3989d24-28a2-11e9-9eab-8e57f341003b" in namespace "e2e-tests-secrets-bmsww" to be "success or failure"
Feb  4 17:31:54.310: INFO: Pod "pod-secrets-c3989d24-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.77599ms
Feb  4 17:31:56.316: INFO: Pod "pod-secrets-c3989d24-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011382055s
Feb  4 17:31:58.333: INFO: Pod "pod-secrets-c3989d24-28a2-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028287488s
STEP: Saw pod success
Feb  4 17:31:58.333: INFO: Pod "pod-secrets-c3989d24-28a2-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:31:58.343: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-secrets-c3989d24-28a2-11e9-9eab-8e57f341003b container secret-volume-test: <nil>
STEP: delete the pod
Feb  4 17:31:58.467: INFO: Waiting for pod pod-secrets-c3989d24-28a2-11e9-9eab-8e57f341003b to disappear
Feb  4 17:31:58.475: INFO: Pod pod-secrets-c3989d24-28a2-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:31:58.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-bmsww" for this suite.
Feb  4 17:32:06.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:32:06.845: INFO: namespace: e2e-tests-secrets-bmsww, resource: bindings, ignored listing per whitelist
Feb  4 17:32:06.985: INFO: namespace e2e-tests-secrets-bmsww deletion completed in 8.502297562s
STEP: Destroying namespace "e2e-tests-secret-namespace-nckl6" for this suite.
Feb  4 17:32:13.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:32:13.789: INFO: namespace: e2e-tests-secret-namespace-nckl6, resource: bindings, ignored listing per whitelist
Feb  4 17:32:14.128: INFO: namespace e2e-tests-secret-namespace-nckl6 deletion completed in 7.14302135s

• [SLOW TEST:20.195 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:32:14.133: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:32:14.708: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb  4 17:32:14.764: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-dx4kg/daemonsets","resourceVersion":"19254"},"items":null}

Feb  4 17:32:14.779: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-dx4kg/pods","resourceVersion":"19254"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:32:14.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-dx4kg" for this suite.
Feb  4 17:32:20.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:32:21.148: INFO: namespace: e2e-tests-daemonsets-dx4kg, resource: bindings, ignored listing per whitelist
Feb  4 17:32:21.333: INFO: namespace e2e-tests-daemonsets-dx4kg deletion completed in 6.521140025s

S [SKIPPING] [7.201 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb  4 17:32:14.708: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:32:21.337: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:32:21.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-mbd9b" for this suite.
Feb  4 17:32:27.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:32:27.716: INFO: namespace: e2e-tests-services-mbd9b, resource: bindings, ignored listing per whitelist
Feb  4 17:32:28.037: INFO: namespace e2e-tests-services-mbd9b deletion completed in 6.415372897s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.703 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:32:28.042: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb  4 17:32:28.314: INFO: Waiting up to 5m0s for pod "client-containers-d7dcb680-28a2-11e9-9eab-8e57f341003b" in namespace "e2e-tests-containers-thhr9" to be "success or failure"
Feb  4 17:32:28.330: INFO: Pod "client-containers-d7dcb680-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.135243ms
Feb  4 17:32:30.338: INFO: Pod "client-containers-d7dcb680-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024351962s
Feb  4 17:32:32.357: INFO: Pod "client-containers-d7dcb680-28a2-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042635585s
STEP: Saw pod success
Feb  4 17:32:32.357: INFO: Pod "client-containers-d7dcb680-28a2-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:32:32.361: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod client-containers-d7dcb680-28a2-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 17:32:32.434: INFO: Waiting for pod client-containers-d7dcb680-28a2-11e9-9eab-8e57f341003b to disappear
Feb  4 17:32:32.476: INFO: Pod client-containers-d7dcb680-28a2-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:32:32.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-thhr9" for this suite.
Feb  4 17:32:38.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:32:38.848: INFO: namespace: e2e-tests-containers-thhr9, resource: bindings, ignored listing per whitelist
Feb  4 17:32:38.938: INFO: namespace e2e-tests-containers-thhr9 deletion completed in 6.448385315s

• [SLOW TEST:10.897 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:32:38.949: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 17:32:39.207: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de5cf912-28a2-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-28nv8" to be "success or failure"
Feb  4 17:32:39.220: INFO: Pod "downwardapi-volume-de5cf912-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.746034ms
Feb  4 17:32:41.227: INFO: Pod "downwardapi-volume-de5cf912-28a2-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01947963s
Feb  4 17:32:43.245: INFO: Pod "downwardapi-volume-de5cf912-28a2-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038001333s
STEP: Saw pod success
Feb  4 17:32:43.245: INFO: Pod "downwardapi-volume-de5cf912-28a2-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:32:43.254: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downwardapi-volume-de5cf912-28a2-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 17:32:43.388: INFO: Waiting for pod downwardapi-volume-de5cf912-28a2-11e9-9eab-8e57f341003b to disappear
Feb  4 17:32:43.393: INFO: Pod downwardapi-volume-de5cf912-28a2-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:32:43.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-28nv8" for this suite.
Feb  4 17:32:49.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:32:49.540: INFO: namespace: e2e-tests-projected-28nv8, resource: bindings, ignored listing per whitelist
Feb  4 17:32:49.827: INFO: namespace e2e-tests-projected-28nv8 deletion completed in 6.4266778s

• [SLOW TEST:10.879 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:32:49.832: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:33:12.061: INFO: Container started at 2019-02-04 17:32:51 +0000 UTC, pod became ready at 2019-02-04 17:33:10 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:33:12.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-gm92x" for this suite.
Feb  4 17:33:36.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:33:36.414: INFO: namespace: e2e-tests-container-probe-gm92x, resource: bindings, ignored listing per whitelist
Feb  4 17:33:36.681: INFO: namespace e2e-tests-container-probe-gm92x deletion completed in 24.610308932s

• [SLOW TEST:46.849 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:33:36.682: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb  4 17:33:36.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:37.708: INFO: stderr: ""
Feb  4 17:33:37.708: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  4 17:33:37.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:38.050: INFO: stderr: ""
Feb  4 17:33:38.050: INFO: stdout: "update-demo-nautilus-589qm update-demo-nautilus-tk8kw "
Feb  4 17:33:38.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-589qm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:38.406: INFO: stderr: ""
Feb  4 17:33:38.406: INFO: stdout: ""
Feb  4 17:33:38.406: INFO: update-demo-nautilus-589qm is created but not running
Feb  4 17:33:43.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:43.707: INFO: stderr: ""
Feb  4 17:33:43.707: INFO: stdout: "update-demo-nautilus-589qm update-demo-nautilus-tk8kw "
Feb  4 17:33:43.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-589qm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:43.986: INFO: stderr: ""
Feb  4 17:33:43.986: INFO: stdout: "true"
Feb  4 17:33:43.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-589qm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:44.258: INFO: stderr: ""
Feb  4 17:33:44.258: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  4 17:33:44.258: INFO: validating pod update-demo-nautilus-589qm
Feb  4 17:33:44.368: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  4 17:33:44.368: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  4 17:33:44.368: INFO: update-demo-nautilus-589qm is verified up and running
Feb  4 17:33:44.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-tk8kw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:44.661: INFO: stderr: ""
Feb  4 17:33:44.661: INFO: stdout: "true"
Feb  4 17:33:44.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-tk8kw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:45.029: INFO: stderr: ""
Feb  4 17:33:45.029: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  4 17:33:45.029: INFO: validating pod update-demo-nautilus-tk8kw
Feb  4 17:33:45.201: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  4 17:33:45.201: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  4 17:33:45.201: INFO: update-demo-nautilus-tk8kw is verified up and running
STEP: scaling down the replication controller
Feb  4 17:33:45.205: INFO: scanned /root for discovery docs: <nil>
Feb  4 17:33:45.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:46.695: INFO: stderr: ""
Feb  4 17:33:46.695: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  4 17:33:46.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:47.000: INFO: stderr: ""
Feb  4 17:33:47.000: INFO: stdout: "update-demo-nautilus-589qm update-demo-nautilus-tk8kw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  4 17:33:52.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:52.267: INFO: stderr: ""
Feb  4 17:33:52.267: INFO: stdout: "update-demo-nautilus-589qm update-demo-nautilus-tk8kw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  4 17:33:57.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:57.558: INFO: stderr: ""
Feb  4 17:33:57.558: INFO: stdout: "update-demo-nautilus-589qm "
Feb  4 17:33:57.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-589qm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:57.884: INFO: stderr: ""
Feb  4 17:33:57.884: INFO: stdout: "true"
Feb  4 17:33:57.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-589qm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:58.300: INFO: stderr: ""
Feb  4 17:33:58.300: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  4 17:33:58.300: INFO: validating pod update-demo-nautilus-589qm
Feb  4 17:33:58.401: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  4 17:33:58.401: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  4 17:33:58.401: INFO: update-demo-nautilus-589qm is verified up and running
STEP: scaling up the replication controller
Feb  4 17:33:58.404: INFO: scanned /root for discovery docs: <nil>
Feb  4 17:33:58.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:33:59.816: INFO: stderr: ""
Feb  4 17:33:59.816: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  4 17:33:59.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:34:00.327: INFO: stderr: ""
Feb  4 17:34:00.327: INFO: stdout: "update-demo-nautilus-589qm update-demo-nautilus-fj45l "
Feb  4 17:34:00.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-589qm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:34:00.762: INFO: stderr: ""
Feb  4 17:34:00.763: INFO: stdout: "true"
Feb  4 17:34:00.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-589qm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:34:01.290: INFO: stderr: ""
Feb  4 17:34:01.290: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  4 17:34:01.290: INFO: validating pod update-demo-nautilus-589qm
Feb  4 17:34:01.308: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  4 17:34:01.308: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  4 17:34:01.308: INFO: update-demo-nautilus-589qm is verified up and running
Feb  4 17:34:01.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-fj45l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:34:01.604: INFO: stderr: ""
Feb  4 17:34:01.604: INFO: stdout: ""
Feb  4 17:34:01.604: INFO: update-demo-nautilus-fj45l is created but not running
Feb  4 17:34:06.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:34:06.921: INFO: stderr: ""
Feb  4 17:34:06.921: INFO: stdout: "update-demo-nautilus-589qm update-demo-nautilus-fj45l "
Feb  4 17:34:06.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-589qm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:34:07.178: INFO: stderr: ""
Feb  4 17:34:07.178: INFO: stdout: "true"
Feb  4 17:34:07.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-589qm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:34:07.451: INFO: stderr: ""
Feb  4 17:34:07.451: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  4 17:34:07.451: INFO: validating pod update-demo-nautilus-589qm
Feb  4 17:34:07.512: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  4 17:34:07.512: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  4 17:34:07.512: INFO: update-demo-nautilus-589qm is verified up and running
Feb  4 17:34:07.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-fj45l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:34:07.794: INFO: stderr: ""
Feb  4 17:34:07.794: INFO: stdout: "true"
Feb  4 17:34:07.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-fj45l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:34:08.055: INFO: stderr: ""
Feb  4 17:34:08.055: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  4 17:34:08.055: INFO: validating pod update-demo-nautilus-fj45l
Feb  4 17:34:08.152: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  4 17:34:08.152: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  4 17:34:08.152: INFO: update-demo-nautilus-fj45l is verified up and running
STEP: using delete to clean up resources
Feb  4 17:34:08.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:34:08.407: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  4 17:34:08.407: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb  4 17:34:08.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-htgkb'
Feb  4 17:34:09.083: INFO: stderr: "No resources found.\n"
Feb  4 17:34:09.083: INFO: stdout: ""
Feb  4 17:34:09.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -l name=update-demo --namespace=e2e-tests-kubectl-htgkb -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  4 17:34:09.586: INFO: stderr: ""
Feb  4 17:34:09.586: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:34:09.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-htgkb" for this suite.
Feb  4 17:34:33.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:34:33.882: INFO: namespace: e2e-tests-kubectl-htgkb, resource: bindings, ignored listing per whitelist
Feb  4 17:34:34.132: INFO: namespace e2e-tests-kubectl-htgkb deletion completed in 24.533653132s

• [SLOW TEST:57.450 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:34:34.135: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 17:34:34.592: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2320d62d-28a3-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-dpbxn" to be "success or failure"
Feb  4 17:34:34.628: INFO: Pod "downwardapi-volume-2320d62d-28a3-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 36.373336ms
Feb  4 17:34:36.634: INFO: Pod "downwardapi-volume-2320d62d-28a3-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042940855s
Feb  4 17:34:38.680: INFO: Pod "downwardapi-volume-2320d62d-28a3-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088555388s
STEP: Saw pod success
Feb  4 17:34:38.680: INFO: Pod "downwardapi-volume-2320d62d-28a3-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:34:38.687: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downwardapi-volume-2320d62d-28a3-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 17:34:38.806: INFO: Waiting for pod downwardapi-volume-2320d62d-28a3-11e9-9eab-8e57f341003b to disappear
Feb  4 17:34:38.812: INFO: Pod downwardapi-volume-2320d62d-28a3-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:34:38.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dpbxn" for this suite.
Feb  4 17:34:44.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:34:45.248: INFO: namespace: e2e-tests-projected-dpbxn, resource: bindings, ignored listing per whitelist
Feb  4 17:34:45.281: INFO: namespace e2e-tests-projected-dpbxn deletion completed in 6.458413694s

• [SLOW TEST:11.147 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:34:45.282: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:34:45.507: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb  4 17:34:45.551: INFO: Number of nodes with available pods: 0
Feb  4 17:34:45.551: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 17:34:46.575: INFO: Number of nodes with available pods: 0
Feb  4 17:34:46.575: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 17:34:47.565: INFO: Number of nodes with available pods: 0
Feb  4 17:34:47.565: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf is running more than one daemon pod
Feb  4 17:34:48.563: INFO: Number of nodes with available pods: 2
Feb  4 17:34:48.563: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb  4 17:34:48.609: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:48.610: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:49.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:49.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:50.634: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:50.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:51.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:51.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:52.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:52.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:53.652: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:53.652: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:54.634: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:54.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:55.633: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:55.633: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:56.631: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:56.631: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:57.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:57.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:58.631: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:58.631: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:59.637: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:34:59.638: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:00.631: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:00.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:01.633: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:01.633: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:02.631: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:02.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:03.705: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:03.705: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:04.638: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:04.638: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:05.633: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:05.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:06.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:06.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:07.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:07.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:08.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:08.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:09.634: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:09.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:10.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:10.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:11.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:11.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:12.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:12.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:13.634: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:13.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:14.641: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:14.641: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:15.635: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:15.635: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:16.634: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:16.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:17.633: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:17.633: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:18.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:18.633: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:19.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:19.633: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:20.631: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:20.631: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:21.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:21.632: INFO: Pod daemon-set-trfqx is not available
Feb  4 17:35:21.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:22.634: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:22.634: INFO: Pod daemon-set-trfqx is not available
Feb  4 17:35:22.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:23.638: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:23.638: INFO: Pod daemon-set-trfqx is not available
Feb  4 17:35:23.638: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:24.633: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:24.633: INFO: Pod daemon-set-trfqx is not available
Feb  4 17:35:24.633: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:25.642: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:25.642: INFO: Pod daemon-set-trfqx is not available
Feb  4 17:35:25.642: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:26.634: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:26.634: INFO: Pod daemon-set-trfqx is not available
Feb  4 17:35:26.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:27.709: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:27.709: INFO: Pod daemon-set-trfqx is not available
Feb  4 17:35:27.709: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:28.639: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:28.639: INFO: Pod daemon-set-trfqx is not available
Feb  4 17:35:28.639: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:29.632: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:29.632: INFO: Pod daemon-set-trfqx is not available
Feb  4 17:35:29.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:30.633: INFO: Wrong image for pod: daemon-set-trfqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:30.633: INFO: Pod daemon-set-trfqx is not available
Feb  4 17:35:30.633: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:31.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:31.634: INFO: Pod daemon-set-xb9zg is not available
Feb  4 17:35:32.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:32.633: INFO: Pod daemon-set-xb9zg is not available
Feb  4 17:35:33.633: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:34.640: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:35.635: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:36.652: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:37.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:38.633: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:39.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:40.651: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:41.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:42.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:43.631: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:44.631: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:45.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:46.636: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:47.654: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:48.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:49.644: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:50.633: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:51.633: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:52.636: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:53.640: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:54.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:55.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:56.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:57.672: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:58.631: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:35:59.631: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:00.642: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:01.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:02.635: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:03.646: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:04.636: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:05.652: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:05.652: INFO: Pod daemon-set-wrffv is not available
Feb  4 17:36:06.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:06.634: INFO: Pod daemon-set-wrffv is not available
Feb  4 17:36:07.631: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:07.631: INFO: Pod daemon-set-wrffv is not available
Feb  4 17:36:08.659: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:08.660: INFO: Pod daemon-set-wrffv is not available
Feb  4 17:36:09.636: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:09.637: INFO: Pod daemon-set-wrffv is not available
Feb  4 17:36:10.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:10.633: INFO: Pod daemon-set-wrffv is not available
Feb  4 17:36:11.632: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:11.632: INFO: Pod daemon-set-wrffv is not available
Feb  4 17:36:12.634: INFO: Wrong image for pod: daemon-set-wrffv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  4 17:36:12.634: INFO: Pod daemon-set-wrffv is not available
Feb  4 17:36:13.631: INFO: Pod daemon-set-xsblh is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb  4 17:36:13.653: INFO: Number of nodes with available pods: 1
Feb  4 17:36:13.653: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 is running more than one daemon pod
Feb  4 17:36:14.667: INFO: Number of nodes with available pods: 1
Feb  4 17:36:14.667: INFO: Node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 is running more than one daemon pod
Feb  4 17:36:15.667: INFO: Number of nodes with available pods: 2
Feb  4 17:36:15.667: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-95jp9, will wait for the garbage collector to delete the pods
Feb  4 17:36:15.773: INFO: Deleting DaemonSet.extensions daemon-set took: 20.906104ms
Feb  4 17:36:15.873: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.232146ms
Feb  4 17:36:22.818: INFO: Number of nodes with available pods: 0
Feb  4 17:36:22.819: INFO: Number of running nodes: 0, number of available pods: 0
Feb  4 17:36:22.825: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-95jp9/daemonsets","resourceVersion":"20146"},"items":null}

Feb  4 17:36:22.833: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-95jp9/pods","resourceVersion":"20146"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:36:22.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-95jp9" for this suite.
Feb  4 17:36:28.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:36:29.259: INFO: namespace: e2e-tests-daemonsets-95jp9, resource: bindings, ignored listing per whitelist
Feb  4 17:36:29.259: INFO: namespace e2e-tests-daemonsets-95jp9 deletion completed in 6.39677819s

• [SLOW TEST:103.977 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:36:29.262: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb  4 17:36:29.479: INFO: Waiting up to 5m0s for pod "pod-679daffd-28a3-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-v65tp" to be "success or failure"
Feb  4 17:36:29.484: INFO: Pod "pod-679daffd-28a3-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.704802ms
Feb  4 17:36:31.490: INFO: Pod "pod-679daffd-28a3-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011223257s
Feb  4 17:36:33.506: INFO: Pod "pod-679daffd-28a3-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026445654s
Feb  4 17:36:35.512: INFO: Pod "pod-679daffd-28a3-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033396114s
STEP: Saw pod success
Feb  4 17:36:35.513: INFO: Pod "pod-679daffd-28a3-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:36:35.517: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-679daffd-28a3-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 17:36:35.587: INFO: Waiting for pod pod-679daffd-28a3-11e9-9eab-8e57f341003b to disappear
Feb  4 17:36:35.596: INFO: Pod pod-679daffd-28a3-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:36:35.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-v65tp" for this suite.
Feb  4 17:36:41.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:36:41.867: INFO: namespace: e2e-tests-emptydir-v65tp, resource: bindings, ignored listing per whitelist
Feb  4 17:36:42.017: INFO: namespace e2e-tests-emptydir-v65tp deletion completed in 6.406201354s

• [SLOW TEST:12.755 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:36:42.018: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-gp5vt
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  4 17:36:42.318: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  4 17:37:06.573: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.105:8080/dial?request=hostName&protocol=udp&host=172.25.0.125&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-gp5vt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:37:06.573: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:37:07.117: INFO: Waiting for endpoints: map[]
Feb  4 17:37:07.127: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.105:8080/dial?request=hostName&protocol=udp&host=172.25.1.104&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-gp5vt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:37:07.127: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:37:07.665: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:37:07.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-gp5vt" for this suite.
Feb  4 17:37:31.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:37:32.024: INFO: namespace: e2e-tests-pod-network-test-gp5vt, resource: bindings, ignored listing per whitelist
Feb  4 17:37:32.107: INFO: namespace e2e-tests-pod-network-test-gp5vt deletion completed in 24.433863642s

• [SLOW TEST:50.090 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:37:32.111: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-9zpml/configmap-test-8d1cbe9d-28a3-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 17:37:32.416: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d1f840a-28a3-11e9-9eab-8e57f341003b" in namespace "e2e-tests-configmap-9zpml" to be "success or failure"
Feb  4 17:37:32.436: INFO: Pod "pod-configmaps-8d1f840a-28a3-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.207083ms
Feb  4 17:37:34.442: INFO: Pod "pod-configmaps-8d1f840a-28a3-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025777384s
Feb  4 17:37:36.448: INFO: Pod "pod-configmaps-8d1f840a-28a3-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031976801s
STEP: Saw pod success
Feb  4 17:37:36.448: INFO: Pod "pod-configmaps-8d1f840a-28a3-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:37:36.453: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-configmaps-8d1f840a-28a3-11e9-9eab-8e57f341003b container env-test: <nil>
STEP: delete the pod
Feb  4 17:37:36.583: INFO: Waiting for pod pod-configmaps-8d1f840a-28a3-11e9-9eab-8e57f341003b to disappear
Feb  4 17:37:36.695: INFO: Pod pod-configmaps-8d1f840a-28a3-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:37:36.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9zpml" for this suite.
Feb  4 17:37:42.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:37:42.758: INFO: namespace: e2e-tests-configmap-9zpml, resource: bindings, ignored listing per whitelist
Feb  4 17:37:43.083: INFO: namespace e2e-tests-configmap-9zpml deletion completed in 6.378819607s

• [SLOW TEST:10.972 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:37:43.086: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-93acf397-28a3-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 17:37:43.427: INFO: Waiting up to 5m0s for pod "pod-secrets-93b08fed-28a3-11e9-9eab-8e57f341003b" in namespace "e2e-tests-secrets-rkcwv" to be "success or failure"
Feb  4 17:37:43.435: INFO: Pod "pod-secrets-93b08fed-28a3-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.609647ms
Feb  4 17:37:45.441: INFO: Pod "pod-secrets-93b08fed-28a3-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014269614s
Feb  4 17:37:47.448: INFO: Pod "pod-secrets-93b08fed-28a3-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020944656s
STEP: Saw pod success
Feb  4 17:37:47.448: INFO: Pod "pod-secrets-93b08fed-28a3-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:37:47.465: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-secrets-93b08fed-28a3-11e9-9eab-8e57f341003b container secret-volume-test: <nil>
STEP: delete the pod
Feb  4 17:37:47.625: INFO: Waiting for pod pod-secrets-93b08fed-28a3-11e9-9eab-8e57f341003b to disappear
Feb  4 17:37:47.632: INFO: Pod pod-secrets-93b08fed-28a3-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:37:47.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-rkcwv" for this suite.
Feb  4 17:37:53.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:37:53.932: INFO: namespace: e2e-tests-secrets-rkcwv, resource: bindings, ignored listing per whitelist
Feb  4 17:37:54.047: INFO: namespace e2e-tests-secrets-rkcwv deletion completed in 6.407944694s

• [SLOW TEST:10.961 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:37:54.051: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:37:54.216: INFO: Creating deployment "test-recreate-deployment"
Feb  4 17:37:54.228: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb  4 17:37:54.243: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb  4 17:37:56.264: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb  4 17:37:56.276: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684898674, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684898674, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684898674, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684898674, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  4 17:37:58.284: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb  4 17:37:58.307: INFO: Updating deployment test-recreate-deployment
Feb  4 17:37:58.307: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  4 17:37:58.487: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-tll8h,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tll8h/deployments/test-recreate-deployment,UID:9a23aeb1-28a3-11e9-9959-0a580af41676,ResourceVersion:20607,Generation:2,CreationTimestamp:2019-02-04 17:37:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-04 17:37:58 +0000 UTC 2019-02-04 17:37:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-04 17:37:58 +0000 UTC 2019-02-04 17:37:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb  4 17:37:58.499: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-tll8h,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tll8h/replicasets/test-recreate-deployment-697fbf54bf,UID:9c9dd06b-28a3-11e9-9a5d-0a580af41e66,ResourceVersion:20606,Generation:1,CreationTimestamp:2019-02-04 17:37:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9a23aeb1-28a3-11e9-9959-0a580af41676 0xc001a1ad67 0xc001a1ad68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  4 17:37:58.499: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb  4 17:37:58.500: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-tll8h,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tll8h/replicasets/test-recreate-deployment-5dfdcc846d,UID:9a261580-28a3-11e9-9a5d-0a580af41e66,ResourceVersion:20595,Generation:2,CreationTimestamp:2019-02-04 17:37:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9a23aeb1-28a3-11e9-9959-0a580af41676 0xc001a1aca7 0xc001a1aca8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  4 17:37:58.507: INFO: Pod "test-recreate-deployment-697fbf54bf-v4xxj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-v4xxj,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-tll8h,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tll8h/pods/test-recreate-deployment-697fbf54bf-v4xxj,UID:9ca0e555-28a3-11e9-9a5d-0a580af41e66,ResourceVersion:20608,Generation:0,CreationTimestamp:2019-02-04 17:37:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 9c9dd06b-28a3-11e9-9a5d-0a580af41e66 0xc001a1b767 0xc001a1b768}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4jjpj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4jjpj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4jjpj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-6rthf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a1b7d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a1b840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:37:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:37:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:37:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:37:58 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-02-04 17:37:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:37:58.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-tll8h" for this suite.
Feb  4 17:38:04.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:38:04.863: INFO: namespace: e2e-tests-deployment-tll8h, resource: bindings, ignored listing per whitelist
Feb  4 17:38:04.900: INFO: namespace e2e-tests-deployment-tll8h deletion completed in 6.385945046s

• [SLOW TEST:10.850 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:38:04.901: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-a0a47f08-28a3-11e9-9eab-8e57f341003b
STEP: Creating configMap with name cm-test-opt-upd-a0a47f5e-28a3-11e9-9eab-8e57f341003b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a0a47f08-28a3-11e9-9eab-8e57f341003b
STEP: Updating configmap cm-test-opt-upd-a0a47f5e-28a3-11e9-9eab-8e57f341003b
STEP: Creating configMap with name cm-test-opt-create-a0a47f95-28a3-11e9-9eab-8e57f341003b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:39:44.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-898dw" for this suite.
Feb  4 17:40:08.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:40:08.380: INFO: namespace: e2e-tests-projected-898dw, resource: bindings, ignored listing per whitelist
Feb  4 17:40:08.522: INFO: namespace e2e-tests-projected-898dw deletion completed in 24.436776182s

• [SLOW TEST:123.622 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:40:08.526: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-jhstv
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-jhstv
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-jhstv
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-jhstv
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-jhstv
Feb  4 17:40:12.807: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-jhstv, name: ss-0, uid: ec9f91a7-28a3-11e9-9a5d-0a580af41e66, status phase: Pending. Waiting for statefulset controller to delete.
Feb  4 17:40:13.157: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-jhstv, name: ss-0, uid: ec9f91a7-28a3-11e9-9a5d-0a580af41e66, status phase: Failed. Waiting for statefulset controller to delete.
Feb  4 17:40:13.171: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-jhstv, name: ss-0, uid: ec9f91a7-28a3-11e9-9a5d-0a580af41e66, status phase: Failed. Waiting for statefulset controller to delete.
Feb  4 17:40:13.187: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-jhstv
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-jhstv
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-jhstv and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  4 17:40:17.265: INFO: Deleting all statefulset in ns e2e-tests-statefulset-jhstv
Feb  4 17:40:17.273: INFO: Scaling statefulset ss to 0
Feb  4 17:40:37.311: INFO: Waiting for statefulset status.replicas updated to 0
Feb  4 17:40:37.330: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:40:37.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-jhstv" for this suite.
Feb  4 17:40:43.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:40:43.448: INFO: namespace: e2e-tests-statefulset-jhstv, resource: bindings, ignored listing per whitelist
Feb  4 17:40:43.760: INFO: namespace e2e-tests-statefulset-jhstv deletion completed in 6.397984993s

• [SLOW TEST:35.235 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:40:43.765: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:40:50.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-zwrh6" for this suite.
Feb  4 17:40:56.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:40:56.590: INFO: namespace: e2e-tests-namespaces-zwrh6, resource: bindings, ignored listing per whitelist
Feb  4 17:40:56.611: INFO: namespace e2e-tests-namespaces-zwrh6 deletion completed in 6.395112799s
STEP: Destroying namespace "e2e-tests-nsdeletetest-9njlr" for this suite.
Feb  4 17:40:56.619: INFO: Namespace e2e-tests-nsdeletetest-9njlr was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-xdtxb" for this suite.
Feb  4 17:41:02.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:41:02.964: INFO: namespace: e2e-tests-nsdeletetest-xdtxb, resource: bindings, ignored listing per whitelist
Feb  4 17:41:03.094: INFO: namespace e2e-tests-nsdeletetest-xdtxb deletion completed in 6.475037251s

• [SLOW TEST:19.330 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:41:03.097: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb  4 17:41:03.400: INFO: Waiting up to 5m0s for pod "var-expansion-0ae208ac-28a4-11e9-9eab-8e57f341003b" in namespace "e2e-tests-var-expansion-pj76q" to be "success or failure"
Feb  4 17:41:03.408: INFO: Pod "var-expansion-0ae208ac-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.106888ms
Feb  4 17:41:05.426: INFO: Pod "var-expansion-0ae208ac-28a4-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026665914s
STEP: Saw pod success
Feb  4 17:41:05.427: INFO: Pod "var-expansion-0ae208ac-28a4-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:41:05.439: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod var-expansion-0ae208ac-28a4-11e9-9eab-8e57f341003b container dapi-container: <nil>
STEP: delete the pod
Feb  4 17:41:05.532: INFO: Waiting for pod var-expansion-0ae208ac-28a4-11e9-9eab-8e57f341003b to disappear
Feb  4 17:41:05.538: INFO: Pod var-expansion-0ae208ac-28a4-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:41:05.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-pj76q" for this suite.
Feb  4 17:41:11.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:41:11.764: INFO: namespace: e2e-tests-var-expansion-pj76q, resource: bindings, ignored listing per whitelist
Feb  4 17:41:11.984: INFO: namespace e2e-tests-var-expansion-pj76q deletion completed in 6.43889717s

• [SLOW TEST:8.887 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:41:11.985: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  4 17:41:12.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-cc89z'
Feb  4 17:41:13.830: INFO: stderr: ""
Feb  4 17:41:13.830: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Feb  4 17:41:13.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-cc89z'
Feb  4 17:41:15.238: INFO: stderr: ""
Feb  4 17:41:15.238: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:41:15.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cc89z" for this suite.
Feb  4 17:41:21.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:41:21.645: INFO: namespace: e2e-tests-kubectl-cc89z, resource: bindings, ignored listing per whitelist
Feb  4 17:41:21.708: INFO: namespace e2e-tests-kubectl-cc89z deletion completed in 6.459006041s

• [SLOW TEST:9.723 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:41:21.711: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-642cx
Feb  4 17:41:26.016: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-642cx
STEP: checking the pod's current state and verifying that restartCount is present
Feb  4 17:41:26.023: INFO: Initial restart count of pod liveness-http is 0
Feb  4 17:41:48.146: INFO: Restart count of pod e2e-tests-container-probe-642cx/liveness-http is now 1 (22.123290822s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:41:48.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-642cx" for this suite.
Feb  4 17:41:54.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:41:54.462: INFO: namespace: e2e-tests-container-probe-642cx, resource: bindings, ignored listing per whitelist
Feb  4 17:41:54.680: INFO: namespace e2e-tests-container-probe-642cx deletion completed in 6.491888996s

• [SLOW TEST:32.969 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:41:54.681: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:41:54.918: INFO: (0) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 25.831018ms)
Feb  4 17:41:55.002: INFO: (1) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 83.995427ms)
Feb  4 17:41:55.013: INFO: (2) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 11.118713ms)
Feb  4 17:41:55.103: INFO: (3) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 89.103476ms)
Feb  4 17:41:55.113: INFO: (4) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 10.374741ms)
Feb  4 17:41:55.125: INFO: (5) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 11.605311ms)
Feb  4 17:41:55.135: INFO: (6) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 9.5056ms)
Feb  4 17:41:55.144: INFO: (7) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 9.476213ms)
Feb  4 17:41:55.155: INFO: (8) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 10.462978ms)
Feb  4 17:41:55.166: INFO: (9) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 10.555089ms)
Feb  4 17:41:55.204: INFO: (10) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 37.765554ms)
Feb  4 17:41:55.214: INFO: (11) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 9.647109ms)
Feb  4 17:41:55.223: INFO: (12) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 9.033635ms)
Feb  4 17:41:55.232: INFO: (13) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 9.554226ms)
Feb  4 17:41:55.242: INFO: (14) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 9.248185ms)
Feb  4 17:41:55.251: INFO: (15) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 8.792027ms)
Feb  4 17:41:55.259: INFO: (16) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 8.826895ms)
Feb  4 17:41:55.270: INFO: (17) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 9.97365ms)
Feb  4 17:41:55.279: INFO: (18) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 9.441698ms)
Feb  4 17:41:55.296: INFO: (19) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 16.968043ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:41:55.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-2bgsq" for this suite.
Feb  4 17:42:01.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:42:01.709: INFO: namespace: e2e-tests-proxy-2bgsq, resource: bindings, ignored listing per whitelist
Feb  4 17:42:01.734: INFO: namespace e2e-tests-proxy-2bgsq deletion completed in 6.431416091s

• [SLOW TEST:7.054 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:42:01.742: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb  4 17:42:02.036: INFO: Waiting up to 5m0s for pod "pod-2dd500c8-28a4-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-266dq" to be "success or failure"
Feb  4 17:42:02.042: INFO: Pod "pod-2dd500c8-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.891941ms
Feb  4 17:42:04.057: INFO: Pod "pod-2dd500c8-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020543147s
Feb  4 17:42:06.065: INFO: Pod "pod-2dd500c8-28a4-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02813444s
STEP: Saw pod success
Feb  4 17:42:06.065: INFO: Pod "pod-2dd500c8-28a4-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:42:06.070: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-2dd500c8-28a4-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 17:42:06.128: INFO: Waiting for pod pod-2dd500c8-28a4-11e9-9eab-8e57f341003b to disappear
Feb  4 17:42:06.139: INFO: Pod pod-2dd500c8-28a4-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:42:06.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-266dq" for this suite.
Feb  4 17:42:12.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:42:12.576: INFO: namespace: e2e-tests-emptydir-266dq, resource: bindings, ignored listing per whitelist
Feb  4 17:42:12.609: INFO: namespace e2e-tests-emptydir-266dq deletion completed in 6.456553605s

• [SLOW TEST:10.868 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:42:12.612: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb  4 17:42:13.356: INFO: created pod pod-service-account-defaultsa
Feb  4 17:42:13.357: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb  4 17:42:13.368: INFO: created pod pod-service-account-mountsa
Feb  4 17:42:13.368: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb  4 17:42:13.386: INFO: created pod pod-service-account-nomountsa
Feb  4 17:42:13.386: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb  4 17:42:13.398: INFO: created pod pod-service-account-defaultsa-mountspec
Feb  4 17:42:13.398: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb  4 17:42:13.420: INFO: created pod pod-service-account-mountsa-mountspec
Feb  4 17:42:13.421: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb  4 17:42:13.428: INFO: created pod pod-service-account-nomountsa-mountspec
Feb  4 17:42:13.429: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb  4 17:42:13.438: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb  4 17:42:13.438: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb  4 17:42:13.465: INFO: created pod pod-service-account-mountsa-nomountspec
Feb  4 17:42:13.465: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb  4 17:42:13.484: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb  4 17:42:13.484: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:42:13.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-fbgqw" for this suite.
Feb  4 17:42:37.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:42:37.913: INFO: namespace: e2e-tests-svcaccounts-fbgqw, resource: bindings, ignored listing per whitelist
Feb  4 17:42:37.991: INFO: namespace e2e-tests-svcaccounts-fbgqw deletion completed in 24.497177326s

• [SLOW TEST:25.380 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:42:37.996: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-nvhjt
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-nvhjt
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-nvhjt
Feb  4 17:42:38.340: INFO: Found 0 stateful pods, waiting for 1
Feb  4 17:42:48.361: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb  4 17:42:48.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-nvhjt ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  4 17:42:49.348: INFO: stderr: ""
Feb  4 17:42:49.348: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  4 17:42:49.348: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  4 17:42:49.410: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb  4 17:42:59.434: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  4 17:42:59.434: INFO: Waiting for statefulset status.replicas updated to 0
Feb  4 17:42:59.461: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb  4 17:42:59.461: INFO: ss-0  machine-kubermatic-conformancecluster-61wiwtyxah-6rthf  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:38 +0000 UTC  }]
Feb  4 17:42:59.461: INFO: 
Feb  4 17:42:59.461: INFO: StatefulSet ss has not reached scale 3, at 1
Feb  4 17:43:00.468: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995264658s
Feb  4 17:43:01.518: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.9879101s
Feb  4 17:43:02.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9378192s
Feb  4 17:43:03.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.925436588s
Feb  4 17:43:04.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.914534778s
Feb  4 17:43:05.595: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.908022954s
Feb  4 17:43:06.602: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.860194555s
Feb  4 17:43:07.610: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.853598622s
Feb  4 17:43:08.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 845.723483ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-nvhjt
Feb  4 17:43:09.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-nvhjt ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 17:43:10.476: INFO: stderr: ""
Feb  4 17:43:10.476: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  4 17:43:10.476: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  4 17:43:10.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-nvhjt ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 17:43:11.370: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb  4 17:43:11.370: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  4 17:43:11.370: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  4 17:43:11.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-nvhjt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  4 17:43:12.301: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb  4 17:43:12.301: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  4 17:43:12.301: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  4 17:43:12.310: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  4 17:43:12.310: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  4 17:43:12.310: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb  4 17:43:12.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-nvhjt ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  4 17:43:13.222: INFO: stderr: ""
Feb  4 17:43:13.222: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  4 17:43:13.222: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  4 17:43:13.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-nvhjt ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  4 17:43:14.019: INFO: stderr: ""
Feb  4 17:43:14.019: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  4 17:43:14.019: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  4 17:43:14.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 exec --namespace=e2e-tests-statefulset-nvhjt ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  4 17:43:14.819: INFO: stderr: ""
Feb  4 17:43:14.819: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  4 17:43:14.819: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  4 17:43:14.819: INFO: Waiting for statefulset status.replicas updated to 0
Feb  4 17:43:14.825: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb  4 17:43:24.878: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  4 17:43:24.878: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb  4 17:43:24.878: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb  4 17:43:24.924: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb  4 17:43:24.924: INFO: ss-0  machine-kubermatic-conformancecluster-61wiwtyxah-6rthf  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:38 +0000 UTC  }]
Feb  4 17:43:24.924: INFO: ss-1  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:24.924: INFO: ss-2  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:24.924: INFO: 
Feb  4 17:43:24.924: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  4 17:43:25.932: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb  4 17:43:25.932: INFO: ss-0  machine-kubermatic-conformancecluster-61wiwtyxah-6rthf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:38 +0000 UTC  }]
Feb  4 17:43:25.932: INFO: ss-1  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:25.932: INFO: ss-2  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:25.932: INFO: 
Feb  4 17:43:25.932: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  4 17:43:26.940: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb  4 17:43:26.940: INFO: ss-0  machine-kubermatic-conformancecluster-61wiwtyxah-6rthf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:38 +0000 UTC  }]
Feb  4 17:43:26.940: INFO: ss-1  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:26.940: INFO: ss-2  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:26.940: INFO: 
Feb  4 17:43:26.940: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  4 17:43:27.951: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb  4 17:43:27.952: INFO: ss-1  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:27.952: INFO: ss-2  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:27.952: INFO: 
Feb  4 17:43:27.952: INFO: StatefulSet ss has not reached scale 0, at 2
Feb  4 17:43:28.958: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb  4 17:43:28.958: INFO: ss-1  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:28.958: INFO: ss-2  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:28.959: INFO: 
Feb  4 17:43:28.959: INFO: StatefulSet ss has not reached scale 0, at 2
Feb  4 17:43:29.967: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb  4 17:43:29.968: INFO: ss-1  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:29.968: INFO: ss-2  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:29.968: INFO: 
Feb  4 17:43:29.968: INFO: StatefulSet ss has not reached scale 0, at 2
Feb  4 17:43:31.002: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb  4 17:43:31.002: INFO: ss-1  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:31.002: INFO: ss-2  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:31.002: INFO: 
Feb  4 17:43:31.002: INFO: StatefulSet ss has not reached scale 0, at 2
Feb  4 17:43:32.009: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb  4 17:43:32.009: INFO: ss-1  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:32.009: INFO: ss-2  machine-kubermatic-conformancecluster-61wiwtyxah-t5p54  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:43:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:42:59 +0000 UTC  }]
Feb  4 17:43:32.010: INFO: 
Feb  4 17:43:32.010: INFO: StatefulSet ss has not reached scale 0, at 2
Feb  4 17:43:33.016: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.903574787s
Feb  4 17:43:34.030: INFO: Verifying statefulset ss doesn't scale past 0 for another 897.107134ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-nvhjt
Feb  4 17:43:35.049: INFO: Scaling statefulset ss to 0
Feb  4 17:43:35.071: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  4 17:43:35.076: INFO: Deleting all statefulset in ns e2e-tests-statefulset-nvhjt
Feb  4 17:43:35.081: INFO: Scaling statefulset ss to 0
Feb  4 17:43:35.095: INFO: Waiting for statefulset status.replicas updated to 0
Feb  4 17:43:35.100: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:43:35.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-nvhjt" for this suite.
Feb  4 17:43:41.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:43:41.239: INFO: namespace: e2e-tests-statefulset-nvhjt, resource: bindings, ignored listing per whitelist
Feb  4 17:43:41.543: INFO: namespace e2e-tests-statefulset-nvhjt deletion completed in 6.377955913s

• [SLOW TEST:63.548 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:43:41.547: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb  4 17:43:41.739: INFO: Waiting up to 5m0s for pod "client-containers-694290cd-28a4-11e9-9eab-8e57f341003b" in namespace "e2e-tests-containers-ptxt5" to be "success or failure"
Feb  4 17:43:41.744: INFO: Pod "client-containers-694290cd-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.995899ms
Feb  4 17:43:43.752: INFO: Pod "client-containers-694290cd-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012854684s
Feb  4 17:43:45.769: INFO: Pod "client-containers-694290cd-28a4-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029727359s
STEP: Saw pod success
Feb  4 17:43:45.769: INFO: Pod "client-containers-694290cd-28a4-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:43:45.773: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod client-containers-694290cd-28a4-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 17:43:45.821: INFO: Waiting for pod client-containers-694290cd-28a4-11e9-9eab-8e57f341003b to disappear
Feb  4 17:43:45.829: INFO: Pod client-containers-694290cd-28a4-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:43:45.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-ptxt5" for this suite.
Feb  4 17:43:51.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:43:51.950: INFO: namespace: e2e-tests-containers-ptxt5, resource: bindings, ignored listing per whitelist
Feb  4 17:43:52.232: INFO: namespace e2e-tests-containers-ptxt5 deletion completed in 6.396352753s

• [SLOW TEST:10.686 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:43:52.239: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-js4g9
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-js4g9
STEP: Deleting pre-stop pod
Feb  4 17:44:09.604: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:44:09.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-js4g9" for this suite.
Feb  4 17:44:49.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:44:49.989: INFO: namespace: e2e-tests-prestop-js4g9, resource: bindings, ignored listing per whitelist
Feb  4 17:44:50.081: INFO: namespace e2e-tests-prestop-js4g9 deletion completed in 40.443197198s

• [SLOW TEST:57.843 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:44:50.092: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-9236991a-28a4-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 17:44:50.462: INFO: Waiting up to 5m0s for pod "pod-secrets-9237c6b7-28a4-11e9-9eab-8e57f341003b" in namespace "e2e-tests-secrets-vx8cr" to be "success or failure"
Feb  4 17:44:50.471: INFO: Pod "pod-secrets-9237c6b7-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.21212ms
Feb  4 17:44:52.478: INFO: Pod "pod-secrets-9237c6b7-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01629757s
Feb  4 17:44:54.504: INFO: Pod "pod-secrets-9237c6b7-28a4-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042427325s
STEP: Saw pod success
Feb  4 17:44:54.504: INFO: Pod "pod-secrets-9237c6b7-28a4-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:44:54.512: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-secrets-9237c6b7-28a4-11e9-9eab-8e57f341003b container secret-volume-test: <nil>
STEP: delete the pod
Feb  4 17:44:54.867: INFO: Waiting for pod pod-secrets-9237c6b7-28a4-11e9-9eab-8e57f341003b to disappear
Feb  4 17:44:54.875: INFO: Pod pod-secrets-9237c6b7-28a4-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:44:54.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-vx8cr" for this suite.
Feb  4 17:45:02.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:45:03.223: INFO: namespace: e2e-tests-secrets-vx8cr, resource: bindings, ignored listing per whitelist
Feb  4 17:45:03.539: INFO: namespace e2e-tests-secrets-vx8cr deletion completed in 8.652696698s

• [SLOW TEST:13.447 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:45:03.543: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9a42afba-28a4-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 17:45:04.102: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a43c1c6-28a4-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-4w876" to be "success or failure"
Feb  4 17:45:04.107: INFO: Pod "pod-projected-configmaps-9a43c1c6-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.01076ms
Feb  4 17:45:06.205: INFO: Pod "pod-projected-configmaps-9a43c1c6-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.102902212s
Feb  4 17:45:08.223: INFO: Pod "pod-projected-configmaps-9a43c1c6-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.121103076s
Feb  4 17:45:10.247: INFO: Pod "pod-projected-configmaps-9a43c1c6-28a4-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.144382402s
STEP: Saw pod success
Feb  4 17:45:10.250: INFO: Pod "pod-projected-configmaps-9a43c1c6-28a4-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:45:10.257: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-projected-configmaps-9a43c1c6-28a4-11e9-9eab-8e57f341003b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 17:45:10.395: INFO: Waiting for pod pod-projected-configmaps-9a43c1c6-28a4-11e9-9eab-8e57f341003b to disappear
Feb  4 17:45:10.401: INFO: Pod pod-projected-configmaps-9a43c1c6-28a4-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:45:10.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4w876" for this suite.
Feb  4 17:45:16.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:45:16.721: INFO: namespace: e2e-tests-projected-4w876, resource: bindings, ignored listing per whitelist
Feb  4 17:45:16.869: INFO: namespace e2e-tests-projected-4w876 deletion completed in 6.461076307s

• [SLOW TEST:13.326 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:45:16.873: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 17:45:17.097: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a21990a6-28a4-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-pb28n" to be "success or failure"
Feb  4 17:45:17.102: INFO: Pod "downwardapi-volume-a21990a6-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201212ms
Feb  4 17:45:19.147: INFO: Pod "downwardapi-volume-a21990a6-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049645508s
Feb  4 17:45:21.153: INFO: Pod "downwardapi-volume-a21990a6-28a4-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056037487s
STEP: Saw pod success
Feb  4 17:45:21.153: INFO: Pod "downwardapi-volume-a21990a6-28a4-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:45:21.158: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downwardapi-volume-a21990a6-28a4-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 17:45:21.577: INFO: Waiting for pod downwardapi-volume-a21990a6-28a4-11e9-9eab-8e57f341003b to disappear
Feb  4 17:45:21.627: INFO: Pod downwardapi-volume-a21990a6-28a4-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:45:21.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pb28n" for this suite.
Feb  4 17:45:27.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:45:28.738: INFO: namespace: e2e-tests-downward-api-pb28n, resource: bindings, ignored listing per whitelist
Feb  4 17:45:28.858: INFO: namespace e2e-tests-downward-api-pb28n deletion completed in 7.142570016s

• [SLOW TEST:11.985 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:45:28.863: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Feb  4 17:45:29.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-98phl'
Feb  4 17:45:30.305: INFO: stderr: ""
Feb  4 17:45:30.305: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb  4 17:45:31.321: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 17:45:31.321: INFO: Found 0 / 1
Feb  4 17:45:32.313: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 17:45:32.313: INFO: Found 0 / 1
Feb  4 17:45:33.312: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 17:45:33.313: INFO: Found 1 / 1
Feb  4 17:45:33.313: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  4 17:45:33.321: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 17:45:33.321: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb  4 17:45:33.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 logs redis-master-zqv6q redis-master --namespace=e2e-tests-kubectl-98phl'
Feb  4 17:45:34.006: INFO: stderr: ""
Feb  4 17:45:34.006: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 04 Feb 17:45:32.248 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 04 Feb 17:45:32.248 # Server started, Redis version 3.2.12\n1:M 04 Feb 17:45:32.248 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 04 Feb 17:45:32.248 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb  4 17:45:34.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 log redis-master-zqv6q redis-master --namespace=e2e-tests-kubectl-98phl --tail=1'
Feb  4 17:45:34.404: INFO: stderr: ""
Feb  4 17:45:34.404: INFO: stdout: "1:M 04 Feb 17:45:32.248 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb  4 17:45:34.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 log redis-master-zqv6q redis-master --namespace=e2e-tests-kubectl-98phl --limit-bytes=1'
Feb  4 17:45:34.802: INFO: stderr: ""
Feb  4 17:45:34.803: INFO: stdout: " "
STEP: exposing timestamps
Feb  4 17:45:34.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 log redis-master-zqv6q redis-master --namespace=e2e-tests-kubectl-98phl --tail=1 --timestamps'
Feb  4 17:45:35.200: INFO: stderr: ""
Feb  4 17:45:35.200: INFO: stdout: "2019-02-04T17:45:32.271994387Z 1:M 04 Feb 17:45:32.248 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb  4 17:45:37.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 log redis-master-zqv6q redis-master --namespace=e2e-tests-kubectl-98phl --since=1s'
Feb  4 17:45:38.042: INFO: stderr: ""
Feb  4 17:45:38.042: INFO: stdout: ""
Feb  4 17:45:38.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 log redis-master-zqv6q redis-master --namespace=e2e-tests-kubectl-98phl --since=24h'
Feb  4 17:45:38.399: INFO: stderr: ""
Feb  4 17:45:38.399: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 04 Feb 17:45:32.248 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 04 Feb 17:45:32.248 # Server started, Redis version 3.2.12\n1:M 04 Feb 17:45:32.248 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 04 Feb 17:45:32.248 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Feb  4 17:45:38.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-98phl'
Feb  4 17:45:38.715: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  4 17:45:38.715: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb  4 17:45:38.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-98phl'
Feb  4 17:45:39.295: INFO: stderr: "No resources found.\n"
Feb  4 17:45:39.295: INFO: stdout: ""
Feb  4 17:45:39.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -l name=nginx --namespace=e2e-tests-kubectl-98phl -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  4 17:45:39.795: INFO: stderr: ""
Feb  4 17:45:39.795: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:45:39.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-98phl" for this suite.
Feb  4 17:45:45.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:45:46.015: INFO: namespace: e2e-tests-kubectl-98phl, resource: bindings, ignored listing per whitelist
Feb  4 17:45:46.264: INFO: namespace e2e-tests-kubectl-98phl deletion completed in 6.454252532s

• [SLOW TEST:17.402 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:45:46.285: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:45:46.543: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb  4 17:45:46.557: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb  4 17:45:51.575: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  4 17:45:51.575: INFO: Creating deployment "test-rolling-update-deployment"
Feb  4 17:45:51.667: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb  4 17:45:51.693: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb  4 17:45:53.708: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb  4 17:45:53.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684899151, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684899151, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684899151, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684899151, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  4 17:45:55.722: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  4 17:45:55.738: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-wc6nb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-wc6nb/deployments/test-rolling-update-deployment,UID:b6ab68f4-28a4-11e9-9959-0a580af41676,ResourceVersion:22628,Generation:1,CreationTimestamp:2019-02-04 17:45:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-04 17:45:51 +0000 UTC 2019-02-04 17:45:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-04 17:45:54 +0000 UTC 2019-02-04 17:45:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb  4 17:45:55.744: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-wc6nb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-wc6nb/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:b6bccaa9-28a4-11e9-9a5d-0a580af41e66,ResourceVersion:22617,Generation:1,CreationTimestamp:2019-02-04 17:45:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b6ab68f4-28a4-11e9-9959-0a580af41676 0xc001c0c4a7 0xc001c0c4a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb  4 17:45:55.744: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb  4 17:45:55.744: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-wc6nb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-wc6nb/replicasets/test-rolling-update-controller,UID:b3ab7054-28a4-11e9-9959-0a580af41676,ResourceVersion:22627,Generation:2,CreationTimestamp:2019-02-04 17:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b6ab68f4-28a4-11e9-9959-0a580af41676 0xc001c0c2e7 0xc001c0c2e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  4 17:45:55.750: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-sddl4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-sddl4,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-wc6nb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-wc6nb/pods/test-rolling-update-deployment-68b55d7bc6-sddl4,UID:b6be17a7-28a4-11e9-9a5d-0a580af41e66,ResourceVersion:22616,Generation:0,CreationTimestamp:2019-02-04 17:45:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 b6bccaa9-28a4-11e9-9a5d-0a580af41e66 0xc000837a17 0xc000837a18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-g9w5r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-g9w5r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-g9w5r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-61wiwtyxah-t5p54,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000837c10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000837dc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:45:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:45:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:45:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-04 17:45:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:172.25.1.120,StartTime:2019-02-04 17:45:51 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-04 17:45:53 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://282a2cb5c4641d2fb2ad607b58c5b0a6f266e2ef58c892806cd92c6a5eb2f623}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:45:55.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-wc6nb" for this suite.
Feb  4 17:46:01.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:46:02.079: INFO: namespace: e2e-tests-deployment-wc6nb, resource: bindings, ignored listing per whitelist
Feb  4 17:46:02.206: INFO: namespace e2e-tests-deployment-wc6nb deletion completed in 6.449462792s

• [SLOW TEST:15.921 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:46:02.207: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 17:46:02.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd2113d3-28a4-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-2jlzj" to be "success or failure"
Feb  4 17:46:02.456: INFO: Pod "downwardapi-volume-bd2113d3-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.991551ms
Feb  4 17:46:04.464: INFO: Pod "downwardapi-volume-bd2113d3-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014864915s
Feb  4 17:46:06.472: INFO: Pod "downwardapi-volume-bd2113d3-28a4-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023416475s
STEP: Saw pod success
Feb  4 17:46:06.472: INFO: Pod "downwardapi-volume-bd2113d3-28a4-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:46:06.477: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod downwardapi-volume-bd2113d3-28a4-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 17:46:06.691: INFO: Waiting for pod downwardapi-volume-bd2113d3-28a4-11e9-9eab-8e57f341003b to disappear
Feb  4 17:46:06.697: INFO: Pod downwardapi-volume-bd2113d3-28a4-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:46:06.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2jlzj" for this suite.
Feb  4 17:46:12.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:46:13.082: INFO: namespace: e2e-tests-projected-2jlzj, resource: bindings, ignored listing per whitelist
Feb  4 17:46:13.157: INFO: namespace e2e-tests-projected-2jlzj deletion completed in 6.453383395s

• [SLOW TEST:10.950 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:46:13.158: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  4 17:46:18.342: INFO: Successfully updated pod "annotationupdatec3ab71f9-28a4-11e9-9eab-8e57f341003b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:46:20.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gvbd6" for this suite.
Feb  4 17:46:44.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:46:44.534: INFO: namespace: e2e-tests-downward-api-gvbd6, resource: bindings, ignored listing per whitelist
Feb  4 17:46:44.813: INFO: namespace e2e-tests-downward-api-gvbd6 deletion completed in 24.414325151s

• [SLOW TEST:31.655 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:46:44.817: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:46:49.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-2m4nm" for this suite.
Feb  4 17:46:55.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:46:55.212: INFO: namespace: e2e-tests-kubelet-test-2m4nm, resource: bindings, ignored listing per whitelist
Feb  4 17:46:55.653: INFO: namespace e2e-tests-kubelet-test-2m4nm deletion completed in 6.532872698s

• [SLOW TEST:10.837 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:46:55.658: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0204 17:46:57.050948      13 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  4 17:46:57.051: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:46:57.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-49szj" for this suite.
Feb  4 17:47:03.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:47:03.327: INFO: namespace: e2e-tests-gc-49szj, resource: bindings, ignored listing per whitelist
Feb  4 17:47:03.421: INFO: namespace e2e-tests-gc-49szj deletion completed in 6.361800317s

• [SLOW TEST:7.764 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:47:03.422: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb  4 17:47:03.695: INFO: Waiting up to 5m0s for pod "pod-e1a351a2-28a4-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-khmdp" to be "success or failure"
Feb  4 17:47:03.700: INFO: Pod "pod-e1a351a2-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.479538ms
Feb  4 17:47:05.800: INFO: Pod "pod-e1a351a2-28a4-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105053255s
Feb  4 17:47:07.812: INFO: Pod "pod-e1a351a2-28a4-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.117062921s
STEP: Saw pod success
Feb  4 17:47:07.812: INFO: Pod "pod-e1a351a2-28a4-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:47:07.817: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-e1a351a2-28a4-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 17:47:07.920: INFO: Waiting for pod pod-e1a351a2-28a4-11e9-9eab-8e57f341003b to disappear
Feb  4 17:47:07.926: INFO: Pod pod-e1a351a2-28a4-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:47:07.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-khmdp" for this suite.
Feb  4 17:47:13.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:47:14.292: INFO: namespace: e2e-tests-emptydir-khmdp, resource: bindings, ignored listing per whitelist
Feb  4 17:47:14.387: INFO: namespace e2e-tests-emptydir-khmdp deletion completed in 6.449810304s

• [SLOW TEST:10.969 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:47:14.392: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:47:14.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-vs7lb" for this suite.
Feb  4 17:47:38.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:47:39.129: INFO: namespace: e2e-tests-pods-vs7lb, resource: bindings, ignored listing per whitelist
Feb  4 17:47:39.356: INFO: namespace e2e-tests-pods-vs7lb deletion completed in 24.50659676s

• [SLOW TEST:24.965 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:47:39.361: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:47:39.635: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:47:44.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-r9p24" for this suite.
Feb  4 17:48:40.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:48:40.259: INFO: namespace: e2e-tests-pods-r9p24, resource: bindings, ignored listing per whitelist
Feb  4 17:48:40.527: INFO: namespace e2e-tests-pods-r9p24 deletion completed in 56.471041295s

• [SLOW TEST:61.166 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:48:40.527: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb  4 17:48:47.059: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  4 17:48:47.069: INFO: Pod pod-with-poststart-http-hook still exists
Feb  4 17:48:49.070: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  4 17:48:49.103: INFO: Pod pod-with-poststart-http-hook still exists
Feb  4 17:48:51.079: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  4 17:48:51.084: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:48:51.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-dtlxl" for this suite.
Feb  4 17:49:15.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:49:15.383: INFO: namespace: e2e-tests-container-lifecycle-hook-dtlxl, resource: bindings, ignored listing per whitelist
Feb  4 17:49:15.536: INFO: namespace e2e-tests-container-lifecycle-hook-dtlxl deletion completed in 24.445510175s

• [SLOW TEST:35.009 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:49:15.541: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-5plg5
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  4 17:49:15.858: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  4 17:49:40.109: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.124:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-5plg5 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:49:40.109: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:49:40.728: INFO: Found all expected endpoints: [netserver-0]
Feb  4 17:49:40.736: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.147:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-5plg5 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:49:40.736: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:49:41.287: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:49:41.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-5plg5" for this suite.
Feb  4 17:50:05.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:50:05.452: INFO: namespace: e2e-tests-pod-network-test-5plg5, resource: bindings, ignored listing per whitelist
Feb  4 17:50:05.750: INFO: namespace e2e-tests-pod-network-test-5plg5 deletion completed in 24.450708263s

• [SLOW TEST:50.210 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:50:05.758: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  4 17:50:10.548: INFO: Successfully updated pod "annotationupdate4e46e2e7-28a5-11e9-9eab-8e57f341003b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:50:12.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2s7hh" for this suite.
Feb  4 17:50:36.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:50:36.936: INFO: namespace: e2e-tests-projected-2s7hh, resource: bindings, ignored listing per whitelist
Feb  4 17:50:37.176: INFO: namespace e2e-tests-projected-2s7hh deletion completed in 24.472346336s

• [SLOW TEST:31.418 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:50:37.181: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb  4 17:50:45.581: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-sc8vm PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:50:45.581: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:50:46.049: INFO: Exec stderr: ""
Feb  4 17:50:46.049: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-sc8vm PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:50:46.049: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:50:46.495: INFO: Exec stderr: ""
Feb  4 17:50:46.495: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-sc8vm PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:50:46.495: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:50:47.010: INFO: Exec stderr: ""
Feb  4 17:50:47.011: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-sc8vm PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:50:47.011: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:50:47.465: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb  4 17:50:47.465: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-sc8vm PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:50:47.465: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:50:48.063: INFO: Exec stderr: ""
Feb  4 17:50:48.063: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-sc8vm PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:50:48.063: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:50:48.543: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb  4 17:50:48.543: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-sc8vm PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:50:48.543: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:50:49.165: INFO: Exec stderr: ""
Feb  4 17:50:49.165: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-sc8vm PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:50:49.165: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:50:49.642: INFO: Exec stderr: ""
Feb  4 17:50:49.642: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-sc8vm PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:50:49.643: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:50:50.149: INFO: Exec stderr: ""
Feb  4 17:50:50.149: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-sc8vm PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  4 17:50:50.149: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
Feb  4 17:50:50.617: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:50:50.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-sc8vm" for this suite.
Feb  4 17:51:30.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:51:31.134: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-sc8vm, resource: bindings, ignored listing per whitelist
Feb  4 17:51:31.155: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-sc8vm deletion completed in 40.530231276s

• [SLOW TEST:53.974 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:51:31.156: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 17:51:31.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81386c8b-28a5-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-gpj2j" to be "success or failure"
Feb  4 17:51:31.452: INFO: Pod "downwardapi-volume-81386c8b-28a5-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.345935ms
Feb  4 17:51:33.477: INFO: Pod "downwardapi-volume-81386c8b-28a5-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037289699s
Feb  4 17:51:35.482: INFO: Pod "downwardapi-volume-81386c8b-28a5-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042817337s
STEP: Saw pod success
Feb  4 17:51:35.483: INFO: Pod "downwardapi-volume-81386c8b-28a5-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:51:35.487: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downwardapi-volume-81386c8b-28a5-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 17:51:35.733: INFO: Waiting for pod downwardapi-volume-81386c8b-28a5-11e9-9eab-8e57f341003b to disappear
Feb  4 17:51:35.739: INFO: Pod downwardapi-volume-81386c8b-28a5-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:51:35.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gpj2j" for this suite.
Feb  4 17:51:41.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:51:41.938: INFO: namespace: e2e-tests-projected-gpj2j, resource: bindings, ignored listing per whitelist
Feb  4 17:51:42.178: INFO: namespace e2e-tests-projected-gpj2j deletion completed in 6.428671446s

• [SLOW TEST:11.023 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:51:42.183: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb  4 17:51:42.533: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-296264527 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:51:42.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nxkbq" for this suite.
Feb  4 17:51:48.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:51:49.119: INFO: namespace: e2e-tests-kubectl-nxkbq, resource: bindings, ignored listing per whitelist
Feb  4 17:51:49.224: INFO: namespace e2e-tests-kubectl-nxkbq deletion completed in 6.369076956s

• [SLOW TEST:7.041 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:51:49.228: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-6qs26
Feb  4 17:51:51.787: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-6qs26
STEP: checking the pod's current state and verifying that restartCount is present
Feb  4 17:51:51.797: INFO: Initial restart count of pod liveness-exec is 0
Feb  4 17:52:44.138: INFO: Restart count of pod e2e-tests-container-probe-6qs26/liveness-exec is now 1 (52.341450709s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:52:44.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-6qs26" for this suite.
Feb  4 17:52:50.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:52:50.420: INFO: namespace: e2e-tests-container-probe-6qs26, resource: bindings, ignored listing per whitelist
Feb  4 17:52:50.631: INFO: namespace e2e-tests-container-probe-6qs26 deletion completed in 6.457588757s

• [SLOW TEST:61.403 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:52:50.636: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:52:55.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-s7766" for this suite.
Feb  4 17:53:35.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:53:35.334: INFO: namespace: e2e-tests-kubelet-test-s7766, resource: bindings, ignored listing per whitelist
Feb  4 17:53:35.608: INFO: namespace e2e-tests-kubelet-test-s7766 deletion completed in 40.402794357s

• [SLOW TEST:44.975 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:53:35.616: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:53:42.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-mnt4x" for this suite.
Feb  4 17:54:04.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:54:05.299: INFO: namespace: e2e-tests-replication-controller-mnt4x, resource: bindings, ignored listing per whitelist
Feb  4 17:54:05.422: INFO: namespace e2e-tests-replication-controller-mnt4x deletion completed in 22.516865424s

• [SLOW TEST:29.807 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:54:05.427: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb  4 17:54:09.677: INFO: Pod pod-hostip-dd2156f7-28a5-11e9-9eab-8e57f341003b has hostIP: 192.168.1.10
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:54:09.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-vqzxf" for this suite.
Feb  4 17:54:31.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:54:32.049: INFO: namespace: e2e-tests-pods-vqzxf, resource: bindings, ignored listing per whitelist
Feb  4 17:54:32.325: INFO: namespace e2e-tests-pods-vqzxf deletion completed in 22.641297511s

• [SLOW TEST:26.898 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:54:32.330: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-ed330c6f-28a5-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 17:54:32.599: INFO: Waiting up to 5m0s for pod "pod-configmaps-ed346a5a-28a5-11e9-9eab-8e57f341003b" in namespace "e2e-tests-configmap-hj2gd" to be "success or failure"
Feb  4 17:54:32.612: INFO: Pod "pod-configmaps-ed346a5a-28a5-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019866ms
Feb  4 17:54:34.619: INFO: Pod "pod-configmaps-ed346a5a-28a5-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018120128s
Feb  4 17:54:36.625: INFO: Pod "pod-configmaps-ed346a5a-28a5-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024540303s
STEP: Saw pod success
Feb  4 17:54:36.625: INFO: Pod "pod-configmaps-ed346a5a-28a5-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:54:36.630: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-configmaps-ed346a5a-28a5-11e9-9eab-8e57f341003b container configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 17:54:36.730: INFO: Waiting for pod pod-configmaps-ed346a5a-28a5-11e9-9eab-8e57f341003b to disappear
Feb  4 17:54:36.735: INFO: Pod pod-configmaps-ed346a5a-28a5-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:54:36.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hj2gd" for this suite.
Feb  4 17:54:42.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:54:42.909: INFO: namespace: e2e-tests-configmap-hj2gd, resource: bindings, ignored listing per whitelist
Feb  4 17:54:43.103: INFO: namespace e2e-tests-configmap-hj2gd deletion completed in 6.361363675s

• [SLOW TEST:10.774 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:54:43.104: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb  4 17:54:43.377: INFO: Waiting up to 5m0s for pod "var-expansion-f39dbb2e-28a5-11e9-9eab-8e57f341003b" in namespace "e2e-tests-var-expansion-qtxds" to be "success or failure"
Feb  4 17:54:43.389: INFO: Pod "var-expansion-f39dbb2e-28a5-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00332ms
Feb  4 17:54:45.399: INFO: Pod "var-expansion-f39dbb2e-28a5-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021480032s
Feb  4 17:54:47.502: INFO: Pod "var-expansion-f39dbb2e-28a5-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.124698327s
STEP: Saw pod success
Feb  4 17:54:47.502: INFO: Pod "var-expansion-f39dbb2e-28a5-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:54:47.507: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod var-expansion-f39dbb2e-28a5-11e9-9eab-8e57f341003b container dapi-container: <nil>
STEP: delete the pod
Feb  4 17:54:47.694: INFO: Waiting for pod var-expansion-f39dbb2e-28a5-11e9-9eab-8e57f341003b to disappear
Feb  4 17:54:47.699: INFO: Pod var-expansion-f39dbb2e-28a5-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:54:47.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-qtxds" for this suite.
Feb  4 17:54:53.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:54:53.910: INFO: namespace: e2e-tests-var-expansion-qtxds, resource: bindings, ignored listing per whitelist
Feb  4 17:54:54.158: INFO: namespace e2e-tests-var-expansion-qtxds deletion completed in 6.450279291s

• [SLOW TEST:11.054 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:54:54.162: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:54:58.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-sqss6" for this suite.
Feb  4 17:55:04.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:55:04.741: INFO: namespace: e2e-tests-emptydir-wrapper-sqss6, resource: bindings, ignored listing per whitelist
Feb  4 17:55:05.002: INFO: namespace e2e-tests-emptydir-wrapper-sqss6 deletion completed in 6.479477089s

• [SLOW TEST:10.840 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:55:05.010: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb  4 17:55:05.283: INFO: Waiting up to 5m0s for pod "pod-00a99c46-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-dm6st" to be "success or failure"
Feb  4 17:55:05.288: INFO: Pod "pod-00a99c46-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.937979ms
Feb  4 17:55:07.296: INFO: Pod "pod-00a99c46-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012699917s
Feb  4 17:55:09.304: INFO: Pod "pod-00a99c46-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021021921s
STEP: Saw pod success
Feb  4 17:55:09.304: INFO: Pod "pod-00a99c46-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:55:09.309: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-00a99c46-28a6-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 17:55:09.438: INFO: Waiting for pod pod-00a99c46-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 17:55:09.442: INFO: Pod pod-00a99c46-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:55:09.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dm6st" for this suite.
Feb  4 17:55:15.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:55:15.649: INFO: namespace: e2e-tests-emptydir-dm6st, resource: bindings, ignored listing per whitelist
Feb  4 17:55:15.799: INFO: namespace e2e-tests-emptydir-dm6st deletion completed in 6.350006651s

• [SLOW TEST:10.792 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:55:15.804: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb  4 17:55:16.072: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-296264527 proxy --unix-socket=/tmp/kubectl-proxy-unix170716770/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:55:16.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-b5856" for this suite.
Feb  4 17:55:22.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:55:22.565: INFO: namespace: e2e-tests-kubectl-b5856, resource: bindings, ignored listing per whitelist
Feb  4 17:55:22.738: INFO: namespace e2e-tests-kubectl-b5856 deletion completed in 6.444102736s

• [SLOW TEST:6.935 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:55:22.739: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb  4 17:55:23.003: INFO: Waiting up to 5m0s for pod "pod-0b3f4c4a-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-b9r9b" to be "success or failure"
Feb  4 17:55:23.014: INFO: Pod "pod-0b3f4c4a-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.926398ms
Feb  4 17:55:25.054: INFO: Pod "pod-0b3f4c4a-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050442598s
Feb  4 17:55:27.062: INFO: Pod "pod-0b3f4c4a-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058324889s
STEP: Saw pod success
Feb  4 17:55:27.062: INFO: Pod "pod-0b3f4c4a-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:55:27.069: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-0b3f4c4a-28a6-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 17:55:27.209: INFO: Waiting for pod pod-0b3f4c4a-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 17:55:27.218: INFO: Pod pod-0b3f4c4a-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:55:27.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-b9r9b" for this suite.
Feb  4 17:55:33.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:55:33.519: INFO: namespace: e2e-tests-emptydir-b9r9b, resource: bindings, ignored listing per whitelist
Feb  4 17:55:33.598: INFO: namespace e2e-tests-emptydir-b9r9b deletion completed in 6.368158866s

• [SLOW TEST:10.859 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:55:33.599: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  4 17:55:33.884: INFO: Waiting up to 5m0s for pod "downward-api-11bb3260-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-stmjx" to be "success or failure"
Feb  4 17:55:33.889: INFO: Pod "downward-api-11bb3260-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.368471ms
Feb  4 17:55:35.915: INFO: Pod "downward-api-11bb3260-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031426823s
Feb  4 17:55:37.922: INFO: Pod "downward-api-11bb3260-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03848906s
STEP: Saw pod success
Feb  4 17:55:37.923: INFO: Pod "downward-api-11bb3260-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:55:37.928: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downward-api-11bb3260-28a6-11e9-9eab-8e57f341003b container dapi-container: <nil>
STEP: delete the pod
Feb  4 17:55:38.063: INFO: Waiting for pod downward-api-11bb3260-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 17:55:38.073: INFO: Pod downward-api-11bb3260-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:55:38.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-stmjx" for this suite.
Feb  4 17:55:44.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:55:44.361: INFO: namespace: e2e-tests-downward-api-stmjx, resource: bindings, ignored listing per whitelist
Feb  4 17:55:44.493: INFO: namespace e2e-tests-downward-api-stmjx deletion completed in 6.413890814s

• [SLOW TEST:10.895 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:55:44.495: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb  4 17:55:45.322: INFO: Waiting up to 5m0s for pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-7pqv5" in namespace "e2e-tests-svcaccounts-dm2q5" to be "success or failure"
Feb  4 17:55:45.335: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-7pqv5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.277069ms
Feb  4 17:55:47.393: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-7pqv5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071345497s
Feb  4 17:55:49.434: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-7pqv5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.112658556s
STEP: Saw pod success
Feb  4 17:55:49.434: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-7pqv5" satisfied condition "success or failure"
Feb  4 17:55:49.439: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-7pqv5 container token-test: <nil>
STEP: delete the pod
Feb  4 17:55:49.677: INFO: Waiting for pod pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-7pqv5 to disappear
Feb  4 17:55:49.687: INFO: Pod pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-7pqv5 no longer exists
STEP: Creating a pod to test consume service account root CA
Feb  4 17:55:49.866: INFO: Waiting up to 5m0s for pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-n98xz" in namespace "e2e-tests-svcaccounts-dm2q5" to be "success or failure"
Feb  4 17:55:49.951: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-n98xz": Phase="Pending", Reason="", readiness=false. Elapsed: 84.774057ms
Feb  4 17:55:51.961: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-n98xz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094381062s
Feb  4 17:55:53.969: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-n98xz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.102484392s
STEP: Saw pod success
Feb  4 17:55:53.969: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-n98xz" satisfied condition "success or failure"
Feb  4 17:55:53.976: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-n98xz container root-ca-test: <nil>
STEP: delete the pod
Feb  4 17:55:54.142: INFO: Waiting for pod pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-n98xz to disappear
Feb  4 17:55:54.206: INFO: Pod pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-n98xz no longer exists
STEP: Creating a pod to test consume service account namespace
Feb  4 17:55:54.227: INFO: Waiting up to 5m0s for pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-vjbwx" in namespace "e2e-tests-svcaccounts-dm2q5" to be "success or failure"
Feb  4 17:55:54.237: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-vjbwx": Phase="Pending", Reason="", readiness=false. Elapsed: 9.325372ms
Feb  4 17:55:56.243: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-vjbwx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015879593s
Feb  4 17:55:58.285: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-vjbwx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057806341s
STEP: Saw pod success
Feb  4 17:55:58.285: INFO: Pod "pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-vjbwx" satisfied condition "success or failure"
Feb  4 17:55:58.290: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-vjbwx container namespace-test: <nil>
STEP: delete the pod
Feb  4 17:55:58.381: INFO: Waiting for pod pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-vjbwx to disappear
Feb  4 17:55:58.395: INFO: Pod pod-service-account-188c60a1-28a6-11e9-9eab-8e57f341003b-vjbwx no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:55:58.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-dm2q5" for this suite.
Feb  4 17:56:06.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:56:06.718: INFO: namespace: e2e-tests-svcaccounts-dm2q5, resource: bindings, ignored listing per whitelist
Feb  4 17:56:06.818: INFO: namespace e2e-tests-svcaccounts-dm2q5 deletion completed in 8.414083725s

• [SLOW TEST:22.324 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:56:06.827: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb  4 17:56:07.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 cluster-info'
Feb  4 17:56:08.081: INFO: stderr: ""
Feb  4 17:56:08.081: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:56:08.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pqtns" for this suite.
Feb  4 17:56:14.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:56:14.514: INFO: namespace: e2e-tests-kubectl-pqtns, resource: bindings, ignored listing per whitelist
Feb  4 17:56:14.569: INFO: namespace e2e-tests-kubectl-pqtns deletion completed in 6.477628075s

• [SLOW TEST:7.743 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:56:14.571: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-2a4fb24c-28a6-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 17:56:15.130: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2a50f937-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-jd7ct" to be "success or failure"
Feb  4 17:56:15.171: INFO: Pod "pod-projected-configmaps-2a50f937-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 40.804297ms
Feb  4 17:56:17.178: INFO: Pod "pod-projected-configmaps-2a50f937-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048114386s
Feb  4 17:56:19.192: INFO: Pod "pod-projected-configmaps-2a50f937-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061440989s
STEP: Saw pod success
Feb  4 17:56:19.192: INFO: Pod "pod-projected-configmaps-2a50f937-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:56:19.198: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-projected-configmaps-2a50f937-28a6-11e9-9eab-8e57f341003b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 17:56:19.400: INFO: Waiting for pod pod-projected-configmaps-2a50f937-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 17:56:19.405: INFO: Pod pod-projected-configmaps-2a50f937-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:56:19.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jd7ct" for this suite.
Feb  4 17:56:27.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:56:27.566: INFO: namespace: e2e-tests-projected-jd7ct, resource: bindings, ignored listing per whitelist
Feb  4 17:56:28.061: INFO: namespace e2e-tests-projected-jd7ct deletion completed in 8.649572669s

• [SLOW TEST:13.491 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:56:28.065: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-322b1b4f-28a6-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 17:56:28.315: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-322ce92c-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-7r52m" to be "success or failure"
Feb  4 17:56:28.329: INFO: Pod "pod-projected-secrets-322ce92c-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.574351ms
Feb  4 17:56:30.368: INFO: Pod "pod-projected-secrets-322ce92c-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052878234s
Feb  4 17:56:32.397: INFO: Pod "pod-projected-secrets-322ce92c-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081692868s
STEP: Saw pod success
Feb  4 17:56:32.397: INFO: Pod "pod-projected-secrets-322ce92c-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:56:32.402: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-projected-secrets-322ce92c-28a6-11e9-9eab-8e57f341003b container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  4 17:56:32.574: INFO: Waiting for pod pod-projected-secrets-322ce92c-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 17:56:32.582: INFO: Pod pod-projected-secrets-322ce92c-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:56:32.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7r52m" for this suite.
Feb  4 17:56:38.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:56:38.862: INFO: namespace: e2e-tests-projected-7r52m, resource: bindings, ignored listing per whitelist
Feb  4 17:56:38.982: INFO: namespace e2e-tests-projected-7r52m deletion completed in 6.390602285s

• [SLOW TEST:10.917 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:56:38.989: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb  4 17:56:39.261: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-a,UID:38b6e51d-28a6-11e9-9959-0a580af41676,ResourceVersion:25109,Generation:0,CreationTimestamp:2019-02-04 17:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  4 17:56:39.261: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-a,UID:38b6e51d-28a6-11e9-9959-0a580af41676,ResourceVersion:25109,Generation:0,CreationTimestamp:2019-02-04 17:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb  4 17:56:49.291: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-a,UID:38b6e51d-28a6-11e9-9959-0a580af41676,ResourceVersion:25135,Generation:0,CreationTimestamp:2019-02-04 17:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb  4 17:56:49.291: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-a,UID:38b6e51d-28a6-11e9-9959-0a580af41676,ResourceVersion:25135,Generation:0,CreationTimestamp:2019-02-04 17:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb  4 17:56:59.322: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-a,UID:38b6e51d-28a6-11e9-9959-0a580af41676,ResourceVersion:25161,Generation:0,CreationTimestamp:2019-02-04 17:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  4 17:56:59.322: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-a,UID:38b6e51d-28a6-11e9-9959-0a580af41676,ResourceVersion:25161,Generation:0,CreationTimestamp:2019-02-04 17:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb  4 17:57:09.359: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-a,UID:38b6e51d-28a6-11e9-9959-0a580af41676,ResourceVersion:25186,Generation:0,CreationTimestamp:2019-02-04 17:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  4 17:57:09.359: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-a,UID:38b6e51d-28a6-11e9-9959-0a580af41676,ResourceVersion:25186,Generation:0,CreationTimestamp:2019-02-04 17:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb  4 17:57:19.388: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-b,UID:50a15148-28a6-11e9-9959-0a580af41676,ResourceVersion:25213,Generation:0,CreationTimestamp:2019-02-04 17:57:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  4 17:57:19.388: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-b,UID:50a15148-28a6-11e9-9959-0a580af41676,ResourceVersion:25213,Generation:0,CreationTimestamp:2019-02-04 17:57:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb  4 17:57:29.428: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-b,UID:50a15148-28a6-11e9-9959-0a580af41676,ResourceVersion:25239,Generation:0,CreationTimestamp:2019-02-04 17:57:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  4 17:57:29.428: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lzrwx,SelfLink:/api/v1/namespaces/e2e-tests-watch-lzrwx/configmaps/e2e-watch-test-configmap-b,UID:50a15148-28a6-11e9-9959-0a580af41676,ResourceVersion:25239,Generation:0,CreationTimestamp:2019-02-04 17:57:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:57:39.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-lzrwx" for this suite.
Feb  4 17:57:45.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:57:45.618: INFO: namespace: e2e-tests-watch-lzrwx, resource: bindings, ignored listing per whitelist
Feb  4 17:57:45.850: INFO: namespace e2e-tests-watch-lzrwx deletion completed in 6.385401399s

• [SLOW TEST:66.862 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:57:45.854: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wd4nf A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-wd4nf;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wd4nf A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-wd4nf;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wd4nf.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-wd4nf.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wd4nf.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-wd4nf.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wd4nf.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wd4nf.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wd4nf.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wd4nf.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wd4nf.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-wd4nf.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wd4nf.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-wd4nf.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-wd4nf.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 238.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.238_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wd4nf A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-wd4nf;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wd4nf A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-wd4nf;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wd4nf.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-wd4nf.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wd4nf.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-wd4nf.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wd4nf.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wd4nf.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wd4nf.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wd4nf.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wd4nf.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-wd4nf.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wd4nf.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-wd4nf.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-wd4nf.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 238.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.238_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  4 17:58:38.505: INFO: DNS probes using e2e-tests-dns-wd4nf/dns-test-6088a434-28a6-11e9-9eab-8e57f341003b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:58:38.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-wd4nf" for this suite.
Feb  4 17:58:44.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:58:45.222: INFO: namespace: e2e-tests-dns-wd4nf, resource: bindings, ignored listing per whitelist
Feb  4 17:58:45.262: INFO: namespace e2e-tests-dns-wd4nf deletion completed in 6.442095076s

• [SLOW TEST:59.408 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:58:45.263: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-83ecf620-28a6-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 17:58:45.479: INFO: Waiting up to 5m0s for pod "pod-configmaps-83ee3019-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-configmap-znlcq" to be "success or failure"
Feb  4 17:58:45.487: INFO: Pod "pod-configmaps-83ee3019-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.428607ms
Feb  4 17:58:47.507: INFO: Pod "pod-configmaps-83ee3019-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027453372s
STEP: Saw pod success
Feb  4 17:58:47.507: INFO: Pod "pod-configmaps-83ee3019-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:58:47.514: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-configmaps-83ee3019-28a6-11e9-9eab-8e57f341003b container configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 17:58:47.708: INFO: Waiting for pod pod-configmaps-83ee3019-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 17:58:47.715: INFO: Pod pod-configmaps-83ee3019-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:58:47.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-znlcq" for this suite.
Feb  4 17:58:53.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:58:53.816: INFO: namespace: e2e-tests-configmap-znlcq, resource: bindings, ignored listing per whitelist
Feb  4 17:58:54.073: INFO: namespace e2e-tests-configmap-znlcq deletion completed in 6.348768999s

• [SLOW TEST:8.810 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:58:54.079: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 17:58:54.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 version'
Feb  4 17:58:54.521: INFO: stderr: ""
Feb  4 17:58:54.521: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.2\", GitCommit:\"cff46ab41ff0bb44d8584413b598ad8360ec1def\", GitTreeState:\"clean\", BuildDate:\"2019-01-10T23:28:14Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:58:54.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l9hcv" for this suite.
Feb  4 17:59:00.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:59:00.849: INFO: namespace: e2e-tests-kubectl-l9hcv, resource: bindings, ignored listing per whitelist
Feb  4 17:59:01.021: INFO: namespace e2e-tests-kubectl-l9hcv deletion completed in 6.463121746s

• [SLOW TEST:6.943 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:59:01.025: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-8d52f386-28a6-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 17:59:01.257: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d54250d-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-configmap-7szlg" to be "success or failure"
Feb  4 17:59:01.266: INFO: Pod "pod-configmaps-8d54250d-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.80153ms
Feb  4 17:59:03.293: INFO: Pod "pod-configmaps-8d54250d-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035881598s
STEP: Saw pod success
Feb  4 17:59:03.293: INFO: Pod "pod-configmaps-8d54250d-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:59:03.300: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-configmaps-8d54250d-28a6-11e9-9eab-8e57f341003b container configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 17:59:03.507: INFO: Waiting for pod pod-configmaps-8d54250d-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 17:59:03.513: INFO: Pod pod-configmaps-8d54250d-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:59:03.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7szlg" for this suite.
Feb  4 17:59:09.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:59:09.653: INFO: namespace: e2e-tests-configmap-7szlg, resource: bindings, ignored listing per whitelist
Feb  4 17:59:09.902: INFO: namespace e2e-tests-configmap-7szlg deletion completed in 6.373046058s

• [SLOW TEST:8.878 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:59:09.907: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 17:59:11.308: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92a8b87b-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-46kpl" to be "success or failure"
Feb  4 17:59:11.384: INFO: Pod "downwardapi-volume-92a8b87b-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 75.931278ms
Feb  4 17:59:13.464: INFO: Pod "downwardapi-volume-92a8b87b-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.156309396s
Feb  4 17:59:15.470: INFO: Pod "downwardapi-volume-92a8b87b-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.161965063s
STEP: Saw pod success
Feb  4 17:59:15.470: INFO: Pod "downwardapi-volume-92a8b87b-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 17:59:15.475: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod downwardapi-volume-92a8b87b-28a6-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 17:59:15.629: INFO: Waiting for pod downwardapi-volume-92a8b87b-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 17:59:15.640: INFO: Pod downwardapi-volume-92a8b87b-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:59:15.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-46kpl" for this suite.
Feb  4 17:59:21.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:59:22.075: INFO: namespace: e2e-tests-projected-46kpl, resource: bindings, ignored listing per whitelist
Feb  4 17:59:22.109: INFO: namespace e2e-tests-projected-46kpl deletion completed in 6.463565329s

• [SLOW TEST:12.202 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:59:22.110: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0204 17:59:52.937100      13 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  4 17:59:52.937: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 17:59:52.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-2xk6j" for this suite.
Feb  4 17:59:58.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 17:59:59.344: INFO: namespace: e2e-tests-gc-2xk6j, resource: bindings, ignored listing per whitelist
Feb  4 17:59:59.344: INFO: namespace e2e-tests-gc-2xk6j deletion completed in 6.400106164s

• [SLOW TEST:37.234 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 17:59:59.345: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-b01d145e-28a6-11e9-9eab-8e57f341003b
STEP: Creating secret with name s-test-opt-upd-b01d14b4-28a6-11e9-9eab-8e57f341003b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b01d145e-28a6-11e9-9eab-8e57f341003b
STEP: Updating secret s-test-opt-upd-b01d14b4-28a6-11e9-9eab-8e57f341003b
STEP: Creating secret with name s-test-opt-create-b01d14ec-28a6-11e9-9eab-8e57f341003b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:00:08.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-j98xp" for this suite.
Feb  4 18:00:32.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:00:32.451: INFO: namespace: e2e-tests-secrets-j98xp, resource: bindings, ignored listing per whitelist
Feb  4 18:00:32.726: INFO: namespace e2e-tests-secrets-j98xp deletion completed in 24.577962451s

• [SLOW TEST:33.382 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:00:32.727: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  4 18:00:32.934: INFO: Waiting up to 5m0s for pod "downward-api-c3fa97e3-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-5n8n4" to be "success or failure"
Feb  4 18:00:32.939: INFO: Pod "downward-api-c3fa97e3-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.059895ms
Feb  4 18:00:35.035: INFO: Pod "downward-api-c3fa97e3-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100824558s
Feb  4 18:00:37.052: INFO: Pod "downward-api-c3fa97e3-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.117748439s
STEP: Saw pod success
Feb  4 18:00:37.052: INFO: Pod "downward-api-c3fa97e3-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:00:37.057: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downward-api-c3fa97e3-28a6-11e9-9eab-8e57f341003b container dapi-container: <nil>
STEP: delete the pod
Feb  4 18:00:37.200: INFO: Waiting for pod downward-api-c3fa97e3-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 18:00:37.212: INFO: Pod downward-api-c3fa97e3-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:00:37.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5n8n4" for this suite.
Feb  4 18:00:43.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:00:43.335: INFO: namespace: e2e-tests-downward-api-5n8n4, resource: bindings, ignored listing per whitelist
Feb  4 18:00:43.548: INFO: namespace e2e-tests-downward-api-5n8n4 deletion completed in 6.328947376s

• [SLOW TEST:10.821 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:00:43.552: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  4 18:00:43.773: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:00:48.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-2xj7j" for this suite.
Feb  4 18:00:56.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:00:56.504: INFO: namespace: e2e-tests-init-container-2xj7j, resource: bindings, ignored listing per whitelist
Feb  4 18:00:56.524: INFO: namespace e2e-tests-init-container-2xj7j deletion completed in 8.410420025s

• [SLOW TEST:12.973 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:00:56.525: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 18:00:56.866: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d239c104-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-downward-api-74cwt" to be "success or failure"
Feb  4 18:00:56.882: INFO: Pod "downwardapi-volume-d239c104-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.676951ms
Feb  4 18:00:58.896: INFO: Pod "downwardapi-volume-d239c104-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030342536s
Feb  4 18:01:00.902: INFO: Pod "downwardapi-volume-d239c104-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036488391s
STEP: Saw pod success
Feb  4 18:01:00.902: INFO: Pod "downwardapi-volume-d239c104-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:01:00.909: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downwardapi-volume-d239c104-28a6-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 18:01:01.046: INFO: Waiting for pod downwardapi-volume-d239c104-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 18:01:01.054: INFO: Pod downwardapi-volume-d239c104-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:01:01.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-74cwt" for this suite.
Feb  4 18:01:07.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:01:07.382: INFO: namespace: e2e-tests-downward-api-74cwt, resource: bindings, ignored listing per whitelist
Feb  4 18:01:07.493: INFO: namespace e2e-tests-downward-api-74cwt deletion completed in 6.43294223s

• [SLOW TEST:10.968 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:01:07.493: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb  4 18:01:07.828: INFO: Waiting up to 5m0s for pod "var-expansion-d8c6ab53-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-var-expansion-v8n2j" to be "success or failure"
Feb  4 18:01:07.836: INFO: Pod "var-expansion-d8c6ab53-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.311926ms
Feb  4 18:01:09.851: INFO: Pod "var-expansion-d8c6ab53-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022458075s
Feb  4 18:01:11.863: INFO: Pod "var-expansion-d8c6ab53-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034775204s
STEP: Saw pod success
Feb  4 18:01:11.863: INFO: Pod "var-expansion-d8c6ab53-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:01:11.869: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod var-expansion-d8c6ab53-28a6-11e9-9eab-8e57f341003b container dapi-container: <nil>
STEP: delete the pod
Feb  4 18:01:12.049: INFO: Waiting for pod var-expansion-d8c6ab53-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 18:01:12.054: INFO: Pod var-expansion-d8c6ab53-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:01:12.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-v8n2j" for this suite.
Feb  4 18:01:18.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:01:18.424: INFO: namespace: e2e-tests-var-expansion-v8n2j, resource: bindings, ignored listing per whitelist
Feb  4 18:01:18.527: INFO: namespace e2e-tests-var-expansion-v8n2j deletion completed in 6.46340479s

• [SLOW TEST:11.034 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:01:18.528: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-df48f3ee-28a6-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 18:01:18.797: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-df4ddd82-28a6-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-bx5cs" to be "success or failure"
Feb  4 18:01:18.820: INFO: Pod "pod-projected-secrets-df4ddd82-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.574741ms
Feb  4 18:01:20.844: INFO: Pod "pod-projected-secrets-df4ddd82-28a6-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046893879s
Feb  4 18:01:22.852: INFO: Pod "pod-projected-secrets-df4ddd82-28a6-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054954909s
STEP: Saw pod success
Feb  4 18:01:22.852: INFO: Pod "pod-projected-secrets-df4ddd82-28a6-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:01:22.863: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-projected-secrets-df4ddd82-28a6-11e9-9eab-8e57f341003b container secret-volume-test: <nil>
STEP: delete the pod
Feb  4 18:01:23.003: INFO: Waiting for pod pod-projected-secrets-df4ddd82-28a6-11e9-9eab-8e57f341003b to disappear
Feb  4 18:01:23.030: INFO: Pod pod-projected-secrets-df4ddd82-28a6-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:01:23.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bx5cs" for this suite.
Feb  4 18:01:31.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:01:31.599: INFO: namespace: e2e-tests-projected-bx5cs, resource: bindings, ignored listing per whitelist
Feb  4 18:01:31.627: INFO: namespace e2e-tests-projected-bx5cs deletion completed in 8.578245459s

• [SLOW TEST:13.100 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:01:31.628: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-sqss8
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-sqss8 to expose endpoints map[]
Feb  4 18:01:32.030: INFO: Get endpoints failed (11.900265ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb  4 18:01:33.041: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-sqss8 exposes endpoints map[] (1.022526549s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-sqss8
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-sqss8 to expose endpoints map[pod1:[100]]
Feb  4 18:01:36.115: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-sqss8 exposes endpoints map[pod1:[100]] (3.054168623s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-sqss8
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-sqss8 to expose endpoints map[pod1:[100] pod2:[101]]
Feb  4 18:01:40.296: INFO: Unexpected endpoints: found map[e7d4a572-28a6-11e9-9959-0a580af41676:[100]], expected map[pod1:[100] pod2:[101]] (4.172243669s elapsed, will retry)
Feb  4 18:01:41.339: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-sqss8 exposes endpoints map[pod1:[100] pod2:[101]] (5.215246457s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-sqss8
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-sqss8 to expose endpoints map[pod2:[101]]
Feb  4 18:01:42.388: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-sqss8 exposes endpoints map[pod2:[101]] (1.028161875s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-sqss8
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-sqss8 to expose endpoints map[]
Feb  4 18:01:42.435: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-sqss8 exposes endpoints map[] (6.568403ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:01:42.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-sqss8" for this suite.
Feb  4 18:02:06.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:02:07.007: INFO: namespace: e2e-tests-services-sqss8, resource: bindings, ignored listing per whitelist
Feb  4 18:02:07.007: INFO: namespace e2e-tests-services-sqss8 deletion completed in 24.51113152s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:35.380 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:02:07.011: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-kv796
I0204 18:02:07.275070      13 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-kv796, replica count: 1
I0204 18:02:08.325556      13 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0204 18:02:09.325848      13 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0204 18:02:10.326076      13 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  4 18:02:10.444: INFO: Created: latency-svc-tzwv2
Feb  4 18:02:10.625: INFO: Got endpoints: latency-svc-tzwv2 [199.556177ms]
Feb  4 18:02:10.653: INFO: Created: latency-svc-gd9nw
Feb  4 18:02:10.653: INFO: Created: latency-svc-2z9h7
Feb  4 18:02:10.659: INFO: Got endpoints: latency-svc-2z9h7 [32.857301ms]
Feb  4 18:02:10.674: INFO: Got endpoints: latency-svc-gd9nw [41.304523ms]
Feb  4 18:02:10.674: INFO: Created: latency-svc-84frx
Feb  4 18:02:10.684: INFO: Created: latency-svc-ddvzx
Feb  4 18:02:10.684: INFO: Got endpoints: latency-svc-84frx [43.248856ms]
Feb  4 18:02:10.689: INFO: Created: latency-svc-chvzt
Feb  4 18:02:10.712: INFO: Created: latency-svc-842mv
Feb  4 18:02:10.721: INFO: Got endpoints: latency-svc-842mv [94.654229ms]
Feb  4 18:02:10.722: INFO: Got endpoints: latency-svc-ddvzx [88.716222ms]
Feb  4 18:02:10.723: INFO: Got endpoints: latency-svc-chvzt [96.057089ms]
Feb  4 18:02:10.756: INFO: Created: latency-svc-mq8fj
Feb  4 18:02:10.775: INFO: Created: latency-svc-nfk84
Feb  4 18:02:10.811: INFO: Created: latency-svc-h5gv4
Feb  4 18:02:10.821: INFO: Created: latency-svc-mmzww
Feb  4 18:02:10.834: INFO: Created: latency-svc-sdnxk
Feb  4 18:02:10.851: INFO: Created: latency-svc-h5pbp
Feb  4 18:02:10.864: INFO: Created: latency-svc-g2zr7
Feb  4 18:02:10.879: INFO: Created: latency-svc-xt42l
Feb  4 18:02:10.891: INFO: Got endpoints: latency-svc-sdnxk [249.944441ms]
Feb  4 18:02:10.893: INFO: Got endpoints: latency-svc-mq8fj [259.324269ms]
Feb  4 18:02:10.898: INFO: Got endpoints: latency-svc-nfk84 [264.552577ms]
Feb  4 18:02:10.899: INFO: Got endpoints: latency-svc-h5gv4 [263.874226ms]
Feb  4 18:02:10.902: INFO: Got endpoints: latency-svc-mmzww [260.61234ms]
Feb  4 18:02:10.902: INFO: Got endpoints: latency-svc-xt42l [255.497163ms]
Feb  4 18:02:10.903: INFO: Got endpoints: latency-svc-h5pbp [261.281489ms]
Feb  4 18:02:10.903: INFO: Got endpoints: latency-svc-g2zr7 [261.238781ms]
Feb  4 18:02:10.912: INFO: Created: latency-svc-8kj5j
Feb  4 18:02:10.912: INFO: Got endpoints: latency-svc-8kj5j [265.003706ms]
Feb  4 18:02:10.912: INFO: Created: latency-svc-gfmjm
Feb  4 18:02:10.926: INFO: Created: latency-svc-c7mvg
Feb  4 18:02:10.960: INFO: Got endpoints: latency-svc-gfmjm [300.795051ms]
Feb  4 18:02:10.960: INFO: Created: latency-svc-lf5gf
Feb  4 18:02:10.966: INFO: Got endpoints: latency-svc-lf5gf [282.533616ms]
Feb  4 18:02:10.967: INFO: Got endpoints: latency-svc-c7mvg [293.013767ms]
Feb  4 18:02:10.975: INFO: Created: latency-svc-5pcj8
Feb  4 18:02:10.996: INFO: Got endpoints: latency-svc-5pcj8 [274.404805ms]
Feb  4 18:02:10.996: INFO: Created: latency-svc-n2t5b
Feb  4 18:02:11.010: INFO: Created: latency-svc-dwh48
Feb  4 18:02:11.060: INFO: Created: latency-svc-6c9wp
Feb  4 18:02:11.076: INFO: Created: latency-svc-kfxgp
Feb  4 18:02:11.098: INFO: Created: latency-svc-gqf28
Feb  4 18:02:11.119: INFO: Created: latency-svc-7szrq
Feb  4 18:02:11.155: INFO: Created: latency-svc-8g9g9
Feb  4 18:02:11.165: INFO: Created: latency-svc-tbrnc
Feb  4 18:02:11.189: INFO: Created: latency-svc-5tw2n
Feb  4 18:02:11.205: INFO: Created: latency-svc-sthf8
Feb  4 18:02:11.213: INFO: Got endpoints: latency-svc-dwh48 [489.445283ms]
Feb  4 18:02:11.213: INFO: Got endpoints: latency-svc-n2t5b [491.578298ms]
Feb  4 18:02:11.214: INFO: Got endpoints: latency-svc-5tw2n [311.292679ms]
Feb  4 18:02:11.214: INFO: Got endpoints: latency-svc-6c9wp [322.219986ms]
Feb  4 18:02:11.214: INFO: Got endpoints: latency-svc-kfxgp [321.487071ms]
Feb  4 18:02:11.215: INFO: Got endpoints: latency-svc-gqf28 [316.271031ms]
Feb  4 18:02:11.215: INFO: Got endpoints: latency-svc-7szrq [316.089225ms]
Feb  4 18:02:11.223: INFO: Got endpoints: latency-svc-8g9g9 [321.531576ms]
Feb  4 18:02:11.224: INFO: Got endpoints: latency-svc-tbrnc [321.528439ms]
Feb  4 18:02:11.224: INFO: Created: latency-svc-6bxcw
Feb  4 18:02:11.225: INFO: Got endpoints: latency-svc-sthf8 [313.347593ms]
Feb  4 18:02:11.260: INFO: Created: latency-svc-gcr6d
Feb  4 18:02:11.264: INFO: Got endpoints: latency-svc-6bxcw [352.498849ms]
Feb  4 18:02:11.269: INFO: Created: latency-svc-jnknt
Feb  4 18:02:11.355: INFO: Created: latency-svc-spc8k
Feb  4 18:02:11.469: INFO: Created: latency-svc-mwrph
Feb  4 18:02:11.483: INFO: Got endpoints: latency-svc-spc8k [516.520142ms]
Feb  4 18:02:11.487: INFO: Got endpoints: latency-svc-gcr6d [526.931471ms]
Feb  4 18:02:11.489: INFO: Got endpoints: latency-svc-jnknt [522.679149ms]
Feb  4 18:02:11.555: INFO: Created: latency-svc-wtq5l
Feb  4 18:02:11.556: INFO: Got endpoints: latency-svc-mwrph [560.427738ms]
Feb  4 18:02:11.564: INFO: Got endpoints: latency-svc-wtq5l [351.457742ms]
Feb  4 18:02:11.581: INFO: Created: latency-svc-447pj
Feb  4 18:02:11.581: INFO: Created: latency-svc-vnp7h
Feb  4 18:02:11.596: INFO: Created: latency-svc-z866j
Feb  4 18:02:11.609: INFO: Created: latency-svc-vxm9z
Feb  4 18:02:11.668: INFO: Created: latency-svc-27mgf
Feb  4 18:02:11.688: INFO: Created: latency-svc-w6spt
Feb  4 18:02:11.719: INFO: Created: latency-svc-hd8bw
Feb  4 18:02:11.757: INFO: Got endpoints: latency-svc-w6spt [541.901515ms]
Feb  4 18:02:11.757: INFO: Got endpoints: latency-svc-vnp7h [544.152521ms]
Feb  4 18:02:11.758: INFO: Got endpoints: latency-svc-447pj [543.835669ms]
Feb  4 18:02:11.758: INFO: Got endpoints: latency-svc-z866j [543.802448ms]
Feb  4 18:02:11.758: INFO: Got endpoints: latency-svc-vxm9z [543.884088ms]
Feb  4 18:02:11.759: INFO: Got endpoints: latency-svc-27mgf [543.847905ms]
Feb  4 18:02:11.759: INFO: Got endpoints: latency-svc-hd8bw [535.957773ms]
Feb  4 18:02:11.766: INFO: Created: latency-svc-4xvgg
Feb  4 18:02:11.779: INFO: Got endpoints: latency-svc-4xvgg [555.245777ms]
Feb  4 18:02:11.780: INFO: Created: latency-svc-jjhlg
Feb  4 18:02:11.786: INFO: Created: latency-svc-h6xcd
Feb  4 18:02:11.808: INFO: Got endpoints: latency-svc-h6xcd [543.631034ms]
Feb  4 18:02:11.811: INFO: Got endpoints: latency-svc-jjhlg [586.117037ms]
Feb  4 18:02:11.811: INFO: Created: latency-svc-b9trn
Feb  4 18:02:11.868: INFO: Created: latency-svc-fpcf8
Feb  4 18:02:11.883: INFO: Created: latency-svc-ttdhn
Feb  4 18:02:11.901: INFO: Created: latency-svc-qjcsb
Feb  4 18:02:11.923: INFO: Created: latency-svc-qcqwz
Feb  4 18:02:11.967: INFO: Created: latency-svc-nntld
Feb  4 18:02:11.991: INFO: Created: latency-svc-87mds
Feb  4 18:02:11.996: INFO: Got endpoints: latency-svc-nntld [238.480145ms]
Feb  4 18:02:11.996: INFO: Got endpoints: latency-svc-b9trn [509.733911ms]
Feb  4 18:02:11.996: INFO: Got endpoints: latency-svc-fpcf8 [509.52105ms]
Feb  4 18:02:11.997: INFO: Got endpoints: latency-svc-ttdhn [507.411816ms]
Feb  4 18:02:11.997: INFO: Got endpoints: latency-svc-qjcsb [440.754544ms]
Feb  4 18:02:11.997: INFO: Got endpoints: latency-svc-qcqwz [432.752558ms]
Feb  4 18:02:12.005: INFO: Got endpoints: latency-svc-87mds [247.553888ms]
Feb  4 18:02:12.020: INFO: Created: latency-svc-gr974
Feb  4 18:02:12.035: INFO: Got endpoints: latency-svc-gr974 [277.694591ms]
Feb  4 18:02:12.036: INFO: Created: latency-svc-gdwql
Feb  4 18:02:12.063: INFO: Got endpoints: latency-svc-gdwql [305.081742ms]
Feb  4 18:02:12.067: INFO: Created: latency-svc-5bh7r
Feb  4 18:02:12.075: INFO: Created: latency-svc-7f9gb
Feb  4 18:02:12.075: INFO: Got endpoints: latency-svc-5bh7r [316.285113ms]
Feb  4 18:02:12.095: INFO: Created: latency-svc-llqcv
Feb  4 18:02:12.112: INFO: Created: latency-svc-fspn9
Feb  4 18:02:12.119: INFO: Created: latency-svc-ntnn5
Feb  4 18:02:12.169: INFO: Created: latency-svc-b29nw
Feb  4 18:02:12.197: INFO: Created: latency-svc-srnzm
Feb  4 18:02:12.215: INFO: Created: latency-svc-df5m9
Feb  4 18:02:12.228: INFO: Created: latency-svc-9vnkv
Feb  4 18:02:12.241: INFO: Created: latency-svc-tcjj5
Feb  4 18:02:12.279: INFO: Created: latency-svc-7rtjv
Feb  4 18:02:12.293: INFO: Created: latency-svc-8xqht
Feb  4 18:02:12.307: INFO: Created: latency-svc-q8wq7
Feb  4 18:02:12.315: INFO: Created: latency-svc-xg496
Feb  4 18:02:12.321: INFO: Created: latency-svc-zzms9
Feb  4 18:02:12.334: INFO: Created: latency-svc-2v5l5
Feb  4 18:02:12.342: INFO: Got endpoints: latency-svc-df5m9 [346.121816ms]
Feb  4 18:02:12.343: INFO: Got endpoints: latency-svc-7f9gb [583.92974ms]
Feb  4 18:02:12.343: INFO: Got endpoints: latency-svc-llqcv [584.006728ms]
Feb  4 18:02:12.349: INFO: Got endpoints: latency-svc-fspn9 [570.339847ms]
Feb  4 18:02:12.350: INFO: Got endpoints: latency-svc-ntnn5 [541.687456ms]
Feb  4 18:02:12.350: INFO: Got endpoints: latency-svc-b29nw [539.493244ms]
Feb  4 18:02:12.351: INFO: Got endpoints: latency-svc-srnzm [355.205193ms]
Feb  4 18:02:12.353: INFO: Got endpoints: latency-svc-tcjj5 [356.241489ms]
Feb  4 18:02:12.361: INFO: Got endpoints: latency-svc-9vnkv [364.482632ms]
Feb  4 18:02:12.367: INFO: Created: latency-svc-qx9fw
Feb  4 18:02:12.404: INFO: Created: latency-svc-bg47s
Feb  4 18:02:12.420: INFO: Created: latency-svc-7lbb5
Feb  4 18:02:12.420: INFO: Got endpoints: latency-svc-7rtjv [422.55412ms]
Feb  4 18:02:12.420: INFO: Created: latency-svc-sxfxn
Feb  4 18:02:12.426: INFO: Created: latency-svc-8czqq
Feb  4 18:02:12.497: INFO: Created: latency-svc-7tprn
Feb  4 18:02:12.518: INFO: Created: latency-svc-v8zrd
Feb  4 18:02:12.547: INFO: Created: latency-svc-5lkv9
Feb  4 18:02:12.572: INFO: Created: latency-svc-lm2w7
Feb  4 18:02:12.579: INFO: Got endpoints: latency-svc-xg496 [543.967659ms]
Feb  4 18:02:12.580: INFO: Got endpoints: latency-svc-8xqht [583.060249ms]
Feb  4 18:02:12.581: INFO: Got endpoints: latency-svc-q8wq7 [575.42249ms]
Feb  4 18:02:12.599: INFO: Created: latency-svc-xxjmb
Feb  4 18:02:12.623: INFO: Got endpoints: latency-svc-zzms9 [553.25862ms]
Feb  4 18:02:12.627: INFO: Created: latency-svc-9mc8s
Feb  4 18:02:12.634: INFO: Created: latency-svc-pvdn9
Feb  4 18:02:12.650: INFO: Created: latency-svc-mtbrn
Feb  4 18:02:12.664: INFO: Created: latency-svc-g9jhd
Feb  4 18:02:12.833: INFO: Got endpoints: latency-svc-7lbb5 [489.358456ms]
Feb  4 18:02:12.833: INFO: Got endpoints: latency-svc-2v5l5 [758.389224ms]
Feb  4 18:02:12.834: INFO: Got endpoints: latency-svc-qx9fw [491.47241ms]
Feb  4 18:02:12.834: INFO: Got endpoints: latency-svc-bg47s [491.618257ms]
Feb  4 18:02:12.853: INFO: Got endpoints: latency-svc-sxfxn [503.032284ms]
Feb  4 18:02:12.853: INFO: Created: latency-svc-8mr4b
Feb  4 18:02:12.856: INFO: Created: latency-svc-ljv2h
Feb  4 18:02:12.875: INFO: Created: latency-svc-mkvcx
Feb  4 18:02:12.895: INFO: Created: latency-svc-tkt8g
Feb  4 18:02:12.900: INFO: Got endpoints: latency-svc-8czqq [549.879444ms]
Feb  4 18:02:12.900: INFO: Created: latency-svc-t9wd8
Feb  4 18:02:12.925: INFO: Created: latency-svc-rpw5d
Feb  4 18:02:13.079: INFO: Got endpoints: latency-svc-5lkv9 [718.658196ms]
Feb  4 18:02:13.080: INFO: Got endpoints: latency-svc-7tprn [729.698938ms]
Feb  4 18:02:13.081: INFO: Got endpoints: latency-svc-v8zrd [729.66373ms]
Feb  4 18:02:13.097: INFO: Created: latency-svc-hsgqp
Feb  4 18:02:13.114: INFO: Created: latency-svc-qt9ks
Feb  4 18:02:13.115: INFO: Got endpoints: latency-svc-lm2w7 [754.182877ms]
Feb  4 18:02:13.124: INFO: Created: latency-svc-77lzz
Feb  4 18:02:13.134: INFO: Created: latency-svc-sdpwx
Feb  4 18:02:13.153: INFO: Got endpoints: latency-svc-xxjmb [732.73972ms]
Feb  4 18:02:13.172: INFO: Created: latency-svc-pb9fs
Feb  4 18:02:13.322: INFO: Got endpoints: latency-svc-mtbrn [741.733598ms]
Feb  4 18:02:13.323: INFO: Got endpoints: latency-svc-9mc8s [743.317209ms]
Feb  4 18:02:13.325: INFO: Got endpoints: latency-svc-pvdn9 [744.575923ms]
Feb  4 18:02:13.349: INFO: Created: latency-svc-cf4mb
Feb  4 18:02:13.356: INFO: Got endpoints: latency-svc-g9jhd [733.27129ms]
Feb  4 18:02:13.357: INFO: Created: latency-svc-f2rq6
Feb  4 18:02:13.375: INFO: Created: latency-svc-tkbbm
Feb  4 18:02:13.375: INFO: Created: latency-svc-tz79f
Feb  4 18:02:13.415: INFO: Got endpoints: latency-svc-8mr4b [582.79885ms]
Feb  4 18:02:13.433: INFO: Created: latency-svc-nmjjj
Feb  4 18:02:13.625: INFO: Got endpoints: latency-svc-t9wd8 [772.062771ms]
Feb  4 18:02:13.625: INFO: Got endpoints: latency-svc-ljv2h [792.096868ms]
Feb  4 18:02:13.626: INFO: Got endpoints: latency-svc-mkvcx [791.981829ms]
Feb  4 18:02:13.626: INFO: Got endpoints: latency-svc-tkt8g [791.825008ms]
Feb  4 18:02:13.655: INFO: Created: latency-svc-7hssr
Feb  4 18:02:13.655: INFO: Got endpoints: latency-svc-rpw5d [754.975575ms]
Feb  4 18:02:13.655: INFO: Created: latency-svc-q9bk7
Feb  4 18:02:13.667: INFO: Created: latency-svc-kb7cr
Feb  4 18:02:13.676: INFO: Created: latency-svc-vmc48
Feb  4 18:02:13.690: INFO: Created: latency-svc-prcb9
Feb  4 18:02:13.703: INFO: Got endpoints: latency-svc-hsgqp [622.991792ms]
Feb  4 18:02:13.758: INFO: Created: latency-svc-vtfbg
Feb  4 18:02:13.869: INFO: Got endpoints: latency-svc-sdpwx [754.291412ms]
Feb  4 18:02:13.869: INFO: Got endpoints: latency-svc-qt9ks [789.616576ms]
Feb  4 18:02:13.869: INFO: Got endpoints: latency-svc-77lzz [788.632498ms]
Feb  4 18:02:13.910: INFO: Created: latency-svc-vmwjl
Feb  4 18:02:13.913: INFO: Got endpoints: latency-svc-pb9fs [760.7154ms]
Feb  4 18:02:13.920: INFO: Created: latency-svc-g8tn9
Feb  4 18:02:13.937: INFO: Created: latency-svc-jztgg
Feb  4 18:02:13.945: INFO: Created: latency-svc-jgwf2
Feb  4 18:02:13.958: INFO: Got endpoints: latency-svc-cf4mb [634.736547ms]
Feb  4 18:02:13.977: INFO: Created: latency-svc-846cs
Feb  4 18:02:14.176: INFO: Got endpoints: latency-svc-nmjjj [760.158866ms]
Feb  4 18:02:14.180: INFO: Got endpoints: latency-svc-f2rq6 [857.962998ms]
Feb  4 18:02:14.181: INFO: Got endpoints: latency-svc-tz79f [856.115761ms]
Feb  4 18:02:14.181: INFO: Got endpoints: latency-svc-tkbbm [824.973937ms]
Feb  4 18:02:14.199: INFO: Created: latency-svc-4x2b9
Feb  4 18:02:14.218: INFO: Created: latency-svc-j2k9d
Feb  4 18:02:14.224: INFO: Got endpoints: latency-svc-7hssr [598.957414ms]
Feb  4 18:02:14.252: INFO: Created: latency-svc-fvf8l
Feb  4 18:02:14.260: INFO: Created: latency-svc-pxh98
Feb  4 18:02:14.268: INFO: Got endpoints: latency-svc-q9bk7 [642.488416ms]
Feb  4 18:02:14.273: INFO: Created: latency-svc-7nrfm
Feb  4 18:02:14.292: INFO: Created: latency-svc-bt28h
Feb  4 18:02:14.394: INFO: Got endpoints: latency-svc-vmc48 [767.556955ms]
Feb  4 18:02:14.397: INFO: Got endpoints: latency-svc-kb7cr [771.468224ms]
Feb  4 18:02:14.406: INFO: Got endpoints: latency-svc-prcb9 [751.106858ms]
Feb  4 18:02:14.413: INFO: Created: latency-svc-j8dgq
Feb  4 18:02:14.428: INFO: Created: latency-svc-tj4tk
Feb  4 18:02:14.436: INFO: Created: latency-svc-82d67
Feb  4 18:02:14.466: INFO: Got endpoints: latency-svc-vtfbg [762.489551ms]
Feb  4 18:02:14.514: INFO: Created: latency-svc-4pzng
Feb  4 18:02:14.684: INFO: Got endpoints: latency-svc-jgwf2 [770.707921ms]
Feb  4 18:02:14.685: INFO: Got endpoints: latency-svc-vmwjl [815.393568ms]
Feb  4 18:02:14.685: INFO: Got endpoints: latency-svc-g8tn9 [811.979288ms]
Feb  4 18:02:14.686: INFO: Got endpoints: latency-svc-jztgg [811.902922ms]
Feb  4 18:02:14.715: INFO: Got endpoints: latency-svc-846cs [757.023195ms]
Feb  4 18:02:14.715: INFO: Created: latency-svc-c744p
Feb  4 18:02:14.719: INFO: Created: latency-svc-5wndz
Feb  4 18:02:14.743: INFO: Created: latency-svc-qs7z9
Feb  4 18:02:14.750: INFO: Created: latency-svc-pq8km
Feb  4 18:02:14.764: INFO: Got endpoints: latency-svc-4x2b9 [587.955844ms]
Feb  4 18:02:14.774: INFO: Created: latency-svc-m94mt
Feb  4 18:02:14.820: INFO: Created: latency-svc-njfdh
Feb  4 18:02:14.907: INFO: Got endpoints: latency-svc-pxh98 [726.108151ms]
Feb  4 18:02:14.912: INFO: Got endpoints: latency-svc-j2k9d [731.318881ms]
Feb  4 18:02:14.912: INFO: Got endpoints: latency-svc-fvf8l [731.314053ms]
Feb  4 18:02:14.932: INFO: Created: latency-svc-v7cnt
Feb  4 18:02:14.938: INFO: Created: latency-svc-psvfm
Feb  4 18:02:14.946: INFO: Got endpoints: latency-svc-7nrfm [722.177789ms]
Feb  4 18:02:14.947: INFO: Created: latency-svc-nlh6x
Feb  4 18:02:14.966: INFO: Created: latency-svc-jbddp
Feb  4 18:02:15.003: INFO: Got endpoints: latency-svc-bt28h [735.409445ms]
Feb  4 18:02:15.030: INFO: Created: latency-svc-h9d8s
Feb  4 18:02:15.168: INFO: Got endpoints: latency-svc-82d67 [761.446175ms]
Feb  4 18:02:15.172: INFO: Got endpoints: latency-svc-j8dgq [777.513876ms]
Feb  4 18:02:15.172: INFO: Got endpoints: latency-svc-tj4tk [774.790123ms]
Feb  4 18:02:15.204: INFO: Created: latency-svc-m9m97
Feb  4 18:02:15.207: INFO: Got endpoints: latency-svc-4pzng [741.526204ms]
Feb  4 18:02:15.219: INFO: Created: latency-svc-g8pls
Feb  4 18:02:15.251: INFO: Created: latency-svc-9b78b
Feb  4 18:02:15.255: INFO: Got endpoints: latency-svc-c744p [571.115689ms]
Feb  4 18:02:15.269: INFO: Created: latency-svc-mfgx7
Feb  4 18:02:15.292: INFO: Created: latency-svc-tjbjb
Feb  4 18:02:15.411: INFO: Got endpoints: latency-svc-pq8km [726.755253ms]
Feb  4 18:02:15.412: INFO: Got endpoints: latency-svc-5wndz [726.844004ms]
Feb  4 18:02:15.413: INFO: Got endpoints: latency-svc-qs7z9 [726.942887ms]
Feb  4 18:02:15.428: INFO: Created: latency-svc-5khww
Feb  4 18:02:15.437: INFO: Created: latency-svc-mjpg8
Feb  4 18:02:15.444: INFO: Created: latency-svc-7p288
Feb  4 18:02:15.457: INFO: Got endpoints: latency-svc-m94mt [742.35948ms]
Feb  4 18:02:15.475: INFO: Created: latency-svc-5jg55
Feb  4 18:02:15.664: INFO: Got endpoints: latency-svc-njfdh [896.951266ms]
Feb  4 18:02:15.668: INFO: Got endpoints: latency-svc-psvfm [756.323724ms]
Feb  4 18:02:15.673: INFO: Got endpoints: latency-svc-v7cnt [764.803174ms]
Feb  4 18:02:15.685: INFO: Got endpoints: latency-svc-nlh6x [772.439259ms]
Feb  4 18:02:15.694: INFO: Created: latency-svc-j67kx
Feb  4 18:02:15.711: INFO: Got endpoints: latency-svc-jbddp [765.424548ms]
Feb  4 18:02:15.712: INFO: Created: latency-svc-9gxsr
Feb  4 18:02:15.715: INFO: Created: latency-svc-wmb6d
Feb  4 18:02:15.746: INFO: Created: latency-svc-9mxng
Feb  4 18:02:15.760: INFO: Created: latency-svc-8pg5w
Feb  4 18:02:15.762: INFO: Got endpoints: latency-svc-h9d8s [758.36472ms]
Feb  4 18:02:15.785: INFO: Created: latency-svc-qfscm
Feb  4 18:02:15.872: INFO: Got endpoints: latency-svc-m9m97 [703.758257ms]
Feb  4 18:02:15.876: INFO: Got endpoints: latency-svc-g8pls [704.094461ms]
Feb  4 18:02:15.898: INFO: Created: latency-svc-zcqkh
Feb  4 18:02:15.907: INFO: Got endpoints: latency-svc-9b78b [735.172788ms]
Feb  4 18:02:15.923: INFO: Created: latency-svc-6gqnp
Feb  4 18:02:15.935: INFO: Created: latency-svc-qlw7w
Feb  4 18:02:15.958: INFO: Got endpoints: latency-svc-mfgx7 [749.939783ms]
Feb  4 18:02:16.002: INFO: Created: latency-svc-8skts
Feb  4 18:02:16.083: INFO: Got endpoints: latency-svc-5khww [671.846708ms]
Feb  4 18:02:16.084: INFO: Got endpoints: latency-svc-tjbjb [828.747922ms]
Feb  4 18:02:16.102: INFO: Created: latency-svc-pt28w
Feb  4 18:02:16.103: INFO: Got endpoints: latency-svc-mjpg8 [691.266773ms]
Feb  4 18:02:16.111: INFO: Created: latency-svc-v6bw8
Feb  4 18:02:16.122: INFO: Created: latency-svc-pq9s4
Feb  4 18:02:16.153: INFO: Got endpoints: latency-svc-7p288 [740.23648ms]
Feb  4 18:02:16.173: INFO: Created: latency-svc-q86l8
Feb  4 18:02:16.212: INFO: Got endpoints: latency-svc-5jg55 [754.257706ms]
Feb  4 18:02:16.233: INFO: Created: latency-svc-x7vlt
Feb  4 18:02:16.250: INFO: Got endpoints: latency-svc-j67kx [582.579539ms]
Feb  4 18:02:16.263: INFO: Created: latency-svc-swvpt
Feb  4 18:02:16.298: INFO: Got endpoints: latency-svc-9gxsr [630.268818ms]
Feb  4 18:02:16.319: INFO: Created: latency-svc-psbgg
Feb  4 18:02:16.358: INFO: Got endpoints: latency-svc-wmb6d [685.490985ms]
Feb  4 18:02:16.377: INFO: Created: latency-svc-p9jfx
Feb  4 18:02:16.399: INFO: Got endpoints: latency-svc-9mxng [714.704158ms]
Feb  4 18:02:16.419: INFO: Created: latency-svc-ll2cw
Feb  4 18:02:16.473: INFO: Got endpoints: latency-svc-8pg5w [761.730228ms]
Feb  4 18:02:16.508: INFO: Created: latency-svc-ljm2p
Feb  4 18:02:16.514: INFO: Got endpoints: latency-svc-qfscm [751.673923ms]
Feb  4 18:02:16.540: INFO: Created: latency-svc-v2mxn
Feb  4 18:02:16.556: INFO: Got endpoints: latency-svc-zcqkh [683.716312ms]
Feb  4 18:02:16.588: INFO: Created: latency-svc-p2vz8
Feb  4 18:02:16.654: INFO: Got endpoints: latency-svc-qlw7w [746.373901ms]
Feb  4 18:02:16.654: INFO: Got endpoints: latency-svc-6gqnp [778.585163ms]
Feb  4 18:02:16.673: INFO: Created: latency-svc-mz86f
Feb  4 18:02:16.682: INFO: Created: latency-svc-dhbwt
Feb  4 18:02:16.702: INFO: Got endpoints: latency-svc-8skts [738.908613ms]
Feb  4 18:02:16.741: INFO: Created: latency-svc-7vdt4
Feb  4 18:02:16.753: INFO: Got endpoints: latency-svc-pt28w [669.600467ms]
Feb  4 18:02:16.771: INFO: Created: latency-svc-l5dtj
Feb  4 18:02:16.807: INFO: Got endpoints: latency-svc-v6bw8 [722.463425ms]
Feb  4 18:02:16.831: INFO: Created: latency-svc-gfjgs
Feb  4 18:02:16.849: INFO: Got endpoints: latency-svc-pq9s4 [745.463629ms]
Feb  4 18:02:16.867: INFO: Created: latency-svc-g8dbv
Feb  4 18:02:16.915: INFO: Got endpoints: latency-svc-q86l8 [762.477903ms]
Feb  4 18:02:16.935: INFO: Created: latency-svc-bctq9
Feb  4 18:02:16.951: INFO: Got endpoints: latency-svc-x7vlt [737.585276ms]
Feb  4 18:02:16.991: INFO: Created: latency-svc-kn2k7
Feb  4 18:02:16.998: INFO: Got endpoints: latency-svc-swvpt [748.246859ms]
Feb  4 18:02:17.013: INFO: Created: latency-svc-cq96r
Feb  4 18:02:17.129: INFO: Got endpoints: latency-svc-p9jfx [765.360452ms]
Feb  4 18:02:17.134: INFO: Got endpoints: latency-svc-psbgg [835.133698ms]
Feb  4 18:02:17.155: INFO: Created: latency-svc-jndwb
Feb  4 18:02:17.157: INFO: Got endpoints: latency-svc-ll2cw [757.025276ms]
Feb  4 18:02:17.162: INFO: Created: latency-svc-mz9w5
Feb  4 18:02:17.181: INFO: Created: latency-svc-hsvz6
Feb  4 18:02:17.207: INFO: Got endpoints: latency-svc-ljm2p [733.421243ms]
Feb  4 18:02:17.226: INFO: Created: latency-svc-n6s5b
Feb  4 18:02:17.259: INFO: Got endpoints: latency-svc-v2mxn [745.203272ms]
Feb  4 18:02:17.282: INFO: Created: latency-svc-n6xdd
Feb  4 18:02:17.301: INFO: Got endpoints: latency-svc-p2vz8 [745.140834ms]
Feb  4 18:02:17.319: INFO: Created: latency-svc-qfhvq
Feb  4 18:02:17.371: INFO: Got endpoints: latency-svc-mz86f [716.925761ms]
Feb  4 18:02:17.386: INFO: Created: latency-svc-dfgnr
Feb  4 18:02:17.403: INFO: Got endpoints: latency-svc-dhbwt [749.487279ms]
Feb  4 18:02:17.425: INFO: Created: latency-svc-n5pkv
Feb  4 18:02:17.456: INFO: Got endpoints: latency-svc-7vdt4 [754.26635ms]
Feb  4 18:02:17.474: INFO: Created: latency-svc-hmknz
Feb  4 18:02:17.511: INFO: Got endpoints: latency-svc-l5dtj [758.130155ms]
Feb  4 18:02:17.537: INFO: Created: latency-svc-gq4rq
Feb  4 18:02:17.559: INFO: Got endpoints: latency-svc-gfjgs [752.744783ms]
Feb  4 18:02:17.579: INFO: Created: latency-svc-qggb8
Feb  4 18:02:17.610: INFO: Got endpoints: latency-svc-g8dbv [760.638847ms]
Feb  4 18:02:17.628: INFO: Created: latency-svc-l947p
Feb  4 18:02:17.653: INFO: Got endpoints: latency-svc-bctq9 [737.046237ms]
Feb  4 18:02:17.675: INFO: Created: latency-svc-5jx46
Feb  4 18:02:17.709: INFO: Got endpoints: latency-svc-kn2k7 [758.092705ms]
Feb  4 18:02:17.731: INFO: Created: latency-svc-mphmt
Feb  4 18:02:17.751: INFO: Got endpoints: latency-svc-cq96r [753.037424ms]
Feb  4 18:02:17.771: INFO: Created: latency-svc-gq65j
Feb  4 18:02:17.799: INFO: Got endpoints: latency-svc-jndwb [670.008369ms]
Feb  4 18:02:17.820: INFO: Created: latency-svc-hbssx
Feb  4 18:02:17.865: INFO: Got endpoints: latency-svc-mz9w5 [730.985475ms]
Feb  4 18:02:17.882: INFO: Created: latency-svc-t9c5f
Feb  4 18:02:17.898: INFO: Got endpoints: latency-svc-hsvz6 [741.201087ms]
Feb  4 18:02:17.915: INFO: Created: latency-svc-n5v8v
Feb  4 18:02:17.982: INFO: Got endpoints: latency-svc-n6s5b [775.283903ms]
Feb  4 18:02:18.003: INFO: Created: latency-svc-dlkkv
Feb  4 18:02:18.023: INFO: Got endpoints: latency-svc-n6xdd [764.218353ms]
Feb  4 18:02:18.058: INFO: Got endpoints: latency-svc-qfhvq [756.571637ms]
Feb  4 18:02:18.062: INFO: Created: latency-svc-gvf29
Feb  4 18:02:18.077: INFO: Created: latency-svc-npwdc
Feb  4 18:02:18.103: INFO: Got endpoints: latency-svc-dfgnr [731.956617ms]
Feb  4 18:02:18.125: INFO: Created: latency-svc-nc7q2
Feb  4 18:02:18.155: INFO: Got endpoints: latency-svc-n5pkv [751.282691ms]
Feb  4 18:02:18.181: INFO: Created: latency-svc-bbkjx
Feb  4 18:02:18.218: INFO: Got endpoints: latency-svc-hmknz [761.384181ms]
Feb  4 18:02:18.244: INFO: Created: latency-svc-kmv9s
Feb  4 18:02:18.255: INFO: Got endpoints: latency-svc-gq4rq [739.653373ms]
Feb  4 18:02:18.275: INFO: Created: latency-svc-pf8mp
Feb  4 18:02:18.303: INFO: Got endpoints: latency-svc-qggb8 [743.879819ms]
Feb  4 18:02:18.327: INFO: Created: latency-svc-dgbrj
Feb  4 18:02:18.354: INFO: Got endpoints: latency-svc-l947p [743.456706ms]
Feb  4 18:02:18.382: INFO: Created: latency-svc-w7pkg
Feb  4 18:02:18.403: INFO: Got endpoints: latency-svc-5jx46 [750.426069ms]
Feb  4 18:02:18.427: INFO: Created: latency-svc-4r44p
Feb  4 18:02:18.471: INFO: Got endpoints: latency-svc-mphmt [761.873263ms]
Feb  4 18:02:18.499: INFO: Got endpoints: latency-svc-gq65j [747.678616ms]
Feb  4 18:02:18.550: INFO: Got endpoints: latency-svc-hbssx [750.240779ms]
Feb  4 18:02:18.604: INFO: Got endpoints: latency-svc-t9c5f [738.538807ms]
Feb  4 18:02:18.656: INFO: Got endpoints: latency-svc-n5v8v [754.029718ms]
Feb  4 18:02:18.722: INFO: Got endpoints: latency-svc-dlkkv [739.469511ms]
Feb  4 18:02:18.759: INFO: Got endpoints: latency-svc-gvf29 [735.056332ms]
Feb  4 18:02:18.824: INFO: Got endpoints: latency-svc-npwdc [765.361034ms]
Feb  4 18:02:18.854: INFO: Got endpoints: latency-svc-nc7q2 [747.527756ms]
Feb  4 18:02:18.904: INFO: Got endpoints: latency-svc-bbkjx [746.620049ms]
Feb  4 18:02:18.962: INFO: Got endpoints: latency-svc-kmv9s [743.233688ms]
Feb  4 18:02:19.011: INFO: Got endpoints: latency-svc-pf8mp [755.331708ms]
Feb  4 18:02:19.054: INFO: Got endpoints: latency-svc-dgbrj [746.774218ms]
Feb  4 18:02:19.111: INFO: Got endpoints: latency-svc-w7pkg [757.435528ms]
Feb  4 18:02:19.151: INFO: Got endpoints: latency-svc-4r44p [744.122064ms]
Feb  4 18:02:19.156: INFO: Latencies: [32.857301ms 41.304523ms 43.248856ms 88.716222ms 94.654229ms 96.057089ms 238.480145ms 247.553888ms 249.944441ms 255.497163ms 259.324269ms 260.61234ms 261.238781ms 261.281489ms 263.874226ms 264.552577ms 265.003706ms 274.404805ms 277.694591ms 282.533616ms 293.013767ms 300.795051ms 305.081742ms 311.292679ms 313.347593ms 316.089225ms 316.271031ms 316.285113ms 321.487071ms 321.528439ms 321.531576ms 322.219986ms 346.121816ms 351.457742ms 352.498849ms 355.205193ms 356.241489ms 364.482632ms 422.55412ms 432.752558ms 440.754544ms 489.358456ms 489.445283ms 491.47241ms 491.578298ms 491.618257ms 503.032284ms 507.411816ms 509.52105ms 509.733911ms 516.520142ms 522.679149ms 526.931471ms 535.957773ms 539.493244ms 541.687456ms 541.901515ms 543.631034ms 543.802448ms 543.835669ms 543.847905ms 543.884088ms 543.967659ms 544.152521ms 549.879444ms 553.25862ms 555.245777ms 560.427738ms 570.339847ms 571.115689ms 575.42249ms 582.579539ms 582.79885ms 583.060249ms 583.92974ms 584.006728ms 586.117037ms 587.955844ms 598.957414ms 622.991792ms 630.268818ms 634.736547ms 642.488416ms 669.600467ms 670.008369ms 671.846708ms 683.716312ms 685.490985ms 691.266773ms 703.758257ms 704.094461ms 714.704158ms 716.925761ms 718.658196ms 722.177789ms 722.463425ms 726.108151ms 726.755253ms 726.844004ms 726.942887ms 729.66373ms 729.698938ms 730.985475ms 731.314053ms 731.318881ms 731.956617ms 732.73972ms 733.27129ms 733.421243ms 735.056332ms 735.172788ms 735.409445ms 737.046237ms 737.585276ms 738.538807ms 738.908613ms 739.469511ms 739.653373ms 740.23648ms 741.201087ms 741.526204ms 741.733598ms 742.35948ms 743.233688ms 743.317209ms 743.456706ms 743.879819ms 744.122064ms 744.575923ms 745.140834ms 745.203272ms 745.463629ms 746.373901ms 746.620049ms 746.774218ms 747.527756ms 747.678616ms 748.246859ms 749.487279ms 749.939783ms 750.240779ms 750.426069ms 751.106858ms 751.282691ms 751.673923ms 752.744783ms 753.037424ms 754.029718ms 754.182877ms 754.257706ms 754.26635ms 754.291412ms 754.975575ms 755.331708ms 756.323724ms 756.571637ms 757.023195ms 757.025276ms 757.435528ms 758.092705ms 758.130155ms 758.36472ms 758.389224ms 760.158866ms 760.638847ms 760.7154ms 761.384181ms 761.446175ms 761.730228ms 761.873263ms 762.477903ms 762.489551ms 764.218353ms 764.803174ms 765.360452ms 765.361034ms 765.424548ms 767.556955ms 770.707921ms 771.468224ms 772.062771ms 772.439259ms 774.790123ms 775.283903ms 777.513876ms 778.585163ms 788.632498ms 789.616576ms 791.825008ms 791.981829ms 792.096868ms 811.902922ms 811.979288ms 815.393568ms 824.973937ms 828.747922ms 835.133698ms 856.115761ms 857.962998ms 896.951266ms]
Feb  4 18:02:19.156: INFO: 50 %ile: 729.66373ms
Feb  4 18:02:19.156: INFO: 90 %ile: 772.062771ms
Feb  4 18:02:19.157: INFO: 99 %ile: 857.962998ms
Feb  4 18:02:19.157: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:02:19.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-kv796" for this suite.
Feb  4 18:02:39.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:02:39.453: INFO: namespace: e2e-tests-svc-latency-kv796, resource: bindings, ignored listing per whitelist
Feb  4 18:02:39.537: INFO: namespace e2e-tests-svc-latency-kv796 deletion completed in 20.370783057s

• [SLOW TEST:32.526 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:02:39.539: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-0f998764-28a7-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume secrets
Feb  4 18:02:39.817: INFO: Waiting up to 5m0s for pod "pod-secrets-0f9bb144-28a7-11e9-9eab-8e57f341003b" in namespace "e2e-tests-secrets-lcn57" to be "success or failure"
Feb  4 18:02:39.821: INFO: Pod "pod-secrets-0f9bb144-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.189803ms
Feb  4 18:02:41.828: INFO: Pod "pod-secrets-0f9bb144-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010542797s
Feb  4 18:02:43.841: INFO: Pod "pod-secrets-0f9bb144-28a7-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023732206s
STEP: Saw pod success
Feb  4 18:02:43.841: INFO: Pod "pod-secrets-0f9bb144-28a7-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:02:43.905: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-secrets-0f9bb144-28a7-11e9-9eab-8e57f341003b container secret-volume-test: <nil>
STEP: delete the pod
Feb  4 18:02:44.043: INFO: Waiting for pod pod-secrets-0f9bb144-28a7-11e9-9eab-8e57f341003b to disappear
Feb  4 18:02:44.052: INFO: Pod pod-secrets-0f9bb144-28a7-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:02:44.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-lcn57" for this suite.
Feb  4 18:02:50.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:02:50.231: INFO: namespace: e2e-tests-secrets-lcn57, resource: bindings, ignored listing per whitelist
Feb  4 18:02:50.500: INFO: namespace e2e-tests-secrets-lcn57 deletion completed in 6.439949857s

• [SLOW TEST:10.962 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:02:50.505: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  4 18:02:50.761: INFO: (0) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 15.838204ms)
Feb  4 18:02:50.817: INFO: (1) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 56.588721ms)
Feb  4 18:02:50.831: INFO: (2) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 13.838947ms)
Feb  4 18:02:50.840: INFO: (3) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 8.56902ms)
Feb  4 18:02:50.856: INFO: (4) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 16.010509ms)
Feb  4 18:02:50.908: INFO: (5) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 52.361386ms)
Feb  4 18:02:50.997: INFO: (6) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 88.404747ms)
Feb  4 18:02:51.104: INFO: (7) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 107.332838ms)
Feb  4 18:02:51.206: INFO: (8) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 101.222957ms)
Feb  4 18:02:51.223: INFO: (9) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 16.935118ms)
Feb  4 18:02:51.246: INFO: (10) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 22.974642ms)
Feb  4 18:02:51.259: INFO: (11) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 13.413837ms)
Feb  4 18:02:51.271: INFO: (12) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 11.128795ms)
Feb  4 18:02:51.282: INFO: (13) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 11.49118ms)
Feb  4 18:02:51.292: INFO: (14) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 9.797955ms)
Feb  4 18:02:51.337: INFO: (15) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 44.556439ms)
Feb  4 18:02:51.353: INFO: (16) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 16.30168ms)
Feb  4 18:02:51.402: INFO: (17) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 48.56284ms)
Feb  4 18:02:51.416: INFO: (18) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 14.637949ms)
Feb  4 18:02:51.474: INFO: (19) /api/v1/nodes/machine-kubermatic-conformancecluster-61wiwtyxah-6rthf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="bootstrap.... (200; 57.154137ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:02:51.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-xpd7n" for this suite.
Feb  4 18:02:57.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:02:57.690: INFO: namespace: e2e-tests-proxy-xpd7n, resource: bindings, ignored listing per whitelist
Feb  4 18:02:57.875: INFO: namespace e2e-tests-proxy-xpd7n deletion completed in 6.394298279s

• [SLOW TEST:7.371 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:02:57.880: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-1a86a5bc-28a7-11e9-9eab-8e57f341003b
STEP: Creating secret with name s-test-opt-upd-1a86a665-28a7-11e9-9eab-8e57f341003b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-1a86a5bc-28a7-11e9-9eab-8e57f341003b
STEP: Updating secret s-test-opt-upd-1a86a665-28a7-11e9-9eab-8e57f341003b
STEP: Creating secret with name s-test-opt-create-1a86a6a2-28a7-11e9-9eab-8e57f341003b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:03:04.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n82wg" for this suite.
Feb  4 18:03:28.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:03:28.918: INFO: namespace: e2e-tests-projected-n82wg, resource: bindings, ignored listing per whitelist
Feb  4 18:03:29.117: INFO: namespace e2e-tests-projected-n82wg deletion completed in 24.38022722s

• [SLOW TEST:31.238 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:03:29.122: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb  4 18:03:29.412: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-wwr65,SelfLink:/api/v1/namespaces/e2e-tests-watch-wwr65/configmaps/e2e-watch-test-watch-closed,UID:2d2c9c3c-28a7-11e9-9959-0a580af41676,ResourceVersion:27922,Generation:0,CreationTimestamp:2019-02-04 18:03:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  4 18:03:29.412: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-wwr65,SelfLink:/api/v1/namespaces/e2e-tests-watch-wwr65/configmaps/e2e-watch-test-watch-closed,UID:2d2c9c3c-28a7-11e9-9959-0a580af41676,ResourceVersion:27923,Generation:0,CreationTimestamp:2019-02-04 18:03:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb  4 18:03:29.437: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-wwr65,SelfLink:/api/v1/namespaces/e2e-tests-watch-wwr65/configmaps/e2e-watch-test-watch-closed,UID:2d2c9c3c-28a7-11e9-9959-0a580af41676,ResourceVersion:27924,Generation:0,CreationTimestamp:2019-02-04 18:03:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  4 18:03:29.438: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-wwr65,SelfLink:/api/v1/namespaces/e2e-tests-watch-wwr65/configmaps/e2e-watch-test-watch-closed,UID:2d2c9c3c-28a7-11e9-9959-0a580af41676,ResourceVersion:27925,Generation:0,CreationTimestamp:2019-02-04 18:03:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:03:29.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-wwr65" for this suite.
Feb  4 18:03:35.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:03:35.754: INFO: namespace: e2e-tests-watch-wwr65, resource: bindings, ignored listing per whitelist
Feb  4 18:03:35.854: INFO: namespace e2e-tests-watch-wwr65 deletion completed in 6.408750227s

• [SLOW TEST:6.733 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:03:35.858: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb  4 18:03:36.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-vcsk8'
Feb  4 18:03:36.843: INFO: stderr: ""
Feb  4 18:03:36.843: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  4 18:03:36.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vcsk8'
Feb  4 18:03:37.369: INFO: stderr: ""
Feb  4 18:03:37.369: INFO: stdout: "update-demo-nautilus-mv5vx update-demo-nautilus-tw8cr "
Feb  4 18:03:37.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-mv5vx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vcsk8'
Feb  4 18:03:37.885: INFO: stderr: ""
Feb  4 18:03:37.885: INFO: stdout: ""
Feb  4 18:03:37.885: INFO: update-demo-nautilus-mv5vx is created but not running
Feb  4 18:03:42.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vcsk8'
Feb  4 18:03:43.157: INFO: stderr: ""
Feb  4 18:03:43.157: INFO: stdout: "update-demo-nautilus-mv5vx update-demo-nautilus-tw8cr "
Feb  4 18:03:43.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-mv5vx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vcsk8'
Feb  4 18:03:43.439: INFO: stderr: ""
Feb  4 18:03:43.439: INFO: stdout: "true"
Feb  4 18:03:43.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-mv5vx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vcsk8'
Feb  4 18:03:43.732: INFO: stderr: ""
Feb  4 18:03:43.732: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  4 18:03:43.732: INFO: validating pod update-demo-nautilus-mv5vx
Feb  4 18:03:43.890: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  4 18:03:43.890: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  4 18:03:43.890: INFO: update-demo-nautilus-mv5vx is verified up and running
Feb  4 18:03:43.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-tw8cr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vcsk8'
Feb  4 18:03:44.148: INFO: stderr: ""
Feb  4 18:03:44.148: INFO: stdout: "true"
Feb  4 18:03:44.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods update-demo-nautilus-tw8cr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vcsk8'
Feb  4 18:03:44.415: INFO: stderr: ""
Feb  4 18:03:44.415: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  4 18:03:44.415: INFO: validating pod update-demo-nautilus-tw8cr
Feb  4 18:03:44.590: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  4 18:03:44.590: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  4 18:03:44.590: INFO: update-demo-nautilus-tw8cr is verified up and running
STEP: using delete to clean up resources
Feb  4 18:03:44.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vcsk8'
Feb  4 18:03:44.934: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  4 18:03:44.934: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb  4 18:03:44.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-vcsk8'
Feb  4 18:03:45.509: INFO: stderr: "No resources found.\n"
Feb  4 18:03:45.509: INFO: stdout: ""
Feb  4 18:03:45.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 get pods -l name=update-demo --namespace=e2e-tests-kubectl-vcsk8 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  4 18:03:46.047: INFO: stderr: ""
Feb  4 18:03:46.047: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:03:46.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vcsk8" for this suite.
Feb  4 18:04:10.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:04:10.389: INFO: namespace: e2e-tests-kubectl-vcsk8, resource: bindings, ignored listing per whitelist
Feb  4 18:04:10.524: INFO: namespace e2e-tests-kubectl-vcsk8 deletion completed in 24.469844845s

• [SLOW TEST:34.666 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:04:10.529: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-45d5be7b-28a7-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 18:04:10.809: INFO: Waiting up to 5m0s for pod "pod-configmaps-45d6c30d-28a7-11e9-9eab-8e57f341003b" in namespace "e2e-tests-configmap-t8vdf" to be "success or failure"
Feb  4 18:04:10.814: INFO: Pod "pod-configmaps-45d6c30d-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.152128ms
Feb  4 18:04:12.821: INFO: Pod "pod-configmaps-45d6c30d-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011407412s
Feb  4 18:04:14.828: INFO: Pod "pod-configmaps-45d6c30d-28a7-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019225077s
STEP: Saw pod success
Feb  4 18:04:14.829: INFO: Pod "pod-configmaps-45d6c30d-28a7-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:04:14.910: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-configmaps-45d6c30d-28a7-11e9-9eab-8e57f341003b container configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 18:04:15.109: INFO: Waiting for pod pod-configmaps-45d6c30d-28a7-11e9-9eab-8e57f341003b to disappear
Feb  4 18:04:15.114: INFO: Pod pod-configmaps-45d6c30d-28a7-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:04:15.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-t8vdf" for this suite.
Feb  4 18:04:21.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:04:21.250: INFO: namespace: e2e-tests-configmap-t8vdf, resource: bindings, ignored listing per whitelist
Feb  4 18:04:21.516: INFO: namespace e2e-tests-configmap-t8vdf deletion completed in 6.394516208s

• [SLOW TEST:10.987 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:04:21.520: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb  4 18:04:21.774: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  4 18:04:21.787: INFO: Waiting for terminating namespaces to be deleted...
Feb  4 18:04:21.791: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf before test
Feb  4 18:04:21.865: INFO: node-exporter-wzptg from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.866: INFO: 	Container node-exporter ready: true, restart count 0
Feb  4 18:04:21.869: INFO: sonobuoy-systemd-logs-daemon-set-fde55ff1e0804945-6mf79 from heptio-sonobuoy started at 2019-02-04 16:18:54 +0000 UTC (2 container statuses recorded)
Feb  4 18:04:21.869: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb  4 18:04:21.869: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb  4 18:04:21.869: INFO: openvpn-client-7c9496f697-sq426 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (2 container statuses recorded)
Feb  4 18:04:21.869: INFO: 	Container dnat-controller ready: true, restart count 0
Feb  4 18:04:21.869: INFO: 	Container openvpn-client ready: true, restart count 0
Feb  4 18:04:21.869: INFO: kube-dns-6fcf8486c9-fp64s from kube-system started at 2019-02-04 16:17:59 +0000 UTC (3 container statuses recorded)
Feb  4 18:04:21.869: INFO: 	Container dnsmasq ready: true, restart count 0
Feb  4 18:04:21.869: INFO: 	Container kubedns ready: true, restart count 0
Feb  4 18:04:21.870: INFO: 	Container sidecar ready: true, restart count 0
Feb  4 18:04:21.870: INFO: kube-dns-6fcf8486c9-cfvbk from kube-system started at 2019-02-04 16:16:04 +0000 UTC (3 container statuses recorded)
Feb  4 18:04:21.870: INFO: 	Container dnsmasq ready: true, restart count 0
Feb  4 18:04:21.870: INFO: 	Container kubedns ready: true, restart count 0
Feb  4 18:04:21.870: INFO: 	Container sidecar ready: true, restart count 0
Feb  4 18:04:21.870: INFO: kube-proxy-lwvdf from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.870: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  4 18:04:21.870: INFO: ark-5f9c9897c5-gmcbs from heptio-ark started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.870: INFO: 	Container ark ready: true, restart count 3
Feb  4 18:04:21.870: INFO: cluster-autoscaler-5f8478765d-8rjp2 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.870: INFO: 	Container cluster-autoscaler ready: true, restart count 1
Feb  4 18:04:21.870: INFO: restic-9gtnw from heptio-ark started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.870: INFO: 	Container ark ready: true, restart count 0
Feb  4 18:04:21.870: INFO: kubernetes-dashboard-657fd84c97-4t2v8 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.870: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb  4 18:04:21.870: INFO: kube-dns-autoscaler-6686fffcd4-q7xr8 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.870: INFO: 	Container autoscaler ready: true, restart count 0
Feb  4 18:04:21.870: INFO: canal-7fvph from kube-system started at 2019-02-04 16:16:04 +0000 UTC (3 container statuses recorded)
Feb  4 18:04:21.870: INFO: 	Container calico-node ready: true, restart count 0
Feb  4 18:04:21.870: INFO: 	Container install-cni ready: true, restart count 0
Feb  4 18:04:21.870: INFO: 	Container kube-flannel ready: true, restart count 0
Feb  4 18:04:21.870: INFO: tiller-deploy-7cf4c86f8-ntzqt from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.870: INFO: 	Container tiller ready: true, restart count 0
Feb  4 18:04:21.870: INFO: metrics-server-6b98b49585-cwwl9 from kube-system started at 2019-02-04 16:16:04 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.870: INFO: 	Container metrics-server ready: true, restart count 0
Feb  4 18:04:21.870: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 before test
Feb  4 18:04:21.931: INFO: sonobuoy-e2e-job-45efe79f680344de from heptio-sonobuoy started at 2019-02-04 16:18:54 +0000 UTC (2 container statuses recorded)
Feb  4 18:04:21.931: INFO: 	Container e2e ready: true, restart count 0
Feb  4 18:04:21.935: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  4 18:04:21.935: INFO: sonobuoy-systemd-logs-daemon-set-fde55ff1e0804945-k8lln from heptio-sonobuoy started at 2019-02-04 16:18:54 +0000 UTC (2 container statuses recorded)
Feb  4 18:04:21.935: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb  4 18:04:21.935: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb  4 18:04:21.935: INFO: restic-2rnhq from heptio-ark started at 2019-02-04 16:17:55 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.935: INFO: 	Container ark ready: true, restart count 0
Feb  4 18:04:21.935: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-04 16:18:44 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.935: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  4 18:04:21.935: INFO: node-exporter-xtlhp from kube-system started at 2019-02-04 16:17:55 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.935: INFO: 	Container node-exporter ready: true, restart count 0
Feb  4 18:04:21.935: INFO: canal-9jx2t from kube-system started at 2019-02-04 16:17:55 +0000 UTC (3 container statuses recorded)
Feb  4 18:04:21.935: INFO: 	Container calico-node ready: true, restart count 0
Feb  4 18:04:21.935: INFO: 	Container install-cni ready: true, restart count 0
Feb  4 18:04:21.935: INFO: 	Container kube-flannel ready: true, restart count 0
Feb  4 18:04:21.935: INFO: kube-proxy-7bbjk from kube-system started at 2019-02-04 16:17:55 +0000 UTC (1 container statuses recorded)
Feb  4 18:04:21.935: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15803b4a46513184], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:04:23.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-8vxqw" for this suite.
Feb  4 18:04:29.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:04:29.348: INFO: namespace: e2e-tests-sched-pred-8vxqw, resource: bindings, ignored listing per whitelist
Feb  4 18:04:29.393: INFO: namespace e2e-tests-sched-pred-8vxqw deletion completed in 6.375503609s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.875 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:04:29.395: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-lldtx in namespace e2e-tests-proxy-42pq4
I0204 18:04:29.702125      13 runners.go:184] Created replication controller with name: proxy-service-lldtx, namespace: e2e-tests-proxy-42pq4, replica count: 1
I0204 18:04:30.761406      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0204 18:04:31.761632      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0204 18:04:32.761877      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0204 18:04:33.762094      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0204 18:04:34.765110      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0204 18:04:35.769491      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0204 18:04:36.771565      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0204 18:04:37.771765      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0204 18:04:38.772003      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0204 18:04:39.772248      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0204 18:04:40.772576      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0204 18:04:41.772802      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0204 18:04:42.773028      13 runners.go:184] proxy-service-lldtx Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  4 18:04:42.808: INFO: setup took 13.146831824s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb  4 18:04:42.910: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 100.0006ms)
Feb  4 18:04:42.928: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 117.625363ms)
Feb  4 18:04:42.930: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 117.326495ms)
Feb  4 18:04:42.931: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 120.548439ms)
Feb  4 18:04:42.932: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 121.959839ms)
Feb  4 18:04:42.932: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 122.518251ms)
Feb  4 18:04:42.932: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 123.88575ms)
Feb  4 18:04:42.933: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 123.43317ms)
Feb  4 18:04:42.954: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 141.759405ms)
Feb  4 18:04:42.954: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 142.067786ms)
Feb  4 18:04:42.957: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 147.475373ms)
Feb  4 18:04:43.011: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 199.704837ms)
Feb  4 18:04:43.014: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 204.867741ms)
Feb  4 18:04:43.014: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 202.433745ms)
Feb  4 18:04:43.014: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 203.013129ms)
Feb  4 18:04:43.014: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 202.807579ms)
Feb  4 18:04:43.042: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 27.069081ms)
Feb  4 18:04:43.042: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 27.252269ms)
Feb  4 18:04:43.042: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 26.647672ms)
Feb  4 18:04:43.042: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 27.352489ms)
Feb  4 18:04:43.042: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 27.018486ms)
Feb  4 18:04:43.045: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 30.943005ms)
Feb  4 18:04:43.048: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 33.285521ms)
Feb  4 18:04:43.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 33.876553ms)
Feb  4 18:04:43.050: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 34.313057ms)
Feb  4 18:04:43.050: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 34.846051ms)
Feb  4 18:04:43.050: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 34.844633ms)
Feb  4 18:04:43.050: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 35.8319ms)
Feb  4 18:04:43.050: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 35.750816ms)
Feb  4 18:04:43.050: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 34.555713ms)
Feb  4 18:04:43.102: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 86.903108ms)
Feb  4 18:04:43.102: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 87.665955ms)
Feb  4 18:04:43.112: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 8.50778ms)
Feb  4 18:04:43.114: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 10.225232ms)
Feb  4 18:04:43.115: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 11.60004ms)
Feb  4 18:04:43.116: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 11.730317ms)
Feb  4 18:04:43.118: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 13.22318ms)
Feb  4 18:04:43.119: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 13.976224ms)
Feb  4 18:04:43.119: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 13.51087ms)
Feb  4 18:04:43.119: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 13.362029ms)
Feb  4 18:04:43.119: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 14.694669ms)
Feb  4 18:04:43.123: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 18.108745ms)
Feb  4 18:04:43.125: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 22.470218ms)
Feb  4 18:04:43.125: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 22.879853ms)
Feb  4 18:04:43.126: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 19.738373ms)
Feb  4 18:04:43.126: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 19.013127ms)
Feb  4 18:04:43.126: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 23.160091ms)
Feb  4 18:04:43.127: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 20.217855ms)
Feb  4 18:04:43.202: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 74.570404ms)
Feb  4 18:04:43.208: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 80.429795ms)
Feb  4 18:04:43.208: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 78.763318ms)
Feb  4 18:04:43.209: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 80.910107ms)
Feb  4 18:04:43.210: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 81.191088ms)
Feb  4 18:04:43.210: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 82.180303ms)
Feb  4 18:04:43.210: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 82.05161ms)
Feb  4 18:04:43.211: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 81.267478ms)
Feb  4 18:04:43.211: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 83.791027ms)
Feb  4 18:04:43.211: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 81.850889ms)
Feb  4 18:04:43.211: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 82.356648ms)
Feb  4 18:04:43.211: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 79.451284ms)
Feb  4 18:04:43.212: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 82.633233ms)
Feb  4 18:04:43.212: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 85.357893ms)
Feb  4 18:04:43.218: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 90.41217ms)
Feb  4 18:04:43.218: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 87.820395ms)
Feb  4 18:04:43.320: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 100.609195ms)
Feb  4 18:04:43.320: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 101.383104ms)
Feb  4 18:04:43.320: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 101.27884ms)
Feb  4 18:04:43.320: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 98.912796ms)
Feb  4 18:04:43.320: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 101.85378ms)
Feb  4 18:04:43.320: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 100.256892ms)
Feb  4 18:04:43.320: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 100.902373ms)
Feb  4 18:04:43.320: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 99.950927ms)
Feb  4 18:04:43.320: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 100.2539ms)
Feb  4 18:04:43.320: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 100.91464ms)
Feb  4 18:04:43.322: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 102.438606ms)
Feb  4 18:04:43.323: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 105.123822ms)
Feb  4 18:04:43.326: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 104.851115ms)
Feb  4 18:04:43.326: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 105.135365ms)
Feb  4 18:04:43.326: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 107.320042ms)
Feb  4 18:04:43.326: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 106.979899ms)
Feb  4 18:04:43.340: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 14.166501ms)
Feb  4 18:04:43.341: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 13.437597ms)
Feb  4 18:04:43.342: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 15.556657ms)
Feb  4 18:04:43.342: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 14.772067ms)
Feb  4 18:04:43.346: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 16.814953ms)
Feb  4 18:04:43.347: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 19.872316ms)
Feb  4 18:04:43.347: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 20.654489ms)
Feb  4 18:04:43.348: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 21.109143ms)
Feb  4 18:04:43.349: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 20.646034ms)
Feb  4 18:04:43.349: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 21.380541ms)
Feb  4 18:04:43.349: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 20.9934ms)
Feb  4 18:04:43.350: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 21.017758ms)
Feb  4 18:04:43.352: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 23.08008ms)
Feb  4 18:04:43.353: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 24.459417ms)
Feb  4 18:04:43.354: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 24.835048ms)
Feb  4 18:04:43.354: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 25.605963ms)
Feb  4 18:04:43.408: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 54.027609ms)
Feb  4 18:04:43.409: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 53.378837ms)
Feb  4 18:04:43.409: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 51.535699ms)
Feb  4 18:04:43.409: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 53.372178ms)
Feb  4 18:04:43.409: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 53.022452ms)
Feb  4 18:04:43.409: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 51.605659ms)
Feb  4 18:04:43.410: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 52.908345ms)
Feb  4 18:04:43.412: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 56.923473ms)
Feb  4 18:04:43.414: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 57.057635ms)
Feb  4 18:04:43.414: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 59.717876ms)
Feb  4 18:04:43.414: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 57.225479ms)
Feb  4 18:04:43.415: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 59.11355ms)
Feb  4 18:04:43.416: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 58.86248ms)
Feb  4 18:04:43.416: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 58.109926ms)
Feb  4 18:04:43.418: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 63.057693ms)
Feb  4 18:04:43.419: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 63.307652ms)
Feb  4 18:04:43.446: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 26.698914ms)
Feb  4 18:04:43.447: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 27.421262ms)
Feb  4 18:04:43.448: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 28.325076ms)
Feb  4 18:04:43.448: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 28.068021ms)
Feb  4 18:04:43.448: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 28.014303ms)
Feb  4 18:04:43.448: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 28.323347ms)
Feb  4 18:04:43.448: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 28.738644ms)
Feb  4 18:04:43.448: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 28.187818ms)
Feb  4 18:04:43.448: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 29.551536ms)
Feb  4 18:04:43.448: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 29.027788ms)
Feb  4 18:04:43.448: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 28.888961ms)
Feb  4 18:04:43.448: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 29.498244ms)
Feb  4 18:04:43.503: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 84.21302ms)
Feb  4 18:04:43.503: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 84.539046ms)
Feb  4 18:04:43.503: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 84.268042ms)
Feb  4 18:04:43.504: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 84.971009ms)
Feb  4 18:04:43.611: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 106.629938ms)
Feb  4 18:04:43.612: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 106.380798ms)
Feb  4 18:04:43.612: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 107.096869ms)
Feb  4 18:04:43.612: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 106.992387ms)
Feb  4 18:04:43.612: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 107.350538ms)
Feb  4 18:04:43.612: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 107.103224ms)
Feb  4 18:04:43.612: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 108.427663ms)
Feb  4 18:04:43.612: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 107.111041ms)
Feb  4 18:04:43.613: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 107.937694ms)
Feb  4 18:04:43.613: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 108.075845ms)
Feb  4 18:04:43.614: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 109.422384ms)
Feb  4 18:04:43.614: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 110.112736ms)
Feb  4 18:04:43.614: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 110.15225ms)
Feb  4 18:04:43.615: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 110.366336ms)
Feb  4 18:04:43.615: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 110.479771ms)
Feb  4 18:04:43.616: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 111.358318ms)
Feb  4 18:04:43.702: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 86.400626ms)
Feb  4 18:04:43.706: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 85.271186ms)
Feb  4 18:04:43.706: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 85.772274ms)
Feb  4 18:04:43.707: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 89.954138ms)
Feb  4 18:04:43.707: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 90.085343ms)
Feb  4 18:04:43.708: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 88.325748ms)
Feb  4 18:04:43.708: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 92.046596ms)
Feb  4 18:04:43.709: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 91.124149ms)
Feb  4 18:04:43.709: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 92.3994ms)
Feb  4 18:04:43.709: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 89.110493ms)
Feb  4 18:04:43.709: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 89.600973ms)
Feb  4 18:04:43.709: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 89.342239ms)
Feb  4 18:04:43.710: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 90.267863ms)
Feb  4 18:04:43.710: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 92.908837ms)
Feb  4 18:04:43.710: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 93.324976ms)
Feb  4 18:04:43.714: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 97.687695ms)
Feb  4 18:04:43.726: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 12.041069ms)
Feb  4 18:04:43.727: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 12.823016ms)
Feb  4 18:04:43.729: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 12.236426ms)
Feb  4 18:04:43.733: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 17.451438ms)
Feb  4 18:04:43.733: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 18.934238ms)
Feb  4 18:04:43.733: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 17.899371ms)
Feb  4 18:04:43.734: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 19.138222ms)
Feb  4 18:04:43.734: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 18.817204ms)
Feb  4 18:04:43.734: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 17.989332ms)
Feb  4 18:04:43.742: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 26.660124ms)
Feb  4 18:04:43.743: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 25.609489ms)
Feb  4 18:04:43.743: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 29.341972ms)
Feb  4 18:04:43.743: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 26.791592ms)
Feb  4 18:04:43.744: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 27.174361ms)
Feb  4 18:04:43.745: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 28.712326ms)
Feb  4 18:04:43.745: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 30.385896ms)
Feb  4 18:04:43.806: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 57.559232ms)
Feb  4 18:04:43.815: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 68.372469ms)
Feb  4 18:04:43.815: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 68.824855ms)
Feb  4 18:04:43.816: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 68.376693ms)
Feb  4 18:04:43.816: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 68.520296ms)
Feb  4 18:04:43.817: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 69.936092ms)
Feb  4 18:04:43.817: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 70.705889ms)
Feb  4 18:04:43.818: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 72.515335ms)
Feb  4 18:04:43.818: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 71.16827ms)
Feb  4 18:04:43.818: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 70.378503ms)
Feb  4 18:04:43.818: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 67.062773ms)
Feb  4 18:04:43.818: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 69.861321ms)
Feb  4 18:04:43.820: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 74.124062ms)
Feb  4 18:04:43.820: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 71.843704ms)
Feb  4 18:04:43.821: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 73.498997ms)
Feb  4 18:04:43.821: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 75.001476ms)
Feb  4 18:04:43.830: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 9.254052ms)
Feb  4 18:04:43.836: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 14.757355ms)
Feb  4 18:04:43.836: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 14.958028ms)
Feb  4 18:04:43.838: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 15.861759ms)
Feb  4 18:04:43.838: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 16.079638ms)
Feb  4 18:04:43.838: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 15.7159ms)
Feb  4 18:04:43.838: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 16.136507ms)
Feb  4 18:04:43.838: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 16.898907ms)
Feb  4 18:04:43.838: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 16.631431ms)
Feb  4 18:04:43.839: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 16.662012ms)
Feb  4 18:04:43.902: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 79.455295ms)
Feb  4 18:04:43.905: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 82.650142ms)
Feb  4 18:04:43.942: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 119.868338ms)
Feb  4 18:04:43.942: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 120.009262ms)
Feb  4 18:04:43.943: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 120.700244ms)
Feb  4 18:04:43.943: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 120.851596ms)
Feb  4 18:04:43.957: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 13.382524ms)
Feb  4 18:04:43.964: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 16.692857ms)
Feb  4 18:04:43.964: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 20.346808ms)
Feb  4 18:04:43.964: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 19.101997ms)
Feb  4 18:04:43.964: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 17.142652ms)
Feb  4 18:04:43.965: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 18.453358ms)
Feb  4 18:04:43.965: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 19.39875ms)
Feb  4 18:04:43.965: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 21.519254ms)
Feb  4 18:04:43.965: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 20.866471ms)
Feb  4 18:04:44.026: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 80.09994ms)
Feb  4 18:04:44.029: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 83.83684ms)
Feb  4 18:04:44.029: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 85.253416ms)
Feb  4 18:04:44.029: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 85.10488ms)
Feb  4 18:04:44.030: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 84.70216ms)
Feb  4 18:04:44.030: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 83.92248ms)
Feb  4 18:04:44.031: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 86.051983ms)
Feb  4 18:04:44.051: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 18.296145ms)
Feb  4 18:04:44.113: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 79.754171ms)
Feb  4 18:04:44.114: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 81.548636ms)
Feb  4 18:04:44.115: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 80.185564ms)
Feb  4 18:04:44.116: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 84.515948ms)
Feb  4 18:04:44.116: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 82.839599ms)
Feb  4 18:04:44.116: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 80.907786ms)
Feb  4 18:04:44.117: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 81.508646ms)
Feb  4 18:04:44.117: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 85.300635ms)
Feb  4 18:04:44.117: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 81.971597ms)
Feb  4 18:04:44.117: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 81.723988ms)
Feb  4 18:04:44.118: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 85.208326ms)
Feb  4 18:04:44.118: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 83.981373ms)
Feb  4 18:04:44.119: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 87.74408ms)
Feb  4 18:04:44.120: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 89.204258ms)
Feb  4 18:04:44.121: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 85.143755ms)
Feb  4 18:04:44.133: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 12.392636ms)
Feb  4 18:04:44.134: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 11.631364ms)
Feb  4 18:04:44.135: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 13.066031ms)
Feb  4 18:04:44.135: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 13.120081ms)
Feb  4 18:04:44.135: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 13.260011ms)
Feb  4 18:04:44.137: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 14.46996ms)
Feb  4 18:04:44.137: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 14.19646ms)
Feb  4 18:04:44.138: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 13.539154ms)
Feb  4 18:04:44.138: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 14.40799ms)
Feb  4 18:04:44.138: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 17.093071ms)
Feb  4 18:04:44.138: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 16.507012ms)
Feb  4 18:04:44.138: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 15.608848ms)
Feb  4 18:04:44.142: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 18.59844ms)
Feb  4 18:04:44.142: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 21.151142ms)
Feb  4 18:04:44.143: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 18.986386ms)
Feb  4 18:04:44.143: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 19.792402ms)
Feb  4 18:04:44.202: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 58.328487ms)
Feb  4 18:04:44.204: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 59.670902ms)
Feb  4 18:04:44.204: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 58.906887ms)
Feb  4 18:04:44.204: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 58.508515ms)
Feb  4 18:04:44.204: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 59.84694ms)
Feb  4 18:04:44.204: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 61.481206ms)
Feb  4 18:04:44.204: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 58.436921ms)
Feb  4 18:04:44.204: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 60.898888ms)
Feb  4 18:04:44.205: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 59.335679ms)
Feb  4 18:04:44.205: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 61.560621ms)
Feb  4 18:04:44.205: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 59.854022ms)
Feb  4 18:04:44.208: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 62.88839ms)
Feb  4 18:04:44.210: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 65.366641ms)
Feb  4 18:04:44.211: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 67.495391ms)
Feb  4 18:04:44.211: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 64.82461ms)
Feb  4 18:04:44.211: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 66.950654ms)
Feb  4 18:04:44.222: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 10.528279ms)
Feb  4 18:04:44.222: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 10.770142ms)
Feb  4 18:04:44.230: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 18.21778ms)
Feb  4 18:04:44.230: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 19.086081ms)
Feb  4 18:04:44.231: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 18.984537ms)
Feb  4 18:04:44.231: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 18.600621ms)
Feb  4 18:04:44.232: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 20.245293ms)
Feb  4 18:04:44.233: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 20.897426ms)
Feb  4 18:04:44.240: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 27.846508ms)
Feb  4 18:04:44.241: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 29.293264ms)
Feb  4 18:04:44.242: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 29.352165ms)
Feb  4 18:04:44.243: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 30.885088ms)
Feb  4 18:04:44.243: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 30.138444ms)
Feb  4 18:04:44.243: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 30.328608ms)
Feb  4 18:04:44.243: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 31.241388ms)
Feb  4 18:04:44.245: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 33.853828ms)
Feb  4 18:04:44.315: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 66.376749ms)
Feb  4 18:04:44.316: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 69.544221ms)
Feb  4 18:04:44.316: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 70.16186ms)
Feb  4 18:04:44.316: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 70.101475ms)
Feb  4 18:04:44.316: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 67.513362ms)
Feb  4 18:04:44.316: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 70.958746ms)
Feb  4 18:04:44.317: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 70.910195ms)
Feb  4 18:04:44.318: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 70.818814ms)
Feb  4 18:04:44.319: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 67.623168ms)
Feb  4 18:04:44.319: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 67.244462ms)
Feb  4 18:04:44.319: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 67.102836ms)
Feb  4 18:04:44.319: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 69.930123ms)
Feb  4 18:04:44.323: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 74.388714ms)
Feb  4 18:04:44.324: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 77.120564ms)
Feb  4 18:04:44.324: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 75.194433ms)
Feb  4 18:04:44.324: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 72.856665ms)
Feb  4 18:04:44.339: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4/proxy/rewriteme"... (200; 12.636957ms)
Feb  4 18:04:44.339: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:1080/proxy/rewri... (200; 12.468557ms)
Feb  4 18:04:44.339: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:162/proxy/: bar (200; 14.380521ms)
Feb  4 18:04:44.402: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname2/proxy/: tls qux (200; 77.113974ms)
Feb  4 18:04:44.402: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname1/proxy/: foo (200; 76.751537ms)
Feb  4 18:04:44.403: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname1/proxy/: foo (200; 77.605923ms)
Feb  4 18:04:44.403: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:460/proxy/: tls baz (200; 75.555843ms)
Feb  4 18:04:44.403: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:1080/proxy/... (200; 76.497099ms)
Feb  4 18:04:44.403: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:162/proxy/: bar (200; 76.29246ms)
Feb  4 18:04:44.403: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/https:proxy-service-lldtx:tlsportname1/proxy/: tls baz (200; 76.925491ms)
Feb  4 18:04:44.404: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:443/proxy/... (200; 76.218436ms)
Feb  4 18:04:44.404: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/https:proxy-service-lldtx-txfn4:462/proxy/: tls qux (200; 72.469738ms)
Feb  4 18:04:44.404: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/http:proxy-service-lldtx:portname2/proxy/: bar (200; 78.854314ms)
Feb  4 18:04:44.409: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/services/proxy-service-lldtx:portname2/proxy/: bar (200; 84.22281ms)
Feb  4 18:04:44.409: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/proxy-service-lldtx-txfn4:160/proxy/: foo (200; 77.659023ms)
Feb  4 18:04:44.409: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-42pq4/pods/http:proxy-service-lldtx-txfn4:160/proxy/: foo (200; 77.485954ms)
STEP: deleting ReplicationController proxy-service-lldtx in namespace e2e-tests-proxy-42pq4, will wait for the garbage collector to delete the pods
Feb  4 18:04:44.497: INFO: Deleting ReplicationController proxy-service-lldtx took: 30.640898ms
Feb  4 18:04:44.599: INFO: Terminating ReplicationController proxy-service-lldtx pods took: 101.564619ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:04:51.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-42pq4" for this suite.
Feb  4 18:04:57.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:04:57.453: INFO: namespace: e2e-tests-proxy-42pq4, resource: bindings, ignored listing per whitelist
Feb  4 18:04:57.734: INFO: namespace e2e-tests-proxy-42pq4 deletion completed in 6.513129906s

• [SLOW TEST:28.343 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:04:57.741: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb  4 18:04:58.039: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb  4 18:04:58.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:04:58.783: INFO: stderr: ""
Feb  4 18:04:58.784: INFO: stdout: "service/redis-slave created\n"
Feb  4 18:04:58.784: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb  4 18:04:58.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:04:59.495: INFO: stderr: ""
Feb  4 18:04:59.495: INFO: stdout: "service/redis-master created\n"
Feb  4 18:04:59.496: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb  4 18:04:59.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:05:00.421: INFO: stderr: ""
Feb  4 18:05:00.421: INFO: stdout: "service/frontend created\n"
Feb  4 18:05:00.422: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb  4 18:05:00.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:05:01.099: INFO: stderr: ""
Feb  4 18:05:01.099: INFO: stdout: "deployment.extensions/frontend created\n"
Feb  4 18:05:01.100: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb  4 18:05:01.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:05:01.873: INFO: stderr: ""
Feb  4 18:05:01.873: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb  4 18:05:01.873: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb  4 18:05:01.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:05:03.123: INFO: stderr: ""
Feb  4 18:05:03.123: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb  4 18:05:03.123: INFO: Waiting for all frontend pods to be Running.
Feb  4 18:06:09.091: INFO: Waiting for frontend to serve content.
Feb  4 18:06:14.427: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb  4 18:06:24.968: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb  4 18:06:30.092: INFO: Trying to add a new entry to the guestbook.
Feb  4 18:06:30.121: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb  4 18:06:30.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:06:36.033: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  4 18:06:36.033: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb  4 18:06:36.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:06:36.334: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  4 18:06:36.334: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb  4 18:06:36.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:06:36.771: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  4 18:06:36.771: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb  4 18:06:36.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:06:37.104: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  4 18:06:37.104: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb  4 18:06:37.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:06:37.214: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  4 18:06:37.214: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb  4 18:06:37.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dtpxs'
Feb  4 18:06:37.663: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  4 18:06:37.663: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:06:37.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dtpxs" for this suite.
Feb  4 18:07:25.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:07:25.941: INFO: namespace: e2e-tests-kubectl-dtpxs, resource: bindings, ignored listing per whitelist
Feb  4 18:07:26.129: INFO: namespace e2e-tests-kubectl-dtpxs deletion completed in 48.438470249s

• [SLOW TEST:148.388 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:07:26.134: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-8gglq/configmap-test-ba6dbb3d-28a7-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 18:07:26.416: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba6eb853-28a7-11e9-9eab-8e57f341003b" in namespace "e2e-tests-configmap-8gglq" to be "success or failure"
Feb  4 18:07:26.421: INFO: Pod "pod-configmaps-ba6eb853-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.028739ms
Feb  4 18:07:28.436: INFO: Pod "pod-configmaps-ba6eb853-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019503577s
Feb  4 18:07:30.451: INFO: Pod "pod-configmaps-ba6eb853-28a7-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034289853s
STEP: Saw pod success
Feb  4 18:07:30.451: INFO: Pod "pod-configmaps-ba6eb853-28a7-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:07:30.455: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-configmaps-ba6eb853-28a7-11e9-9eab-8e57f341003b container env-test: <nil>
STEP: delete the pod
Feb  4 18:07:30.584: INFO: Waiting for pod pod-configmaps-ba6eb853-28a7-11e9-9eab-8e57f341003b to disappear
Feb  4 18:07:30.629: INFO: Pod pod-configmaps-ba6eb853-28a7-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:07:30.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8gglq" for this suite.
Feb  4 18:07:36.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:07:36.908: INFO: namespace: e2e-tests-configmap-8gglq, resource: bindings, ignored listing per whitelist
Feb  4 18:07:37.070: INFO: namespace e2e-tests-configmap-8gglq deletion completed in 6.434021125s

• [SLOW TEST:10.936 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:07:37.073: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb  4 18:07:37.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 create -f - --namespace=e2e-tests-kubectl-5q68f'
Feb  4 18:07:38.296: INFO: stderr: ""
Feb  4 18:07:38.296: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb  4 18:07:39.309: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 18:07:39.309: INFO: Found 0 / 1
Feb  4 18:07:40.303: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 18:07:40.303: INFO: Found 0 / 1
Feb  4 18:07:41.308: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 18:07:41.308: INFO: Found 1 / 1
Feb  4 18:07:41.308: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb  4 18:07:41.313: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 18:07:41.313: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  4 18:07:41.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 patch pod redis-master-r8gkl --namespace=e2e-tests-kubectl-5q68f -p {"metadata":{"annotations":{"x":"y"}}}'
Feb  4 18:07:41.603: INFO: stderr: ""
Feb  4 18:07:41.603: INFO: stdout: "pod/redis-master-r8gkl patched\n"
STEP: checking annotations
Feb  4 18:07:41.609: INFO: Selector matched 1 pods for map[app:redis]
Feb  4 18:07:41.609: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:07:41.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5q68f" for this suite.
Feb  4 18:08:05.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:08:05.988: INFO: namespace: e2e-tests-kubectl-5q68f, resource: bindings, ignored listing per whitelist
Feb  4 18:08:06.147: INFO: namespace e2e-tests-kubectl-5q68f deletion completed in 24.52585943s

• [SLOW TEST:29.076 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:08:06.148: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  4 18:08:06.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-g7qkg'
Feb  4 18:08:06.478: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  4 18:08:06.478: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb  4 18:08:06.509: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-dc495]
Feb  4 18:08:06.509: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-dc495" in namespace "e2e-tests-kubectl-g7qkg" to be "running and ready"
Feb  4 18:08:06.518: INFO: Pod "e2e-test-nginx-rc-dc495": Phase="Pending", Reason="", readiness=false. Elapsed: 8.66124ms
Feb  4 18:08:08.524: INFO: Pod "e2e-test-nginx-rc-dc495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015053838s
Feb  4 18:08:10.532: INFO: Pod "e2e-test-nginx-rc-dc495": Phase="Running", Reason="", readiness=true. Elapsed: 4.022668785s
Feb  4 18:08:10.535: INFO: Pod "e2e-test-nginx-rc-dc495" satisfied condition "running and ready"
Feb  4 18:08:10.535: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-dc495]
Feb  4 18:08:10.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-g7qkg'
Feb  4 18:08:10.760: INFO: stderr: ""
Feb  4 18:08:10.760: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Feb  4 18:08:10.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-296264527 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-g7qkg'
Feb  4 18:08:10.884: INFO: stderr: ""
Feb  4 18:08:10.884: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:08:10.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-g7qkg" for this suite.
Feb  4 18:08:17.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:08:17.352: INFO: namespace: e2e-tests-kubectl-g7qkg, resource: bindings, ignored listing per whitelist
Feb  4 18:08:17.427: INFO: namespace e2e-tests-kubectl-g7qkg deletion completed in 6.533805618s

• [SLOW TEST:11.280 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:08:17.432: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  4 18:08:17.745: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9073530-28a7-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-plnc9" to be "success or failure"
Feb  4 18:08:17.752: INFO: Pod "downwardapi-volume-d9073530-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.749049ms
Feb  4 18:08:19.763: INFO: Pod "downwardapi-volume-d9073530-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018006353s
Feb  4 18:08:21.769: INFO: Pod "downwardapi-volume-d9073530-28a7-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024670795s
STEP: Saw pod success
Feb  4 18:08:21.769: INFO: Pod "downwardapi-volume-d9073530-28a7-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:08:21.811: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod downwardapi-volume-d9073530-28a7-11e9-9eab-8e57f341003b container client-container: <nil>
STEP: delete the pod
Feb  4 18:08:21.972: INFO: Waiting for pod downwardapi-volume-d9073530-28a7-11e9-9eab-8e57f341003b to disappear
Feb  4 18:08:21.978: INFO: Pod downwardapi-volume-d9073530-28a7-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:08:21.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-plnc9" for this suite.
Feb  4 18:08:28.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:08:28.157: INFO: namespace: e2e-tests-projected-plnc9, resource: bindings, ignored listing per whitelist
Feb  4 18:08:28.409: INFO: namespace e2e-tests-projected-plnc9 deletion completed in 6.418430894s

• [SLOW TEST:10.981 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:08:28.413: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-df8bac53-28a7-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 18:08:28.698: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-df8d2ec2-28a7-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-bwcl6" to be "success or failure"
Feb  4 18:08:28.705: INFO: Pod "pod-projected-configmaps-df8d2ec2-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.368701ms
Feb  4 18:08:30.713: INFO: Pod "pod-projected-configmaps-df8d2ec2-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014766183s
Feb  4 18:08:32.719: INFO: Pod "pod-projected-configmaps-df8d2ec2-28a7-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021072352s
STEP: Saw pod success
Feb  4 18:08:32.719: INFO: Pod "pod-projected-configmaps-df8d2ec2-28a7-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:08:32.724: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-projected-configmaps-df8d2ec2-28a7-11e9-9eab-8e57f341003b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 18:08:32.835: INFO: Waiting for pod pod-projected-configmaps-df8d2ec2-28a7-11e9-9eab-8e57f341003b to disappear
Feb  4 18:08:32.842: INFO: Pod pod-projected-configmaps-df8d2ec2-28a7-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:08:32.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bwcl6" for this suite.
Feb  4 18:08:38.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:08:39.053: INFO: namespace: e2e-tests-projected-bwcl6, resource: bindings, ignored listing per whitelist
Feb  4 18:08:39.197: INFO: namespace e2e-tests-projected-bwcl6 deletion completed in 6.347551632s

• [SLOW TEST:10.784 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:08:39.202: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb  4 18:08:39.448: INFO: Waiting up to 5m0s for pod "pod-e5f1c6a2-28a7-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-8v5db" to be "success or failure"
Feb  4 18:08:39.452: INFO: Pod "pod-e5f1c6a2-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.425852ms
Feb  4 18:08:41.486: INFO: Pod "pod-e5f1c6a2-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037616752s
Feb  4 18:08:43.495: INFO: Pod "pod-e5f1c6a2-28a7-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046984727s
STEP: Saw pod success
Feb  4 18:08:43.495: INFO: Pod "pod-e5f1c6a2-28a7-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:08:43.500: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-e5f1c6a2-28a7-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 18:08:43.606: INFO: Waiting for pod pod-e5f1c6a2-28a7-11e9-9eab-8e57f341003b to disappear
Feb  4 18:08:43.629: INFO: Pod pod-e5f1c6a2-28a7-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:08:43.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8v5db" for this suite.
Feb  4 18:08:49.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:08:50.020: INFO: namespace: e2e-tests-emptydir-8v5db, resource: bindings, ignored listing per whitelist
Feb  4 18:08:50.084: INFO: namespace e2e-tests-emptydir-8v5db deletion completed in 6.432707852s

• [SLOW TEST:10.882 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:08:50.087: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-ec75a77a-28a7-11e9-9eab-8e57f341003b
STEP: Creating a pod to test consume configMaps
Feb  4 18:08:50.358: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ec773371-28a7-11e9-9eab-8e57f341003b" in namespace "e2e-tests-projected-t55qk" to be "success or failure"
Feb  4 18:08:50.370: INFO: Pod "pod-projected-configmaps-ec773371-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.530455ms
Feb  4 18:08:52.376: INFO: Pod "pod-projected-configmaps-ec773371-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018070068s
Feb  4 18:08:54.384: INFO: Pod "pod-projected-configmaps-ec773371-28a7-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025543585s
STEP: Saw pod success
Feb  4 18:08:54.384: INFO: Pod "pod-projected-configmaps-ec773371-28a7-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:08:54.389: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-projected-configmaps-ec773371-28a7-11e9-9eab-8e57f341003b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  4 18:08:54.433: INFO: Waiting for pod pod-projected-configmaps-ec773371-28a7-11e9-9eab-8e57f341003b to disappear
Feb  4 18:08:54.438: INFO: Pod pod-projected-configmaps-ec773371-28a7-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:08:54.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t55qk" for this suite.
Feb  4 18:09:00.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:09:00.864: INFO: namespace: e2e-tests-projected-t55qk, resource: bindings, ignored listing per whitelist
Feb  4 18:09:01.025: INFO: namespace e2e-tests-projected-t55qk deletion completed in 6.575249657s

• [SLOW TEST:10.939 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:09:01.030: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb  4 18:09:01.283: INFO: Waiting up to 5m0s for pod "pod-f2f9f253-28a7-11e9-9eab-8e57f341003b" in namespace "e2e-tests-emptydir-qgm56" to be "success or failure"
Feb  4 18:09:01.294: INFO: Pod "pod-f2f9f253-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.65494ms
Feb  4 18:09:03.303: INFO: Pod "pod-f2f9f253-28a7-11e9-9eab-8e57f341003b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019905838s
Feb  4 18:09:05.313: INFO: Pod "pod-f2f9f253-28a7-11e9-9eab-8e57f341003b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030356882s
STEP: Saw pod success
Feb  4 18:09:05.313: INFO: Pod "pod-f2f9f253-28a7-11e9-9eab-8e57f341003b" satisfied condition "success or failure"
Feb  4 18:09:05.319: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-t5p54 pod pod-f2f9f253-28a7-11e9-9eab-8e57f341003b container test-container: <nil>
STEP: delete the pod
Feb  4 18:09:05.510: INFO: Waiting for pod pod-f2f9f253-28a7-11e9-9eab-8e57f341003b to disappear
Feb  4 18:09:05.517: INFO: Pod pod-f2f9f253-28a7-11e9-9eab-8e57f341003b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:09:05.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qgm56" for this suite.
Feb  4 18:09:11.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:09:11.747: INFO: namespace: e2e-tests-emptydir-qgm56, resource: bindings, ignored listing per whitelist
Feb  4 18:09:11.926: INFO: namespace e2e-tests-emptydir-qgm56 deletion completed in 6.401083531s

• [SLOW TEST:10.897 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:09:11.932: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0204 18:09:52.399222      13 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  4 18:09:52.399: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:09:52.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-vgxgz" for this suite.
Feb  4 18:10:00.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:10:00.636: INFO: namespace: e2e-tests-gc-vgxgz, resource: bindings, ignored listing per whitelist
Feb  4 18:10:01.006: INFO: namespace e2e-tests-gc-vgxgz deletion completed in 8.600868612s

• [SLOW TEST:49.074 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  4 18:10:01.008: INFO: >>> kubeConfig: /tmp/kubeconfig-296264527
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb  4 18:10:01.323: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-bnv9r" to be "success or failure"
Feb  4 18:10:01.335: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 11.447692ms
Feb  4 18:10:03.358: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034786701s
Feb  4 18:10:05.365: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04203649s
STEP: Saw pod success
Feb  4 18:10:05.365: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb  4 18:10:05.372: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-61wiwtyxah-6rthf pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb  4 18:10:05.458: INFO: Waiting for pod pod-host-path-test to disappear
Feb  4 18:10:05.484: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  4 18:10:05.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-bnv9r" for this suite.
Feb  4 18:10:11.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  4 18:10:11.718: INFO: namespace: e2e-tests-hostpath-bnv9r, resource: bindings, ignored listing per whitelist
Feb  4 18:10:12.056: INFO: namespace e2e-tests-hostpath-bnv9r deletion completed in 6.562477454s

• [SLOW TEST:11.048 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSFeb  4 18:10:12.060: INFO: Running AfterSuite actions on all nodes
Feb  4 18:10:12.062: INFO: Running AfterSuite actions on node 1
Feb  4 18:10:12.062: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 6569.721 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h49m34.168668623s
Test Suite Passed
