I0208 22:20:15.821314      15 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-762798943
I0208 22:20:15.821497      15 e2e.go:224] Starting e2e run "b51f581c-2bef-11e9-a89e-229fb9a7b2a7" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1549664414 - Will randomize all specs
Will run 201 of 1946 specs

Feb  8 22:20:16.011: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 22:20:16.013: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb  8 22:20:16.022: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb  8 22:20:16.053: INFO: 17 / 17 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb  8 22:20:16.053: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Feb  8 22:20:16.053: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb  8 22:20:16.063: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb  8 22:20:16.063: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb  8 22:20:16.063: INFO: e2e test version: v1.13.0
Feb  8 22:20:16.064: INFO: kube-apiserver version: v1.13.2
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:20:16.065: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-probe
Feb  8 22:20:16.159: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:21:16.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-4n7sx" for this suite.
Feb  8 22:21:38.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:21:38.217: INFO: namespace: e2e-tests-container-probe-4n7sx, resource: bindings, ignored listing per whitelist
Feb  8 22:21:38.291: INFO: namespace e2e-tests-container-probe-4n7sx deletion completed in 22.112447343s

• [SLOW TEST:82.227 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:21:38.292: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:21:38.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zh4w7" for this suite.
Feb  8 22:22:00.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:22:00.524: INFO: namespace: e2e-tests-pods-zh4w7, resource: bindings, ignored listing per whitelist
Feb  8 22:22:00.524: INFO: namespace e2e-tests-pods-zh4w7 deletion completed in 22.115172935s

• [SLOW TEST:22.233 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:22:00.525: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb  8 22:22:00.601: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-k5qgt" to be "success or failure"
Feb  8 22:22:00.609: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 7.261282ms
Feb  8 22:22:02.616: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014556645s
Feb  8 22:22:04.619: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01758205s
STEP: Saw pod success
Feb  8 22:22:04.619: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb  8 22:22:04.622: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb  8 22:22:04.669: INFO: Waiting for pod pod-host-path-test to disappear
Feb  8 22:22:04.684: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:22:04.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-k5qgt" for this suite.
Feb  8 22:22:10.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:22:10.734: INFO: namespace: e2e-tests-hostpath-k5qgt, resource: bindings, ignored listing per whitelist
Feb  8 22:22:10.814: INFO: namespace e2e-tests-hostpath-k5qgt deletion completed in 6.125995008s

• [SLOW TEST:10.290 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:22:10.815: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-fa5a6aae-2bef-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 22:22:10.909: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fa5b25f1-2bef-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-cvjf7" to be "success or failure"
Feb  8 22:22:10.916: INFO: Pod "pod-projected-secrets-fa5b25f1-2bef-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.238087ms
Feb  8 22:22:12.921: INFO: Pod "pod-projected-secrets-fa5b25f1-2bef-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011496268s
Feb  8 22:22:14.925: INFO: Pod "pod-projected-secrets-fa5b25f1-2bef-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015662507s
STEP: Saw pod success
Feb  8 22:22:14.925: INFO: Pod "pod-projected-secrets-fa5b25f1-2bef-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:22:14.928: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-projected-secrets-fa5b25f1-2bef-11e9-a89e-229fb9a7b2a7 container secret-volume-test: <nil>
STEP: delete the pod
Feb  8 22:22:14.975: INFO: Waiting for pod pod-projected-secrets-fa5b25f1-2bef-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:22:14.978: INFO: Pod pod-projected-secrets-fa5b25f1-2bef-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:22:14.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cvjf7" for this suite.
Feb  8 22:22:20.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:22:21.015: INFO: namespace: e2e-tests-projected-cvjf7, resource: bindings, ignored listing per whitelist
Feb  8 22:22:21.095: INFO: namespace e2e-tests-projected-cvjf7 deletion completed in 6.112546984s

• [SLOW TEST:10.280 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:22:21.095: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-0079d538-2bf0-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 22:22:21.183: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-007a9cd6-2bf0-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-p2n4z" to be "success or failure"
Feb  8 22:22:21.193: INFO: Pod "pod-projected-configmaps-007a9cd6-2bf0-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.895027ms
Feb  8 22:22:23.199: INFO: Pod "pod-projected-configmaps-007a9cd6-2bf0-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016745776s
Feb  8 22:22:25.203: INFO: Pod "pod-projected-configmaps-007a9cd6-2bf0-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020557596s
STEP: Saw pod success
Feb  8 22:22:25.203: INFO: Pod "pod-projected-configmaps-007a9cd6-2bf0-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:22:25.207: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-projected-configmaps-007a9cd6-2bf0-11e9-a89e-229fb9a7b2a7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 22:22:25.238: INFO: Waiting for pod pod-projected-configmaps-007a9cd6-2bf0-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:22:25.248: INFO: Pod pod-projected-configmaps-007a9cd6-2bf0-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:22:25.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p2n4z" for this suite.
Feb  8 22:22:31.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:22:31.312: INFO: namespace: e2e-tests-projected-p2n4z, resource: bindings, ignored listing per whitelist
Feb  8 22:22:31.360: INFO: namespace e2e-tests-projected-p2n4z deletion completed in 6.107682957s

• [SLOW TEST:10.265 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:22:31.360: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-06983b6c-2bf0-11e9-a89e-229fb9a7b2a7
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:22:35.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8fm5w" for this suite.
Feb  8 22:22:57.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:22:57.564: INFO: namespace: e2e-tests-configmap-8fm5w, resource: bindings, ignored listing per whitelist
Feb  8 22:22:57.604: INFO: namespace e2e-tests-configmap-8fm5w deletion completed in 22.112774686s

• [SLOW TEST:26.244 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:22:57.604: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:23:01.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-bdqlm" for this suite.
Feb  8 22:23:07.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:23:07.865: INFO: namespace: e2e-tests-emptydir-wrapper-bdqlm, resource: bindings, ignored listing per whitelist
Feb  8 22:23:07.871: INFO: namespace e2e-tests-emptydir-wrapper-bdqlm deletion completed in 6.105253095s

• [SLOW TEST:10.267 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:23:07.871: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb  8 22:23:07.980: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-hdhch,SelfLink:/api/v1/namespaces/e2e-tests-watch-hdhch/configmaps/e2e-watch-test-resource-version,UID:1c5cd9e2-2bf0-11e9-a0ab-42010a8a0033,ResourceVersion:1851,Generation:0,CreationTimestamp:2019-02-08 22:23:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  8 22:23:07.981: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-hdhch,SelfLink:/api/v1/namespaces/e2e-tests-watch-hdhch/configmaps/e2e-watch-test-resource-version,UID:1c5cd9e2-2bf0-11e9-a0ab-42010a8a0033,ResourceVersion:1852,Generation:0,CreationTimestamp:2019-02-08 22:23:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:23:07.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-hdhch" for this suite.
Feb  8 22:23:13.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:23:14.067: INFO: namespace: e2e-tests-watch-hdhch, resource: bindings, ignored listing per whitelist
Feb  8 22:23:14.089: INFO: namespace e2e-tests-watch-hdhch deletion completed in 6.105429967s

• [SLOW TEST:6.219 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:23:14.090: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:23:14.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-m2rg9" for this suite.
Feb  8 22:23:36.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:23:36.222: INFO: namespace: e2e-tests-kubelet-test-m2rg9, resource: bindings, ignored listing per whitelist
Feb  8 22:23:36.309: INFO: namespace e2e-tests-kubelet-test-m2rg9 deletion completed in 22.125678371s

• [SLOW TEST:22.219 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:23:36.309: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 22:23:36.394: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d4f263b-2bf0-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-qntl2" to be "success or failure"
Feb  8 22:23:36.403: INFO: Pod "downwardapi-volume-2d4f263b-2bf0-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.280907ms
Feb  8 22:23:38.408: INFO: Pod "downwardapi-volume-2d4f263b-2bf0-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013755337s
Feb  8 22:23:40.412: INFO: Pod "downwardapi-volume-2d4f263b-2bf0-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017708226s
STEP: Saw pod success
Feb  8 22:23:40.412: INFO: Pod "downwardapi-volume-2d4f263b-2bf0-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:23:40.416: INFO: Trying to get logs from node netztbred3-worker-1 pod downwardapi-volume-2d4f263b-2bf0-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 22:23:40.448: INFO: Waiting for pod downwardapi-volume-2d4f263b-2bf0-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:23:40.457: INFO: Pod downwardapi-volume-2d4f263b-2bf0-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:23:40.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qntl2" for this suite.
Feb  8 22:23:46.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:23:46.544: INFO: namespace: e2e-tests-projected-qntl2, resource: bindings, ignored listing per whitelist
Feb  8 22:23:46.571: INFO: namespace e2e-tests-projected-qntl2 deletion completed in 6.109470362s

• [SLOW TEST:10.261 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:23:46.571: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb  8 22:23:47.174: INFO: created pod pod-service-account-defaultsa
Feb  8 22:23:47.174: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb  8 22:23:47.186: INFO: created pod pod-service-account-mountsa
Feb  8 22:23:47.186: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb  8 22:23:47.209: INFO: created pod pod-service-account-nomountsa
Feb  8 22:23:47.209: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb  8 22:23:47.220: INFO: created pod pod-service-account-defaultsa-mountspec
Feb  8 22:23:47.220: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb  8 22:23:47.239: INFO: created pod pod-service-account-mountsa-mountspec
Feb  8 22:23:47.239: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb  8 22:23:47.257: INFO: created pod pod-service-account-nomountsa-mountspec
Feb  8 22:23:47.257: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb  8 22:23:47.279: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb  8 22:23:47.279: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb  8 22:23:47.294: INFO: created pod pod-service-account-mountsa-nomountspec
Feb  8 22:23:47.294: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb  8 22:23:47.319: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb  8 22:23:47.319: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:23:47.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-ghl4q" for this suite.
Feb  8 22:23:53.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:23:53.418: INFO: namespace: e2e-tests-svcaccounts-ghl4q, resource: bindings, ignored listing per whitelist
Feb  8 22:23:53.476: INFO: namespace e2e-tests-svcaccounts-ghl4q deletion completed in 6.139154323s

• [SLOW TEST:6.905 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:23:53.476: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-r8wb5
Feb  8 22:23:57.563: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-r8wb5
STEP: checking the pod's current state and verifying that restartCount is present
Feb  8 22:23:57.566: INFO: Initial restart count of pod liveness-http is 0
Feb  8 22:24:15.605: INFO: Restart count of pod e2e-tests-container-probe-r8wb5/liveness-http is now 1 (18.039140444s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:24:15.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-r8wb5" for this suite.
Feb  8 22:24:21.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:24:21.776: INFO: namespace: e2e-tests-container-probe-r8wb5, resource: bindings, ignored listing per whitelist
Feb  8 22:24:21.789: INFO: namespace e2e-tests-container-probe-r8wb5 deletion completed in 6.157284266s

• [SLOW TEST:28.313 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:24:21.790: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-dvbd
STEP: Creating a pod to test atomic-volume-subpath
Feb  8 22:24:21.888: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dvbd" in namespace "e2e-tests-subpath-74xjt" to be "success or failure"
Feb  8 22:24:21.896: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.71434ms
Feb  8 22:24:23.900: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012027384s
Feb  8 22:24:25.904: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Running", Reason="", readiness=false. Elapsed: 4.016084494s
Feb  8 22:24:27.909: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Running", Reason="", readiness=false. Elapsed: 6.020305522s
Feb  8 22:24:29.913: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Running", Reason="", readiness=false. Elapsed: 8.024540601s
Feb  8 22:24:31.917: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Running", Reason="", readiness=false. Elapsed: 10.028805317s
Feb  8 22:24:33.922: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Running", Reason="", readiness=false. Elapsed: 12.033343285s
Feb  8 22:24:35.926: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Running", Reason="", readiness=false. Elapsed: 14.037478865s
Feb  8 22:24:37.930: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Running", Reason="", readiness=false. Elapsed: 16.041697813s
Feb  8 22:24:39.934: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Running", Reason="", readiness=false. Elapsed: 18.046114842s
Feb  8 22:24:41.939: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Running", Reason="", readiness=false. Elapsed: 20.050459917s
Feb  8 22:24:43.943: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Running", Reason="", readiness=false. Elapsed: 22.055000304s
Feb  8 22:24:45.954: INFO: Pod "pod-subpath-test-projected-dvbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.065153291s
STEP: Saw pod success
Feb  8 22:24:45.954: INFO: Pod "pod-subpath-test-projected-dvbd" satisfied condition "success or failure"
Feb  8 22:24:45.957: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-subpath-test-projected-dvbd container test-container-subpath-projected-dvbd: <nil>
STEP: delete the pod
Feb  8 22:24:45.993: INFO: Waiting for pod pod-subpath-test-projected-dvbd to disappear
Feb  8 22:24:46.008: INFO: Pod pod-subpath-test-projected-dvbd no longer exists
STEP: Deleting pod pod-subpath-test-projected-dvbd
Feb  8 22:24:46.008: INFO: Deleting pod "pod-subpath-test-projected-dvbd" in namespace "e2e-tests-subpath-74xjt"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:24:46.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-74xjt" for this suite.
Feb  8 22:24:52.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:24:52.081: INFO: namespace: e2e-tests-subpath-74xjt, resource: bindings, ignored listing per whitelist
Feb  8 22:24:52.130: INFO: namespace e2e-tests-subpath-74xjt deletion completed in 6.107964546s

• [SLOW TEST:30.341 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:24:52.130: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 22:24:52.250: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5a84963f-2bf0-11e9-a0ab-42010a8a0033", Controller:(*bool)(0xc00132fb96), BlockOwnerDeletion:(*bool)(0xc00132fb97)}}
Feb  8 22:24:52.265: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5a809a30-2bf0-11e9-a0ab-42010a8a0033", Controller:(*bool)(0xc0005cae16), BlockOwnerDeletion:(*bool)(0xc0005cae17)}}
Feb  8 22:24:52.275: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5a81f68c-2bf0-11e9-a0ab-42010a8a0033", Controller:(*bool)(0xc000dbdbee), BlockOwnerDeletion:(*bool)(0xc000dbdbef)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:24:57.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-pz862" for this suite.
Feb  8 22:25:03.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:25:03.374: INFO: namespace: e2e-tests-gc-pz862, resource: bindings, ignored listing per whitelist
Feb  8 22:25:03.409: INFO: namespace e2e-tests-gc-pz862 deletion completed in 6.110730263s

• [SLOW TEST:11.279 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:25:03.410: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb  8 22:25:03.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:03.898: INFO: stderr: ""
Feb  8 22:25:03.898: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  8 22:25:03.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:03.987: INFO: stderr: ""
Feb  8 22:25:03.987: INFO: stdout: "update-demo-nautilus-4zh7w update-demo-nautilus-54jg5 "
Feb  8 22:25:03.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-4zh7w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:04.066: INFO: stderr: ""
Feb  8 22:25:04.066: INFO: stdout: ""
Feb  8 22:25:04.066: INFO: update-demo-nautilus-4zh7w is created but not running
Feb  8 22:25:09.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:09.144: INFO: stderr: ""
Feb  8 22:25:09.144: INFO: stdout: "update-demo-nautilus-4zh7w update-demo-nautilus-54jg5 "
Feb  8 22:25:09.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-4zh7w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:09.219: INFO: stderr: ""
Feb  8 22:25:09.219: INFO: stdout: "true"
Feb  8 22:25:09.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-4zh7w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:09.297: INFO: stderr: ""
Feb  8 22:25:09.297: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  8 22:25:09.297: INFO: validating pod update-demo-nautilus-4zh7w
Feb  8 22:25:09.305: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  8 22:25:09.305: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  8 22:25:09.305: INFO: update-demo-nautilus-4zh7w is verified up and running
Feb  8 22:25:09.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-54jg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:09.381: INFO: stderr: ""
Feb  8 22:25:09.381: INFO: stdout: "true"
Feb  8 22:25:09.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-54jg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:09.465: INFO: stderr: ""
Feb  8 22:25:09.466: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  8 22:25:09.466: INFO: validating pod update-demo-nautilus-54jg5
Feb  8 22:25:09.473: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  8 22:25:09.473: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  8 22:25:09.473: INFO: update-demo-nautilus-54jg5 is verified up and running
STEP: rolling-update to new replication controller
Feb  8 22:25:09.474: INFO: scanned /root for discovery docs: <nil>
Feb  8 22:25:09.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:31.931: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb  8 22:25:31.931: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  8 22:25:31.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:32.015: INFO: stderr: ""
Feb  8 22:25:32.015: INFO: stdout: "update-demo-kitten-2ntkx update-demo-kitten-57lff "
Feb  8 22:25:32.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-kitten-2ntkx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:32.087: INFO: stderr: ""
Feb  8 22:25:32.087: INFO: stdout: "true"
Feb  8 22:25:32.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-kitten-2ntkx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:32.184: INFO: stderr: ""
Feb  8 22:25:32.184: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb  8 22:25:32.184: INFO: validating pod update-demo-kitten-2ntkx
Feb  8 22:25:32.191: INFO: got data: {
  "image": "kitten.jpg"
}

Feb  8 22:25:32.191: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb  8 22:25:32.191: INFO: update-demo-kitten-2ntkx is verified up and running
Feb  8 22:25:32.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-kitten-57lff -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:32.267: INFO: stderr: ""
Feb  8 22:25:32.267: INFO: stdout: "true"
Feb  8 22:25:32.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-kitten-57lff -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-phnfk'
Feb  8 22:25:32.349: INFO: stderr: ""
Feb  8 22:25:32.349: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb  8 22:25:32.349: INFO: validating pod update-demo-kitten-57lff
Feb  8 22:25:32.356: INFO: got data: {
  "image": "kitten.jpg"
}

Feb  8 22:25:32.356: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb  8 22:25:32.356: INFO: update-demo-kitten-57lff is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:25:32.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-phnfk" for this suite.
Feb  8 22:25:54.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:25:54.404: INFO: namespace: e2e-tests-kubectl-phnfk, resource: bindings, ignored listing per whitelist
Feb  8 22:25:54.464: INFO: namespace e2e-tests-kubectl-phnfk deletion completed in 22.103504001s

• [SLOW TEST:51.055 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:25:54.465: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-gw24
STEP: Creating a pod to test atomic-volume-subpath
Feb  8 22:25:54.555: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gw24" in namespace "e2e-tests-subpath-4m7fw" to be "success or failure"
Feb  8 22:25:54.567: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Pending", Reason="", readiness=false. Elapsed: 11.847966ms
Feb  8 22:25:56.571: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016225134s
Feb  8 22:25:58.575: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Running", Reason="", readiness=false. Elapsed: 4.020397593s
Feb  8 22:26:00.579: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Running", Reason="", readiness=false. Elapsed: 6.024451756s
Feb  8 22:26:02.583: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Running", Reason="", readiness=false. Elapsed: 8.028381342s
Feb  8 22:26:04.587: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Running", Reason="", readiness=false. Elapsed: 10.032312848s
Feb  8 22:26:06.592: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Running", Reason="", readiness=false. Elapsed: 12.03656993s
Feb  8 22:26:08.596: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Running", Reason="", readiness=false. Elapsed: 14.040996252s
Feb  8 22:26:10.600: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Running", Reason="", readiness=false. Elapsed: 16.044830677s
Feb  8 22:26:12.604: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Running", Reason="", readiness=false. Elapsed: 18.049086117s
Feb  8 22:26:14.608: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Running", Reason="", readiness=false. Elapsed: 20.053069327s
Feb  8 22:26:16.612: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Running", Reason="", readiness=false. Elapsed: 22.057136326s
Feb  8 22:26:18.616: INFO: Pod "pod-subpath-test-configmap-gw24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.060938011s
STEP: Saw pod success
Feb  8 22:26:18.616: INFO: Pod "pod-subpath-test-configmap-gw24" satisfied condition "success or failure"
Feb  8 22:26:18.619: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-subpath-test-configmap-gw24 container test-container-subpath-configmap-gw24: <nil>
STEP: delete the pod
Feb  8 22:26:18.650: INFO: Waiting for pod pod-subpath-test-configmap-gw24 to disappear
Feb  8 22:26:18.653: INFO: Pod pod-subpath-test-configmap-gw24 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gw24
Feb  8 22:26:18.653: INFO: Deleting pod "pod-subpath-test-configmap-gw24" in namespace "e2e-tests-subpath-4m7fw"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:26:18.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-4m7fw" for this suite.
Feb  8 22:26:24.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:26:24.698: INFO: namespace: e2e-tests-subpath-4m7fw, resource: bindings, ignored listing per whitelist
Feb  8 22:26:24.765: INFO: namespace e2e-tests-subpath-4m7fw deletion completed in 6.104812862s

• [SLOW TEST:30.300 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:26:24.765: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  8 22:26:27.383: INFO: Successfully updated pod "labelsupdate91b71edc-2bf0-11e9-a89e-229fb9a7b2a7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:26:29.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kskt8" for this suite.
Feb  8 22:26:51.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:26:51.519: INFO: namespace: e2e-tests-downward-api-kskt8, resource: bindings, ignored listing per whitelist
Feb  8 22:26:51.531: INFO: namespace e2e-tests-downward-api-kskt8 deletion completed in 22.12273021s

• [SLOW TEST:26.766 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:26:51.531: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-a1aee392-2bf0-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 22:26:51.641: INFO: Waiting up to 5m0s for pod "pod-configmaps-a1afa560-2bf0-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-configmap-76rvw" to be "success or failure"
Feb  8 22:26:51.645: INFO: Pod "pod-configmaps-a1afa560-2bf0-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247771ms
Feb  8 22:26:53.650: INFO: Pod "pod-configmaps-a1afa560-2bf0-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008877155s
Feb  8 22:26:55.655: INFO: Pod "pod-configmaps-a1afa560-2bf0-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013345813s
STEP: Saw pod success
Feb  8 22:26:55.655: INFO: Pod "pod-configmaps-a1afa560-2bf0-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:26:55.658: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-configmaps-a1afa560-2bf0-11e9-a89e-229fb9a7b2a7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 22:26:55.687: INFO: Waiting for pod pod-configmaps-a1afa560-2bf0-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:26:55.696: INFO: Pod pod-configmaps-a1afa560-2bf0-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:26:55.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-76rvw" for this suite.
Feb  8 22:27:01.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:27:01.776: INFO: namespace: e2e-tests-configmap-76rvw, resource: bindings, ignored listing per whitelist
Feb  8 22:27:01.805: INFO: namespace e2e-tests-configmap-76rvw deletion completed in 6.102861588s

• [SLOW TEST:10.274 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:27:01.806: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 22:27:01.876: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:27:07.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-55nx7" for this suite.
Feb  8 22:27:47.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:27:48.008: INFO: namespace: e2e-tests-pods-55nx7, resource: bindings, ignored listing per whitelist
Feb  8 22:27:48.034: INFO: namespace e2e-tests-pods-55nx7 deletion completed in 40.109680634s

• [SLOW TEST:46.229 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:27:48.035: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 22:27:48.123: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb  8 22:27:48.134: INFO: Number of nodes with available pods: 0
Feb  8 22:27:48.134: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb  8 22:27:48.173: INFO: Number of nodes with available pods: 0
Feb  8 22:27:48.173: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:27:49.177: INFO: Number of nodes with available pods: 0
Feb  8 22:27:49.177: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:27:50.177: INFO: Number of nodes with available pods: 0
Feb  8 22:27:50.177: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:27:51.177: INFO: Number of nodes with available pods: 1
Feb  8 22:27:51.177: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb  8 22:27:51.204: INFO: Number of nodes with available pods: 1
Feb  8 22:27:51.204: INFO: Number of running nodes: 0, number of available pods: 1
Feb  8 22:27:52.208: INFO: Number of nodes with available pods: 0
Feb  8 22:27:52.208: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb  8 22:27:52.231: INFO: Number of nodes with available pods: 0
Feb  8 22:27:52.231: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:27:53.237: INFO: Number of nodes with available pods: 0
Feb  8 22:27:53.237: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:27:54.236: INFO: Number of nodes with available pods: 0
Feb  8 22:27:54.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:27:55.236: INFO: Number of nodes with available pods: 0
Feb  8 22:27:55.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:27:56.236: INFO: Number of nodes with available pods: 0
Feb  8 22:27:56.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:27:57.236: INFO: Number of nodes with available pods: 0
Feb  8 22:27:57.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:27:58.236: INFO: Number of nodes with available pods: 0
Feb  8 22:27:58.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:27:59.236: INFO: Number of nodes with available pods: 0
Feb  8 22:27:59.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:00.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:00.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:01.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:01.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:02.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:02.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:03.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:03.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:04.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:04.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:05.235: INFO: Number of nodes with available pods: 0
Feb  8 22:28:05.235: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:06.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:06.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:07.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:07.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:08.237: INFO: Number of nodes with available pods: 0
Feb  8 22:28:08.237: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:09.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:09.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:10.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:10.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:11.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:11.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:12.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:12.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:13.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:13.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:14.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:14.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:15.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:15.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:16.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:16.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:17.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:17.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:18.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:18.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:19.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:19.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:20.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:20.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:21.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:21.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:22.235: INFO: Number of nodes with available pods: 0
Feb  8 22:28:22.235: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:23.242: INFO: Number of nodes with available pods: 0
Feb  8 22:28:23.242: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:24.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:24.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:25.235: INFO: Number of nodes with available pods: 0
Feb  8 22:28:25.235: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:26.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:26.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:27.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:27.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:28.237: INFO: Number of nodes with available pods: 0
Feb  8 22:28:28.237: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:29.236: INFO: Number of nodes with available pods: 0
Feb  8 22:28:29.236: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:28:30.236: INFO: Number of nodes with available pods: 1
Feb  8 22:28:30.236: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-k6dgq, will wait for the garbage collector to delete the pods
Feb  8 22:28:30.310: INFO: Deleting DaemonSet.extensions daemon-set took: 15.101858ms
Feb  8 22:28:30.410: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.296089ms
Feb  8 22:29:04.115: INFO: Number of nodes with available pods: 0
Feb  8 22:29:04.115: INFO: Number of running nodes: 0, number of available pods: 0
Feb  8 22:29:04.120: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-k6dgq/daemonsets","resourceVersion":"3021"},"items":null}

Feb  8 22:29:04.124: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-k6dgq/pods","resourceVersion":"3021"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:29:04.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-k6dgq" for this suite.
Feb  8 22:29:10.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:29:10.231: INFO: namespace: e2e-tests-daemonsets-k6dgq, resource: bindings, ignored listing per whitelist
Feb  8 22:29:10.265: INFO: namespace e2e-tests-daemonsets-k6dgq deletion completed in 6.104861915s

• [SLOW TEST:82.231 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:29:10.266: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb  8 22:29:10.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 api-versions'
Feb  8 22:29:10.425: INFO: stderr: ""
Feb  8 22:29:10.425: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:29:10.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qp6tp" for this suite.
Feb  8 22:29:16.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:29:16.508: INFO: namespace: e2e-tests-kubectl-qp6tp, resource: bindings, ignored listing per whitelist
Feb  8 22:29:16.539: INFO: namespace e2e-tests-kubectl-qp6tp deletion completed in 6.109572859s

• [SLOW TEST:6.274 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:29:16.540: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb  8 22:29:16.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-748h9'
Feb  8 22:29:16.794: INFO: stderr: ""
Feb  8 22:29:16.794: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb  8 22:29:17.798: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 22:29:17.798: INFO: Found 0 / 1
Feb  8 22:29:18.798: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 22:29:18.798: INFO: Found 0 / 1
Feb  8 22:29:19.798: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 22:29:19.798: INFO: Found 0 / 1
Feb  8 22:29:20.798: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 22:29:20.798: INFO: Found 1 / 1
Feb  8 22:29:20.798: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb  8 22:29:20.801: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 22:29:20.801: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  8 22:29:20.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 patch pod redis-master-9cwlz --namespace=e2e-tests-kubectl-748h9 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb  8 22:29:20.889: INFO: stderr: ""
Feb  8 22:29:20.889: INFO: stdout: "pod/redis-master-9cwlz patched\n"
STEP: checking annotations
Feb  8 22:29:20.892: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 22:29:20.892: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:29:20.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-748h9" for this suite.
Feb  8 22:29:42.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:29:42.996: INFO: namespace: e2e-tests-kubectl-748h9, resource: bindings, ignored listing per whitelist
Feb  8 22:29:43.010: INFO: namespace e2e-tests-kubectl-748h9 deletion completed in 22.113403785s

• [SLOW TEST:26.470 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:29:43.010: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 22:29:43.098: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07e23c93-2bf1-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-4rrzt" to be "success or failure"
Feb  8 22:29:43.111: INFO: Pod "downwardapi-volume-07e23c93-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.352159ms
Feb  8 22:29:45.116: INFO: Pod "downwardapi-volume-07e23c93-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018001758s
Feb  8 22:29:47.120: INFO: Pod "downwardapi-volume-07e23c93-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02179787s
STEP: Saw pod success
Feb  8 22:29:47.120: INFO: Pod "downwardapi-volume-07e23c93-2bf1-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:29:47.123: INFO: Trying to get logs from node netztbred3-worker-1 pod downwardapi-volume-07e23c93-2bf1-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 22:29:47.157: INFO: Waiting for pod downwardapi-volume-07e23c93-2bf1-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:29:47.163: INFO: Pod downwardapi-volume-07e23c93-2bf1-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:29:47.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4rrzt" for this suite.
Feb  8 22:29:53.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:29:53.264: INFO: namespace: e2e-tests-projected-4rrzt, resource: bindings, ignored listing per whitelist
Feb  8 22:29:53.276: INFO: namespace e2e-tests-projected-4rrzt deletion completed in 6.109594588s

• [SLOW TEST:10.267 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:29:53.277: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb  8 22:30:01.418: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  8 22:30:01.422: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  8 22:30:03.422: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  8 22:30:03.426: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  8 22:30:05.422: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  8 22:30:05.426: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  8 22:30:07.422: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  8 22:30:07.426: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  8 22:30:09.422: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  8 22:30:09.426: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  8 22:30:11.422: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  8 22:30:11.426: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  8 22:30:13.422: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  8 22:30:13.427: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  8 22:30:15.422: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  8 22:30:15.427: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  8 22:30:17.422: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  8 22:30:17.427: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  8 22:30:19.422: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  8 22:30:19.426: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  8 22:30:21.422: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  8 22:30:21.426: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:30:21.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-bzbmm" for this suite.
Feb  8 22:30:43.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:30:43.505: INFO: namespace: e2e-tests-container-lifecycle-hook-bzbmm, resource: bindings, ignored listing per whitelist
Feb  8 22:30:43.560: INFO: namespace e2e-tests-container-lifecycle-hook-bzbmm deletion completed in 22.117550707s

• [SLOW TEST:50.283 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:30:43.560: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:30:49.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-dhh2w" for this suite.
Feb  8 22:30:55.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:30:55.963: INFO: namespace: e2e-tests-namespaces-dhh2w, resource: bindings, ignored listing per whitelist
Feb  8 22:30:55.963: INFO: namespace e2e-tests-namespaces-dhh2w deletion completed in 6.113522764s
STEP: Destroying namespace "e2e-tests-nsdeletetest-l2kk9" for this suite.
Feb  8 22:30:55.965: INFO: Namespace e2e-tests-nsdeletetest-l2kk9 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-m7ztg" for this suite.
Feb  8 22:31:01.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:31:02.050: INFO: namespace: e2e-tests-nsdeletetest-m7ztg, resource: bindings, ignored listing per whitelist
Feb  8 22:31:02.082: INFO: namespace e2e-tests-nsdeletetest-m7ztg deletion completed in 6.117077231s

• [SLOW TEST:18.523 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:31:02.083: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-37029c92-2bf1-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 22:31:02.171: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-37035ced-2bf1-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-xj6cf" to be "success or failure"
Feb  8 22:31:02.181: INFO: Pod "pod-projected-secrets-37035ced-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.113319ms
Feb  8 22:31:04.185: INFO: Pod "pod-projected-secrets-37035ced-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014067095s
STEP: Saw pod success
Feb  8 22:31:04.185: INFO: Pod "pod-projected-secrets-37035ced-2bf1-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:31:04.188: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-projected-secrets-37035ced-2bf1-11e9-a89e-229fb9a7b2a7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  8 22:31:04.215: INFO: Waiting for pod pod-projected-secrets-37035ced-2bf1-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:31:04.219: INFO: Pod pod-projected-secrets-37035ced-2bf1-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:31:04.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xj6cf" for this suite.
Feb  8 22:31:10.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:31:10.320: INFO: namespace: e2e-tests-projected-xj6cf, resource: bindings, ignored listing per whitelist
Feb  8 22:31:10.335: INFO: namespace e2e-tests-projected-xj6cf deletion completed in 6.111424038s

• [SLOW TEST:8.252 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:31:10.335: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  8 22:31:10.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:31:10.496: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  8 22:31:10.496: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb  8 22:31:10.509: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Feb  8 22:31:10.515: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb  8 22:31:10.523: INFO: scanned /root for discovery docs: <nil>
Feb  8 22:31:10.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:31:26.368: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb  8 22:31:26.368: INFO: stdout: "Created e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e\nScaling up e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb  8 22:31:26.368: INFO: stdout: "Created e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e\nScaling up e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb  8 22:31:26.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:31:26.448: INFO: stderr: ""
Feb  8 22:31:26.448: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:31:31.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:31:31.528: INFO: stderr: ""
Feb  8 22:31:31.528: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:31:36.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:31:36.611: INFO: stderr: ""
Feb  8 22:31:36.612: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:31:41.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:31:41.692: INFO: stderr: ""
Feb  8 22:31:41.692: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:31:46.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:31:46.773: INFO: stderr: ""
Feb  8 22:31:46.773: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:31:51.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:31:51.850: INFO: stderr: ""
Feb  8 22:31:51.850: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:31:56.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:31:56.929: INFO: stderr: ""
Feb  8 22:31:56.929: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:32:01.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:02.018: INFO: stderr: ""
Feb  8 22:32:02.019: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:32:07.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:07.116: INFO: stderr: ""
Feb  8 22:32:07.116: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:32:12.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:12.217: INFO: stderr: ""
Feb  8 22:32:12.217: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:32:17.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:17.292: INFO: stderr: ""
Feb  8 22:32:17.292: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:32:22.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:22.368: INFO: stderr: ""
Feb  8 22:32:22.368: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:32:27.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:27.450: INFO: stderr: ""
Feb  8 22:32:27.450: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:32:32.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:32.547: INFO: stderr: ""
Feb  8 22:32:32.547: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:32:37.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:37.631: INFO: stderr: ""
Feb  8 22:32:37.631: INFO: stdout: "e2e-test-nginx-rc-f5hd2 e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb  8 22:32:42.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:42.711: INFO: stderr: ""
Feb  8 22:32:42.711: INFO: stdout: "e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d "
Feb  8 22:32:42.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:42.788: INFO: stderr: ""
Feb  8 22:32:42.788: INFO: stdout: "true"
Feb  8 22:32:42.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:42.865: INFO: stderr: ""
Feb  8 22:32:42.865: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb  8 22:32:42.865: INFO: e2e-test-nginx-rc-fff1790337d3773510882dc3a55d062e-znp9d is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Feb  8 22:32:42.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ckhr2'
Feb  8 22:32:42.965: INFO: stderr: ""
Feb  8 22:32:42.965: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:32:42.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ckhr2" for this suite.
Feb  8 22:33:04.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:33:05.043: INFO: namespace: e2e-tests-kubectl-ckhr2, resource: bindings, ignored listing per whitelist
Feb  8 22:33:05.105: INFO: namespace e2e-tests-kubectl-ckhr2 deletion completed in 22.129559905s

• [SLOW TEST:114.770 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:33:05.105: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 22:33:05.190: INFO: Creating ReplicaSet my-hostname-basic-80586c9f-2bf1-11e9-a89e-229fb9a7b2a7
Feb  8 22:33:05.202: INFO: Pod name my-hostname-basic-80586c9f-2bf1-11e9-a89e-229fb9a7b2a7: Found 0 pods out of 1
Feb  8 22:33:10.209: INFO: Pod name my-hostname-basic-80586c9f-2bf1-11e9-a89e-229fb9a7b2a7: Found 1 pods out of 1
Feb  8 22:33:10.209: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-80586c9f-2bf1-11e9-a89e-229fb9a7b2a7" is running
Feb  8 22:33:10.224: INFO: Pod "my-hostname-basic-80586c9f-2bf1-11e9-a89e-229fb9a7b2a7-v8sdk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-08 22:33:05 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-08 22:33:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-08 22:33:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-08 22:33:05 +0000 UTC Reason: Message:}])
Feb  8 22:33:10.224: INFO: Trying to dial the pod
Feb  8 22:33:15.238: INFO: Controller my-hostname-basic-80586c9f-2bf1-11e9-a89e-229fb9a7b2a7: Got expected result from replica 1 [my-hostname-basic-80586c9f-2bf1-11e9-a89e-229fb9a7b2a7-v8sdk]: "my-hostname-basic-80586c9f-2bf1-11e9-a89e-229fb9a7b2a7-v8sdk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:33:15.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-7cchn" for this suite.
Feb  8 22:33:21.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:33:21.291: INFO: namespace: e2e-tests-replicaset-7cchn, resource: bindings, ignored listing per whitelist
Feb  8 22:33:21.363: INFO: namespace e2e-tests-replicaset-7cchn deletion completed in 6.120864682s

• [SLOW TEST:16.258 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:33:21.363: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-8a068bc2-2bf1-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 22:33:21.446: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a07470a-2bf1-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-configmap-gw2b7" to be "success or failure"
Feb  8 22:33:21.451: INFO: Pod "pod-configmaps-8a07470a-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.382834ms
Feb  8 22:33:23.455: INFO: Pod "pod-configmaps-8a07470a-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009351103s
Feb  8 22:33:25.459: INFO: Pod "pod-configmaps-8a07470a-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012986838s
STEP: Saw pod success
Feb  8 22:33:25.459: INFO: Pod "pod-configmaps-8a07470a-2bf1-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:33:25.463: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-configmaps-8a07470a-2bf1-11e9-a89e-229fb9a7b2a7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 22:33:25.494: INFO: Waiting for pod pod-configmaps-8a07470a-2bf1-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:33:25.498: INFO: Pod pod-configmaps-8a07470a-2bf1-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:33:25.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gw2b7" for this suite.
Feb  8 22:33:31.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:33:31.588: INFO: namespace: e2e-tests-configmap-gw2b7, resource: bindings, ignored listing per whitelist
Feb  8 22:33:31.603: INFO: namespace e2e-tests-configmap-gw2b7 deletion completed in 6.101414097s

• [SLOW TEST:10.240 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:33:31.603: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb  8 22:33:31.681: INFO: Waiting up to 5m0s for pod "pod-9021053f-2bf1-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-nmcqd" to be "success or failure"
Feb  8 22:33:31.685: INFO: Pod "pod-9021053f-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.683114ms
Feb  8 22:33:33.689: INFO: Pod "pod-9021053f-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008749151s
STEP: Saw pod success
Feb  8 22:33:33.689: INFO: Pod "pod-9021053f-2bf1-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:33:33.692: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-9021053f-2bf1-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 22:33:33.719: INFO: Waiting for pod pod-9021053f-2bf1-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:33:33.726: INFO: Pod pod-9021053f-2bf1-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:33:33.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nmcqd" for this suite.
Feb  8 22:33:39.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:33:39.826: INFO: namespace: e2e-tests-emptydir-nmcqd, resource: bindings, ignored listing per whitelist
Feb  8 22:33:39.841: INFO: namespace e2e-tests-emptydir-nmcqd deletion completed in 6.111828848s

• [SLOW TEST:8.238 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:33:39.841: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Feb  8 22:33:39.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-s27xj'
Feb  8 22:33:40.104: INFO: stderr: ""
Feb  8 22:33:40.104: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb  8 22:33:41.109: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 22:33:41.109: INFO: Found 0 / 1
Feb  8 22:33:42.109: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 22:33:42.109: INFO: Found 0 / 1
Feb  8 22:33:43.109: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 22:33:43.109: INFO: Found 1 / 1
Feb  8 22:33:43.109: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  8 22:33:43.112: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 22:33:43.112: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb  8 22:33:43.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 logs redis-master-kdqh4 redis-master --namespace=e2e-tests-kubectl-s27xj'
Feb  8 22:33:43.199: INFO: stderr: ""
Feb  8 22:33:43.199: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Feb 22:33:41.493 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Feb 22:33:41.493 # Server started, Redis version 3.2.12\n1:M 08 Feb 22:33:41.493 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Feb 22:33:41.493 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb  8 22:33:43.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 log redis-master-kdqh4 redis-master --namespace=e2e-tests-kubectl-s27xj --tail=1'
Feb  8 22:33:43.284: INFO: stderr: ""
Feb  8 22:33:43.284: INFO: stdout: "1:M 08 Feb 22:33:41.493 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb  8 22:33:43.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 log redis-master-kdqh4 redis-master --namespace=e2e-tests-kubectl-s27xj --limit-bytes=1'
Feb  8 22:33:43.371: INFO: stderr: ""
Feb  8 22:33:43.371: INFO: stdout: " "
STEP: exposing timestamps
Feb  8 22:33:43.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 log redis-master-kdqh4 redis-master --namespace=e2e-tests-kubectl-s27xj --tail=1 --timestamps'
Feb  8 22:33:43.481: INFO: stderr: ""
Feb  8 22:33:43.481: INFO: stdout: "2019-02-08T22:33:41.49383138Z 1:M 08 Feb 22:33:41.493 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb  8 22:33:45.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 log redis-master-kdqh4 redis-master --namespace=e2e-tests-kubectl-s27xj --since=1s'
Feb  8 22:33:46.080: INFO: stderr: ""
Feb  8 22:33:46.080: INFO: stdout: ""
Feb  8 22:33:46.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 log redis-master-kdqh4 redis-master --namespace=e2e-tests-kubectl-s27xj --since=24h'
Feb  8 22:33:46.189: INFO: stderr: ""
Feb  8 22:33:46.189: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Feb 22:33:41.493 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Feb 22:33:41.493 # Server started, Redis version 3.2.12\n1:M 08 Feb 22:33:41.493 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Feb 22:33:41.493 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Feb  8 22:33:46.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-s27xj'
Feb  8 22:33:46.283: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  8 22:33:46.283: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb  8 22:33:46.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-s27xj'
Feb  8 22:33:46.370: INFO: stderr: "No resources found.\n"
Feb  8 22:33:46.370: INFO: stdout: ""
Feb  8 22:33:46.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -l name=nginx --namespace=e2e-tests-kubectl-s27xj -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  8 22:33:46.454: INFO: stderr: ""
Feb  8 22:33:46.454: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:33:46.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-s27xj" for this suite.
Feb  8 22:34:08.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:34:08.547: INFO: namespace: e2e-tests-kubectl-s27xj, resource: bindings, ignored listing per whitelist
Feb  8 22:34:08.574: INFO: namespace e2e-tests-kubectl-s27xj deletion completed in 22.114530255s

• [SLOW TEST:28.733 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:34:08.574: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 22:34:08.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 version'
Feb  8 22:34:08.708: INFO: stderr: ""
Feb  8 22:34:08.709: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.2\", GitCommit:\"cff46ab41ff0bb44d8584413b598ad8360ec1def\", GitTreeState:\"clean\", BuildDate:\"2019-01-10T23:28:14Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:34:08.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hhlpc" for this suite.
Feb  8 22:34:14.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:34:14.816: INFO: namespace: e2e-tests-kubectl-hhlpc, resource: bindings, ignored listing per whitelist
Feb  8 22:34:14.823: INFO: namespace e2e-tests-kubectl-hhlpc deletion completed in 6.110051652s

• [SLOW TEST:6.249 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:34:14.823: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb  8 22:34:14.911: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-n899s,SelfLink:/api/v1/namespaces/e2e-tests-watch-n899s/configmaps/e2e-watch-test-label-changed,UID:a9e3a192-2bf1-11e9-a0ab-42010a8a0033,ResourceVersion:3939,Generation:0,CreationTimestamp:2019-02-08 22:34:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  8 22:34:14.911: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-n899s,SelfLink:/api/v1/namespaces/e2e-tests-watch-n899s/configmaps/e2e-watch-test-label-changed,UID:a9e3a192-2bf1-11e9-a0ab-42010a8a0033,ResourceVersion:3940,Generation:0,CreationTimestamp:2019-02-08 22:34:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb  8 22:34:14.911: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-n899s,SelfLink:/api/v1/namespaces/e2e-tests-watch-n899s/configmaps/e2e-watch-test-label-changed,UID:a9e3a192-2bf1-11e9-a0ab-42010a8a0033,ResourceVersion:3941,Generation:0,CreationTimestamp:2019-02-08 22:34:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb  8 22:34:24.955: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-n899s,SelfLink:/api/v1/namespaces/e2e-tests-watch-n899s/configmaps/e2e-watch-test-label-changed,UID:a9e3a192-2bf1-11e9-a0ab-42010a8a0033,ResourceVersion:3959,Generation:0,CreationTimestamp:2019-02-08 22:34:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  8 22:34:24.955: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-n899s,SelfLink:/api/v1/namespaces/e2e-tests-watch-n899s/configmaps/e2e-watch-test-label-changed,UID:a9e3a192-2bf1-11e9-a0ab-42010a8a0033,ResourceVersion:3960,Generation:0,CreationTimestamp:2019-02-08 22:34:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb  8 22:34:24.955: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-n899s,SelfLink:/api/v1/namespaces/e2e-tests-watch-n899s/configmaps/e2e-watch-test-label-changed,UID:a9e3a192-2bf1-11e9-a0ab-42010a8a0033,ResourceVersion:3961,Generation:0,CreationTimestamp:2019-02-08 22:34:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:34:24.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-n899s" for this suite.
Feb  8 22:34:30.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:34:31.059: INFO: namespace: e2e-tests-watch-n899s, resource: bindings, ignored listing per whitelist
Feb  8 22:34:31.070: INFO: namespace e2e-tests-watch-n899s deletion completed in 6.108274085s

• [SLOW TEST:16.247 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:34:31.070: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-q9f7
STEP: Creating a pod to test atomic-volume-subpath
Feb  8 22:34:31.154: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-q9f7" in namespace "e2e-tests-subpath-bk8dm" to be "success or failure"
Feb  8 22:34:31.158: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.421047ms
Feb  8 22:34:33.162: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008305144s
Feb  8 22:34:35.166: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Running", Reason="", readiness=false. Elapsed: 4.012726409s
Feb  8 22:34:37.171: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Running", Reason="", readiness=false. Elapsed: 6.017096103s
Feb  8 22:34:39.175: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Running", Reason="", readiness=false. Elapsed: 8.021224468s
Feb  8 22:34:41.179: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Running", Reason="", readiness=false. Elapsed: 10.02515582s
Feb  8 22:34:43.183: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Running", Reason="", readiness=false. Elapsed: 12.02908821s
Feb  8 22:34:45.187: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Running", Reason="", readiness=false. Elapsed: 14.0334673s
Feb  8 22:34:47.191: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Running", Reason="", readiness=false. Elapsed: 16.037698805s
Feb  8 22:34:49.196: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Running", Reason="", readiness=false. Elapsed: 18.041998576s
Feb  8 22:34:51.199: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Running", Reason="", readiness=false. Elapsed: 20.045681414s
Feb  8 22:34:53.203: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Running", Reason="", readiness=false. Elapsed: 22.049490686s
Feb  8 22:34:55.208: INFO: Pod "pod-subpath-test-secret-q9f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.054006346s
STEP: Saw pod success
Feb  8 22:34:55.208: INFO: Pod "pod-subpath-test-secret-q9f7" satisfied condition "success or failure"
Feb  8 22:34:55.211: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-subpath-test-secret-q9f7 container test-container-subpath-secret-q9f7: <nil>
STEP: delete the pod
Feb  8 22:34:55.241: INFO: Waiting for pod pod-subpath-test-secret-q9f7 to disappear
Feb  8 22:34:55.254: INFO: Pod pod-subpath-test-secret-q9f7 no longer exists
STEP: Deleting pod pod-subpath-test-secret-q9f7
Feb  8 22:34:55.254: INFO: Deleting pod "pod-subpath-test-secret-q9f7" in namespace "e2e-tests-subpath-bk8dm"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:34:55.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-bk8dm" for this suite.
Feb  8 22:35:01.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:35:01.287: INFO: namespace: e2e-tests-subpath-bk8dm, resource: bindings, ignored listing per whitelist
Feb  8 22:35:01.362: INFO: namespace e2e-tests-subpath-bk8dm deletion completed in 6.099207702s

• [SLOW TEST:30.292 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:35:01.362: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 22:35:01.460: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5a3d9e3-2bf1-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-5jlqs" to be "success or failure"
Feb  8 22:35:01.474: INFO: Pod "downwardapi-volume-c5a3d9e3-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.3919ms
Feb  8 22:35:03.479: INFO: Pod "downwardapi-volume-c5a3d9e3-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0188524s
STEP: Saw pod success
Feb  8 22:35:03.479: INFO: Pod "downwardapi-volume-c5a3d9e3-2bf1-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:35:03.483: INFO: Trying to get logs from node netztbred3-worker-3 pod downwardapi-volume-c5a3d9e3-2bf1-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 22:35:03.512: INFO: Waiting for pod downwardapi-volume-c5a3d9e3-2bf1-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:35:03.516: INFO: Pod downwardapi-volume-c5a3d9e3-2bf1-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:35:03.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5jlqs" for this suite.
Feb  8 22:35:09.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:35:09.550: INFO: namespace: e2e-tests-downward-api-5jlqs, resource: bindings, ignored listing per whitelist
Feb  8 22:35:09.644: INFO: namespace e2e-tests-downward-api-5jlqs deletion completed in 6.124417158s

• [SLOW TEST:8.282 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:35:09.644: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-ca9030f0-2bf1-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 22:35:09.722: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ca910c84-2bf1-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-kqmpt" to be "success or failure"
Feb  8 22:35:09.729: INFO: Pod "pod-projected-secrets-ca910c84-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.872414ms
Feb  8 22:35:11.734: INFO: Pod "pod-projected-secrets-ca910c84-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012445909s
STEP: Saw pod success
Feb  8 22:35:11.734: INFO: Pod "pod-projected-secrets-ca910c84-2bf1-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:35:11.737: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-projected-secrets-ca910c84-2bf1-11e9-a89e-229fb9a7b2a7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  8 22:35:11.767: INFO: Waiting for pod pod-projected-secrets-ca910c84-2bf1-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:35:11.783: INFO: Pod pod-projected-secrets-ca910c84-2bf1-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:35:11.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kqmpt" for this suite.
Feb  8 22:35:17.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:35:17.842: INFO: namespace: e2e-tests-projected-kqmpt, resource: bindings, ignored listing per whitelist
Feb  8 22:35:17.898: INFO: namespace e2e-tests-projected-kqmpt deletion completed in 6.110832896s

• [SLOW TEST:8.254 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:35:17.898: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-cf7c1cee-2bf1-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 22:35:17.981: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cf7cd9a2-2bf1-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-rllkw" to be "success or failure"
Feb  8 22:35:17.994: INFO: Pod "pod-projected-secrets-cf7cd9a2-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.419733ms
Feb  8 22:35:19.998: INFO: Pod "pod-projected-secrets-cf7cd9a2-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016332845s
Feb  8 22:35:22.003: INFO: Pod "pod-projected-secrets-cf7cd9a2-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021818813s
STEP: Saw pod success
Feb  8 22:35:22.003: INFO: Pod "pod-projected-secrets-cf7cd9a2-2bf1-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:35:22.006: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-projected-secrets-cf7cd9a2-2bf1-11e9-a89e-229fb9a7b2a7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  8 22:35:22.034: INFO: Waiting for pod pod-projected-secrets-cf7cd9a2-2bf1-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:35:22.044: INFO: Pod pod-projected-secrets-cf7cd9a2-2bf1-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:35:22.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rllkw" for this suite.
Feb  8 22:35:28.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:35:28.137: INFO: namespace: e2e-tests-projected-rllkw, resource: bindings, ignored listing per whitelist
Feb  8 22:35:28.193: INFO: namespace e2e-tests-projected-rllkw deletion completed in 6.143427619s

• [SLOW TEST:10.295 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:35:28.193: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-h82lf
I0208 22:35:28.321101      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-h82lf, replica count: 1
I0208 22:35:29.371558      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0208 22:35:30.371772      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  8 22:35:30.488: INFO: Created: latency-svc-mqqvh
Feb  8 22:35:30.500: INFO: Got endpoints: latency-svc-mqqvh [28.43164ms]
Feb  8 22:35:30.520: INFO: Created: latency-svc-bswzf
Feb  8 22:35:30.530: INFO: Created: latency-svc-6rctv
Feb  8 22:35:30.535: INFO: Got endpoints: latency-svc-bswzf [34.635132ms]
Feb  8 22:35:30.540: INFO: Got endpoints: latency-svc-6rctv [38.218416ms]
Feb  8 22:35:30.546: INFO: Created: latency-svc-498ss
Feb  8 22:35:30.561: INFO: Got endpoints: latency-svc-498ss [60.019194ms]
Feb  8 22:35:30.561: INFO: Created: latency-svc-w9f64
Feb  8 22:35:30.570: INFO: Got endpoints: latency-svc-w9f64 [69.124685ms]
Feb  8 22:35:30.575: INFO: Created: latency-svc-wwqgd
Feb  8 22:35:30.583: INFO: Created: latency-svc-krn7x
Feb  8 22:35:30.586: INFO: Got endpoints: latency-svc-wwqgd [85.051215ms]
Feb  8 22:35:30.591: INFO: Got endpoints: latency-svc-krn7x [89.905013ms]
Feb  8 22:35:30.599: INFO: Created: latency-svc-mjlbk
Feb  8 22:35:30.615: INFO: Got endpoints: latency-svc-mjlbk [113.044754ms]
Feb  8 22:35:30.620: INFO: Created: latency-svc-6q5jj
Feb  8 22:35:30.641: INFO: Got endpoints: latency-svc-6q5jj [139.308138ms]
Feb  8 22:35:30.649: INFO: Created: latency-svc-cr6bw
Feb  8 22:35:30.656: INFO: Got endpoints: latency-svc-cr6bw [154.693569ms]
Feb  8 22:35:30.662: INFO: Created: latency-svc-7gsbf
Feb  8 22:35:30.670: INFO: Created: latency-svc-8n7jc
Feb  8 22:35:30.673: INFO: Got endpoints: latency-svc-7gsbf [171.389803ms]
Feb  8 22:35:30.681: INFO: Created: latency-svc-cjlwh
Feb  8 22:35:30.684: INFO: Got endpoints: latency-svc-8n7jc [182.601482ms]
Feb  8 22:35:30.692: INFO: Got endpoints: latency-svc-cjlwh [190.590362ms]
Feb  8 22:35:30.701: INFO: Created: latency-svc-kg67x
Feb  8 22:35:30.724: INFO: Got endpoints: latency-svc-kg67x [50.275078ms]
Feb  8 22:35:30.750: INFO: Created: latency-svc-rrc9b
Feb  8 22:35:30.764: INFO: Got endpoints: latency-svc-rrc9b [261.985538ms]
Feb  8 22:35:30.777: INFO: Created: latency-svc-45x5z
Feb  8 22:35:30.797: INFO: Created: latency-svc-dc5jm
Feb  8 22:35:30.802: INFO: Got endpoints: latency-svc-45x5z [299.966711ms]
Feb  8 22:35:30.828: INFO: Got endpoints: latency-svc-dc5jm [325.566896ms]
Feb  8 22:35:30.828: INFO: Created: latency-svc-kv7dx
Feb  8 22:35:30.841: INFO: Got endpoints: latency-svc-kv7dx [306.052277ms]
Feb  8 22:35:30.861: INFO: Created: latency-svc-w5ctf
Feb  8 22:35:30.870: INFO: Created: latency-svc-fqgwg
Feb  8 22:35:30.874: INFO: Got endpoints: latency-svc-w5ctf [334.153948ms]
Feb  8 22:35:30.878: INFO: Got endpoints: latency-svc-fqgwg [316.554562ms]
Feb  8 22:35:30.888: INFO: Created: latency-svc-29smn
Feb  8 22:35:30.911: INFO: Got endpoints: latency-svc-29smn [340.844991ms]
Feb  8 22:35:30.911: INFO: Created: latency-svc-wbqc6
Feb  8 22:35:30.927: INFO: Got endpoints: latency-svc-wbqc6 [340.646411ms]
Feb  8 22:35:30.933: INFO: Created: latency-svc-9skg6
Feb  8 22:35:30.951: INFO: Got endpoints: latency-svc-9skg6 [359.989265ms]
Feb  8 22:35:30.977: INFO: Created: latency-svc-6ph99
Feb  8 22:35:30.995: INFO: Created: latency-svc-g5tng
Feb  8 22:35:31.000: INFO: Got endpoints: latency-svc-6ph99 [385.639302ms]
Feb  8 22:35:31.008: INFO: Got endpoints: latency-svc-g5tng [366.877795ms]
Feb  8 22:35:31.018: INFO: Created: latency-svc-675jw
Feb  8 22:35:31.047: INFO: Got endpoints: latency-svc-675jw [390.32639ms]
Feb  8 22:35:31.054: INFO: Created: latency-svc-kvrwx
Feb  8 22:35:31.062: INFO: Created: latency-svc-qwgjl
Feb  8 22:35:31.063: INFO: Got endpoints: latency-svc-kvrwx [379.075044ms]
Feb  8 22:35:31.077: INFO: Got endpoints: latency-svc-qwgjl [384.901693ms]
Feb  8 22:35:31.081: INFO: Created: latency-svc-pl7bl
Feb  8 22:35:31.090: INFO: Got endpoints: latency-svc-pl7bl [366.562955ms]
Feb  8 22:35:31.095: INFO: Created: latency-svc-txft5
Feb  8 22:35:31.103: INFO: Got endpoints: latency-svc-txft5 [338.789031ms]
Feb  8 22:35:31.123: INFO: Created: latency-svc-kbk7r
Feb  8 22:35:31.136: INFO: Created: latency-svc-wnjbs
Feb  8 22:35:31.141: INFO: Got endpoints: latency-svc-kbk7r [338.498553ms]
Feb  8 22:35:31.147: INFO: Got endpoints: latency-svc-wnjbs [318.729087ms]
Feb  8 22:35:31.175: INFO: Created: latency-svc-8jb7d
Feb  8 22:35:31.185: INFO: Created: latency-svc-6cn5w
Feb  8 22:35:31.188: INFO: Got endpoints: latency-svc-8jb7d [346.610178ms]
Feb  8 22:35:31.199: INFO: Got endpoints: latency-svc-6cn5w [324.528613ms]
Feb  8 22:35:31.203: INFO: Created: latency-svc-2wv9v
Feb  8 22:35:31.213: INFO: Created: latency-svc-v2kk2
Feb  8 22:35:31.216: INFO: Got endpoints: latency-svc-2wv9v [338.531224ms]
Feb  8 22:35:31.221: INFO: Got endpoints: latency-svc-v2kk2 [309.714536ms]
Feb  8 22:35:31.244: INFO: Created: latency-svc-t24zv
Feb  8 22:35:31.257: INFO: Created: latency-svc-b4qlz
Feb  8 22:35:31.258: INFO: Got endpoints: latency-svc-t24zv [330.773515ms]
Feb  8 22:35:31.276: INFO: Got endpoints: latency-svc-b4qlz [324.114889ms]
Feb  8 22:35:31.278: INFO: Created: latency-svc-qhqgp
Feb  8 22:35:31.302: INFO: Created: latency-svc-tftm7
Feb  8 22:35:31.303: INFO: Got endpoints: latency-svc-qhqgp [302.408559ms]
Feb  8 22:35:31.322: INFO: Got endpoints: latency-svc-tftm7 [313.773471ms]
Feb  8 22:35:31.326: INFO: Created: latency-svc-bm52l
Feb  8 22:35:31.340: INFO: Got endpoints: latency-svc-bm52l [292.350215ms]
Feb  8 22:35:31.343: INFO: Created: latency-svc-hqlpl
Feb  8 22:35:31.353: INFO: Got endpoints: latency-svc-hqlpl [289.168425ms]
Feb  8 22:35:31.360: INFO: Created: latency-svc-ld6nf
Feb  8 22:35:31.369: INFO: Got endpoints: latency-svc-ld6nf [291.425931ms]
Feb  8 22:35:31.369: INFO: Created: latency-svc-nrgl7
Feb  8 22:35:31.377: INFO: Got endpoints: latency-svc-nrgl7 [286.911834ms]
Feb  8 22:35:31.386: INFO: Created: latency-svc-449fl
Feb  8 22:35:31.391: INFO: Got endpoints: latency-svc-449fl [287.781165ms]
Feb  8 22:35:31.396: INFO: Created: latency-svc-ns9x5
Feb  8 22:35:31.403: INFO: Got endpoints: latency-svc-ns9x5 [262.113271ms]
Feb  8 22:35:31.410: INFO: Created: latency-svc-p2b94
Feb  8 22:35:31.416: INFO: Got endpoints: latency-svc-p2b94 [269.002924ms]
Feb  8 22:35:31.420: INFO: Created: latency-svc-8kqr2
Feb  8 22:35:31.426: INFO: Got endpoints: latency-svc-8kqr2 [238.078813ms]
Feb  8 22:35:31.433: INFO: Created: latency-svc-qv2rf
Feb  8 22:35:31.445: INFO: Created: latency-svc-jsjbg
Feb  8 22:35:31.446: INFO: Got endpoints: latency-svc-qv2rf [246.774926ms]
Feb  8 22:35:31.455: INFO: Created: latency-svc-zdzsf
Feb  8 22:35:31.466: INFO: Created: latency-svc-bglkl
Feb  8 22:35:31.480: INFO: Created: latency-svc-7jxk6
Feb  8 22:35:31.508: INFO: Got endpoints: latency-svc-jsjbg [292.051586ms]
Feb  8 22:35:31.511: INFO: Created: latency-svc-5v6gk
Feb  8 22:35:31.520: INFO: Created: latency-svc-hwgqq
Feb  8 22:35:31.529: INFO: Created: latency-svc-9lhl5
Feb  8 22:35:31.535: INFO: Created: latency-svc-lq4vb
Feb  8 22:35:31.544: INFO: Created: latency-svc-5l42r
Feb  8 22:35:31.549: INFO: Got endpoints: latency-svc-zdzsf [328.447094ms]
Feb  8 22:35:31.556: INFO: Created: latency-svc-wdw64
Feb  8 22:35:31.568: INFO: Created: latency-svc-2tfzw
Feb  8 22:35:31.578: INFO: Created: latency-svc-kbn45
Feb  8 22:35:31.584: INFO: Created: latency-svc-mmlzj
Feb  8 22:35:31.598: INFO: Created: latency-svc-njr7l
Feb  8 22:35:31.599: INFO: Got endpoints: latency-svc-bglkl [340.704609ms]
Feb  8 22:35:31.608: INFO: Created: latency-svc-bk8h5
Feb  8 22:35:31.614: INFO: Created: latency-svc-txzl8
Feb  8 22:35:31.624: INFO: Created: latency-svc-hl6wv
Feb  8 22:35:31.633: INFO: Created: latency-svc-wzfcn
Feb  8 22:35:31.646: INFO: Got endpoints: latency-svc-7jxk6 [370.352951ms]
Feb  8 22:35:31.661: INFO: Created: latency-svc-k2zzz
Feb  8 22:35:31.694: INFO: Got endpoints: latency-svc-5v6gk [391.081367ms]
Feb  8 22:35:31.709: INFO: Created: latency-svc-b4j2p
Feb  8 22:35:31.743: INFO: Got endpoints: latency-svc-hwgqq [421.254192ms]
Feb  8 22:35:31.758: INFO: Created: latency-svc-b9486
Feb  8 22:35:31.795: INFO: Got endpoints: latency-svc-9lhl5 [455.573787ms]
Feb  8 22:35:31.815: INFO: Created: latency-svc-l492q
Feb  8 22:35:31.842: INFO: Got endpoints: latency-svc-lq4vb [489.408627ms]
Feb  8 22:35:31.857: INFO: Created: latency-svc-kxckd
Feb  8 22:35:31.897: INFO: Got endpoints: latency-svc-5l42r [527.803239ms]
Feb  8 22:35:31.910: INFO: Created: latency-svc-slcr6
Feb  8 22:35:31.945: INFO: Got endpoints: latency-svc-wdw64 [567.986604ms]
Feb  8 22:35:31.959: INFO: Created: latency-svc-ssnhm
Feb  8 22:35:31.994: INFO: Got endpoints: latency-svc-2tfzw [602.935868ms]
Feb  8 22:35:32.008: INFO: Created: latency-svc-kx7fr
Feb  8 22:35:32.053: INFO: Got endpoints: latency-svc-kbn45 [649.864342ms]
Feb  8 22:35:32.068: INFO: Created: latency-svc-jw7bz
Feb  8 22:35:32.094: INFO: Got endpoints: latency-svc-mmlzj [676.609609ms]
Feb  8 22:35:32.105: INFO: Created: latency-svc-7plrn
Feb  8 22:35:32.146: INFO: Got endpoints: latency-svc-njr7l [719.066813ms]
Feb  8 22:35:32.161: INFO: Created: latency-svc-x2rv7
Feb  8 22:35:32.194: INFO: Got endpoints: latency-svc-bk8h5 [747.858422ms]
Feb  8 22:35:32.208: INFO: Created: latency-svc-8llgb
Feb  8 22:35:32.244: INFO: Got endpoints: latency-svc-txzl8 [735.463794ms]
Feb  8 22:35:32.263: INFO: Created: latency-svc-pskx7
Feb  8 22:35:32.294: INFO: Got endpoints: latency-svc-hl6wv [744.140899ms]
Feb  8 22:35:32.309: INFO: Created: latency-svc-tn24m
Feb  8 22:35:32.344: INFO: Got endpoints: latency-svc-wzfcn [744.963359ms]
Feb  8 22:35:32.358: INFO: Created: latency-svc-rj2l4
Feb  8 22:35:32.395: INFO: Got endpoints: latency-svc-k2zzz [748.78872ms]
Feb  8 22:35:32.410: INFO: Created: latency-svc-jjljj
Feb  8 22:35:32.444: INFO: Got endpoints: latency-svc-b4j2p [750.306462ms]
Feb  8 22:35:32.459: INFO: Created: latency-svc-7vsgs
Feb  8 22:35:32.494: INFO: Got endpoints: latency-svc-b9486 [750.39946ms]
Feb  8 22:35:32.508: INFO: Created: latency-svc-98rdj
Feb  8 22:35:32.544: INFO: Got endpoints: latency-svc-l492q [749.120924ms]
Feb  8 22:35:32.560: INFO: Created: latency-svc-stqw6
Feb  8 22:35:32.597: INFO: Got endpoints: latency-svc-kxckd [754.340401ms]
Feb  8 22:35:32.612: INFO: Created: latency-svc-692pf
Feb  8 22:35:32.646: INFO: Got endpoints: latency-svc-slcr6 [749.093114ms]
Feb  8 22:35:32.670: INFO: Created: latency-svc-pzs87
Feb  8 22:35:32.696: INFO: Got endpoints: latency-svc-ssnhm [750.575461ms]
Feb  8 22:35:32.710: INFO: Created: latency-svc-87tnl
Feb  8 22:35:32.745: INFO: Got endpoints: latency-svc-kx7fr [750.852799ms]
Feb  8 22:35:32.775: INFO: Created: latency-svc-tszgf
Feb  8 22:35:32.794: INFO: Got endpoints: latency-svc-jw7bz [741.105568ms]
Feb  8 22:35:32.808: INFO: Created: latency-svc-275jt
Feb  8 22:35:32.848: INFO: Got endpoints: latency-svc-7plrn [753.797318ms]
Feb  8 22:35:32.862: INFO: Created: latency-svc-5cnlh
Feb  8 22:35:32.909: INFO: Got endpoints: latency-svc-x2rv7 [763.823539ms]
Feb  8 22:35:32.921: INFO: Created: latency-svc-wwf9c
Feb  8 22:35:32.956: INFO: Got endpoints: latency-svc-8llgb [761.888888ms]
Feb  8 22:35:32.976: INFO: Created: latency-svc-d4r6w
Feb  8 22:35:32.994: INFO: Got endpoints: latency-svc-pskx7 [750.48504ms]
Feb  8 22:35:33.008: INFO: Created: latency-svc-8jw2r
Feb  8 22:35:33.044: INFO: Got endpoints: latency-svc-tn24m [749.967626ms]
Feb  8 22:35:33.057: INFO: Created: latency-svc-ttswh
Feb  8 22:35:33.094: INFO: Got endpoints: latency-svc-rj2l4 [750.348938ms]
Feb  8 22:35:33.108: INFO: Created: latency-svc-ccw9w
Feb  8 22:35:33.144: INFO: Got endpoints: latency-svc-jjljj [748.854187ms]
Feb  8 22:35:33.159: INFO: Created: latency-svc-mnfdf
Feb  8 22:35:33.193: INFO: Got endpoints: latency-svc-7vsgs [748.93741ms]
Feb  8 22:35:33.206: INFO: Created: latency-svc-t6pgd
Feb  8 22:35:33.244: INFO: Got endpoints: latency-svc-98rdj [749.889603ms]
Feb  8 22:35:33.258: INFO: Created: latency-svc-d2n68
Feb  8 22:35:33.294: INFO: Got endpoints: latency-svc-stqw6 [749.279278ms]
Feb  8 22:35:33.309: INFO: Created: latency-svc-pjdg5
Feb  8 22:35:33.345: INFO: Got endpoints: latency-svc-692pf [747.777317ms]
Feb  8 22:35:33.359: INFO: Created: latency-svc-cs7ks
Feb  8 22:35:33.394: INFO: Got endpoints: latency-svc-pzs87 [747.546816ms]
Feb  8 22:35:33.409: INFO: Created: latency-svc-xdwgt
Feb  8 22:35:33.443: INFO: Got endpoints: latency-svc-87tnl [747.030191ms]
Feb  8 22:35:33.457: INFO: Created: latency-svc-cc5wb
Feb  8 22:35:33.495: INFO: Got endpoints: latency-svc-tszgf [749.743251ms]
Feb  8 22:35:33.508: INFO: Created: latency-svc-vv24l
Feb  8 22:35:33.544: INFO: Got endpoints: latency-svc-275jt [749.135249ms]
Feb  8 22:35:33.558: INFO: Created: latency-svc-tppzw
Feb  8 22:35:33.594: INFO: Got endpoints: latency-svc-5cnlh [746.752264ms]
Feb  8 22:35:33.607: INFO: Created: latency-svc-gp4fp
Feb  8 22:35:33.645: INFO: Got endpoints: latency-svc-wwf9c [735.081093ms]
Feb  8 22:35:33.659: INFO: Created: latency-svc-hft2h
Feb  8 22:35:33.699: INFO: Got endpoints: latency-svc-d4r6w [742.968639ms]
Feb  8 22:35:33.711: INFO: Created: latency-svc-wvbrs
Feb  8 22:35:33.745: INFO: Got endpoints: latency-svc-8jw2r [750.051442ms]
Feb  8 22:35:33.759: INFO: Created: latency-svc-rzs2w
Feb  8 22:35:33.794: INFO: Got endpoints: latency-svc-ttswh [749.946906ms]
Feb  8 22:35:33.810: INFO: Created: latency-svc-5mp9p
Feb  8 22:35:33.848: INFO: Got endpoints: latency-svc-ccw9w [753.443189ms]
Feb  8 22:35:33.869: INFO: Created: latency-svc-6w2bb
Feb  8 22:35:33.894: INFO: Got endpoints: latency-svc-mnfdf [749.73913ms]
Feb  8 22:35:33.907: INFO: Created: latency-svc-c4vss
Feb  8 22:35:33.945: INFO: Got endpoints: latency-svc-t6pgd [751.264597ms]
Feb  8 22:35:33.959: INFO: Created: latency-svc-tqrph
Feb  8 22:35:33.995: INFO: Got endpoints: latency-svc-d2n68 [750.590094ms]
Feb  8 22:35:34.009: INFO: Created: latency-svc-rkkx4
Feb  8 22:35:34.045: INFO: Got endpoints: latency-svc-pjdg5 [751.054521ms]
Feb  8 22:35:34.059: INFO: Created: latency-svc-5qzgq
Feb  8 22:35:34.095: INFO: Got endpoints: latency-svc-cs7ks [749.633309ms]
Feb  8 22:35:34.119: INFO: Created: latency-svc-jz5c9
Feb  8 22:35:34.144: INFO: Got endpoints: latency-svc-xdwgt [750.339192ms]
Feb  8 22:35:34.159: INFO: Created: latency-svc-rkhj5
Feb  8 22:35:34.196: INFO: Got endpoints: latency-svc-cc5wb [752.260894ms]
Feb  8 22:35:34.211: INFO: Created: latency-svc-pfxfh
Feb  8 22:35:34.245: INFO: Got endpoints: latency-svc-vv24l [750.080618ms]
Feb  8 22:35:34.258: INFO: Created: latency-svc-4d6wq
Feb  8 22:35:34.304: INFO: Got endpoints: latency-svc-tppzw [760.388511ms]
Feb  8 22:35:34.337: INFO: Created: latency-svc-kt5z2
Feb  8 22:35:34.351: INFO: Got endpoints: latency-svc-gp4fp [756.879795ms]
Feb  8 22:35:34.401: INFO: Created: latency-svc-5h6vk
Feb  8 22:35:34.402: INFO: Got endpoints: latency-svc-hft2h [756.910488ms]
Feb  8 22:35:34.441: INFO: Created: latency-svc-k9fvx
Feb  8 22:35:34.446: INFO: Got endpoints: latency-svc-wvbrs [747.298127ms]
Feb  8 22:35:34.469: INFO: Created: latency-svc-mn9jt
Feb  8 22:35:34.493: INFO: Got endpoints: latency-svc-rzs2w [748.49801ms]
Feb  8 22:35:34.509: INFO: Created: latency-svc-v7pzs
Feb  8 22:35:34.544: INFO: Got endpoints: latency-svc-5mp9p [749.222218ms]
Feb  8 22:35:34.556: INFO: Created: latency-svc-wjfj7
Feb  8 22:35:34.595: INFO: Got endpoints: latency-svc-6w2bb [747.460315ms]
Feb  8 22:35:34.610: INFO: Created: latency-svc-5hhtn
Feb  8 22:35:34.644: INFO: Got endpoints: latency-svc-c4vss [750.090026ms]
Feb  8 22:35:34.661: INFO: Created: latency-svc-dfbb8
Feb  8 22:35:34.695: INFO: Got endpoints: latency-svc-tqrph [750.080018ms]
Feb  8 22:35:34.713: INFO: Created: latency-svc-xtsg4
Feb  8 22:35:34.752: INFO: Got endpoints: latency-svc-rkkx4 [757.286415ms]
Feb  8 22:35:34.769: INFO: Created: latency-svc-psz7r
Feb  8 22:35:34.794: INFO: Got endpoints: latency-svc-5qzgq [749.003786ms]
Feb  8 22:35:34.809: INFO: Created: latency-svc-89xx4
Feb  8 22:35:34.844: INFO: Got endpoints: latency-svc-jz5c9 [749.206083ms]
Feb  8 22:35:34.857: INFO: Created: latency-svc-qwmkm
Feb  8 22:35:34.893: INFO: Got endpoints: latency-svc-rkhj5 [749.048902ms]
Feb  8 22:35:34.908: INFO: Created: latency-svc-k2b88
Feb  8 22:35:34.959: INFO: Got endpoints: latency-svc-pfxfh [763.083545ms]
Feb  8 22:35:34.988: INFO: Created: latency-svc-hdb9n
Feb  8 22:35:35.002: INFO: Got endpoints: latency-svc-4d6wq [757.644986ms]
Feb  8 22:35:35.033: INFO: Created: latency-svc-hvj5p
Feb  8 22:35:35.048: INFO: Got endpoints: latency-svc-kt5z2 [743.526972ms]
Feb  8 22:35:35.061: INFO: Created: latency-svc-sktv6
Feb  8 22:35:35.095: INFO: Got endpoints: latency-svc-5h6vk [743.494158ms]
Feb  8 22:35:35.109: INFO: Created: latency-svc-tlvwg
Feb  8 22:35:35.144: INFO: Got endpoints: latency-svc-k9fvx [741.940588ms]
Feb  8 22:35:35.158: INFO: Created: latency-svc-lmlqf
Feb  8 22:35:35.194: INFO: Got endpoints: latency-svc-mn9jt [747.703563ms]
Feb  8 22:35:35.209: INFO: Created: latency-svc-jbfdh
Feb  8 22:35:35.246: INFO: Got endpoints: latency-svc-v7pzs [752.287015ms]
Feb  8 22:35:35.258: INFO: Created: latency-svc-r77xs
Feb  8 22:35:35.294: INFO: Got endpoints: latency-svc-wjfj7 [749.790076ms]
Feb  8 22:35:35.304: INFO: Created: latency-svc-hzv82
Feb  8 22:35:35.344: INFO: Got endpoints: latency-svc-5hhtn [748.255898ms]
Feb  8 22:35:35.357: INFO: Created: latency-svc-7gxqf
Feb  8 22:35:35.396: INFO: Got endpoints: latency-svc-dfbb8 [751.573246ms]
Feb  8 22:35:35.408: INFO: Created: latency-svc-7rch9
Feb  8 22:35:35.445: INFO: Got endpoints: latency-svc-xtsg4 [749.391804ms]
Feb  8 22:35:35.457: INFO: Created: latency-svc-49ctl
Feb  8 22:35:35.493: INFO: Got endpoints: latency-svc-psz7r [741.129713ms]
Feb  8 22:35:35.507: INFO: Created: latency-svc-gc8vv
Feb  8 22:35:35.544: INFO: Got endpoints: latency-svc-89xx4 [750.117149ms]
Feb  8 22:35:35.556: INFO: Created: latency-svc-zs2f5
Feb  8 22:35:35.594: INFO: Got endpoints: latency-svc-qwmkm [749.474785ms]
Feb  8 22:35:35.606: INFO: Created: latency-svc-phlvt
Feb  8 22:35:35.643: INFO: Got endpoints: latency-svc-k2b88 [749.117716ms]
Feb  8 22:35:35.656: INFO: Created: latency-svc-zqmfg
Feb  8 22:35:35.695: INFO: Got endpoints: latency-svc-hdb9n [736.569436ms]
Feb  8 22:35:35.705: INFO: Created: latency-svc-rs5s7
Feb  8 22:35:35.743: INFO: Got endpoints: latency-svc-hvj5p [740.076048ms]
Feb  8 22:35:35.757: INFO: Created: latency-svc-6hg7w
Feb  8 22:35:35.796: INFO: Got endpoints: latency-svc-sktv6 [747.332652ms]
Feb  8 22:35:35.807: INFO: Created: latency-svc-gz692
Feb  8 22:35:35.844: INFO: Got endpoints: latency-svc-tlvwg [749.026804ms]
Feb  8 22:35:35.857: INFO: Created: latency-svc-qm7qt
Feb  8 22:35:35.894: INFO: Got endpoints: latency-svc-lmlqf [749.898599ms]
Feb  8 22:35:35.906: INFO: Created: latency-svc-8dxgd
Feb  8 22:35:35.944: INFO: Got endpoints: latency-svc-jbfdh [749.804568ms]
Feb  8 22:35:35.958: INFO: Created: latency-svc-jwhtg
Feb  8 22:35:35.995: INFO: Got endpoints: latency-svc-r77xs [748.890324ms]
Feb  8 22:35:36.006: INFO: Created: latency-svc-ldpxt
Feb  8 22:35:36.043: INFO: Got endpoints: latency-svc-hzv82 [749.315975ms]
Feb  8 22:35:36.057: INFO: Created: latency-svc-4v77m
Feb  8 22:35:36.096: INFO: Got endpoints: latency-svc-7gxqf [751.332745ms]
Feb  8 22:35:36.107: INFO: Created: latency-svc-bbktc
Feb  8 22:35:36.153: INFO: Got endpoints: latency-svc-7rch9 [757.224209ms]
Feb  8 22:35:36.166: INFO: Created: latency-svc-2tdnb
Feb  8 22:35:36.195: INFO: Got endpoints: latency-svc-49ctl [749.908976ms]
Feb  8 22:35:36.207: INFO: Created: latency-svc-cbz8r
Feb  8 22:35:36.243: INFO: Got endpoints: latency-svc-gc8vv [749.518156ms]
Feb  8 22:35:36.256: INFO: Created: latency-svc-8lpm4
Feb  8 22:35:36.295: INFO: Got endpoints: latency-svc-zs2f5 [750.531418ms]
Feb  8 22:35:36.309: INFO: Created: latency-svc-7ktj4
Feb  8 22:35:36.344: INFO: Got endpoints: latency-svc-phlvt [750.174761ms]
Feb  8 22:35:36.358: INFO: Created: latency-svc-wlzxm
Feb  8 22:35:36.395: INFO: Got endpoints: latency-svc-zqmfg [751.689681ms]
Feb  8 22:35:36.406: INFO: Created: latency-svc-8cqc8
Feb  8 22:35:36.443: INFO: Got endpoints: latency-svc-rs5s7 [747.905198ms]
Feb  8 22:35:36.456: INFO: Created: latency-svc-gb9q9
Feb  8 22:35:36.495: INFO: Got endpoints: latency-svc-6hg7w [751.610343ms]
Feb  8 22:35:36.508: INFO: Created: latency-svc-rkt27
Feb  8 22:35:36.543: INFO: Got endpoints: latency-svc-gz692 [747.837981ms]
Feb  8 22:35:36.556: INFO: Created: latency-svc-nzmkm
Feb  8 22:35:36.594: INFO: Got endpoints: latency-svc-qm7qt [749.427135ms]
Feb  8 22:35:36.608: INFO: Created: latency-svc-xqh88
Feb  8 22:35:36.644: INFO: Got endpoints: latency-svc-8dxgd [750.037672ms]
Feb  8 22:35:36.658: INFO: Created: latency-svc-7shtg
Feb  8 22:35:36.695: INFO: Got endpoints: latency-svc-jwhtg [751.028259ms]
Feb  8 22:35:36.709: INFO: Created: latency-svc-q8wxf
Feb  8 22:35:36.743: INFO: Got endpoints: latency-svc-ldpxt [748.579591ms]
Feb  8 22:35:36.757: INFO: Created: latency-svc-kv6fm
Feb  8 22:35:36.795: INFO: Got endpoints: latency-svc-4v77m [752.144917ms]
Feb  8 22:35:36.808: INFO: Created: latency-svc-h6bxh
Feb  8 22:35:36.843: INFO: Got endpoints: latency-svc-bbktc [747.652178ms]
Feb  8 22:35:36.861: INFO: Created: latency-svc-l6thr
Feb  8 22:35:36.899: INFO: Got endpoints: latency-svc-2tdnb [745.837572ms]
Feb  8 22:35:36.911: INFO: Created: latency-svc-vmqgp
Feb  8 22:35:36.955: INFO: Got endpoints: latency-svc-cbz8r [760.153034ms]
Feb  8 22:35:36.976: INFO: Created: latency-svc-4skjg
Feb  8 22:35:36.994: INFO: Got endpoints: latency-svc-8lpm4 [750.122814ms]
Feb  8 22:35:37.006: INFO: Created: latency-svc-nwpvh
Feb  8 22:35:37.045: INFO: Got endpoints: latency-svc-7ktj4 [749.590533ms]
Feb  8 22:35:37.060: INFO: Created: latency-svc-bz2t7
Feb  8 22:35:37.094: INFO: Got endpoints: latency-svc-wlzxm [749.068203ms]
Feb  8 22:35:37.107: INFO: Created: latency-svc-h4qzt
Feb  8 22:35:37.145: INFO: Got endpoints: latency-svc-8cqc8 [749.724394ms]
Feb  8 22:35:37.159: INFO: Created: latency-svc-2t6b9
Feb  8 22:35:37.194: INFO: Got endpoints: latency-svc-gb9q9 [750.517545ms]
Feb  8 22:35:37.207: INFO: Created: latency-svc-89988
Feb  8 22:35:37.243: INFO: Got endpoints: latency-svc-rkt27 [748.503673ms]
Feb  8 22:35:37.257: INFO: Created: latency-svc-jdcfk
Feb  8 22:35:37.294: INFO: Got endpoints: latency-svc-nzmkm [750.067776ms]
Feb  8 22:35:37.309: INFO: Created: latency-svc-52sqf
Feb  8 22:35:37.345: INFO: Got endpoints: latency-svc-xqh88 [750.57376ms]
Feb  8 22:35:37.359: INFO: Created: latency-svc-knwtg
Feb  8 22:35:37.394: INFO: Got endpoints: latency-svc-7shtg [749.796996ms]
Feb  8 22:35:37.406: INFO: Created: latency-svc-46k8k
Feb  8 22:35:37.444: INFO: Got endpoints: latency-svc-q8wxf [748.634618ms]
Feb  8 22:35:37.456: INFO: Created: latency-svc-njw4z
Feb  8 22:35:37.494: INFO: Got endpoints: latency-svc-kv6fm [750.435308ms]
Feb  8 22:35:37.510: INFO: Created: latency-svc-sm7s6
Feb  8 22:35:37.545: INFO: Got endpoints: latency-svc-h6bxh [749.577107ms]
Feb  8 22:35:37.558: INFO: Created: latency-svc-8fkwr
Feb  8 22:35:37.593: INFO: Got endpoints: latency-svc-l6thr [750.027463ms]
Feb  8 22:35:37.606: INFO: Created: latency-svc-5ztk7
Feb  8 22:35:37.645: INFO: Got endpoints: latency-svc-vmqgp [745.578878ms]
Feb  8 22:35:37.657: INFO: Created: latency-svc-s6mfd
Feb  8 22:35:37.696: INFO: Got endpoints: latency-svc-4skjg [740.422671ms]
Feb  8 22:35:37.711: INFO: Created: latency-svc-5xcsz
Feb  8 22:35:37.744: INFO: Got endpoints: latency-svc-nwpvh [750.555701ms]
Feb  8 22:35:37.756: INFO: Created: latency-svc-2ptnj
Feb  8 22:35:37.793: INFO: Got endpoints: latency-svc-bz2t7 [748.353328ms]
Feb  8 22:35:37.807: INFO: Created: latency-svc-2vg9x
Feb  8 22:35:37.845: INFO: Got endpoints: latency-svc-h4qzt [751.127265ms]
Feb  8 22:35:37.858: INFO: Created: latency-svc-2hwsb
Feb  8 22:35:37.894: INFO: Got endpoints: latency-svc-2t6b9 [748.850861ms]
Feb  8 22:35:37.922: INFO: Created: latency-svc-wqjxt
Feb  8 22:35:37.943: INFO: Got endpoints: latency-svc-89988 [748.894052ms]
Feb  8 22:35:37.957: INFO: Created: latency-svc-4k57c
Feb  8 22:35:37.998: INFO: Got endpoints: latency-svc-jdcfk [754.018386ms]
Feb  8 22:35:38.012: INFO: Created: latency-svc-4bt78
Feb  8 22:35:38.044: INFO: Got endpoints: latency-svc-52sqf [750.247536ms]
Feb  8 22:35:38.058: INFO: Created: latency-svc-9zbdg
Feb  8 22:35:38.096: INFO: Got endpoints: latency-svc-knwtg [751.298597ms]
Feb  8 22:35:38.117: INFO: Created: latency-svc-shrh7
Feb  8 22:35:38.143: INFO: Got endpoints: latency-svc-46k8k [749.215141ms]
Feb  8 22:35:38.158: INFO: Created: latency-svc-bnzf2
Feb  8 22:35:38.194: INFO: Got endpoints: latency-svc-njw4z [750.107231ms]
Feb  8 22:35:38.208: INFO: Created: latency-svc-hsjqx
Feb  8 22:35:38.245: INFO: Got endpoints: latency-svc-sm7s6 [750.510423ms]
Feb  8 22:35:38.258: INFO: Created: latency-svc-bxdpp
Feb  8 22:35:38.295: INFO: Got endpoints: latency-svc-8fkwr [749.406569ms]
Feb  8 22:35:38.307: INFO: Created: latency-svc-44ltk
Feb  8 22:35:38.345: INFO: Got endpoints: latency-svc-5ztk7 [751.532956ms]
Feb  8 22:35:38.393: INFO: Got endpoints: latency-svc-s6mfd [748.126ms]
Feb  8 22:35:38.445: INFO: Got endpoints: latency-svc-5xcsz [746.562523ms]
Feb  8 22:35:38.494: INFO: Got endpoints: latency-svc-2ptnj [749.787889ms]
Feb  8 22:35:38.544: INFO: Got endpoints: latency-svc-2vg9x [750.567134ms]
Feb  8 22:35:38.594: INFO: Got endpoints: latency-svc-2hwsb [749.547177ms]
Feb  8 22:35:38.648: INFO: Got endpoints: latency-svc-wqjxt [753.708087ms]
Feb  8 22:35:38.694: INFO: Got endpoints: latency-svc-4k57c [750.793948ms]
Feb  8 22:35:38.743: INFO: Got endpoints: latency-svc-4bt78 [745.857072ms]
Feb  8 22:35:38.794: INFO: Got endpoints: latency-svc-9zbdg [749.708001ms]
Feb  8 22:35:38.844: INFO: Got endpoints: latency-svc-shrh7 [748.170684ms]
Feb  8 22:35:38.907: INFO: Got endpoints: latency-svc-bnzf2 [763.112464ms]
Feb  8 22:35:38.985: INFO: Got endpoints: latency-svc-hsjqx [791.138499ms]
Feb  8 22:35:39.001: INFO: Got endpoints: latency-svc-bxdpp [756.238854ms]
Feb  8 22:35:39.045: INFO: Got endpoints: latency-svc-44ltk [749.703773ms]
Feb  8 22:35:39.045: INFO: Latencies: [34.635132ms 38.218416ms 50.275078ms 60.019194ms 69.124685ms 85.051215ms 89.905013ms 113.044754ms 139.308138ms 154.693569ms 171.389803ms 182.601482ms 190.590362ms 238.078813ms 246.774926ms 261.985538ms 262.113271ms 269.002924ms 286.911834ms 287.781165ms 289.168425ms 291.425931ms 292.051586ms 292.350215ms 299.966711ms 302.408559ms 306.052277ms 309.714536ms 313.773471ms 316.554562ms 318.729087ms 324.114889ms 324.528613ms 325.566896ms 328.447094ms 330.773515ms 334.153948ms 338.498553ms 338.531224ms 338.789031ms 340.646411ms 340.704609ms 340.844991ms 346.610178ms 359.989265ms 366.562955ms 366.877795ms 370.352951ms 379.075044ms 384.901693ms 385.639302ms 390.32639ms 391.081367ms 421.254192ms 455.573787ms 489.408627ms 527.803239ms 567.986604ms 602.935868ms 649.864342ms 676.609609ms 719.066813ms 735.081093ms 735.463794ms 736.569436ms 740.076048ms 740.422671ms 741.105568ms 741.129713ms 741.940588ms 742.968639ms 743.494158ms 743.526972ms 744.140899ms 744.963359ms 745.578878ms 745.837572ms 745.857072ms 746.562523ms 746.752264ms 747.030191ms 747.298127ms 747.332652ms 747.460315ms 747.546816ms 747.652178ms 747.703563ms 747.777317ms 747.837981ms 747.858422ms 747.905198ms 748.126ms 748.170684ms 748.255898ms 748.353328ms 748.49801ms 748.503673ms 748.579591ms 748.634618ms 748.78872ms 748.850861ms 748.854187ms 748.890324ms 748.894052ms 748.93741ms 749.003786ms 749.026804ms 749.048902ms 749.068203ms 749.093114ms 749.117716ms 749.120924ms 749.135249ms 749.206083ms 749.215141ms 749.222218ms 749.279278ms 749.315975ms 749.391804ms 749.406569ms 749.427135ms 749.474785ms 749.518156ms 749.547177ms 749.577107ms 749.590533ms 749.633309ms 749.703773ms 749.708001ms 749.724394ms 749.73913ms 749.743251ms 749.787889ms 749.790076ms 749.796996ms 749.804568ms 749.889603ms 749.898599ms 749.908976ms 749.946906ms 749.967626ms 750.027463ms 750.037672ms 750.051442ms 750.067776ms 750.080018ms 750.080618ms 750.090026ms 750.107231ms 750.117149ms 750.122814ms 750.174761ms 750.247536ms 750.306462ms 750.339192ms 750.348938ms 750.39946ms 750.435308ms 750.48504ms 750.510423ms 750.517545ms 750.531418ms 750.555701ms 750.567134ms 750.57376ms 750.575461ms 750.590094ms 750.793948ms 750.852799ms 751.028259ms 751.054521ms 751.127265ms 751.264597ms 751.298597ms 751.332745ms 751.532956ms 751.573246ms 751.610343ms 751.689681ms 752.144917ms 752.260894ms 752.287015ms 753.443189ms 753.708087ms 753.797318ms 754.018386ms 754.340401ms 756.238854ms 756.879795ms 756.910488ms 757.224209ms 757.286415ms 757.644986ms 760.153034ms 760.388511ms 761.888888ms 763.083545ms 763.112464ms 763.823539ms 791.138499ms]
Feb  8 22:35:39.045: INFO: 50 %ile: 748.850861ms
Feb  8 22:35:39.045: INFO: 90 %ile: 752.260894ms
Feb  8 22:35:39.045: INFO: 99 %ile: 763.823539ms
Feb  8 22:35:39.045: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:35:39.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-h82lf" for this suite.
Feb  8 22:35:51.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:35:51.093: INFO: namespace: e2e-tests-svc-latency-h82lf, resource: bindings, ignored listing per whitelist
Feb  8 22:35:51.163: INFO: namespace e2e-tests-svc-latency-h82lf deletion completed in 12.113665915s

• [SLOW TEST:22.970 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:35:51.163: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb  8 22:35:51.241: INFO: Waiting up to 5m0s for pod "pod-e35029d0-2bf1-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-58j8g" to be "success or failure"
Feb  8 22:35:51.250: INFO: Pod "pod-e35029d0-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.949429ms
Feb  8 22:35:53.259: INFO: Pod "pod-e35029d0-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017421138s
Feb  8 22:35:55.263: INFO: Pod "pod-e35029d0-2bf1-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021313853s
STEP: Saw pod success
Feb  8 22:35:55.263: INFO: Pod "pod-e35029d0-2bf1-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:35:55.266: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-e35029d0-2bf1-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 22:35:55.301: INFO: Waiting for pod pod-e35029d0-2bf1-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:35:55.305: INFO: Pod pod-e35029d0-2bf1-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:35:55.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-58j8g" for this suite.
Feb  8 22:36:01.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:36:01.398: INFO: namespace: e2e-tests-emptydir-58j8g, resource: bindings, ignored listing per whitelist
Feb  8 22:36:01.420: INFO: namespace e2e-tests-emptydir-58j8g deletion completed in 6.11027608s

• [SLOW TEST:10.257 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:36:01.420: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb  8 22:36:06.535: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:36:06.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-4zv7q" for this suite.
Feb  8 22:39:16.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:39:16.659: INFO: namespace: e2e-tests-replicaset-4zv7q, resource: bindings, ignored listing per whitelist
Feb  8 22:39:16.700: INFO: namespace e2e-tests-replicaset-4zv7q deletion completed in 3m10.126446021s

• [SLOW TEST:195.280 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:39:16.700: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-5dd28219-2bf2-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 22:39:16.789: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5dd3f874-2bf2-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-9jvsg" to be "success or failure"
Feb  8 22:39:16.798: INFO: Pod "pod-projected-configmaps-5dd3f874-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.141406ms
Feb  8 22:39:18.802: INFO: Pod "pod-projected-configmaps-5dd3f874-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013140117s
Feb  8 22:39:20.806: INFO: Pod "pod-projected-configmaps-5dd3f874-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017463503s
STEP: Saw pod success
Feb  8 22:39:20.806: INFO: Pod "pod-projected-configmaps-5dd3f874-2bf2-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:39:20.810: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-projected-configmaps-5dd3f874-2bf2-11e9-a89e-229fb9a7b2a7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 22:39:20.843: INFO: Waiting for pod pod-projected-configmaps-5dd3f874-2bf2-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:39:20.849: INFO: Pod pod-projected-configmaps-5dd3f874-2bf2-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:39:20.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9jvsg" for this suite.
Feb  8 22:39:26.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:39:26.921: INFO: namespace: e2e-tests-projected-9jvsg, resource: bindings, ignored listing per whitelist
Feb  8 22:39:26.988: INFO: namespace e2e-tests-projected-9jvsg deletion completed in 6.134178849s

• [SLOW TEST:10.288 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:39:26.988: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-625dh in namespace e2e-tests-proxy-5wh8d
I0208 22:39:27.074729      15 runners.go:184] Created replication controller with name: proxy-service-625dh, namespace: e2e-tests-proxy-5wh8d, replica count: 1
I0208 22:39:28.125167      15 runners.go:184] proxy-service-625dh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0208 22:39:29.125390      15 runners.go:184] proxy-service-625dh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0208 22:39:30.130665      15 runners.go:184] proxy-service-625dh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0208 22:39:31.130906      15 runners.go:184] proxy-service-625dh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0208 22:39:32.131160      15 runners.go:184] proxy-service-625dh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0208 22:39:33.131479      15 runners.go:184] proxy-service-625dh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0208 22:39:34.133680      15 runners.go:184] proxy-service-625dh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0208 22:39:35.133910      15 runners.go:184] proxy-service-625dh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0208 22:39:36.134169      15 runners.go:184] proxy-service-625dh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  8 22:39:36.139: INFO: setup took 9.085519563s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb  8 22:39:36.153: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 13.427147ms)
Feb  8 22:39:36.155: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 15.855729ms)
Feb  8 22:39:36.155: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 15.677012ms)
Feb  8 22:39:36.155: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 15.859163ms)
Feb  8 22:39:36.155: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 15.782834ms)
Feb  8 22:39:36.155: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 15.947757ms)
Feb  8 22:39:36.161: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 21.146069ms)
Feb  8 22:39:36.162: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 22.43985ms)
Feb  8 22:39:36.162: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 22.49064ms)
Feb  8 22:39:36.162: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 22.661193ms)
Feb  8 22:39:36.162: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 22.782002ms)
Feb  8 22:39:36.173: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 34.068805ms)
Feb  8 22:39:36.173: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 34.336303ms)
Feb  8 22:39:36.173: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 33.683757ms)
Feb  8 22:39:36.174: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 33.913732ms)
Feb  8 22:39:36.176: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 36.987798ms)
Feb  8 22:39:36.187: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 10.817065ms)
Feb  8 22:39:36.188: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 10.352162ms)
Feb  8 22:39:36.189: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 11.916754ms)
Feb  8 22:39:36.190: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 11.899291ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 14.363391ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 14.188947ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 14.861755ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 14.519066ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 14.438208ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 14.536493ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 15.059503ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 14.715655ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 15.381892ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 15.402711ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 14.900535ms)
Feb  8 22:39:36.192: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 15.49907ms)
Feb  8 22:39:36.203: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 9.501881ms)
Feb  8 22:39:36.203: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 10.68703ms)
Feb  8 22:39:36.203: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 10.660551ms)
Feb  8 22:39:36.203: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 10.975638ms)
Feb  8 22:39:36.205: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 11.362185ms)
Feb  8 22:39:36.205: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 12.314939ms)
Feb  8 22:39:36.205: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 11.782344ms)
Feb  8 22:39:36.205: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 11.49538ms)
Feb  8 22:39:36.205: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 12.170206ms)
Feb  8 22:39:36.205: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 12.115211ms)
Feb  8 22:39:36.205: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 12.01657ms)
Feb  8 22:39:36.206: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 12.450952ms)
Feb  8 22:39:36.207: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 13.42733ms)
Feb  8 22:39:36.207: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 13.725387ms)
Feb  8 22:39:36.207: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 14.177623ms)
Feb  8 22:39:36.207: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 13.996375ms)
Feb  8 22:39:36.217: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 9.51141ms)
Feb  8 22:39:36.217: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 9.998841ms)
Feb  8 22:39:36.217: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 9.940355ms)
Feb  8 22:39:36.218: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 10.150623ms)
Feb  8 22:39:36.220: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 12.1801ms)
Feb  8 22:39:36.220: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 12.49703ms)
Feb  8 22:39:36.220: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 12.422557ms)
Feb  8 22:39:36.220: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 12.421721ms)
Feb  8 22:39:36.221: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 13.614752ms)
Feb  8 22:39:36.222: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 13.972987ms)
Feb  8 22:39:36.222: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 13.700804ms)
Feb  8 22:39:36.222: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 13.93991ms)
Feb  8 22:39:36.222: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 13.802069ms)
Feb  8 22:39:36.222: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 13.98903ms)
Feb  8 22:39:36.222: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 14.657874ms)
Feb  8 22:39:36.222: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 14.760748ms)
Feb  8 22:39:36.232: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 9.393382ms)
Feb  8 22:39:36.233: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 9.600828ms)
Feb  8 22:39:36.234: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 11.225069ms)
Feb  8 22:39:36.235: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 11.66199ms)
Feb  8 22:39:36.235: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 12.251232ms)
Feb  8 22:39:36.235: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 12.547965ms)
Feb  8 22:39:36.235: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 12.252852ms)
Feb  8 22:39:36.235: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 12.311986ms)
Feb  8 22:39:36.235: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 11.949257ms)
Feb  8 22:39:36.235: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 12.069253ms)
Feb  8 22:39:36.235: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 12.390653ms)
Feb  8 22:39:36.236: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 13.215601ms)
Feb  8 22:39:36.237: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 13.708594ms)
Feb  8 22:39:36.237: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 13.907004ms)
Feb  8 22:39:36.237: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 13.831795ms)
Feb  8 22:39:36.237: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 13.957675ms)
Feb  8 22:39:36.247: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 9.639541ms)
Feb  8 22:39:36.249: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 10.733896ms)
Feb  8 22:39:36.249: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 11.625611ms)
Feb  8 22:39:36.249: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 11.756495ms)
Feb  8 22:39:36.250: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 11.76447ms)
Feb  8 22:39:36.250: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 12.076898ms)
Feb  8 22:39:36.250: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 12.031154ms)
Feb  8 22:39:36.250: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 11.866203ms)
Feb  8 22:39:36.250: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 12.206784ms)
Feb  8 22:39:36.251: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 12.482501ms)
Feb  8 22:39:36.251: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 13.142662ms)
Feb  8 22:39:36.251: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 13.146687ms)
Feb  8 22:39:36.251: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 12.906974ms)
Feb  8 22:39:36.251: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 13.47283ms)
Feb  8 22:39:36.251: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 13.0331ms)
Feb  8 22:39:36.251: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 13.046717ms)
Feb  8 22:39:36.259: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 8.128757ms)
Feb  8 22:39:36.260: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 8.626542ms)
Feb  8 22:39:36.262: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 9.722778ms)
Feb  8 22:39:36.263: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 10.951066ms)
Feb  8 22:39:36.263: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 11.353467ms)
Feb  8 22:39:36.263: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 11.300528ms)
Feb  8 22:39:36.264: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 12.921056ms)
Feb  8 22:39:36.265: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 12.531088ms)
Feb  8 22:39:36.265: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 12.86037ms)
Feb  8 22:39:36.265: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 13.859458ms)
Feb  8 22:39:36.265: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 13.279878ms)
Feb  8 22:39:36.265: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 13.441085ms)
Feb  8 22:39:36.265: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 13.774146ms)
Feb  8 22:39:36.265: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 13.610608ms)
Feb  8 22:39:36.265: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 13.762543ms)
Feb  8 22:39:36.266: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 13.706928ms)
Feb  8 22:39:36.274: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 8.441956ms)
Feb  8 22:39:36.277: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 10.242456ms)
Feb  8 22:39:36.278: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 11.128335ms)
Feb  8 22:39:36.278: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 11.357947ms)
Feb  8 22:39:36.278: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 11.388066ms)
Feb  8 22:39:36.278: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 12.021096ms)
Feb  8 22:39:36.278: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 12.187173ms)
Feb  8 22:39:36.278: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 11.868461ms)
Feb  8 22:39:36.278: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 12.155923ms)
Feb  8 22:39:36.279: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 12.374797ms)
Feb  8 22:39:36.279: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 12.170077ms)
Feb  8 22:39:36.279: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 12.70859ms)
Feb  8 22:39:36.279: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 13.210267ms)
Feb  8 22:39:36.279: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 12.673958ms)
Feb  8 22:39:36.280: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 13.669114ms)
Feb  8 22:39:36.280: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 13.720641ms)
Feb  8 22:39:36.290: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 10.157474ms)
Feb  8 22:39:36.291: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 10.612752ms)
Feb  8 22:39:36.292: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 11.270256ms)
Feb  8 22:39:36.292: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 11.061472ms)
Feb  8 22:39:36.292: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 11.681813ms)
Feb  8 22:39:36.292: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 11.811815ms)
Feb  8 22:39:36.292: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 11.889009ms)
Feb  8 22:39:36.292: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 11.782867ms)
Feb  8 22:39:36.293: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 12.086627ms)
Feb  8 22:39:36.293: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 12.001239ms)
Feb  8 22:39:36.293: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 12.590369ms)
Feb  8 22:39:36.295: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 14.173641ms)
Feb  8 22:39:36.296: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 15.489016ms)
Feb  8 22:39:36.296: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 15.353328ms)
Feb  8 22:39:36.296: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 15.502431ms)
Feb  8 22:39:36.296: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 15.559352ms)
Feb  8 22:39:36.306: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 9.602806ms)
Feb  8 22:39:36.308: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 10.403156ms)
Feb  8 22:39:36.308: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 11.718038ms)
Feb  8 22:39:36.308: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 11.372896ms)
Feb  8 22:39:36.309: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 11.89545ms)
Feb  8 22:39:36.309: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 11.808688ms)
Feb  8 22:39:36.309: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 11.692354ms)
Feb  8 22:39:36.309: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 13.073612ms)
Feb  8 22:39:36.309: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 12.190112ms)
Feb  8 22:39:36.309: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 12.563009ms)
Feb  8 22:39:36.309: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 12.465599ms)
Feb  8 22:39:36.310: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 12.655519ms)
Feb  8 22:39:36.310: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 12.671859ms)
Feb  8 22:39:36.310: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 13.417071ms)
Feb  8 22:39:36.310: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 12.935612ms)
Feb  8 22:39:36.310: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 13.139088ms)
Feb  8 22:39:36.320: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 9.687335ms)
Feb  8 22:39:36.322: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 11.050431ms)
Feb  8 22:39:36.322: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 11.187802ms)
Feb  8 22:39:36.323: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 11.828181ms)
Feb  8 22:39:36.323: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 12.474291ms)
Feb  8 22:39:36.323: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 12.503524ms)
Feb  8 22:39:36.323: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 12.472491ms)
Feb  8 22:39:36.323: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 12.508897ms)
Feb  8 22:39:36.323: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 12.641615ms)
Feb  8 22:39:36.323: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 13.127318ms)
Feb  8 22:39:36.324: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 12.832533ms)
Feb  8 22:39:36.324: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 13.168253ms)
Feb  8 22:39:36.324: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 13.465751ms)
Feb  8 22:39:36.325: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 14.315104ms)
Feb  8 22:39:36.325: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 14.361863ms)
Feb  8 22:39:36.325: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 14.601881ms)
Feb  8 22:39:36.334: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 8.282412ms)
Feb  8 22:39:36.334: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 8.847825ms)
Feb  8 22:39:36.335: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 9.404373ms)
Feb  8 22:39:36.336: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 11.005913ms)
Feb  8 22:39:36.337: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 11.004761ms)
Feb  8 22:39:36.337: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 10.951124ms)
Feb  8 22:39:36.337: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 11.026736ms)
Feb  8 22:39:36.337: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 10.926926ms)
Feb  8 22:39:36.337: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 11.271482ms)
Feb  8 22:39:36.337: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 11.199886ms)
Feb  8 22:39:36.337: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 11.439086ms)
Feb  8 22:39:36.339: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 13.241038ms)
Feb  8 22:39:36.339: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 13.568998ms)
Feb  8 22:39:36.339: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 13.521216ms)
Feb  8 22:39:36.339: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 13.504945ms)
Feb  8 22:39:36.339: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 13.763882ms)
Feb  8 22:39:36.348: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 8.306476ms)
Feb  8 22:39:36.349: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 9.1426ms)
Feb  8 22:39:36.349: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 8.927823ms)
Feb  8 22:39:36.349: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 9.78422ms)
Feb  8 22:39:36.349: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 9.530469ms)
Feb  8 22:39:36.350: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 9.130473ms)
Feb  8 22:39:36.350: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 9.391662ms)
Feb  8 22:39:36.350: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 8.805934ms)
Feb  8 22:39:36.350: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 10.030202ms)
Feb  8 22:39:36.350: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 9.508579ms)
Feb  8 22:39:36.352: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 11.715749ms)
Feb  8 22:39:36.353: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 12.767912ms)
Feb  8 22:39:36.354: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 12.944385ms)
Feb  8 22:39:36.354: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 13.098164ms)
Feb  8 22:39:36.354: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 13.089847ms)
Feb  8 22:39:36.354: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 13.830751ms)
Feb  8 22:39:36.362: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 8.18352ms)
Feb  8 22:39:36.367: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 13.312513ms)
Feb  8 22:39:36.370: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 15.6407ms)
Feb  8 22:39:36.371: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 16.318806ms)
Feb  8 22:39:36.372: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 17.691611ms)
Feb  8 22:39:36.372: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 17.601397ms)
Feb  8 22:39:36.372: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 17.687827ms)
Feb  8 22:39:36.372: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 17.89914ms)
Feb  8 22:39:36.373: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 18.312582ms)
Feb  8 22:39:36.373: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 18.30405ms)
Feb  8 22:39:36.373: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 18.662201ms)
Feb  8 22:39:36.374: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 19.330951ms)
Feb  8 22:39:36.374: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 19.524457ms)
Feb  8 22:39:36.374: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 18.987495ms)
Feb  8 22:39:36.374: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 18.931176ms)
Feb  8 22:39:36.374: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 18.693717ms)
Feb  8 22:39:36.422: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 47.722451ms)
Feb  8 22:39:36.422: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 47.729826ms)
Feb  8 22:39:36.422: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 47.705504ms)
Feb  8 22:39:36.422: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 47.667478ms)
Feb  8 22:39:36.422: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 47.556291ms)
Feb  8 22:39:36.422: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 48.314714ms)
Feb  8 22:39:36.422: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 47.914351ms)
Feb  8 22:39:36.422: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 48.332562ms)
Feb  8 22:39:36.429: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 54.935155ms)
Feb  8 22:39:36.429: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 55.091954ms)
Feb  8 22:39:36.436: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 61.95054ms)
Feb  8 22:39:36.436: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 62.357925ms)
Feb  8 22:39:36.438: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 63.781767ms)
Feb  8 22:39:36.438: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 63.396388ms)
Feb  8 22:39:36.438: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 63.545508ms)
Feb  8 22:39:36.438: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 63.778219ms)
Feb  8 22:39:36.470: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 32.398633ms)
Feb  8 22:39:36.476: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 37.099767ms)
Feb  8 22:39:36.476: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 37.239837ms)
Feb  8 22:39:36.480: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 41.916361ms)
Feb  8 22:39:36.481: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 42.195805ms)
Feb  8 22:39:36.481: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 42.295847ms)
Feb  8 22:39:36.485: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 46.316743ms)
Feb  8 22:39:36.485: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 46.3172ms)
Feb  8 22:39:36.485: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 46.746026ms)
Feb  8 22:39:36.485: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 46.842719ms)
Feb  8 22:39:36.486: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 47.178057ms)
Feb  8 22:39:36.486: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 47.578969ms)
Feb  8 22:39:36.496: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 57.965893ms)
Feb  8 22:39:36.497: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 57.974367ms)
Feb  8 22:39:36.497: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 58.532123ms)
Feb  8 22:39:36.497: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 58.461544ms)
Feb  8 22:39:36.530: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 32.018357ms)
Feb  8 22:39:36.530: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 32.622298ms)
Feb  8 22:39:36.532: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 34.427878ms)
Feb  8 22:39:36.532: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 34.553928ms)
Feb  8 22:39:36.532: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 34.695775ms)
Feb  8 22:39:36.532: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 35.02589ms)
Feb  8 22:39:36.532: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 34.823008ms)
Feb  8 22:39:36.536: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 39.040293ms)
Feb  8 22:39:36.537: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 39.340588ms)
Feb  8 22:39:36.537: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 39.101262ms)
Feb  8 22:39:36.537: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 39.141656ms)
Feb  8 22:39:36.538: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 40.746113ms)
Feb  8 22:39:36.539: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 41.882637ms)
Feb  8 22:39:36.539: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 41.365039ms)
Feb  8 22:39:36.539: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 41.43273ms)
Feb  8 22:39:36.539: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 41.933753ms)
Feb  8 22:39:36.579: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 39.20985ms)
Feb  8 22:39:36.582: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 41.57733ms)
Feb  8 22:39:36.583: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 43.855527ms)
Feb  8 22:39:36.583: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 43.673357ms)
Feb  8 22:39:36.583: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 44.059096ms)
Feb  8 22:39:36.584: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 43.886675ms)
Feb  8 22:39:36.584: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 44.047702ms)
Feb  8 22:39:36.584: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 44.426839ms)
Feb  8 22:39:36.584: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 44.16737ms)
Feb  8 22:39:36.584: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 44.236124ms)
Feb  8 22:39:36.584: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 44.694235ms)
Feb  8 22:39:36.585: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 44.571669ms)
Feb  8 22:39:36.585: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 45.499988ms)
Feb  8 22:39:36.586: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 45.684887ms)
Feb  8 22:39:36.586: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 46.023664ms)
Feb  8 22:39:36.586: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 45.992139ms)
Feb  8 22:39:36.628: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 40.862167ms)
Feb  8 22:39:36.628: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 41.264014ms)
Feb  8 22:39:36.628: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 41.950623ms)
Feb  8 22:39:36.628: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 42.289881ms)
Feb  8 22:39:36.628: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 41.567803ms)
Feb  8 22:39:36.628: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 41.277101ms)
Feb  8 22:39:36.628: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 41.861519ms)
Feb  8 22:39:36.628: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 42.191406ms)
Feb  8 22:39:36.628: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 42.01428ms)
Feb  8 22:39:36.630: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 43.102756ms)
Feb  8 22:39:36.631: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 44.618522ms)
Feb  8 22:39:36.632: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 45.380812ms)
Feb  8 22:39:36.632: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 45.673902ms)
Feb  8 22:39:36.632: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 45.243579ms)
Feb  8 22:39:36.632: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 45.604616ms)
Feb  8 22:39:36.632: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 45.727782ms)
Feb  8 22:39:36.669: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb/proxy/rewriteme"... (200; 36.631631ms)
Feb  8 22:39:36.670: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:162/proxy/: bar (200; 37.829531ms)
Feb  8 22:39:36.673: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:160/proxy/: foo (200; 40.238533ms)
Feb  8 22:39:36.674: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:443/proxy/... (200; 41.206657ms)
Feb  8 22:39:36.674: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:1080/proxy/rewri... (200; 41.27009ms)
Feb  8 22:39:36.674: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:1080/proxy/... (200; 41.171911ms)
Feb  8 22:39:36.674: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:462/proxy/: tls qux (200; 41.573776ms)
Feb  8 22:39:36.674: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname1/proxy/: tls baz (200; 41.53505ms)
Feb  8 22:39:36.675: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/http:proxy-service-625dh-mt4tb:162/proxy/: bar (200; 41.791793ms)
Feb  8 22:39:36.675: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/proxy-service-625dh-mt4tb:160/proxy/: foo (200; 42.027545ms)
Feb  8 22:39:36.675: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/pods/https:proxy-service-625dh-mt4tb:460/proxy/: tls baz (200; 42.12803ms)
Feb  8 22:39:36.675: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/https:proxy-service-625dh:tlsportname2/proxy/: tls qux (200; 42.290475ms)
Feb  8 22:39:36.675: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname1/proxy/: foo (200; 42.61435ms)
Feb  8 22:39:36.676: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/proxy-service-625dh:portname2/proxy/: bar (200; 42.747393ms)
Feb  8 22:39:36.676: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname1/proxy/: foo (200; 43.150151ms)
Feb  8 22:39:36.676: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-5wh8d/services/http:proxy-service-625dh:portname2/proxy/: bar (200; 43.374701ms)
STEP: deleting ReplicationController proxy-service-625dh in namespace e2e-tests-proxy-5wh8d, will wait for the garbage collector to delete the pods
Feb  8 22:39:36.740: INFO: Deleting ReplicationController proxy-service-625dh took: 10.75699ms
Feb  8 22:39:36.840: INFO: Terminating ReplicationController proxy-service-625dh pods took: 100.244811ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:39:49.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-5wh8d" for this suite.
Feb  8 22:39:55.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:39:55.828: INFO: namespace: e2e-tests-proxy-5wh8d, resource: bindings, ignored listing per whitelist
Feb  8 22:39:55.845: INFO: namespace e2e-tests-proxy-5wh8d deletion completed in 6.099969562s

• [SLOW TEST:28.857 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:39:55.845: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb  8 22:39:55.923: INFO: Waiting up to 5m0s for pod "pod-75279d6a-2bf2-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-sl4rh" to be "success or failure"
Feb  8 22:39:55.929: INFO: Pod "pod-75279d6a-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.699733ms
Feb  8 22:39:57.933: INFO: Pod "pod-75279d6a-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00997664s
Feb  8 22:39:59.937: INFO: Pod "pod-75279d6a-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013927284s
STEP: Saw pod success
Feb  8 22:39:59.937: INFO: Pod "pod-75279d6a-2bf2-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:39:59.940: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-75279d6a-2bf2-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 22:39:59.972: INFO: Waiting for pod pod-75279d6a-2bf2-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:39:59.976: INFO: Pod pod-75279d6a-2bf2-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:39:59.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sl4rh" for this suite.
Feb  8 22:40:05.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:40:06.040: INFO: namespace: e2e-tests-emptydir-sl4rh, resource: bindings, ignored listing per whitelist
Feb  8 22:40:06.084: INFO: namespace e2e-tests-emptydir-sl4rh deletion completed in 6.103587363s

• [SLOW TEST:10.238 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:40:06.084: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-7b41d6dd-2bf2-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 22:40:06.167: INFO: Waiting up to 5m0s for pod "pod-secrets-7b42b699-2bf2-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-secrets-jx2vk" to be "success or failure"
Feb  8 22:40:06.174: INFO: Pod "pod-secrets-7b42b699-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.348867ms
Feb  8 22:40:08.178: INFO: Pod "pod-secrets-7b42b699-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010565477s
Feb  8 22:40:10.182: INFO: Pod "pod-secrets-7b42b699-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014548822s
STEP: Saw pod success
Feb  8 22:40:10.182: INFO: Pod "pod-secrets-7b42b699-2bf2-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:40:10.185: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-secrets-7b42b699-2bf2-11e9-a89e-229fb9a7b2a7 container secret-volume-test: <nil>
STEP: delete the pod
Feb  8 22:40:10.210: INFO: Waiting for pod pod-secrets-7b42b699-2bf2-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:40:10.216: INFO: Pod pod-secrets-7b42b699-2bf2-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:40:10.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-jx2vk" for this suite.
Feb  8 22:40:16.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:40:16.264: INFO: namespace: e2e-tests-secrets-jx2vk, resource: bindings, ignored listing per whitelist
Feb  8 22:40:16.323: INFO: namespace e2e-tests-secrets-jx2vk deletion completed in 6.103266309s

• [SLOW TEST:10.240 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:40:16.324: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb  8 22:40:18.951: INFO: Successfully updated pod "pod-update-activedeadlineseconds-815da25a-2bf2-11e9-a89e-229fb9a7b2a7"
Feb  8 22:40:18.951: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-815da25a-2bf2-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-pods-48blb" to be "terminated due to deadline exceeded"
Feb  8 22:40:18.955: INFO: Pod "pod-update-activedeadlineseconds-815da25a-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Running", Reason="", readiness=true. Elapsed: 3.474215ms
Feb  8 22:40:20.959: INFO: Pod "pod-update-activedeadlineseconds-815da25a-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Running", Reason="", readiness=true. Elapsed: 2.0073154s
Feb  8 22:40:22.963: INFO: Pod "pod-update-activedeadlineseconds-815da25a-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.01165254s
Feb  8 22:40:22.963: INFO: Pod "pod-update-activedeadlineseconds-815da25a-2bf2-11e9-a89e-229fb9a7b2a7" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:40:22.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-48blb" for this suite.
Feb  8 22:40:28.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:40:29.007: INFO: namespace: e2e-tests-pods-48blb, resource: bindings, ignored listing per whitelist
Feb  8 22:40:29.071: INFO: namespace e2e-tests-pods-48blb deletion completed in 6.102814929s

• [SLOW TEST:12.747 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:40:29.071: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-zrsqc/secret-test-88f5ccdc-2bf2-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 22:40:29.154: INFO: Waiting up to 5m0s for pod "pod-configmaps-88f67a87-2bf2-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-secrets-zrsqc" to be "success or failure"
Feb  8 22:40:29.158: INFO: Pod "pod-configmaps-88f67a87-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2503ms
Feb  8 22:40:31.162: INFO: Pod "pod-configmaps-88f67a87-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008530017s
STEP: Saw pod success
Feb  8 22:40:31.162: INFO: Pod "pod-configmaps-88f67a87-2bf2-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:40:31.166: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-configmaps-88f67a87-2bf2-11e9-a89e-229fb9a7b2a7 container env-test: <nil>
STEP: delete the pod
Feb  8 22:40:31.192: INFO: Waiting for pod pod-configmaps-88f67a87-2bf2-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:40:31.195: INFO: Pod pod-configmaps-88f67a87-2bf2-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:40:31.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-zrsqc" for this suite.
Feb  8 22:40:37.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:40:37.301: INFO: namespace: e2e-tests-secrets-zrsqc, resource: bindings, ignored listing per whitelist
Feb  8 22:40:37.322: INFO: namespace e2e-tests-secrets-zrsqc deletion completed in 6.12396321s

• [SLOW TEST:8.252 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:40:37.323: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-8de0c88a-2bf2-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 22:40:37.402: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8de17de9-2bf2-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-wn42k" to be "success or failure"
Feb  8 22:40:37.412: INFO: Pod "pod-projected-configmaps-8de17de9-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.710968ms
Feb  8 22:40:39.416: INFO: Pod "pod-projected-configmaps-8de17de9-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013653511s
Feb  8 22:40:41.420: INFO: Pod "pod-projected-configmaps-8de17de9-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017863096s
STEP: Saw pod success
Feb  8 22:40:41.420: INFO: Pod "pod-projected-configmaps-8de17de9-2bf2-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:40:41.424: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-projected-configmaps-8de17de9-2bf2-11e9-a89e-229fb9a7b2a7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 22:40:41.460: INFO: Waiting for pod pod-projected-configmaps-8de17de9-2bf2-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:40:41.469: INFO: Pod pod-projected-configmaps-8de17de9-2bf2-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:40:41.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wn42k" for this suite.
Feb  8 22:40:47.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:40:47.532: INFO: namespace: e2e-tests-projected-wn42k, resource: bindings, ignored listing per whitelist
Feb  8 22:40:47.603: INFO: namespace e2e-tests-projected-wn42k deletion completed in 6.115042185s

• [SLOW TEST:10.280 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:40:47.603: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Feb  8 22:40:47.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-w2rgx'
Feb  8 22:40:48.022: INFO: stderr: ""
Feb  8 22:40:48.022: INFO: stdout: "pod/pause created\n"
Feb  8 22:40:48.022: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb  8 22:40:48.022: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-w2rgx" to be "running and ready"
Feb  8 22:40:48.028: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.930559ms
Feb  8 22:40:50.032: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009368927s
Feb  8 22:40:52.036: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.01339683s
Feb  8 22:40:52.036: INFO: Pod "pause" satisfied condition "running and ready"
Feb  8 22:40:52.036: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb  8 22:40:52.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-w2rgx'
Feb  8 22:40:52.133: INFO: stderr: ""
Feb  8 22:40:52.133: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb  8 22:40:52.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pod pause -L testing-label --namespace=e2e-tests-kubectl-w2rgx'
Feb  8 22:40:52.216: INFO: stderr: ""
Feb  8 22:40:52.216: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb  8 22:40:52.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 label pods pause testing-label- --namespace=e2e-tests-kubectl-w2rgx'
Feb  8 22:40:52.304: INFO: stderr: ""
Feb  8 22:40:52.304: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb  8 22:40:52.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pod pause -L testing-label --namespace=e2e-tests-kubectl-w2rgx'
Feb  8 22:40:52.379: INFO: stderr: ""
Feb  8 22:40:52.379: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Feb  8 22:40:52.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-w2rgx'
Feb  8 22:40:52.472: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  8 22:40:52.472: INFO: stdout: "pod \"pause\" force deleted\n"
Feb  8 22:40:52.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-w2rgx'
Feb  8 22:40:52.559: INFO: stderr: "No resources found.\n"
Feb  8 22:40:52.559: INFO: stdout: ""
Feb  8 22:40:52.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -l name=pause --namespace=e2e-tests-kubectl-w2rgx -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  8 22:40:52.640: INFO: stderr: ""
Feb  8 22:40:52.640: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:40:52.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-w2rgx" for this suite.
Feb  8 22:40:58.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:40:58.741: INFO: namespace: e2e-tests-kubectl-w2rgx, resource: bindings, ignored listing per whitelist
Feb  8 22:40:58.773: INFO: namespace e2e-tests-kubectl-w2rgx deletion completed in 6.128314139s

• [SLOW TEST:11.170 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:40:58.773: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb  8 22:40:58.842: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-762798943 proxy --unix-socket=/tmp/kubectl-proxy-unix512588594/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:40:58.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-c8vst" for this suite.
Feb  8 22:41:04.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:41:04.971: INFO: namespace: e2e-tests-kubectl-c8vst, resource: bindings, ignored listing per whitelist
Feb  8 22:41:05.043: INFO: namespace e2e-tests-kubectl-c8vst deletion completed in 6.135813667s

• [SLOW TEST:6.270 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:41:05.044: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  8 22:41:05.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-62752'
Feb  8 22:41:05.215: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  8 22:41:05.215: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Feb  8 22:41:07.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-62752'
Feb  8 22:41:07.321: INFO: stderr: ""
Feb  8 22:41:07.321: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:41:07.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-62752" for this suite.
Feb  8 22:41:29.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:41:29.368: INFO: namespace: e2e-tests-kubectl-62752, resource: bindings, ignored listing per whitelist
Feb  8 22:41:29.434: INFO: namespace e2e-tests-kubectl-62752 deletion completed in 22.106093709s

• [SLOW TEST:24.390 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:41:29.434: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0208 22:41:39.527547      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  8 22:41:39.527: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:41:39.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-dh2p6" for this suite.
Feb  8 22:41:45.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:41:45.598: INFO: namespace: e2e-tests-gc-dh2p6, resource: bindings, ignored listing per whitelist
Feb  8 22:41:45.624: INFO: namespace e2e-tests-gc-dh2p6 deletion completed in 6.092820278s

• [SLOW TEST:16.190 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:41:45.624: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:41:47.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-h7h4c" for this suite.
Feb  8 22:42:31.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:42:31.836: INFO: namespace: e2e-tests-kubelet-test-h7h4c, resource: bindings, ignored listing per whitelist
Feb  8 22:42:31.873: INFO: namespace e2e-tests-kubelet-test-h7h4c deletion completed in 44.12458729s

• [SLOW TEST:46.250 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:42:31.873: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  8 22:42:31.941: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:42:36.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-lxlbf" for this suite.
Feb  8 22:42:58.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:42:58.175: INFO: namespace: e2e-tests-init-container-lxlbf, resource: bindings, ignored listing per whitelist
Feb  8 22:42:58.194: INFO: namespace e2e-tests-init-container-lxlbf deletion completed in 22.112210022s

• [SLOW TEST:26.320 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:42:58.194: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-e1d7fffb-2bf2-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 22:42:58.277: INFO: Waiting up to 5m0s for pod "pod-secrets-e1d8c6c4-2bf2-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-secrets-77jf5" to be "success or failure"
Feb  8 22:42:58.284: INFO: Pod "pod-secrets-e1d8c6c4-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.617111ms
Feb  8 22:43:00.288: INFO: Pod "pod-secrets-e1d8c6c4-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010816683s
STEP: Saw pod success
Feb  8 22:43:00.288: INFO: Pod "pod-secrets-e1d8c6c4-2bf2-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:43:00.291: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-secrets-e1d8c6c4-2bf2-11e9-a89e-229fb9a7b2a7 container secret-volume-test: <nil>
STEP: delete the pod
Feb  8 22:43:00.321: INFO: Waiting for pod pod-secrets-e1d8c6c4-2bf2-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:43:00.328: INFO: Pod pod-secrets-e1d8c6c4-2bf2-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:43:00.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-77jf5" for this suite.
Feb  8 22:43:06.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:43:06.409: INFO: namespace: e2e-tests-secrets-77jf5, resource: bindings, ignored listing per whitelist
Feb  8 22:43:06.438: INFO: namespace e2e-tests-secrets-77jf5 deletion completed in 6.102400637s

• [SLOW TEST:8.244 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:43:06.438: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 22:43:08.542: INFO: Waiting up to 5m0s for pod "client-envvars-e7f755c5-2bf2-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-pods-2gmt8" to be "success or failure"
Feb  8 22:43:08.550: INFO: Pod "client-envvars-e7f755c5-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.728309ms
Feb  8 22:43:10.554: INFO: Pod "client-envvars-e7f755c5-2bf2-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011409171s
STEP: Saw pod success
Feb  8 22:43:10.554: INFO: Pod "client-envvars-e7f755c5-2bf2-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:43:10.557: INFO: Trying to get logs from node netztbred3-worker-3 pod client-envvars-e7f755c5-2bf2-11e9-a89e-229fb9a7b2a7 container env3cont: <nil>
STEP: delete the pod
Feb  8 22:43:10.582: INFO: Waiting for pod client-envvars-e7f755c5-2bf2-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:43:10.586: INFO: Pod client-envvars-e7f755c5-2bf2-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:43:10.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-2gmt8" for this suite.
Feb  8 22:44:02.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:44:02.681: INFO: namespace: e2e-tests-pods-2gmt8, resource: bindings, ignored listing per whitelist
Feb  8 22:44:02.686: INFO: namespace e2e-tests-pods-2gmt8 deletion completed in 52.096222669s

• [SLOW TEST:56.248 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:44:02.686: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  8 22:44:07.309: INFO: Successfully updated pod "annotationupdate0849a5f2-2bf3-11e9-a89e-229fb9a7b2a7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:44:09.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-slmnp" for this suite.
Feb  8 22:44:31.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:44:31.428: INFO: namespace: e2e-tests-downward-api-slmnp, resource: bindings, ignored listing per whitelist
Feb  8 22:44:31.458: INFO: namespace e2e-tests-downward-api-slmnp deletion completed in 22.123141794s

• [SLOW TEST:28.772 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:44:31.458: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-bn9bm
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-bn9bm
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-bn9bm
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-bn9bm
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-bn9bm
Feb  8 22:44:33.584: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-bn9bm, name: ss-0, uid: 1987a2ee-2bf3-11e9-a0ab-42010a8a0033, status phase: Pending. Waiting for statefulset controller to delete.
Feb  8 22:44:37.782: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-bn9bm, name: ss-0, uid: 1987a2ee-2bf3-11e9-a0ab-42010a8a0033, status phase: Failed. Waiting for statefulset controller to delete.
Feb  8 22:44:37.797: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-bn9bm, name: ss-0, uid: 1987a2ee-2bf3-11e9-a0ab-42010a8a0033, status phase: Failed. Waiting for statefulset controller to delete.
Feb  8 22:44:37.804: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-bn9bm
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-bn9bm
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-bn9bm and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  8 22:44:47.886: INFO: Deleting all statefulset in ns e2e-tests-statefulset-bn9bm
Feb  8 22:44:47.889: INFO: Scaling statefulset ss to 0
Feb  8 22:44:57.909: INFO: Waiting for statefulset status.replicas updated to 0
Feb  8 22:44:57.912: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:44:57.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-bn9bm" for this suite.
Feb  8 22:45:03.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:45:04.010: INFO: namespace: e2e-tests-statefulset-bn9bm, resource: bindings, ignored listing per whitelist
Feb  8 22:45:04.037: INFO: namespace e2e-tests-statefulset-bn9bm deletion completed in 6.099306492s

• [SLOW TEST:32.579 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:45:04.037: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  8 22:45:04.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-bbjhl'
Feb  8 22:45:04.207: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  8 22:45:04.207: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Feb  8 22:45:04.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-bbjhl'
Feb  8 22:45:04.308: INFO: stderr: ""
Feb  8 22:45:04.308: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:45:04.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bbjhl" for this suite.
Feb  8 22:45:24.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:45:24.364: INFO: namespace: e2e-tests-kubectl-bbjhl, resource: bindings, ignored listing per whitelist
Feb  8 22:45:24.429: INFO: namespace e2e-tests-kubectl-bbjhl deletion completed in 20.116131477s

• [SLOW TEST:20.392 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:45:24.429: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb  8 22:45:27.052: INFO: Successfully updated pod "pod-update-39043476-2bf3-11e9-a89e-229fb9a7b2a7"
STEP: verifying the updated pod is in kubernetes
Feb  8 22:45:27.061: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:45:27.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-p8sgw" for this suite.
Feb  8 22:45:49.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:45:49.132: INFO: namespace: e2e-tests-pods-p8sgw, resource: bindings, ignored listing per whitelist
Feb  8 22:45:49.192: INFO: namespace e2e-tests-pods-p8sgw deletion completed in 22.12738393s

• [SLOW TEST:24.763 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:45:49.192: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-4sfzb
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  8 22:45:49.279: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  8 22:46:11.416: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.2.33:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4sfzb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 22:46:11.416: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 22:46:11.556: INFO: Found all expected endpoints: [netserver-0]
Feb  8 22:46:11.559: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.1.39:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4sfzb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 22:46:11.559: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 22:46:11.729: INFO: Found all expected endpoints: [netserver-1]
Feb  8 22:46:11.733: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.3.7:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4sfzb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 22:46:11.733: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 22:46:11.932: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:46:11.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-4sfzb" for this suite.
Feb  8 22:46:33.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:46:33.986: INFO: namespace: e2e-tests-pod-network-test-4sfzb, resource: bindings, ignored listing per whitelist
Feb  8 22:46:34.061: INFO: namespace e2e-tests-pod-network-test-4sfzb deletion completed in 22.124658207s

• [SLOW TEST:44.869 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:46:34.062: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:46:38.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-mjbq9" for this suite.
Feb  8 22:47:20.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:47:20.231: INFO: namespace: e2e-tests-kubelet-test-mjbq9, resource: bindings, ignored listing per whitelist
Feb  8 22:47:20.283: INFO: namespace e2e-tests-kubelet-test-mjbq9 deletion completed in 42.113241s

• [SLOW TEST:46.222 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:47:20.284: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-qb6zz A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-qb6zz;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-qb6zz A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-qb6zz;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-qb6zz.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-qb6zz.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-qb6zz.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-qb6zz.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-qb6zz.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-qb6zz.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-qb6zz.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-qb6zz.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 155.0.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.0.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.0.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.0.155_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-qb6zz A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-qb6zz;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-qb6zz A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-qb6zz.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-qb6zz.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-qb6zz.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-qb6zz.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-qb6zz.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-qb6zz.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-qb6zz.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-qb6zz.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 155.0.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.0.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.0.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.0.155_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  8 22:47:32.487: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:32.490: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:32.493: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-qb6zz from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:32.497: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:32.500: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:32.504: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:32.508: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:32.511: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:32.530: INFO: Lookups using e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-qb6zz jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz jessie_udp@dns-test-service.e2e-tests-dns-qb6zz.svc jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc]

Feb  8 22:47:37.580: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:37.582: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:37.585: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-qb6zz from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:37.589: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:37.592: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:37.595: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:37.598: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:37.601: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:37.619: INFO: Lookups using e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-qb6zz jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz jessie_udp@dns-test-service.e2e-tests-dns-qb6zz.svc jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc]

Feb  8 22:47:42.609: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:42.614: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:42.618: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-qb6zz from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:42.621: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:42.625: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:42.629: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:42.633: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:42.637: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:42.659: INFO: Lookups using e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-qb6zz jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz jessie_udp@dns-test-service.e2e-tests-dns-qb6zz.svc jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc]

Feb  8 22:47:47.598: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:47.601: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:47.607: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-qb6zz from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:47.610: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:47.617: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:47.622: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:47.626: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:47.629: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:47.653: INFO: Lookups using e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-qb6zz jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz jessie_udp@dns-test-service.e2e-tests-dns-qb6zz.svc jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc]

Feb  8 22:47:52.579: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:52.583: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:52.585: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-qb6zz from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:52.588: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:52.591: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:52.594: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:52.598: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:52.601: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc from pod e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7: the server could not find the requested resource (get pods dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7)
Feb  8 22:47:52.627: INFO: Lookups using e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-qb6zz jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz jessie_udp@dns-test-service.e2e-tests-dns-qb6zz.svc jessie_tcp@dns-test-service.e2e-tests-dns-qb6zz.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-qb6zz.svc]

Feb  8 22:47:57.634: INFO: DNS probes using e2e-tests-dns-qb6zz/dns-test-7e14a4bb-2bf3-11e9-a89e-229fb9a7b2a7 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:47:57.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-qb6zz" for this suite.
Feb  8 22:48:03.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:48:03.802: INFO: namespace: e2e-tests-dns-qb6zz, resource: bindings, ignored listing per whitelist
Feb  8 22:48:03.818: INFO: namespace e2e-tests-dns-qb6zz deletion completed in 6.099263823s

• [SLOW TEST:43.535 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:48:03.819: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 22:48:03.911: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9802490b-2bf3-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-4mr28" to be "success or failure"
Feb  8 22:48:03.920: INFO: Pod "downwardapi-volume-9802490b-2bf3-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.146906ms
Feb  8 22:48:05.928: INFO: Pod "downwardapi-volume-9802490b-2bf3-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016727285s
STEP: Saw pod success
Feb  8 22:48:05.928: INFO: Pod "downwardapi-volume-9802490b-2bf3-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:48:05.931: INFO: Trying to get logs from node netztbred3-worker-3 pod downwardapi-volume-9802490b-2bf3-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 22:48:05.959: INFO: Waiting for pod downwardapi-volume-9802490b-2bf3-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:48:05.964: INFO: Pod downwardapi-volume-9802490b-2bf3-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:48:05.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4mr28" for this suite.
Feb  8 22:48:11.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:48:12.077: INFO: namespace: e2e-tests-downward-api-4mr28, resource: bindings, ignored listing per whitelist
Feb  8 22:48:12.080: INFO: namespace e2e-tests-downward-api-4mr28 deletion completed in 6.11065074s

• [SLOW TEST:8.262 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:48:12.081: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 22:48:12.195: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb  8 22:48:12.205: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb  8 22:48:17.209: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  8 22:48:17.209: INFO: Creating deployment "test-rolling-update-deployment"
Feb  8 22:48:17.217: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb  8 22:48:17.226: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb  8 22:48:19.260: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb  8 22:48:19.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685262897, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685262897, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685262899, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685262897, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  8 22:48:21.269: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  8 22:48:21.279: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-zjpsm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zjpsm/deployments/test-rolling-update-deployment,UID:9ff3a8fb-2bf3-11e9-a0ab-42010a8a0033,ResourceVersion:7713,Generation:1,CreationTimestamp:2019-02-08 22:48:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-08 22:48:17 +0000 UTC 2019-02-08 22:48:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-08 22:48:19 +0000 UTC 2019-02-08 22:48:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb  8 22:48:21.282: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-zjpsm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zjpsm/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:9ff7790c-2bf3-11e9-a0ab-42010a8a0033,ResourceVersion:7704,Generation:1,CreationTimestamp:2019-02-08 22:48:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 9ff3a8fb-2bf3-11e9-a0ab-42010a8a0033 0xc00110a237 0xc00110a238}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb  8 22:48:21.282: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb  8 22:48:21.283: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-zjpsm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zjpsm/replicasets/test-rolling-update-controller,UID:9cf68fdd-2bf3-11e9-a0ab-42010a8a0033,ResourceVersion:7712,Generation:2,CreationTimestamp:2019-02-08 22:48:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 9ff3a8fb-2bf3-11e9-a0ab-42010a8a0033 0xc00110a187 0xc00110a188}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  8 22:48:21.286: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-jxp98" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-jxp98,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-zjpsm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zjpsm/pods/test-rolling-update-deployment-68b55d7bc6-jxp98,UID:9ff8bc4e-2bf3-11e9-a0ab-42010a8a0033,ResourceVersion:7703,Generation:0,CreationTimestamp:2019-02-08 22:48:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.2.37/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 9ff7790c-2bf3-11e9-a0ab-42010a8a0033 0xc00110add7 0xc00110add8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bzdm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bzdm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-bzdm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00110ae40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00110ae60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 22:48:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 22:48:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 22:48:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 22:48:17 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.54,PodIP:10.2.2.37,StartTime:2019-02-08 22:48:17 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-08 22:48:18 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://a365f722458da8d1a5726d9bc65bf7316b34d2487a9ce55c1e51df33fa85ff9e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:48:21.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-zjpsm" for this suite.
Feb  8 22:48:27.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:48:27.332: INFO: namespace: e2e-tests-deployment-zjpsm, resource: bindings, ignored listing per whitelist
Feb  8 22:48:27.397: INFO: namespace e2e-tests-deployment-zjpsm deletion completed in 6.106595619s

• [SLOW TEST:15.316 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:48:27.397: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb  8 22:48:27.988: INFO: Waiting up to 5m0s for pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-fdd45" in namespace "e2e-tests-svcaccounts-h5krt" to be "success or failure"
Feb  8 22:48:27.995: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-fdd45": Phase="Pending", Reason="", readiness=false. Elapsed: 7.031574ms
Feb  8 22:48:29.999: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-fdd45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011012282s
Feb  8 22:48:32.004: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-fdd45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015971756s
STEP: Saw pod success
Feb  8 22:48:32.004: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-fdd45" satisfied condition "success or failure"
Feb  8 22:48:32.007: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-fdd45 container token-test: <nil>
STEP: delete the pod
Feb  8 22:48:32.039: INFO: Waiting for pod pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-fdd45 to disappear
Feb  8 22:48:32.053: INFO: Pod pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-fdd45 no longer exists
STEP: Creating a pod to test consume service account root CA
Feb  8 22:48:32.060: INFO: Waiting up to 5m0s for pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-v4pn4" in namespace "e2e-tests-svcaccounts-h5krt" to be "success or failure"
Feb  8 22:48:32.066: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-v4pn4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.436062ms
Feb  8 22:48:34.069: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-v4pn4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009127194s
Feb  8 22:48:36.074: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-v4pn4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013509865s
STEP: Saw pod success
Feb  8 22:48:36.074: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-v4pn4" satisfied condition "success or failure"
Feb  8 22:48:36.077: INFO: Trying to get logs from node netztbred3-worker-2 pod pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-v4pn4 container root-ca-test: <nil>
STEP: delete the pod
Feb  8 22:48:36.128: INFO: Waiting for pod pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-v4pn4 to disappear
Feb  8 22:48:36.136: INFO: Pod pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-v4pn4 no longer exists
STEP: Creating a pod to test consume service account namespace
Feb  8 22:48:36.146: INFO: Waiting up to 5m0s for pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-vv7n5" in namespace "e2e-tests-svcaccounts-h5krt" to be "success or failure"
Feb  8 22:48:36.155: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-vv7n5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.831082ms
Feb  8 22:48:38.160: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-vv7n5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013873121s
Feb  8 22:48:40.164: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-vv7n5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017665813s
STEP: Saw pod success
Feb  8 22:48:40.164: INFO: Pod "pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-vv7n5" satisfied condition "success or failure"
Feb  8 22:48:40.167: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-vv7n5 container namespace-test: <nil>
STEP: delete the pod
Feb  8 22:48:40.197: INFO: Waiting for pod pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-vv7n5 to disappear
Feb  8 22:48:40.205: INFO: Pod pod-service-account-a65dd44c-2bf3-11e9-a89e-229fb9a7b2a7-vv7n5 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:48:40.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-h5krt" for this suite.
Feb  8 22:48:46.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:48:46.252: INFO: namespace: e2e-tests-svcaccounts-h5krt, resource: bindings, ignored listing per whitelist
Feb  8 22:48:46.321: INFO: namespace e2e-tests-svcaccounts-h5krt deletion completed in 6.111837994s

• [SLOW TEST:18.924 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:48:46.321: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-wpfdh
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-wpfdh
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-wpfdh
Feb  8 22:48:46.435: INFO: Found 0 stateful pods, waiting for 1
Feb  8 22:48:56.439: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb  8 22:48:56.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-wpfdh ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  8 22:48:56.682: INFO: stderr: ""
Feb  8 22:48:56.682: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  8 22:48:56.682: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  8 22:48:56.686: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb  8 22:49:06.691: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  8 22:49:06.691: INFO: Waiting for statefulset status.replicas updated to 0
Feb  8 22:49:06.714: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999527s
Feb  8 22:49:07.718: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994481991s
Feb  8 22:49:08.722: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990327546s
Feb  8 22:49:09.726: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986203973s
Feb  8 22:49:10.731: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981810256s
Feb  8 22:49:11.739: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976763096s
Feb  8 22:49:12.743: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.969656923s
Feb  8 22:49:13.748: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.965021581s
Feb  8 22:49:14.752: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960545878s
Feb  8 22:49:15.756: INFO: Verifying statefulset ss doesn't scale past 1 for another 956.386796ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-wpfdh
Feb  8 22:49:16.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-wpfdh ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  8 22:49:16.988: INFO: stderr: ""
Feb  8 22:49:16.988: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  8 22:49:16.988: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  8 22:49:16.996: INFO: Found 1 stateful pods, waiting for 3
Feb  8 22:49:27.000: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  8 22:49:27.000: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  8 22:49:27.000: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb  8 22:49:27.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-wpfdh ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  8 22:49:27.204: INFO: stderr: ""
Feb  8 22:49:27.204: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  8 22:49:27.204: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  8 22:49:27.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-wpfdh ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  8 22:49:27.415: INFO: stderr: ""
Feb  8 22:49:27.415: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  8 22:49:27.415: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  8 22:49:27.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-wpfdh ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  8 22:49:27.693: INFO: stderr: ""
Feb  8 22:49:27.693: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  8 22:49:27.693: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  8 22:49:27.693: INFO: Waiting for statefulset status.replicas updated to 0
Feb  8 22:49:27.696: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb  8 22:49:37.705: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  8 22:49:37.705: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb  8 22:49:37.705: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb  8 22:49:37.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999574s
Feb  8 22:49:38.726: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994043564s
Feb  8 22:49:39.731: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98916387s
Feb  8 22:49:40.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984526229s
Feb  8 22:49:41.740: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979699335s
Feb  8 22:49:42.745: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975202917s
Feb  8 22:49:43.750: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970514973s
Feb  8 22:49:44.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965821556s
Feb  8 22:49:45.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960820651s
Feb  8 22:49:46.764: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.821389ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-wpfdh
Feb  8 22:49:47.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-wpfdh ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  8 22:49:47.991: INFO: stderr: ""
Feb  8 22:49:47.991: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  8 22:49:47.991: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  8 22:49:47.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-wpfdh ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  8 22:49:48.214: INFO: stderr: ""
Feb  8 22:49:48.215: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  8 22:49:48.215: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  8 22:49:48.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-wpfdh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  8 22:49:48.429: INFO: stderr: ""
Feb  8 22:49:48.429: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  8 22:49:48.429: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  8 22:49:48.429: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  8 22:50:08.445: INFO: Deleting all statefulset in ns e2e-tests-statefulset-wpfdh
Feb  8 22:50:08.449: INFO: Scaling statefulset ss to 0
Feb  8 22:50:08.459: INFO: Waiting for statefulset status.replicas updated to 0
Feb  8 22:50:08.462: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:50:08.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-wpfdh" for this suite.
Feb  8 22:50:14.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:50:14.548: INFO: namespace: e2e-tests-statefulset-wpfdh, resource: bindings, ignored listing per whitelist
Feb  8 22:50:14.602: INFO: namespace e2e-tests-statefulset-wpfdh deletion completed in 6.116141764s

• [SLOW TEST:88.281 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:50:14.602: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb  8 22:50:14.668: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  8 22:50:14.675: INFO: Waiting for terminating namespaces to be deleted...
Feb  8 22:50:14.678: INFO: 
Logging pods the kubelet thinks is on node netztbred3-worker-1 before test
Feb  8 22:50:14.685: INFO: tiller-deploy-6f8d4f6c9c-lqj4h from kube-system started at 2019-02-08 22:17:15 +0000 UTC (1 container statuses recorded)
Feb  8 22:50:14.685: INFO: 	Container tiller ready: true, restart count 0
Feb  8 22:50:14.685: INFO: dashboard-proxy-846644569f-6szfm from kube-system started at 2019-02-08 22:17:44 +0000 UTC (1 container statuses recorded)
Feb  8 22:50:14.685: INFO: 	Container nginx ready: true, restart count 0
Feb  8 22:50:14.685: INFO: sonobuoy-systemd-logs-daemon-set-173572836c7f431d-lz98h from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 22:50:14.685: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  8 22:50:14.685: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  8 22:50:14.685: INFO: kube-proxy-q2w28 from kube-system started at 2019-02-08 22:16:28 +0000 UTC (1 container statuses recorded)
Feb  8 22:50:14.685: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  8 22:50:14.685: INFO: calico-node-xctdj from kube-system started at 2019-02-08 22:16:28 +0000 UTC (2 container statuses recorded)
Feb  8 22:50:14.685: INFO: 	Container calico-node ready: true, restart count 0
Feb  8 22:50:14.685: INFO: 	Container install-cni ready: true, restart count 0
Feb  8 22:50:14.685: INFO: 
Logging pods the kubelet thinks is on node netztbred3-worker-2 before test
Feb  8 22:50:14.690: INFO: kube-proxy-mcn8h from kube-system started at 2019-02-08 22:16:30 +0000 UTC (1 container statuses recorded)
Feb  8 22:50:14.690: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  8 22:50:14.690: INFO: calico-node-625hk from kube-system started at 2019-02-08 22:16:30 +0000 UTC (2 container statuses recorded)
Feb  8 22:50:14.690: INFO: 	Container calico-node ready: true, restart count 0
Feb  8 22:50:14.690: INFO: 	Container install-cni ready: true, restart count 0
Feb  8 22:50:14.690: INFO: kubernetes-dashboard-5d8785cf74-69q9c from kube-system started at 2019-02-08 22:17:44 +0000 UTC (1 container statuses recorded)
Feb  8 22:50:14.690: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb  8 22:50:14.690: INFO: sonobuoy-e2e-job-ae557681a2834e81 from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 22:50:14.690: INFO: 	Container e2e ready: true, restart count 0
Feb  8 22:50:14.690: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  8 22:50:14.690: INFO: sonobuoy-systemd-logs-daemon-set-173572836c7f431d-cpmxt from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 22:50:14.690: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  8 22:50:14.690: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  8 22:50:14.690: INFO: 
Logging pods the kubelet thinks is on node netztbred3-worker-3 before test
Feb  8 22:50:14.695: INFO: kube-proxy-587fv from kube-system started at 2019-02-08 22:16:29 +0000 UTC (1 container statuses recorded)
Feb  8 22:50:14.695: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  8 22:50:14.695: INFO: calico-node-9srcf from kube-system started at 2019-02-08 22:16:30 +0000 UTC (2 container statuses recorded)
Feb  8 22:50:14.695: INFO: 	Container calico-node ready: true, restart count 0
Feb  8 22:50:14.695: INFO: 	Container install-cni ready: true, restart count 0
Feb  8 22:50:14.695: INFO: heapster-6cb8b844bb-ggjhz from kube-system started at 2019-02-08 22:17:44 +0000 UTC (1 container statuses recorded)
Feb  8 22:50:14.695: INFO: 	Container heapster ready: true, restart count 0
Feb  8 22:50:14.695: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-08 22:19:48 +0000 UTC (1 container statuses recorded)
Feb  8 22:50:14.695: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  8 22:50:14.695: INFO: sonobuoy-systemd-logs-daemon-set-173572836c7f431d-zncqp from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 22:50:14.695: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  8 22:50:14.695: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15818536389ff7ec], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:50:15.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-d75kt" for this suite.
Feb  8 22:50:21.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:50:21.763: INFO: namespace: e2e-tests-sched-pred-d75kt, resource: bindings, ignored listing per whitelist
Feb  8 22:50:21.836: INFO: namespace e2e-tests-sched-pred-d75kt deletion completed in 6.110885948s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.234 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:50:21.836: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-j27kv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j27kv to expose endpoints map[]
Feb  8 22:50:21.922: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j27kv exposes endpoints map[] (4.370202ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-j27kv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j27kv to expose endpoints map[pod1:[100]]
Feb  8 22:50:23.963: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j27kv exposes endpoints map[pod1:[100]] (2.029916834s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-j27kv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j27kv to expose endpoints map[pod1:[100] pod2:[101]]
Feb  8 22:50:27.033: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j27kv exposes endpoints map[pod2:[101] pod1:[100]] (3.061971832s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-j27kv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j27kv to expose endpoints map[pod2:[101]]
Feb  8 22:50:27.053: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j27kv exposes endpoints map[pod2:[101]] (10.15273ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-j27kv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j27kv to expose endpoints map[]
Feb  8 22:50:27.075: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j27kv exposes endpoints map[] (5.704301ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:50:27.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-j27kv" for this suite.
Feb  8 22:50:49.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:50:49.185: INFO: namespace: e2e-tests-services-j27kv, resource: bindings, ignored listing per whitelist
Feb  8 22:50:49.226: INFO: namespace e2e-tests-services-j27kv deletion completed in 22.111972423s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:27.390 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:50:49.226: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb  8 22:50:49.307: INFO: Pod name pod-release: Found 0 pods out of 1
Feb  8 22:50:54.312: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:50:54.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-9mqzk" for this suite.
Feb  8 22:51:00.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:51:00.433: INFO: namespace: e2e-tests-replication-controller-9mqzk, resource: bindings, ignored listing per whitelist
Feb  8 22:51:00.491: INFO: namespace e2e-tests-replication-controller-9mqzk deletion completed in 6.141946722s

• [SLOW TEST:11.265 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:51:00.496: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb  8 22:51:00.572: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb  8 22:51:00.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:00.944: INFO: stderr: ""
Feb  8 22:51:00.944: INFO: stdout: "service/redis-slave created\n"
Feb  8 22:51:00.944: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb  8 22:51:00.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:01.143: INFO: stderr: ""
Feb  8 22:51:01.143: INFO: stdout: "service/redis-master created\n"
Feb  8 22:51:01.143: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb  8 22:51:01.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:01.341: INFO: stderr: ""
Feb  8 22:51:01.341: INFO: stdout: "service/frontend created\n"
Feb  8 22:51:01.343: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb  8 22:51:01.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:01.576: INFO: stderr: ""
Feb  8 22:51:01.576: INFO: stdout: "deployment.extensions/frontend created\n"
Feb  8 22:51:01.576: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb  8 22:51:01.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:01.883: INFO: stderr: ""
Feb  8 22:51:01.883: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb  8 22:51:01.883: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb  8 22:51:01.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:02.134: INFO: stderr: ""
Feb  8 22:51:02.134: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb  8 22:51:02.134: INFO: Waiting for all frontend pods to be Running.
Feb  8 22:51:17.189: INFO: Waiting for frontend to serve content.
Feb  8 22:51:22.208: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb  8 22:51:27.222: INFO: Trying to add a new entry to the guestbook.
Feb  8 22:51:27.233: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb  8 22:51:27.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:27.345: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  8 22:51:27.345: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb  8 22:51:27.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:27.467: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  8 22:51:27.467: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb  8 22:51:27.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:27.585: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  8 22:51:27.585: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb  8 22:51:27.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:27.690: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  8 22:51:27.690: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb  8 22:51:27.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:27.845: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  8 22:51:27.845: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb  8 22:51:27.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cvg82'
Feb  8 22:51:28.042: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  8 22:51:28.042: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:51:28.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cvg82" for this suite.
Feb  8 22:52:12.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:52:12.101: INFO: namespace: e2e-tests-kubectl-cvg82, resource: bindings, ignored listing per whitelist
Feb  8 22:52:12.166: INFO: namespace e2e-tests-kubectl-cvg82 deletion completed in 44.117394638s

• [SLOW TEST:71.670 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:52:12.166: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  8 22:52:12.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-2tqnd'
Feb  8 22:52:12.322: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  8 22:52:12.322: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb  8 22:52:12.351: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-96l4f]
Feb  8 22:52:12.351: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-96l4f" in namespace "e2e-tests-kubectl-2tqnd" to be "running and ready"
Feb  8 22:52:12.367: INFO: Pod "e2e-test-nginx-rc-96l4f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.091365ms
Feb  8 22:52:14.373: INFO: Pod "e2e-test-nginx-rc-96l4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021821304s
Feb  8 22:52:16.376: INFO: Pod "e2e-test-nginx-rc-96l4f": Phase="Running", Reason="", readiness=true. Elapsed: 4.025641612s
Feb  8 22:52:16.377: INFO: Pod "e2e-test-nginx-rc-96l4f" satisfied condition "running and ready"
Feb  8 22:52:16.377: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-96l4f]
Feb  8 22:52:16.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-2tqnd'
Feb  8 22:52:16.486: INFO: stderr: ""
Feb  8 22:52:16.486: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Feb  8 22:52:16.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-2tqnd'
Feb  8 22:52:16.577: INFO: stderr: ""
Feb  8 22:52:16.577: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:52:16.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2tqnd" for this suite.
Feb  8 22:52:22.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:52:22.654: INFO: namespace: e2e-tests-kubectl-2tqnd, resource: bindings, ignored listing per whitelist
Feb  8 22:52:22.682: INFO: namespace e2e-tests-kubectl-2tqnd deletion completed in 6.101977658s

• [SLOW TEST:10.516 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:52:22.682: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 22:52:22.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32570a16-2bf4-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-frkgv" to be "success or failure"
Feb  8 22:52:22.834: INFO: Pod "downwardapi-volume-32570a16-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.312529ms
Feb  8 22:52:24.838: INFO: Pod "downwardapi-volume-32570a16-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010443396s
STEP: Saw pod success
Feb  8 22:52:24.838: INFO: Pod "downwardapi-volume-32570a16-2bf4-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:52:24.841: INFO: Trying to get logs from node netztbred3-worker-3 pod downwardapi-volume-32570a16-2bf4-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 22:52:24.871: INFO: Waiting for pod downwardapi-volume-32570a16-2bf4-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:52:24.875: INFO: Pod downwardapi-volume-32570a16-2bf4-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:52:24.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-frkgv" for this suite.
Feb  8 22:52:30.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:52:30.998: INFO: namespace: e2e-tests-downward-api-frkgv, resource: bindings, ignored listing per whitelist
Feb  8 22:52:31.002: INFO: namespace e2e-tests-downward-api-frkgv deletion completed in 6.12323777s

• [SLOW TEST:8.320 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:52:31.002: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  8 22:52:33.617: INFO: Successfully updated pod "annotationupdate374327e4-2bf4-11e9-a89e-229fb9a7b2a7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:52:35.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-knpx2" for this suite.
Feb  8 22:52:57.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:52:57.690: INFO: namespace: e2e-tests-projected-knpx2, resource: bindings, ignored listing per whitelist
Feb  8 22:52:57.736: INFO: namespace e2e-tests-projected-knpx2 deletion completed in 22.095203091s

• [SLOW TEST:26.734 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:52:57.736: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb  8 22:52:57.829: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:52:57.832: INFO: Number of nodes with available pods: 0
Feb  8 22:52:57.832: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:52:58.838: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:52:58.841: INFO: Number of nodes with available pods: 0
Feb  8 22:52:58.841: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:52:59.840: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:52:59.846: INFO: Number of nodes with available pods: 1
Feb  8 22:52:59.846: INFO: Node netztbred3-worker-2 is running more than one daemon pod
Feb  8 22:53:00.837: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:53:00.841: INFO: Number of nodes with available pods: 1
Feb  8 22:53:00.841: INFO: Node netztbred3-worker-2 is running more than one daemon pod
Feb  8 22:53:01.838: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:53:01.841: INFO: Number of nodes with available pods: 3
Feb  8 22:53:01.841: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb  8 22:53:01.864: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:53:01.883: INFO: Number of nodes with available pods: 2
Feb  8 22:53:01.883: INFO: Node netztbred3-worker-2 is running more than one daemon pod
Feb  8 22:53:02.888: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:53:02.892: INFO: Number of nodes with available pods: 2
Feb  8 22:53:02.892: INFO: Node netztbred3-worker-2 is running more than one daemon pod
Feb  8 22:53:03.888: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:53:03.892: INFO: Number of nodes with available pods: 2
Feb  8 22:53:03.892: INFO: Node netztbred3-worker-2 is running more than one daemon pod
Feb  8 22:53:04.888: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:53:04.891: INFO: Number of nodes with available pods: 3
Feb  8 22:53:04.891: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-7pqrs, will wait for the garbage collector to delete the pods
Feb  8 22:53:04.964: INFO: Deleting DaemonSet.extensions daemon-set took: 14.398723ms
Feb  8 22:53:05.064: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.192441ms
Feb  8 22:53:47.868: INFO: Number of nodes with available pods: 0
Feb  8 22:53:47.868: INFO: Number of running nodes: 0, number of available pods: 0
Feb  8 22:53:47.870: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-7pqrs/daemonsets","resourceVersion":"9052"},"items":null}

Feb  8 22:53:47.873: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-7pqrs/pods","resourceVersion":"9052"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:53:47.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-7pqrs" for this suite.
Feb  8 22:53:53.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:53:53.949: INFO: namespace: e2e-tests-daemonsets-7pqrs, resource: bindings, ignored listing per whitelist
Feb  8 22:53:53.984: INFO: namespace e2e-tests-daemonsets-7pqrs deletion completed in 6.09657964s

• [SLOW TEST:56.248 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:53:53.985: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb  8 22:53:54.056: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-a,UID:68b99727-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9097,Generation:0,CreationTimestamp:2019-02-08 22:53:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  8 22:53:54.056: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-a,UID:68b99727-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9097,Generation:0,CreationTimestamp:2019-02-08 22:53:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb  8 22:54:04.065: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-a,UID:68b99727-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9113,Generation:0,CreationTimestamp:2019-02-08 22:53:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb  8 22:54:04.065: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-a,UID:68b99727-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9113,Generation:0,CreationTimestamp:2019-02-08 22:53:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb  8 22:54:14.075: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-a,UID:68b99727-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9133,Generation:0,CreationTimestamp:2019-02-08 22:53:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  8 22:54:14.075: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-a,UID:68b99727-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9133,Generation:0,CreationTimestamp:2019-02-08 22:53:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb  8 22:54:24.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-a,UID:68b99727-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9149,Generation:0,CreationTimestamp:2019-02-08 22:53:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  8 22:54:24.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-a,UID:68b99727-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9149,Generation:0,CreationTimestamp:2019-02-08 22:53:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb  8 22:54:34.094: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-b,UID:8096776b-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9166,Generation:0,CreationTimestamp:2019-02-08 22:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  8 22:54:34.094: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-b,UID:8096776b-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9166,Generation:0,CreationTimestamp:2019-02-08 22:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb  8 22:54:44.103: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-b,UID:8096776b-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9185,Generation:0,CreationTimestamp:2019-02-08 22:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  8 22:54:44.103: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-w7wql,SelfLink:/api/v1/namespaces/e2e-tests-watch-w7wql/configmaps/e2e-watch-test-configmap-b,UID:8096776b-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9185,Generation:0,CreationTimestamp:2019-02-08 22:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:54:54.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-w7wql" for this suite.
Feb  8 22:55:00.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:55:00.216: INFO: namespace: e2e-tests-watch-w7wql, resource: bindings, ignored listing per whitelist
Feb  8 22:55:00.217: INFO: namespace e2e-tests-watch-w7wql deletion completed in 6.108903246s

• [SLOW TEST:66.233 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:55:00.217: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb  8 22:55:02.317: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-90349f48-2bf4-11e9-a89e-229fb9a7b2a7,GenerateName:,Namespace:e2e-tests-events-mj4ph,SelfLink:/api/v1/namespaces/e2e-tests-events-mj4ph/pods/send-events-90349f48-2bf4-11e9-a89e-229fb9a7b2a7,UID:903521df-2bf4-11e9-a0ab-42010a8a0033,ResourceVersion:9234,Generation:0,CreationTimestamp:2019-02-08 22:55:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 289317915,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.2.45/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d5smw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d5smw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-d5smw true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a65670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a65690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 22:55:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 22:55:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 22:55:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 22:55:00 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.54,PodIP:10.2.2.45,StartTime:2019-02-08 22:55:00 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-08 22:55:01 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://f86ecacbc873107ac63115db85ddc4e9ab8323bc04d7d1de342106f84b1c6268}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb  8 22:55:04.322: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb  8 22:55:06.326: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:55:06.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-mj4ph" for this suite.
Feb  8 22:55:40.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:55:40.410: INFO: namespace: e2e-tests-events-mj4ph, resource: bindings, ignored listing per whitelist
Feb  8 22:55:40.439: INFO: namespace e2e-tests-events-mj4ph deletion completed in 34.097801046s

• [SLOW TEST:40.222 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:55:40.440: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 22:55:40.518: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a82d78bb-2bf4-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-5jtg4" to be "success or failure"
Feb  8 22:55:40.531: INFO: Pod "downwardapi-volume-a82d78bb-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.286291ms
Feb  8 22:55:42.542: INFO: Pod "downwardapi-volume-a82d78bb-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023716277s
STEP: Saw pod success
Feb  8 22:55:42.542: INFO: Pod "downwardapi-volume-a82d78bb-2bf4-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:55:42.545: INFO: Trying to get logs from node netztbred3-worker-1 pod downwardapi-volume-a82d78bb-2bf4-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 22:55:42.576: INFO: Waiting for pod downwardapi-volume-a82d78bb-2bf4-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:55:42.591: INFO: Pod downwardapi-volume-a82d78bb-2bf4-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:55:42.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5jtg4" for this suite.
Feb  8 22:55:48.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:55:48.681: INFO: namespace: e2e-tests-projected-5jtg4, resource: bindings, ignored listing per whitelist
Feb  8 22:55:48.699: INFO: namespace e2e-tests-projected-5jtg4 deletion completed in 6.103322299s

• [SLOW TEST:8.259 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:55:48.699: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb  8 22:55:52.796: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-ad18784e-2bf4-11e9-a89e-229fb9a7b2a7", GenerateName:"", Namespace:"e2e-tests-pods-zt2ws", SelfLink:"/api/v1/namespaces/e2e-tests-pods-zt2ws/pods/pod-submit-remove-ad18784e-2bf4-11e9-a89e-229fb9a7b2a7", UID:"ad19a978-2bf4-11e9-a0ab-42010a8a0033", ResourceVersion:"9367", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63685263348, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"758744935"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.2.2.46/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-zp7j5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001ca8340), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zp7j5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001f7c268), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"netztbred3-worker-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00210a060), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f7c2b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f7c2d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001f7c2d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001f7c2dc)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685263348, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685263351, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685263351, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685263348, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.138.0.54", PodIP:"10.2.2.46", StartTime:(*v1.Time)(0xc001b08a00), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001b08a20), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://3e6cba7f661d78af4c726feb1f5d4855c9502268e02737981693bcd5b01df199"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:55:59.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zt2ws" for this suite.
Feb  8 22:56:05.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:56:05.775: INFO: namespace: e2e-tests-pods-zt2ws, resource: bindings, ignored listing per whitelist
Feb  8 22:56:05.836: INFO: namespace e2e-tests-pods-zt2ws deletion completed in 6.108671446s

• [SLOW TEST:17.137 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:56:05.836: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-rppc9/configmap-test-b750d9a6-2bf4-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 22:56:05.920: INFO: Waiting up to 5m0s for pod "pod-configmaps-b751c4f9-2bf4-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-configmap-rppc9" to be "success or failure"
Feb  8 22:56:05.929: INFO: Pod "pod-configmaps-b751c4f9-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.686374ms
Feb  8 22:56:07.936: INFO: Pod "pod-configmaps-b751c4f9-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016084592s
STEP: Saw pod success
Feb  8 22:56:07.936: INFO: Pod "pod-configmaps-b751c4f9-2bf4-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:56:07.940: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-configmaps-b751c4f9-2bf4-11e9-a89e-229fb9a7b2a7 container env-test: <nil>
STEP: delete the pod
Feb  8 22:56:07.964: INFO: Waiting for pod pod-configmaps-b751c4f9-2bf4-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:56:07.970: INFO: Pod pod-configmaps-b751c4f9-2bf4-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:56:07.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rppc9" for this suite.
Feb  8 22:56:13.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:56:14.064: INFO: namespace: e2e-tests-configmap-rppc9, resource: bindings, ignored listing per whitelist
Feb  8 22:56:14.088: INFO: namespace e2e-tests-configmap-rppc9 deletion completed in 6.113645824s

• [SLOW TEST:8.252 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:56:14.089: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb  8 22:56:14.176: INFO: Waiting up to 5m0s for pod "pod-bc3d2350-2bf4-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-7k2pt" to be "success or failure"
Feb  8 22:56:14.185: INFO: Pod "pod-bc3d2350-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.509121ms
Feb  8 22:56:16.189: INFO: Pod "pod-bc3d2350-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012417747s
Feb  8 22:56:18.193: INFO: Pod "pod-bc3d2350-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016645943s
STEP: Saw pod success
Feb  8 22:56:18.193: INFO: Pod "pod-bc3d2350-2bf4-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:56:18.196: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-bc3d2350-2bf4-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 22:56:18.231: INFO: Waiting for pod pod-bc3d2350-2bf4-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:56:18.236: INFO: Pod pod-bc3d2350-2bf4-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:56:18.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7k2pt" for this suite.
Feb  8 22:56:24.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:56:24.345: INFO: namespace: e2e-tests-emptydir-7k2pt, resource: bindings, ignored listing per whitelist
Feb  8 22:56:24.350: INFO: namespace e2e-tests-emptydir-7k2pt deletion completed in 6.105635688s

• [SLOW TEST:10.261 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:56:24.350: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb  8 22:56:24.425: INFO: Waiting up to 5m0s for pod "pod-c2592c91-2bf4-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-r6mtq" to be "success or failure"
Feb  8 22:56:24.434: INFO: Pod "pod-c2592c91-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.431087ms
Feb  8 22:56:26.438: INFO: Pod "pod-c2592c91-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013668294s
Feb  8 22:56:28.442: INFO: Pod "pod-c2592c91-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017424535s
STEP: Saw pod success
Feb  8 22:56:28.442: INFO: Pod "pod-c2592c91-2bf4-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:56:28.445: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-c2592c91-2bf4-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 22:56:28.471: INFO: Waiting for pod pod-c2592c91-2bf4-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:56:28.482: INFO: Pod pod-c2592c91-2bf4-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:56:28.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-r6mtq" for this suite.
Feb  8 22:56:34.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:56:34.526: INFO: namespace: e2e-tests-emptydir-r6mtq, resource: bindings, ignored listing per whitelist
Feb  8 22:56:34.606: INFO: namespace e2e-tests-emptydir-r6mtq deletion completed in 6.119028115s

• [SLOW TEST:10.256 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:56:34.606: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb  8 22:56:36.702: INFO: Pod pod-hostip-c876eca3-2bf4-11e9-a89e-229fb9a7b2a7 has hostIP: 10.138.0.54
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:56:36.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-fj85g" for this suite.
Feb  8 22:56:58.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:56:58.755: INFO: namespace: e2e-tests-pods-fj85g, resource: bindings, ignored listing per whitelist
Feb  8 22:56:58.804: INFO: namespace e2e-tests-pods-fj85g deletion completed in 22.097112147s

• [SLOW TEST:24.198 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:56:58.804: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:57:26.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-6wds5" for this suite.
Feb  8 22:57:32.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:57:32.293: INFO: namespace: e2e-tests-container-runtime-6wds5, resource: bindings, ignored listing per whitelist
Feb  8 22:57:32.305: INFO: namespace e2e-tests-container-runtime-6wds5 deletion completed in 6.11773463s

• [SLOW TEST:33.500 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:57:32.305: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb  8 22:57:32.407: INFO: Waiting up to 5m0s for pod "var-expansion-eade0c73-2bf4-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-var-expansion-qmclw" to be "success or failure"
Feb  8 22:57:32.415: INFO: Pod "var-expansion-eade0c73-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.58346ms
Feb  8 22:57:34.418: INFO: Pod "var-expansion-eade0c73-2bf4-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011053388s
STEP: Saw pod success
Feb  8 22:57:34.419: INFO: Pod "var-expansion-eade0c73-2bf4-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:57:34.421: INFO: Trying to get logs from node netztbred3-worker-3 pod var-expansion-eade0c73-2bf4-11e9-a89e-229fb9a7b2a7 container dapi-container: <nil>
STEP: delete the pod
Feb  8 22:57:34.444: INFO: Waiting for pod var-expansion-eade0c73-2bf4-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:57:34.451: INFO: Pod var-expansion-eade0c73-2bf4-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:57:34.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-qmclw" for this suite.
Feb  8 22:57:40.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:57:40.536: INFO: namespace: e2e-tests-var-expansion-qmclw, resource: bindings, ignored listing per whitelist
Feb  8 22:57:40.549: INFO: namespace e2e-tests-var-expansion-qmclw deletion completed in 6.093511717s

• [SLOW TEST:8.244 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:57:40.549: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-zt2dd
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  8 22:57:40.658: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  8 22:58:02.782: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.57:8080/dial?request=hostName&protocol=udp&host=10.2.1.56&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-zt2dd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 22:58:02.782: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 22:58:02.917: INFO: Waiting for endpoints: map[]
Feb  8 22:58:02.921: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.57:8080/dial?request=hostName&protocol=udp&host=10.2.2.51&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-zt2dd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 22:58:02.921: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 22:58:03.065: INFO: Waiting for endpoints: map[]
Feb  8 22:58:03.069: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.57:8080/dial?request=hostName&protocol=udp&host=10.2.3.15&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-zt2dd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 22:58:03.069: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 22:58:03.214: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:58:03.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-zt2dd" for this suite.
Feb  8 22:58:25.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:58:25.282: INFO: namespace: e2e-tests-pod-network-test-zt2dd, resource: bindings, ignored listing per whitelist
Feb  8 22:58:25.323: INFO: namespace e2e-tests-pod-network-test-zt2dd deletion completed in 22.103666742s

• [SLOW TEST:44.774 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:58:25.323: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb  8 22:58:25.422: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:25.425: INFO: Number of nodes with available pods: 0
Feb  8 22:58:25.425: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:26.430: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:26.433: INFO: Number of nodes with available pods: 0
Feb  8 22:58:26.433: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:27.432: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:27.447: INFO: Number of nodes with available pods: 2
Feb  8 22:58:27.447: INFO: Node netztbred3-worker-3 is running more than one daemon pod
Feb  8 22:58:28.430: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:28.433: INFO: Number of nodes with available pods: 3
Feb  8 22:58:28.433: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb  8 22:58:28.453: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:28.457: INFO: Number of nodes with available pods: 2
Feb  8 22:58:28.457: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:29.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:29.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:29.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:30.461: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:30.464: INFO: Number of nodes with available pods: 2
Feb  8 22:58:30.464: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:31.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:31.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:31.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:32.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:32.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:32.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:33.461: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:33.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:33.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:34.461: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:34.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:34.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:35.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:35.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:35.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:36.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:36.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:36.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:37.461: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:37.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:37.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:38.461: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:38.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:38.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:39.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:39.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:39.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:40.461: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:40.464: INFO: Number of nodes with available pods: 2
Feb  8 22:58:40.464: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:41.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:41.466: INFO: Number of nodes with available pods: 2
Feb  8 22:58:41.466: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:42.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:42.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:42.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:43.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:43.466: INFO: Number of nodes with available pods: 2
Feb  8 22:58:43.466: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:44.461: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:44.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:44.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:45.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:45.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:45.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:46.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:46.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:46.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:47.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:47.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:47.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:48.461: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:48.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:48.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:49.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:49.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:49.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:50.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:50.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:50.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:51.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:51.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:51.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:52.469: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:52.476: INFO: Number of nodes with available pods: 2
Feb  8 22:58:52.476: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:53.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:53.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:53.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:54.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:54.466: INFO: Number of nodes with available pods: 2
Feb  8 22:58:54.466: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:55.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:55.466: INFO: Number of nodes with available pods: 2
Feb  8 22:58:55.466: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:56.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:56.465: INFO: Number of nodes with available pods: 2
Feb  8 22:58:56.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:57.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:57.466: INFO: Number of nodes with available pods: 2
Feb  8 22:58:57.466: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:58.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:58.466: INFO: Number of nodes with available pods: 2
Feb  8 22:58:58.466: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:58:59.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:58:59.466: INFO: Number of nodes with available pods: 2
Feb  8 22:58:59.466: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:59:00.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:59:00.466: INFO: Number of nodes with available pods: 2
Feb  8 22:59:00.466: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:59:01.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:59:01.465: INFO: Number of nodes with available pods: 2
Feb  8 22:59:01.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:59:02.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:59:02.466: INFO: Number of nodes with available pods: 2
Feb  8 22:59:02.466: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:59:03.462: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:59:03.465: INFO: Number of nodes with available pods: 2
Feb  8 22:59:03.465: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:59:04.463: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:59:04.466: INFO: Number of nodes with available pods: 2
Feb  8 22:59:04.466: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 22:59:05.463: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 22:59:05.467: INFO: Number of nodes with available pods: 3
Feb  8 22:59:05.467: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-8gjvp, will wait for the garbage collector to delete the pods
Feb  8 22:59:05.535: INFO: Deleting DaemonSet.extensions daemon-set took: 11.769982ms
Feb  8 22:59:05.636: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.227738ms
Feb  8 22:59:47.840: INFO: Number of nodes with available pods: 0
Feb  8 22:59:47.840: INFO: Number of running nodes: 0, number of available pods: 0
Feb  8 22:59:47.843: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-8gjvp/daemonsets","resourceVersion":"10137"},"items":null}

Feb  8 22:59:47.846: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-8gjvp/pods","resourceVersion":"10137"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:59:47.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-8gjvp" for this suite.
Feb  8 22:59:53.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 22:59:53.897: INFO: namespace: e2e-tests-daemonsets-8gjvp, resource: bindings, ignored listing per whitelist
Feb  8 22:59:53.970: INFO: namespace e2e-tests-daemonsets-8gjvp deletion completed in 6.108000542s

• [SLOW TEST:88.647 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 22:59:53.970: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 22:59:54.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f4bd729-2bf5-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-2xpw8" to be "success or failure"
Feb  8 22:59:54.071: INFO: Pod "downwardapi-volume-3f4bd729-2bf5-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.010963ms
Feb  8 22:59:56.075: INFO: Pod "downwardapi-volume-3f4bd729-2bf5-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011700557s
Feb  8 22:59:58.080: INFO: Pod "downwardapi-volume-3f4bd729-2bf5-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015898486s
STEP: Saw pod success
Feb  8 22:59:58.080: INFO: Pod "downwardapi-volume-3f4bd729-2bf5-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 22:59:58.083: INFO: Trying to get logs from node netztbred3-worker-1 pod downwardapi-volume-3f4bd729-2bf5-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 22:59:58.115: INFO: Waiting for pod downwardapi-volume-3f4bd729-2bf5-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 22:59:58.121: INFO: Pod downwardapi-volume-3f4bd729-2bf5-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 22:59:58.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2xpw8" for this suite.
Feb  8 23:00:04.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:00:04.166: INFO: namespace: e2e-tests-downward-api-2xpw8, resource: bindings, ignored listing per whitelist
Feb  8 23:00:04.229: INFO: namespace e2e-tests-downward-api-2xpw8 deletion completed in 6.102997591s

• [SLOW TEST:10.259 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:00:04.230: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  8 23:00:04.299: INFO: PodSpec: initContainers in spec.initContainers
Feb  8 23:00:48.673: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4568d997-2bf5-11e9-a89e-229fb9a7b2a7", GenerateName:"", Namespace:"e2e-tests-init-container-k67sg", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-k67sg/pods/pod-init-4568d997-2bf5-11e9-a89e-229fb9a7b2a7", UID:"45695b7d-2bf5-11e9-a0ab-42010a8a0033", ResourceVersion:"10321", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63685263604, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"299206709"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.2.2.53/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-79ct4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001c6c280), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-79ct4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-79ct4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-79ct4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0019492c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"netztbred3-worker-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00177cf60), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001949b00)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001949b20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001949b28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001949b2c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685263604, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685263604, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685263604, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685263604, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.138.0.54", PodIP:"10.2.2.53", StartTime:(*v1.Time)(0xc00114a6c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00036df10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00036df80)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://4debf906623020de2aed49d4fa0f09eefee2fcdef5cff8ac251dc5ef6c53ad1f"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00114a740), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00114a700), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:00:48.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-k67sg" for this suite.
Feb  8 23:01:10.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:01:10.716: INFO: namespace: e2e-tests-init-container-k67sg, resource: bindings, ignored listing per whitelist
Feb  8 23:01:10.780: INFO: namespace e2e-tests-init-container-k67sg deletion completed in 22.101322198s

• [SLOW TEST:66.550 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:01:10.781: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-fspjl
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb  8 23:01:10.863: INFO: Found 0 stateful pods, waiting for 3
Feb  8 23:01:20.869: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  8 23:01:20.869: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  8 23:01:20.869: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb  8 23:01:20.902: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb  8 23:01:30.943: INFO: Updating stateful set ss2
Feb  8 23:01:30.957: INFO: Waiting for Pod e2e-tests-statefulset-fspjl/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb  8 23:01:41.073: INFO: Found 2 stateful pods, waiting for 3
Feb  8 23:01:51.078: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  8 23:01:51.078: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  8 23:01:51.078: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb  8 23:01:51.106: INFO: Updating stateful set ss2
Feb  8 23:01:51.114: INFO: Waiting for Pod e2e-tests-statefulset-fspjl/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  8 23:02:01.147: INFO: Updating stateful set ss2
Feb  8 23:02:01.157: INFO: Waiting for StatefulSet e2e-tests-statefulset-fspjl/ss2 to complete update
Feb  8 23:02:01.158: INFO: Waiting for Pod e2e-tests-statefulset-fspjl/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  8 23:02:11.165: INFO: Deleting all statefulset in ns e2e-tests-statefulset-fspjl
Feb  8 23:02:11.167: INFO: Scaling statefulset ss2 to 0
Feb  8 23:02:41.191: INFO: Waiting for statefulset status.replicas updated to 0
Feb  8 23:02:41.194: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:02:41.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-fspjl" for this suite.
Feb  8 23:02:47.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:02:47.357: INFO: namespace: e2e-tests-statefulset-fspjl, resource: bindings, ignored listing per whitelist
Feb  8 23:02:47.360: INFO: namespace e2e-tests-statefulset-fspjl deletion completed in 6.133453321s

• [SLOW TEST:96.579 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:02:47.360: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0208 23:02:53.487046      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  8 23:02:53.487: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:02:53.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-q7zkp" for this suite.
Feb  8 23:02:59.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:02:59.545: INFO: namespace: e2e-tests-gc-q7zkp, resource: bindings, ignored listing per whitelist
Feb  8 23:02:59.589: INFO: namespace e2e-tests-gc-q7zkp deletion completed in 6.098414449s

• [SLOW TEST:12.229 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:02:59.589: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:03:23.687: INFO: Container started at 2019-02-08 23:03:01 +0000 UTC, pod became ready at 2019-02-08 23:03:23 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:03:23.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-jbjjx" for this suite.
Feb  8 23:03:45.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:03:45.806: INFO: namespace: e2e-tests-container-probe-jbjjx, resource: bindings, ignored listing per whitelist
Feb  8 23:03:45.825: INFO: namespace e2e-tests-container-probe-jbjjx deletion completed in 22.133217148s

• [SLOW TEST:46.235 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:03:45.825: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  8 23:03:45.892: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:03:48.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-gsvgz" for this suite.
Feb  8 23:03:54.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:03:55.005: INFO: namespace: e2e-tests-init-container-gsvgz, resource: bindings, ignored listing per whitelist
Feb  8 23:03:55.066: INFO: namespace e2e-tests-init-container-gsvgz deletion completed in 6.09616369s

• [SLOW TEST:9.241 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:03:55.066: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-nc7vg.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-nc7vg.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-nc7vg.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-nc7vg.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-nc7vg.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-nc7vg.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  8 23:04:07.224: INFO: DNS probes using e2e-tests-dns-nc7vg/dns-test-cefeb8ab-2bf5-11e9-a89e-229fb9a7b2a7 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:04:07.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-nc7vg" for this suite.
Feb  8 23:04:13.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:04:13.307: INFO: namespace: e2e-tests-dns-nc7vg, resource: bindings, ignored listing per whitelist
Feb  8 23:04:13.354: INFO: namespace e2e-tests-dns-nc7vg deletion completed in 6.107207576s

• [SLOW TEST:18.288 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:04:13.354: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-bgtf9
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  8 23:04:13.424: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  8 23:04:33.598: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.2.63:8080/dial?request=hostName&protocol=http&host=10.2.2.62&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-bgtf9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:04:33.598: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:04:33.739: INFO: Waiting for endpoints: map[]
Feb  8 23:04:33.742: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.2.63:8080/dial?request=hostName&protocol=http&host=10.2.3.23&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-bgtf9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:04:33.742: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:04:33.895: INFO: Waiting for endpoints: map[]
Feb  8 23:04:33.898: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.2.63:8080/dial?request=hostName&protocol=http&host=10.2.1.68&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-bgtf9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:04:33.898: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:04:34.056: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:04:34.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-bgtf9" for this suite.
Feb  8 23:04:56.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:04:56.160: INFO: namespace: e2e-tests-pod-network-test-bgtf9, resource: bindings, ignored listing per whitelist
Feb  8 23:04:56.172: INFO: namespace e2e-tests-pod-network-test-bgtf9 deletion completed in 22.110609881s

• [SLOW TEST:42.818 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:04:56.172: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  8 23:04:56.238: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:05:00.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-fp52t" for this suite.
Feb  8 23:05:06.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:05:06.280: INFO: namespace: e2e-tests-init-container-fp52t, resource: bindings, ignored listing per whitelist
Feb  8 23:05:06.347: INFO: namespace e2e-tests-init-container-fp52t deletion completed in 6.097472796s

• [SLOW TEST:10.175 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:05:06.347: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb  8 23:05:06.418: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  8 23:05:06.426: INFO: Waiting for terminating namespaces to be deleted...
Feb  8 23:05:06.428: INFO: 
Logging pods the kubelet thinks is on node netztbred3-worker-1 before test
Feb  8 23:05:06.436: INFO: calico-node-xctdj from kube-system started at 2019-02-08 22:16:28 +0000 UTC (2 container statuses recorded)
Feb  8 23:05:06.436: INFO: 	Container calico-node ready: true, restart count 0
Feb  8 23:05:06.436: INFO: 	Container install-cni ready: true, restart count 0
Feb  8 23:05:06.436: INFO: tiller-deploy-6f8d4f6c9c-lqj4h from kube-system started at 2019-02-08 22:17:15 +0000 UTC (1 container statuses recorded)
Feb  8 23:05:06.436: INFO: 	Container tiller ready: true, restart count 0
Feb  8 23:05:06.436: INFO: dashboard-proxy-846644569f-6szfm from kube-system started at 2019-02-08 22:17:44 +0000 UTC (1 container statuses recorded)
Feb  8 23:05:06.436: INFO: 	Container nginx ready: true, restart count 0
Feb  8 23:05:06.436: INFO: sonobuoy-systemd-logs-daemon-set-173572836c7f431d-lz98h from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 23:05:06.436: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  8 23:05:06.436: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  8 23:05:06.436: INFO: kube-proxy-q2w28 from kube-system started at 2019-02-08 22:16:28 +0000 UTC (1 container statuses recorded)
Feb  8 23:05:06.436: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  8 23:05:06.436: INFO: 
Logging pods the kubelet thinks is on node netztbred3-worker-2 before test
Feb  8 23:05:06.443: INFO: kubernetes-dashboard-5d8785cf74-69q9c from kube-system started at 2019-02-08 22:17:44 +0000 UTC (1 container statuses recorded)
Feb  8 23:05:06.443: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb  8 23:05:06.443: INFO: sonobuoy-e2e-job-ae557681a2834e81 from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 23:05:06.443: INFO: 	Container e2e ready: true, restart count 0
Feb  8 23:05:06.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  8 23:05:06.443: INFO: sonobuoy-systemd-logs-daemon-set-173572836c7f431d-cpmxt from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 23:05:06.444: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  8 23:05:06.444: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  8 23:05:06.444: INFO: kube-proxy-mcn8h from kube-system started at 2019-02-08 22:16:30 +0000 UTC (1 container statuses recorded)
Feb  8 23:05:06.444: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  8 23:05:06.444: INFO: calico-node-625hk from kube-system started at 2019-02-08 22:16:30 +0000 UTC (2 container statuses recorded)
Feb  8 23:05:06.444: INFO: 	Container calico-node ready: true, restart count 0
Feb  8 23:05:06.444: INFO: 	Container install-cni ready: true, restart count 0
Feb  8 23:05:06.444: INFO: 
Logging pods the kubelet thinks is on node netztbred3-worker-3 before test
Feb  8 23:05:06.451: INFO: calico-node-9srcf from kube-system started at 2019-02-08 22:16:30 +0000 UTC (2 container statuses recorded)
Feb  8 23:05:06.451: INFO: 	Container calico-node ready: true, restart count 0
Feb  8 23:05:06.451: INFO: 	Container install-cni ready: true, restart count 0
Feb  8 23:05:06.451: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-08 22:19:48 +0000 UTC (1 container statuses recorded)
Feb  8 23:05:06.451: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  8 23:05:06.451: INFO: kube-proxy-587fv from kube-system started at 2019-02-08 22:16:29 +0000 UTC (1 container statuses recorded)
Feb  8 23:05:06.451: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  8 23:05:06.451: INFO: heapster-6cb8b844bb-ggjhz from kube-system started at 2019-02-08 22:17:44 +0000 UTC (1 container statuses recorded)
Feb  8 23:05:06.451: INFO: 	Container heapster ready: true, restart count 0
Feb  8 23:05:06.451: INFO: sonobuoy-systemd-logs-daemon-set-173572836c7f431d-zncqp from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 23:05:06.451: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  8 23:05:06.451: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-fbe9dfa1-2bf5-11e9-a89e-229fb9a7b2a7 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-fbe9dfa1-2bf5-11e9-a89e-229fb9a7b2a7 off the node netztbred3-worker-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fbe9dfa1-2bf5-11e9-a89e-229fb9a7b2a7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:05:12.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-hrjlm" for this suite.
Feb  8 23:05:30.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:05:30.608: INFO: namespace: e2e-tests-sched-pred-hrjlm, resource: bindings, ignored listing per whitelist
Feb  8 23:05:30.666: INFO: namespace e2e-tests-sched-pred-hrjlm deletion completed in 18.103525125s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:24.319 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:05:30.666: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-07faf018-2bf6-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 23:05:30.751: INFO: Waiting up to 5m0s for pod "pod-configmaps-07fb929d-2bf6-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-configmap-8jjxb" to be "success or failure"
Feb  8 23:05:30.757: INFO: Pod "pod-configmaps-07fb929d-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.247248ms
Feb  8 23:05:32.761: INFO: Pod "pod-configmaps-07fb929d-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01095776s
STEP: Saw pod success
Feb  8 23:05:32.762: INFO: Pod "pod-configmaps-07fb929d-2bf6-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:05:32.765: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-configmaps-07fb929d-2bf6-11e9-a89e-229fb9a7b2a7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 23:05:32.794: INFO: Waiting for pod pod-configmaps-07fb929d-2bf6-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:05:32.801: INFO: Pod pod-configmaps-07fb929d-2bf6-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:05:32.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8jjxb" for this suite.
Feb  8 23:05:38.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:05:38.898: INFO: namespace: e2e-tests-configmap-8jjxb, resource: bindings, ignored listing per whitelist
Feb  8 23:05:38.906: INFO: namespace e2e-tests-configmap-8jjxb deletion completed in 6.102032026s

• [SLOW TEST:8.240 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:05:38.907: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-0ced0a71-2bf6-11e9-a89e-229fb9a7b2a7
STEP: Creating secret with name secret-projected-all-test-volume-0ced0a56-2bf6-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb  8 23:05:39.052: INFO: Waiting up to 5m0s for pod "projected-volume-0ced0a15-2bf6-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-wrfcm" to be "success or failure"
Feb  8 23:05:39.065: INFO: Pod "projected-volume-0ced0a15-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.885945ms
Feb  8 23:05:41.068: INFO: Pod "projected-volume-0ced0a15-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016161656s
STEP: Saw pod success
Feb  8 23:05:41.068: INFO: Pod "projected-volume-0ced0a15-2bf6-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:05:41.072: INFO: Trying to get logs from node netztbred3-worker-1 pod projected-volume-0ced0a15-2bf6-11e9-a89e-229fb9a7b2a7 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb  8 23:05:41.100: INFO: Waiting for pod projected-volume-0ced0a15-2bf6-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:05:41.122: INFO: Pod projected-volume-0ced0a15-2bf6-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:05:41.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wrfcm" for this suite.
Feb  8 23:05:47.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:05:47.154: INFO: namespace: e2e-tests-projected-wrfcm, resource: bindings, ignored listing per whitelist
Feb  8 23:05:47.229: INFO: namespace e2e-tests-projected-wrfcm deletion completed in 6.103665918s

• [SLOW TEST:8.323 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:05:47.230: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  8 23:05:47.310: INFO: Waiting up to 5m0s for pod "downward-api-11da5967-2bf6-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-jntmq" to be "success or failure"
Feb  8 23:05:47.313: INFO: Pod "downward-api-11da5967-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881034ms
Feb  8 23:05:49.317: INFO: Pod "downward-api-11da5967-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007811685s
Feb  8 23:05:51.322: INFO: Pod "downward-api-11da5967-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011991423s
STEP: Saw pod success
Feb  8 23:05:51.322: INFO: Pod "downward-api-11da5967-2bf6-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:05:51.325: INFO: Trying to get logs from node netztbred3-worker-3 pod downward-api-11da5967-2bf6-11e9-a89e-229fb9a7b2a7 container dapi-container: <nil>
STEP: delete the pod
Feb  8 23:05:51.350: INFO: Waiting for pod downward-api-11da5967-2bf6-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:05:51.352: INFO: Pod downward-api-11da5967-2bf6-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:05:51.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jntmq" for this suite.
Feb  8 23:05:57.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:05:57.414: INFO: namespace: e2e-tests-downward-api-jntmq, resource: bindings, ignored listing per whitelist
Feb  8 23:05:57.458: INFO: namespace e2e-tests-downward-api-jntmq deletion completed in 6.101149748s

• [SLOW TEST:10.228 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:05:57.458: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb  8 23:05:57.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 cluster-info'
Feb  8 23:05:57.729: INFO: stderr: ""
Feb  8 23:05:57.729: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443\x1b[0m\n\x1b[0;32mHeapster\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443/api/v1/namespaces/kube-system/services/heapster/proxy\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:05:57.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-kcmws" for this suite.
Feb  8 23:06:03.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:06:03.770: INFO: namespace: e2e-tests-kubectl-kcmws, resource: bindings, ignored listing per whitelist
Feb  8 23:06:03.838: INFO: namespace e2e-tests-kubectl-kcmws deletion completed in 6.104888717s

• [SLOW TEST:6.380 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:06:03.838: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:06:07.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-6xlwl" for this suite.
Feb  8 23:06:13.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:06:13.972: INFO: namespace: e2e-tests-kubelet-test-6xlwl, resource: bindings, ignored listing per whitelist
Feb  8 23:06:14.022: INFO: namespace e2e-tests-kubelet-test-6xlwl deletion completed in 6.100402645s

• [SLOW TEST:10.183 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:06:14.022: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-21d337df-2bf6-11e9-a89e-229fb9a7b2a7
STEP: Creating configMap with name cm-test-opt-upd-21d3381d-2bf6-11e9-a89e-229fb9a7b2a7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-21d337df-2bf6-11e9-a89e-229fb9a7b2a7
STEP: Updating configmap cm-test-opt-upd-21d3381d-2bf6-11e9-a89e-229fb9a7b2a7
STEP: Creating configMap with name cm-test-opt-create-21d3382f-2bf6-11e9-a89e-229fb9a7b2a7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:07:42.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-n2gql" for this suite.
Feb  8 23:08:04.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:08:04.698: INFO: namespace: e2e-tests-configmap-n2gql, resource: bindings, ignored listing per whitelist
Feb  8 23:08:04.759: INFO: namespace e2e-tests-configmap-n2gql deletion completed in 22.11201927s

• [SLOW TEST:110.737 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:08:04.759: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb  8 23:08:04.838: INFO: Waiting up to 5m0s for pod "var-expansion-63d38535-2bf6-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-var-expansion-hhq5x" to be "success or failure"
Feb  8 23:08:04.844: INFO: Pod "var-expansion-63d38535-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.359898ms
Feb  8 23:08:06.848: INFO: Pod "var-expansion-63d38535-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010718304s
Feb  8 23:08:08.867: INFO: Pod "var-expansion-63d38535-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029623852s
STEP: Saw pod success
Feb  8 23:08:08.867: INFO: Pod "var-expansion-63d38535-2bf6-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:08:08.873: INFO: Trying to get logs from node netztbred3-worker-1 pod var-expansion-63d38535-2bf6-11e9-a89e-229fb9a7b2a7 container dapi-container: <nil>
STEP: delete the pod
Feb  8 23:08:08.908: INFO: Waiting for pod var-expansion-63d38535-2bf6-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:08:08.920: INFO: Pod var-expansion-63d38535-2bf6-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:08:08.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-hhq5x" for this suite.
Feb  8 23:08:14.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:08:14.978: INFO: namespace: e2e-tests-var-expansion-hhq5x, resource: bindings, ignored listing per whitelist
Feb  8 23:08:15.033: INFO: namespace e2e-tests-var-expansion-hhq5x deletion completed in 6.100195003s

• [SLOW TEST:10.274 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:08:15.033: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb  8 23:08:15.106: INFO: Waiting up to 5m0s for pod "pod-69f283b9-2bf6-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-st952" to be "success or failure"
Feb  8 23:08:15.115: INFO: Pod "pod-69f283b9-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.203633ms
Feb  8 23:08:17.119: INFO: Pod "pod-69f283b9-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013366181s
Feb  8 23:08:19.123: INFO: Pod "pod-69f283b9-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017276637s
STEP: Saw pod success
Feb  8 23:08:19.123: INFO: Pod "pod-69f283b9-2bf6-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:08:19.126: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-69f283b9-2bf6-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:08:19.153: INFO: Waiting for pod pod-69f283b9-2bf6-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:08:19.159: INFO: Pod pod-69f283b9-2bf6-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:08:19.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-st952" for this suite.
Feb  8 23:08:25.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:08:25.231: INFO: namespace: e2e-tests-emptydir-st952, resource: bindings, ignored listing per whitelist
Feb  8 23:08:25.267: INFO: namespace e2e-tests-emptydir-st952 deletion completed in 6.10424637s

• [SLOW TEST:10.235 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:08:25.268: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 23:08:25.347: INFO: Waiting up to 5m0s for pod "downwardapi-volume-700d23b0-2bf6-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-gkdbm" to be "success or failure"
Feb  8 23:08:25.357: INFO: Pod "downwardapi-volume-700d23b0-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.239511ms
Feb  8 23:08:27.360: INFO: Pod "downwardapi-volume-700d23b0-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013534358s
STEP: Saw pod success
Feb  8 23:08:27.360: INFO: Pod "downwardapi-volume-700d23b0-2bf6-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:08:27.363: INFO: Trying to get logs from node netztbred3-worker-1 pod downwardapi-volume-700d23b0-2bf6-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 23:08:27.398: INFO: Waiting for pod downwardapi-volume-700d23b0-2bf6-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:08:27.402: INFO: Pod downwardapi-volume-700d23b0-2bf6-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:08:27.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gkdbm" for this suite.
Feb  8 23:08:33.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:08:33.522: INFO: namespace: e2e-tests-projected-gkdbm, resource: bindings, ignored listing per whitelist
Feb  8 23:08:33.526: INFO: namespace e2e-tests-projected-gkdbm deletion completed in 6.1052116s

• [SLOW TEST:8.258 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:08:33.526: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 23:08:33.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74f800b4-2bf6-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-7b5mf" to be "success or failure"
Feb  8 23:08:33.615: INFO: Pod "downwardapi-volume-74f800b4-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.730662ms
Feb  8 23:08:35.620: INFO: Pod "downwardapi-volume-74f800b4-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015080344s
STEP: Saw pod success
Feb  8 23:08:35.620: INFO: Pod "downwardapi-volume-74f800b4-2bf6-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:08:35.622: INFO: Trying to get logs from node netztbred3-worker-3 pod downwardapi-volume-74f800b4-2bf6-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 23:08:35.643: INFO: Waiting for pod downwardapi-volume-74f800b4-2bf6-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:08:35.649: INFO: Pod downwardapi-volume-74f800b4-2bf6-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:08:35.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7b5mf" for this suite.
Feb  8 23:08:41.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:08:41.733: INFO: namespace: e2e-tests-downward-api-7b5mf, resource: bindings, ignored listing per whitelist
Feb  8 23:08:41.764: INFO: namespace e2e-tests-downward-api-7b5mf deletion completed in 6.111759125s

• [SLOW TEST:8.238 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:08:41.765: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:08:46.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-hd5st" for this suite.
Feb  8 23:09:08.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:09:08.913: INFO: namespace: e2e-tests-replication-controller-hd5st, resource: bindings, ignored listing per whitelist
Feb  8 23:09:08.994: INFO: namespace e2e-tests-replication-controller-hd5st deletion completed in 22.115156896s

• [SLOW TEST:27.229 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:09:08.994: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8a214fba-2bf6-11e9-a89e-229fb9a7b2a7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-8a214fba-2bf6-11e9-a89e-229fb9a7b2a7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:09:13.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t2vz5" for this suite.
Feb  8 23:09:35.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:09:35.241: INFO: namespace: e2e-tests-projected-t2vz5, resource: bindings, ignored listing per whitelist
Feb  8 23:09:35.264: INFO: namespace e2e-tests-projected-t2vz5 deletion completed in 22.095036343s

• [SLOW TEST:26.270 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:09:35.264: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb  8 23:09:35.346: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2snzc,SelfLink:/api/v1/namespaces/e2e-tests-watch-2snzc/configmaps/e2e-watch-test-watch-closed,UID:99c631b2-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12283,Generation:0,CreationTimestamp:2019-02-08 23:09:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  8 23:09:35.346: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2snzc,SelfLink:/api/v1/namespaces/e2e-tests-watch-2snzc/configmaps/e2e-watch-test-watch-closed,UID:99c631b2-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12284,Generation:0,CreationTimestamp:2019-02-08 23:09:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb  8 23:09:35.361: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2snzc,SelfLink:/api/v1/namespaces/e2e-tests-watch-2snzc/configmaps/e2e-watch-test-watch-closed,UID:99c631b2-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12285,Generation:0,CreationTimestamp:2019-02-08 23:09:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  8 23:09:35.362: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2snzc,SelfLink:/api/v1/namespaces/e2e-tests-watch-2snzc/configmaps/e2e-watch-test-watch-closed,UID:99c631b2-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12286,Generation:0,CreationTimestamp:2019-02-08 23:09:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:09:35.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-2snzc" for this suite.
Feb  8 23:09:41.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:09:41.391: INFO: namespace: e2e-tests-watch-2snzc, resource: bindings, ignored listing per whitelist
Feb  8 23:09:41.473: INFO: namespace e2e-tests-watch-2snzc deletion completed in 6.108024493s

• [SLOW TEST:6.209 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:09:41.473: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb  8 23:09:41.541: INFO: Waiting up to 5m0s for pod "client-containers-9d77bbd8-2bf6-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-containers-jhkrp" to be "success or failure"
Feb  8 23:09:41.549: INFO: Pod "client-containers-9d77bbd8-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.856423ms
Feb  8 23:09:43.554: INFO: Pod "client-containers-9d77bbd8-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01218963s
Feb  8 23:09:45.558: INFO: Pod "client-containers-9d77bbd8-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016392893s
STEP: Saw pod success
Feb  8 23:09:45.558: INFO: Pod "client-containers-9d77bbd8-2bf6-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:09:45.561: INFO: Trying to get logs from node netztbred3-worker-1 pod client-containers-9d77bbd8-2bf6-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:09:45.588: INFO: Waiting for pod client-containers-9d77bbd8-2bf6-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:09:45.603: INFO: Pod client-containers-9d77bbd8-2bf6-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:09:45.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-jhkrp" for this suite.
Feb  8 23:09:51.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:09:51.681: INFO: namespace: e2e-tests-containers-jhkrp, resource: bindings, ignored listing per whitelist
Feb  8 23:09:51.713: INFO: namespace e2e-tests-containers-jhkrp deletion completed in 6.105776068s

• [SLOW TEST:10.240 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:09:51.715: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:09:51.798: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:09:53.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4klsb" for this suite.
Feb  8 23:10:48.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:10:48.087: INFO: namespace: e2e-tests-pods-4klsb, resource: bindings, ignored listing per whitelist
Feb  8 23:10:48.099: INFO: namespace e2e-tests-pods-4klsb deletion completed in 54.105557554s

• [SLOW TEST:56.385 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:10:48.100: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb  8 23:10:48.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-8vgv7'
Feb  8 23:10:48.364: INFO: stderr: ""
Feb  8 23:10:48.364: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  8 23:10:48.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8vgv7'
Feb  8 23:10:48.452: INFO: stderr: ""
Feb  8 23:10:48.452: INFO: stdout: "update-demo-nautilus-78hrk update-demo-nautilus-v7sg5 "
Feb  8 23:10:48.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-78hrk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8vgv7'
Feb  8 23:10:48.528: INFO: stderr: ""
Feb  8 23:10:48.528: INFO: stdout: ""
Feb  8 23:10:48.528: INFO: update-demo-nautilus-78hrk is created but not running
Feb  8 23:10:53.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8vgv7'
Feb  8 23:10:53.604: INFO: stderr: ""
Feb  8 23:10:53.604: INFO: stdout: "update-demo-nautilus-78hrk update-demo-nautilus-v7sg5 "
Feb  8 23:10:53.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-78hrk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8vgv7'
Feb  8 23:10:53.678: INFO: stderr: ""
Feb  8 23:10:53.678: INFO: stdout: "true"
Feb  8 23:10:53.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-78hrk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8vgv7'
Feb  8 23:10:53.751: INFO: stderr: ""
Feb  8 23:10:53.751: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  8 23:10:53.751: INFO: validating pod update-demo-nautilus-78hrk
Feb  8 23:10:53.757: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  8 23:10:53.757: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  8 23:10:53.757: INFO: update-demo-nautilus-78hrk is verified up and running
Feb  8 23:10:53.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-v7sg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8vgv7'
Feb  8 23:10:53.830: INFO: stderr: ""
Feb  8 23:10:53.830: INFO: stdout: "true"
Feb  8 23:10:53.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-v7sg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8vgv7'
Feb  8 23:10:53.904: INFO: stderr: ""
Feb  8 23:10:53.904: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  8 23:10:53.904: INFO: validating pod update-demo-nautilus-v7sg5
Feb  8 23:10:53.908: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  8 23:10:53.908: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  8 23:10:53.908: INFO: update-demo-nautilus-v7sg5 is verified up and running
STEP: using delete to clean up resources
Feb  8 23:10:53.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-8vgv7'
Feb  8 23:10:53.991: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  8 23:10:53.991: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb  8 23:10:53.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-8vgv7'
Feb  8 23:10:54.078: INFO: stderr: "No resources found.\n"
Feb  8 23:10:54.079: INFO: stdout: ""
Feb  8 23:10:54.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -l name=update-demo --namespace=e2e-tests-kubectl-8vgv7 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  8 23:10:54.178: INFO: stderr: ""
Feb  8 23:10:54.178: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:10:54.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8vgv7" for this suite.
Feb  8 23:11:00.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:11:00.261: INFO: namespace: e2e-tests-kubectl-8vgv7, resource: bindings, ignored listing per whitelist
Feb  8 23:11:00.289: INFO: namespace e2e-tests-kubectl-8vgv7 deletion completed in 6.10644167s

• [SLOW TEST:12.189 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:11:00.289: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-cc72a643-2bf6-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 23:11:00.372: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cc7442a0-2bf6-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-qzzzg" to be "success or failure"
Feb  8 23:11:00.380: INFO: Pod "pod-projected-secrets-cc7442a0-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.631056ms
Feb  8 23:11:02.384: INFO: Pod "pod-projected-secrets-cc7442a0-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011653696s
Feb  8 23:11:04.388: INFO: Pod "pod-projected-secrets-cc7442a0-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015817395s
STEP: Saw pod success
Feb  8 23:11:04.388: INFO: Pod "pod-projected-secrets-cc7442a0-2bf6-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:11:04.391: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-projected-secrets-cc7442a0-2bf6-11e9-a89e-229fb9a7b2a7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  8 23:11:04.428: INFO: Waiting for pod pod-projected-secrets-cc7442a0-2bf6-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:11:04.432: INFO: Pod pod-projected-secrets-cc7442a0-2bf6-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:11:04.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qzzzg" for this suite.
Feb  8 23:11:10.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:11:10.508: INFO: namespace: e2e-tests-projected-qzzzg, resource: bindings, ignored listing per whitelist
Feb  8 23:11:10.533: INFO: namespace e2e-tests-projected-qzzzg deletion completed in 6.096978615s

• [SLOW TEST:10.244 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:11:10.534: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 23:11:10.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d28d75fd-2bf6-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-rncrm" to be "success or failure"
Feb  8 23:11:10.614: INFO: Pod "downwardapi-volume-d28d75fd-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.277418ms
Feb  8 23:11:12.619: INFO: Pod "downwardapi-volume-d28d75fd-2bf6-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014075894s
STEP: Saw pod success
Feb  8 23:11:12.619: INFO: Pod "downwardapi-volume-d28d75fd-2bf6-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:11:12.622: INFO: Trying to get logs from node netztbred3-worker-3 pod downwardapi-volume-d28d75fd-2bf6-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 23:11:12.651: INFO: Waiting for pod downwardapi-volume-d28d75fd-2bf6-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:11:12.655: INFO: Pod downwardapi-volume-d28d75fd-2bf6-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:11:12.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rncrm" for this suite.
Feb  8 23:11:18.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:11:18.709: INFO: namespace: e2e-tests-projected-rncrm, resource: bindings, ignored listing per whitelist
Feb  8 23:11:18.754: INFO: namespace e2e-tests-projected-rncrm deletion completed in 6.095377514s

• [SLOW TEST:8.220 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:11:18.754: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:11:18.813: INFO: Creating deployment "test-recreate-deployment"
Feb  8 23:11:18.821: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb  8 23:11:18.836: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb  8 23:11:20.844: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb  8 23:11:20.846: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685264278, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685264278, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685264278, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685264278, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  8 23:11:22.850: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb  8 23:11:22.861: INFO: Updating deployment test-recreate-deployment
Feb  8 23:11:22.861: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  8 23:11:22.995: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-qrht6,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qrht6/deployments/test-recreate-deployment,UID:d773a590-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12672,Generation:2,CreationTimestamp:2019-02-08 23:11:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-08 23:11:22 +0000 UTC 2019-02-08 23:11:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-08 23:11:22 +0000 UTC 2019-02-08 23:11:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb  8 23:11:22.999: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-qrht6,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qrht6/replicasets/test-recreate-deployment-697fbf54bf,UID:d9e59f72-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12670,Generation:1,CreationTimestamp:2019-02-08 23:11:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment d773a590-2bf6-11e9-a0ab-42010a8a0033 0xc001da17e7 0xc001da17e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  8 23:11:22.999: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb  8 23:11:22.999: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-qrht6,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qrht6/replicasets/test-recreate-deployment-5dfdcc846d,UID:d77581ba-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12659,Generation:2,CreationTimestamp:2019-02-08 23:11:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment d773a590-2bf6-11e9-a0ab-42010a8a0033 0xc001da1727 0xc001da1728}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  8 23:11:23.002: INFO: Pod "test-recreate-deployment-697fbf54bf-txr6s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-txr6s,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-qrht6,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qrht6/pods/test-recreate-deployment-697fbf54bf-txr6s,UID:d9e74fc7-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12671,Generation:0,CreationTimestamp:2019-02-08 23:11:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf d9e59f72-2bf6-11e9-a0ab-42010a8a0033 0xc002744057 0xc002744058}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4cd86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4cd86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4cd86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027440c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027440e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:11:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:11:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:11:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:11:22 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.54,PodIP:,StartTime:2019-02-08 23:11:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:11:23.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-qrht6" for this suite.
Feb  8 23:11:29.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:11:29.096: INFO: namespace: e2e-tests-deployment-qrht6, resource: bindings, ignored listing per whitelist
Feb  8 23:11:29.125: INFO: namespace e2e-tests-deployment-qrht6 deletion completed in 6.119194879s

• [SLOW TEST:10.371 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:11:29.125: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb  8 23:11:35.258: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  8 23:11:35.261: INFO: Pod pod-with-poststart-http-hook still exists
Feb  8 23:11:37.261: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  8 23:11:37.265: INFO: Pod pod-with-poststart-http-hook still exists
Feb  8 23:11:39.261: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  8 23:11:39.265: INFO: Pod pod-with-poststart-http-hook still exists
Feb  8 23:11:41.261: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  8 23:11:41.265: INFO: Pod pod-with-poststart-http-hook still exists
Feb  8 23:11:43.262: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  8 23:11:43.266: INFO: Pod pod-with-poststart-http-hook still exists
Feb  8 23:11:45.261: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  8 23:11:45.266: INFO: Pod pod-with-poststart-http-hook still exists
Feb  8 23:11:47.261: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  8 23:11:47.265: INFO: Pod pod-with-poststart-http-hook still exists
Feb  8 23:11:49.262: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  8 23:11:49.266: INFO: Pod pod-with-poststart-http-hook still exists
Feb  8 23:11:51.261: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  8 23:11:51.265: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:11:51.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-zgvvr" for this suite.
Feb  8 23:12:13.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:12:13.348: INFO: namespace: e2e-tests-container-lifecycle-hook-zgvvr, resource: bindings, ignored listing per whitelist
Feb  8 23:12:13.380: INFO: namespace e2e-tests-container-lifecycle-hook-zgvvr deletion completed in 22.1109568s

• [SLOW TEST:44.254 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:12:13.380: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:12:13.446: INFO: Creating deployment "nginx-deployment"
Feb  8 23:12:13.454: INFO: Waiting for observed generation 1
Feb  8 23:12:15.464: INFO: Waiting for all required pods to come up
Feb  8 23:12:15.468: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb  8 23:12:19.501: INFO: Waiting for deployment "nginx-deployment" to complete
Feb  8 23:12:19.514: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb  8 23:12:19.523: INFO: Updating deployment nginx-deployment
Feb  8 23:12:19.523: INFO: Waiting for observed generation 2
Feb  8 23:12:21.533: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb  8 23:12:21.536: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb  8 23:12:21.538: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb  8 23:12:21.545: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb  8 23:12:21.545: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb  8 23:12:21.547: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb  8 23:12:21.552: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb  8 23:12:21.552: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb  8 23:12:21.561: INFO: Updating deployment nginx-deployment
Feb  8 23:12:21.562: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb  8 23:12:21.578: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb  8 23:12:21.596: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  8 23:12:21.706: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vl7gt/deployments/nginx-deployment,UID:f8040d12-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13060,Generation:3,CreationTimestamp:2019-02-08 23:12:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-02-08 23:12:19 +0000 UTC 2019-02-08 23:12:13 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.} {Available False 2019-02-08 23:12:21 +0000 UTC 2019-02-08 23:12:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb  8 23:12:21.810: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vl7gt/replicasets/nginx-deployment-65bbdb5f8,UID:fba36c50-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13041,Generation:3,CreationTimestamp:2019-02-08 23:12:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment f8040d12-2bf6-11e9-a0ab-42010a8a0033 0xc002a0dd17 0xc002a0dd18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  8 23:12:21.810: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb  8 23:12:21.810: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vl7gt/replicasets/nginx-deployment-555b55d965,UID:f8066ff7-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13088,Generation:3,CreationTimestamp:2019-02-08 23:12:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment f8040d12-2bf6-11e9-a0ab-42010a8a0033 0xc002a0d937 0xc002a0d938}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb  8 23:12:21.866: INFO: Pod "nginx-deployment-555b55d965-49x7j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-49x7j,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-49x7j,UID:f81031de-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12954,Generation:0,CreationTimestamp:2019-02-08 23:12:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.3.25/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001908e97 0xc001908e98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001908f00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001908f20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.53,PodIP:10.2.3.25,StartTime:2019-02-08 23:12:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-08 23:12:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://8d42e0e2fbeeb330b4f3b12d17d3a46892fcc72ea6e8b0b89f28cccd1ee0c530}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.867: INFO: Pod "nginx-deployment-555b55d965-4btjk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4btjk,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-4btjk,UID:fcea6fa0-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13082,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001908fe0 0xc001908fe1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001909040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001909070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.868: INFO: Pod "nginx-deployment-555b55d965-56lcp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-56lcp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-56lcp,UID:f80ccac4-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12957,Generation:0,CreationTimestamp:2019-02-08 23:12:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.3.24/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001909150 0xc001909151}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019091b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019091d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.53,PodIP:10.2.3.24,StartTime:2019-02-08 23:12:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-08 23:12:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://abf78d97792116baf09feba1df5dce51b97611c1d618c6691f13a803db66d0cf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.868: INFO: Pod "nginx-deployment-555b55d965-5cm6z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5cm6z,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-5cm6z,UID:f81529ff-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12943,Generation:0,CreationTimestamp:2019-02-08 23:12:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.2.79/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc0019092a0 0xc0019092a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001909360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001909380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.54,PodIP:10.2.2.79,StartTime:2019-02-08 23:12:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-08 23:12:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://f46081976b9d0357205f9b15421e4f469734370b6c128785be80cfd547ec2cef}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.869: INFO: Pod "nginx-deployment-555b55d965-6ld84" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6ld84,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-6ld84,UID:fcdb312d-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13072,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001909440 0xc001909441}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019094a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019094c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.53,PodIP:,StartTime:2019-02-08 23:12:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.869: INFO: Pod "nginx-deployment-555b55d965-6wkjp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6wkjp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-6wkjp,UID:fce11d27-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13066,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001909570 0xc001909571}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019095d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019095f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.869: INFO: Pod "nginx-deployment-555b55d965-9jw8t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9jw8t,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-9jw8t,UID:f8102392-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12925,Generation:0,CreationTimestamp:2019-02-08 23:12:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.82/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001909670 0xc001909671}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019096d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019096f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.52,PodIP:10.2.1.82,StartTime:2019-02-08 23:12:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-08 23:12:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://a5edf795863bead51e9458e367d72ab00d2c826ab17b4602aedf5dec8b827bee}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.870: INFO: Pod "nginx-deployment-555b55d965-bb45x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-bb45x,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-bb45x,UID:fce20213-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13097,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc0019097b0 0xc0019097b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001909810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001909830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.52,PodIP:,StartTime:2019-02-08 23:12:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.870: INFO: Pod "nginx-deployment-555b55d965-c5cvl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-c5cvl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-c5cvl,UID:fcdd96b9-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13081,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc0019098e0 0xc0019098e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001909940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001909960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.54,PodIP:,StartTime:2019-02-08 23:12:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.870: INFO: Pod "nginx-deployment-555b55d965-cpgln" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-cpgln,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-cpgln,UID:fce1f8db-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13071,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001909a10 0xc001909a11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001909a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001909a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.871: INFO: Pod "nginx-deployment-555b55d965-jdfzq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jdfzq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-jdfzq,UID:fce164d5-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13067,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001909b00 0xc001909b01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001909b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001909b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.871: INFO: Pod "nginx-deployment-555b55d965-jqt5z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jqt5z,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-jqt5z,UID:fcdde0ea-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13057,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001909bf0 0xc001909bf1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001909c50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001909c70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.871: INFO: Pod "nginx-deployment-555b55d965-lngjh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lngjh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-lngjh,UID:f8102bd7-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12921,Generation:0,CreationTimestamp:2019-02-08 23:12:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.83/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001909cf0 0xc001909cf1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001909d50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001909d70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.52,PodIP:10.2.1.83,StartTime:2019-02-08 23:12:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-08 23:12:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://2bfd51fc413a724e9a90cd3b06d91cf33f71565c09096ee1d8225c93a5fcc130}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.872: INFO: Pod "nginx-deployment-555b55d965-lqsrr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lqsrr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-lqsrr,UID:fcea7739-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13080,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001909e30 0xc001909e31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001909e90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001909eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.872: INFO: Pod "nginx-deployment-555b55d965-mgw8t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mgw8t,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-mgw8t,UID:fcea7d5d-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13087,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001909f20 0xc001909f21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001909f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001909fa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.872: INFO: Pod "nginx-deployment-555b55d965-nfzr9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-nfzr9,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-nfzr9,UID:f8100b83-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12939,Generation:0,CreationTimestamp:2019-02-08 23:12:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.2.77/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001be4020 0xc001be4021}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be4080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be40a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.54,PodIP:10.2.2.77,StartTime:2019-02-08 23:12:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-08 23:12:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://434ba3bb673d217d321eb7d7eb48cbcf73d9284070943d61de792ec1e1716791}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.872: INFO: Pod "nginx-deployment-555b55d965-rhhvz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-rhhvz,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-rhhvz,UID:fce93c7b-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13083,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001be4160 0xc001be4161}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be41c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be41e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.873: INFO: Pod "nginx-deployment-555b55d965-tr5r4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-tr5r4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-tr5r4,UID:f80b363f-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12927,Generation:0,CreationTimestamp:2019-02-08 23:12:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.81/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001be4260 0xc001be4261}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be42c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be42e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.52,PodIP:10.2.1.81,StartTime:2019-02-08 23:12:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-08 23:12:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://13f13229d7d87676d0e0e8b551681b25f5bfedcf6c5d8e89a9686ab870daf746}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.873: INFO: Pod "nginx-deployment-555b55d965-wqqh6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-wqqh6,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-wqqh6,UID:fcea3094-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13084,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001be43a0 0xc001be43a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be4410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be4430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.873: INFO: Pod "nginx-deployment-555b55d965-zwwjh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zwwjh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-555b55d965-zwwjh,UID:f80ca201-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:12949,Generation:0,CreationTimestamp:2019-02-08 23:12:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.2.76/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f8066ff7-2bf6-11e9-a0ab-42010a8a0033 0xc001be44b0 0xc001be44b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be4510} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be4530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:13 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.54,PodIP:10.2.2.76,StartTime:2019-02-08 23:12:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-08 23:12:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://ea0e050e96040218aabb19d09bb4a9f6651e2b26ba351d173a28836c12984e6b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.874: INFO: Pod "nginx-deployment-65bbdb5f8-4sxjx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-4sxjx,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-4sxjx,UID:fce3779f-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13074,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be45f0 0xc001be45f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be4680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be46b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.874: INFO: Pod "nginx-deployment-65bbdb5f8-6cvjr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6cvjr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-6cvjr,UID:fcded2fa-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13095,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be4720 0xc001be4721}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be47a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be47c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.53,PodIP:,StartTime:2019-02-08 23:12:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.874: INFO: Pod "nginx-deployment-65bbdb5f8-6ljmz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6ljmz,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-6ljmz,UID:fcf83959-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13102,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be4880 0xc001be4881}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be48f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be4910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.875: INFO: Pod "nginx-deployment-65bbdb5f8-786l2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-786l2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-786l2,UID:fcf04845-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13092,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be49a0 0xc001be49a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be4a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be4a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.875: INFO: Pod "nginx-deployment-65bbdb5f8-8sb2j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-8sb2j,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-8sb2j,UID:fcf84468-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13103,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be4ad0 0xc001be4ad1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be4b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be4b60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.878: INFO: Pod "nginx-deployment-65bbdb5f8-958rh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-958rh,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-958rh,UID:fbbc379c-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13032,Generation:0,CreationTimestamp:2019-02-08 23:12:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.85/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be4bf0 0xc001be4bf1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be4c60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be4c80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.52,PodIP:,StartTime:2019-02-08 23:12:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.879: INFO: Pod "nginx-deployment-65bbdb5f8-f8xmr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-f8xmr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-f8xmr,UID:fd02f6bb-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13101,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be4d40 0xc001be4d41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be4db0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be4e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.884: INFO: Pod "nginx-deployment-65bbdb5f8-hs4zp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-hs4zp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-hs4zp,UID:fba71a32-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13026,Generation:0,CreationTimestamp:2019-02-08 23:12:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.3.27/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be4eb7 0xc001be4eb8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be4f20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be4f40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.53,PodIP:,StartTime:2019-02-08 23:12:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.885: INFO: Pod "nginx-deployment-65bbdb5f8-mn5dn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-mn5dn,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-mn5dn,UID:fba4b0af-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13027,Generation:0,CreationTimestamp:2019-02-08 23:12:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.84/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be5070 0xc001be5071}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be50e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be5100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.52,PodIP:,StartTime:2019-02-08 23:12:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.885: INFO: Pod "nginx-deployment-65bbdb5f8-qbkmb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-qbkmb,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-qbkmb,UID:fba747ff-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13030,Generation:0,CreationTimestamp:2019-02-08 23:12:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.2.80/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be51d0 0xc001be51d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be5280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be52a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.54,PodIP:,StartTime:2019-02-08 23:12:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.888: INFO: Pod "nginx-deployment-65bbdb5f8-sm77q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-sm77q,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-sm77q,UID:fcf5e881-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13094,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be5360 0xc001be5361}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be5410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be5430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.888: INFO: Pod "nginx-deployment-65bbdb5f8-tqxpb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-tqxpb,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-tqxpb,UID:fce3b1d8-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13073,Generation:0,CreationTimestamp:2019-02-08 23:12:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be54a0 0xc001be54a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be5510} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be5530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  8 23:12:21.888: INFO: Pod "nginx-deployment-65bbdb5f8-vb2vs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-vb2vs,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-vl7gt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vl7gt/pods/nginx-deployment-65bbdb5f8-vb2vs,UID:fbb68242-2bf6-11e9-a0ab-42010a8a0033,ResourceVersion:13034,Generation:0,CreationTimestamp:2019-02-08 23:12:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.2.81/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fba36c50-2bf6-11e9-a0ab-42010a8a0033 0xc001be55e0 0xc001be55e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vw9z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw9z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw9z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001be5650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001be5670}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:12:19 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.54,PodIP:,StartTime:2019-02-08 23:12:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:12:21.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-vl7gt" for this suite.
Feb  8 23:12:29.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:12:30.002: INFO: namespace: e2e-tests-deployment-vl7gt, resource: bindings, ignored listing per whitelist
Feb  8 23:12:30.069: INFO: namespace e2e-tests-deployment-vl7gt deletion completed in 8.138326705s

• [SLOW TEST:16.689 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:12:30.069: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 23:12:30.165: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01f8a275-2bf7-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-cqhln" to be "success or failure"
Feb  8 23:12:30.173: INFO: Pod "downwardapi-volume-01f8a275-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.271358ms
Feb  8 23:12:32.177: INFO: Pod "downwardapi-volume-01f8a275-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012343145s
Feb  8 23:12:34.182: INFO: Pod "downwardapi-volume-01f8a275-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016416715s
STEP: Saw pod success
Feb  8 23:12:34.182: INFO: Pod "downwardapi-volume-01f8a275-2bf7-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:12:34.184: INFO: Trying to get logs from node netztbred3-worker-3 pod downwardapi-volume-01f8a275-2bf7-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 23:12:34.209: INFO: Waiting for pod downwardapi-volume-01f8a275-2bf7-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:12:34.215: INFO: Pod downwardapi-volume-01f8a275-2bf7-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:12:34.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cqhln" for this suite.
Feb  8 23:12:40.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:12:40.305: INFO: namespace: e2e-tests-downward-api-cqhln, resource: bindings, ignored listing per whitelist
Feb  8 23:12:40.312: INFO: namespace e2e-tests-downward-api-cqhln deletion completed in 6.093527657s

• [SLOW TEST:10.243 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:12:40.312: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb  8 23:12:40.379: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-762798943 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:12:40.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xlgrc" for this suite.
Feb  8 23:12:46.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:12:46.541: INFO: namespace: e2e-tests-kubectl-xlgrc, resource: bindings, ignored listing per whitelist
Feb  8 23:12:46.568: INFO: namespace e2e-tests-kubectl-xlgrc deletion completed in 6.115474684s

• [SLOW TEST:6.256 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:12:46.568: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-0bcd4f41-2bf7-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 23:12:46.656: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bcde9d7-2bf7-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-configmap-2gmrq" to be "success or failure"
Feb  8 23:12:46.660: INFO: Pod "pod-configmaps-0bcde9d7-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.17013ms
Feb  8 23:12:48.665: INFO: Pod "pod-configmaps-0bcde9d7-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530238s
Feb  8 23:12:50.669: INFO: Pod "pod-configmaps-0bcde9d7-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012636712s
STEP: Saw pod success
Feb  8 23:12:50.669: INFO: Pod "pod-configmaps-0bcde9d7-2bf7-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:12:50.672: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-configmaps-0bcde9d7-2bf7-11e9-a89e-229fb9a7b2a7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 23:12:50.701: INFO: Waiting for pod pod-configmaps-0bcde9d7-2bf7-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:12:50.710: INFO: Pod pod-configmaps-0bcde9d7-2bf7-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:12:50.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-2gmrq" for this suite.
Feb  8 23:12:56.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:12:56.802: INFO: namespace: e2e-tests-configmap-2gmrq, resource: bindings, ignored listing per whitelist
Feb  8 23:12:56.820: INFO: namespace e2e-tests-configmap-2gmrq deletion completed in 6.105320658s

• [SLOW TEST:10.252 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:12:56.820: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Feb  8 23:13:00.991: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:13:25.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-r876w" for this suite.
Feb  8 23:13:31.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:13:31.149: INFO: namespace: e2e-tests-namespaces-r876w, resource: bindings, ignored listing per whitelist
Feb  8 23:13:31.160: INFO: namespace e2e-tests-namespaces-r876w deletion completed in 6.111976119s
STEP: Destroying namespace "e2e-tests-nsdeletetest-jrvzb" for this suite.
Feb  8 23:13:31.163: INFO: Namespace e2e-tests-nsdeletetest-jrvzb was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-tt68s" for this suite.
Feb  8 23:13:37.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:13:37.205: INFO: namespace: e2e-tests-nsdeletetest-tt68s, resource: bindings, ignored listing per whitelist
Feb  8 23:13:37.265: INFO: namespace e2e-tests-nsdeletetest-tt68s deletion completed in 6.101972873s

• [SLOW TEST:40.445 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:13:37.265: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  8 23:13:37.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-szx58'
Feb  8 23:13:37.445: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  8 23:13:37.445: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Feb  8 23:13:41.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-szx58'
Feb  8 23:13:41.554: INFO: stderr: ""
Feb  8 23:13:41.554: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:13:41.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-szx58" for this suite.
Feb  8 23:14:03.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:14:03.636: INFO: namespace: e2e-tests-kubectl-szx58, resource: bindings, ignored listing per whitelist
Feb  8 23:14:03.673: INFO: namespace e2e-tests-kubectl-szx58 deletion completed in 22.115059723s

• [SLOW TEST:26.409 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:14:03.673: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-39c0971f-2bf7-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 23:14:03.748: INFO: Waiting up to 5m0s for pod "pod-configmaps-39c151be-2bf7-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-configmap-qgc8r" to be "success or failure"
Feb  8 23:14:03.756: INFO: Pod "pod-configmaps-39c151be-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.429171ms
Feb  8 23:14:05.759: INFO: Pod "pod-configmaps-39c151be-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010759985s
Feb  8 23:14:07.763: INFO: Pod "pod-configmaps-39c151be-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014843679s
STEP: Saw pod success
Feb  8 23:14:07.763: INFO: Pod "pod-configmaps-39c151be-2bf7-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:14:07.766: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-configmaps-39c151be-2bf7-11e9-a89e-229fb9a7b2a7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 23:14:07.815: INFO: Waiting for pod pod-configmaps-39c151be-2bf7-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:14:07.823: INFO: Pod pod-configmaps-39c151be-2bf7-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:14:07.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-qgc8r" for this suite.
Feb  8 23:14:13.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:14:13.903: INFO: namespace: e2e-tests-configmap-qgc8r, resource: bindings, ignored listing per whitelist
Feb  8 23:14:13.928: INFO: namespace e2e-tests-configmap-qgc8r deletion completed in 6.100923724s

• [SLOW TEST:10.255 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:14:13.928: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  8 23:14:14.002: INFO: Waiting up to 5m0s for pod "downward-api-3fdd8031-2bf7-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-8stlg" to be "success or failure"
Feb  8 23:14:14.009: INFO: Pod "downward-api-3fdd8031-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.087231ms
Feb  8 23:14:16.013: INFO: Pod "downward-api-3fdd8031-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010924797s
Feb  8 23:14:18.017: INFO: Pod "downward-api-3fdd8031-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014867474s
STEP: Saw pod success
Feb  8 23:14:18.017: INFO: Pod "downward-api-3fdd8031-2bf7-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:14:18.020: INFO: Trying to get logs from node netztbred3-worker-3 pod downward-api-3fdd8031-2bf7-11e9-a89e-229fb9a7b2a7 container dapi-container: <nil>
STEP: delete the pod
Feb  8 23:14:18.046: INFO: Waiting for pod downward-api-3fdd8031-2bf7-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:14:18.050: INFO: Pod downward-api-3fdd8031-2bf7-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:14:18.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8stlg" for this suite.
Feb  8 23:14:24.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:14:24.091: INFO: namespace: e2e-tests-downward-api-8stlg, resource: bindings, ignored listing per whitelist
Feb  8 23:14:24.168: INFO: namespace e2e-tests-downward-api-8stlg deletion completed in 6.11308351s

• [SLOW TEST:10.240 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:14:24.168: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-45f81022-2bf7-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 23:14:24.244: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-45f8d3b4-2bf7-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-pf2mz" to be "success or failure"
Feb  8 23:14:24.253: INFO: Pod "pod-projected-configmaps-45f8d3b4-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.413119ms
Feb  8 23:14:26.263: INFO: Pod "pod-projected-configmaps-45f8d3b4-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018051847s
Feb  8 23:14:28.267: INFO: Pod "pod-projected-configmaps-45f8d3b4-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022240227s
STEP: Saw pod success
Feb  8 23:14:28.267: INFO: Pod "pod-projected-configmaps-45f8d3b4-2bf7-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:14:28.272: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-projected-configmaps-45f8d3b4-2bf7-11e9-a89e-229fb9a7b2a7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 23:14:28.315: INFO: Waiting for pod pod-projected-configmaps-45f8d3b4-2bf7-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:14:28.323: INFO: Pod pod-projected-configmaps-45f8d3b4-2bf7-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:14:28.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pf2mz" for this suite.
Feb  8 23:14:34.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:14:34.398: INFO: namespace: e2e-tests-projected-pf2mz, resource: bindings, ignored listing per whitelist
Feb  8 23:14:34.434: INFO: namespace e2e-tests-projected-pf2mz deletion completed in 6.106142254s

• [SLOW TEST:10.266 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:14:34.435: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-4c184410-2bf7-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 23:14:34.523: INFO: Waiting up to 5m0s for pod "pod-secrets-4c19028f-2bf7-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-secrets-mxz5h" to be "success or failure"
Feb  8 23:14:34.532: INFO: Pod "pod-secrets-4c19028f-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.946535ms
Feb  8 23:14:36.535: INFO: Pod "pod-secrets-4c19028f-2bf7-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012200856s
STEP: Saw pod success
Feb  8 23:14:36.535: INFO: Pod "pod-secrets-4c19028f-2bf7-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:14:36.538: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-secrets-4c19028f-2bf7-11e9-a89e-229fb9a7b2a7 container secret-volume-test: <nil>
STEP: delete the pod
Feb  8 23:14:36.565: INFO: Waiting for pod pod-secrets-4c19028f-2bf7-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:14:36.568: INFO: Pod pod-secrets-4c19028f-2bf7-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:14:36.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mxz5h" for this suite.
Feb  8 23:14:42.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:14:42.666: INFO: namespace: e2e-tests-secrets-mxz5h, resource: bindings, ignored listing per whitelist
Feb  8 23:14:42.670: INFO: namespace e2e-tests-secrets-mxz5h deletion completed in 6.098356589s

• [SLOW TEST:8.235 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:14:42.670: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:14:46.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-2ngn8" for this suite.
Feb  8 23:15:24.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:15:24.845: INFO: namespace: e2e-tests-kubelet-test-2ngn8, resource: bindings, ignored listing per whitelist
Feb  8 23:15:24.881: INFO: namespace e2e-tests-kubelet-test-2ngn8 deletion completed in 38.096863391s

• [SLOW TEST:42.211 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:15:24.882: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-97gdc
Feb  8 23:15:26.978: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-97gdc
STEP: checking the pod's current state and verifying that restartCount is present
Feb  8 23:15:26.982: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:19:27.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-97gdc" for this suite.
Feb  8 23:19:33.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:19:33.637: INFO: namespace: e2e-tests-container-probe-97gdc, resource: bindings, ignored listing per whitelist
Feb  8 23:19:33.637: INFO: namespace e2e-tests-container-probe-97gdc deletion completed in 6.098155306s

• [SLOW TEST:248.756 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:19:33.638: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb  8 23:19:33.717: INFO: namespace e2e-tests-kubectl-ft648
Feb  8 23:19:33.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-ft648'
Feb  8 23:19:34.065: INFO: stderr: ""
Feb  8 23:19:34.065: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb  8 23:19:35.069: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 23:19:35.070: INFO: Found 0 / 1
Feb  8 23:19:36.070: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 23:19:36.070: INFO: Found 1 / 1
Feb  8 23:19:36.070: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  8 23:19:36.074: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 23:19:36.074: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  8 23:19:36.074: INFO: wait on redis-master startup in e2e-tests-kubectl-ft648 
Feb  8 23:19:36.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 logs redis-master-v5p9q redis-master --namespace=e2e-tests-kubectl-ft648'
Feb  8 23:19:36.169: INFO: stderr: ""
Feb  8 23:19:36.169: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Feb 23:19:35.452 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Feb 23:19:35.452 # Server started, Redis version 3.2.12\n1:M 08 Feb 23:19:35.452 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Feb 23:19:35.452 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb  8 23:19:36.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-ft648'
Feb  8 23:19:36.275: INFO: stderr: ""
Feb  8 23:19:36.275: INFO: stdout: "service/rm2 exposed\n"
Feb  8 23:19:36.279: INFO: Service rm2 in namespace e2e-tests-kubectl-ft648 found.
STEP: exposing service
Feb  8 23:19:38.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-ft648'
Feb  8 23:19:38.383: INFO: stderr: ""
Feb  8 23:19:38.383: INFO: stdout: "service/rm3 exposed\n"
Feb  8 23:19:38.388: INFO: Service rm3 in namespace e2e-tests-kubectl-ft648 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:19:40.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ft648" for this suite.
Feb  8 23:20:02.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:20:02.444: INFO: namespace: e2e-tests-kubectl-ft648, resource: bindings, ignored listing per whitelist
Feb  8 23:20:02.497: INFO: namespace e2e-tests-kubectl-ft648 deletion completed in 22.097279035s

• [SLOW TEST:28.860 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:20:02.498: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:20:02.594: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb  8 23:20:02.600: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-fh55c/daemonsets","resourceVersion":"14506"},"items":null}

Feb  8 23:20:02.602: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-fh55c/pods","resourceVersion":"14506"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:20:02.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-fh55c" for this suite.
Feb  8 23:20:08.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:20:08.789: INFO: namespace: e2e-tests-daemonsets-fh55c, resource: bindings, ignored listing per whitelist
Feb  8 23:20:08.809: INFO: namespace e2e-tests-daemonsets-fh55c deletion completed in 6.192013687s

S [SKIPPING] [6.311 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb  8 23:20:02.594: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:20:08.809: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-jldz
STEP: Creating a pod to test atomic-volume-subpath
Feb  8 23:20:08.910: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-jldz" in namespace "e2e-tests-subpath-j8fbz" to be "success or failure"
Feb  8 23:20:08.935: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Pending", Reason="", readiness=false. Elapsed: 24.294168ms
Feb  8 23:20:10.939: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028525642s
Feb  8 23:20:12.944: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Running", Reason="", readiness=false. Elapsed: 4.03382908s
Feb  8 23:20:14.948: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Running", Reason="", readiness=false. Elapsed: 6.03786176s
Feb  8 23:20:16.952: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Running", Reason="", readiness=false. Elapsed: 8.041542174s
Feb  8 23:20:18.956: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Running", Reason="", readiness=false. Elapsed: 10.045905348s
Feb  8 23:20:20.960: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Running", Reason="", readiness=false. Elapsed: 12.049920448s
Feb  8 23:20:22.965: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Running", Reason="", readiness=false. Elapsed: 14.054369335s
Feb  8 23:20:24.969: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Running", Reason="", readiness=false. Elapsed: 16.058335762s
Feb  8 23:20:26.973: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Running", Reason="", readiness=false. Elapsed: 18.062270348s
Feb  8 23:20:28.977: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Running", Reason="", readiness=false. Elapsed: 20.066885605s
Feb  8 23:20:30.982: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Running", Reason="", readiness=false. Elapsed: 22.071377984s
Feb  8 23:20:32.986: INFO: Pod "pod-subpath-test-downwardapi-jldz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.075636511s
STEP: Saw pod success
Feb  8 23:20:32.986: INFO: Pod "pod-subpath-test-downwardapi-jldz" satisfied condition "success or failure"
Feb  8 23:20:32.989: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-subpath-test-downwardapi-jldz container test-container-subpath-downwardapi-jldz: <nil>
STEP: delete the pod
Feb  8 23:20:33.021: INFO: Waiting for pod pod-subpath-test-downwardapi-jldz to disappear
Feb  8 23:20:33.028: INFO: Pod pod-subpath-test-downwardapi-jldz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-jldz
Feb  8 23:20:33.028: INFO: Deleting pod "pod-subpath-test-downwardapi-jldz" in namespace "e2e-tests-subpath-j8fbz"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:20:33.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-j8fbz" for this suite.
Feb  8 23:20:39.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:20:39.091: INFO: namespace: e2e-tests-subpath-j8fbz, resource: bindings, ignored listing per whitelist
Feb  8 23:20:39.133: INFO: namespace e2e-tests-subpath-j8fbz deletion completed in 6.098326902s

• [SLOW TEST:30.323 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:20:39.133: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  8 23:20:39.213: INFO: Waiting up to 5m0s for pod "downward-api-25781395-2bf8-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-682bg" to be "success or failure"
Feb  8 23:20:39.221: INFO: Pod "downward-api-25781395-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041012ms
Feb  8 23:20:41.224: INFO: Pod "downward-api-25781395-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011443316s
STEP: Saw pod success
Feb  8 23:20:41.224: INFO: Pod "downward-api-25781395-2bf8-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:20:41.227: INFO: Trying to get logs from node netztbred3-worker-1 pod downward-api-25781395-2bf8-11e9-a89e-229fb9a7b2a7 container dapi-container: <nil>
STEP: delete the pod
Feb  8 23:20:41.257: INFO: Waiting for pod downward-api-25781395-2bf8-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:20:41.268: INFO: Pod downward-api-25781395-2bf8-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:20:41.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-682bg" for this suite.
Feb  8 23:20:47.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:20:47.383: INFO: namespace: e2e-tests-downward-api-682bg, resource: bindings, ignored listing per whitelist
Feb  8 23:20:47.404: INFO: namespace e2e-tests-downward-api-682bg deletion completed in 6.130958406s

• [SLOW TEST:8.271 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:20:47.404: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb  8 23:20:47.529: INFO: Waiting up to 5m0s for pod "pod-2a6d695e-2bf8-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-qbffn" to be "success or failure"
Feb  8 23:20:47.539: INFO: Pod "pod-2a6d695e-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.143903ms
Feb  8 23:20:49.543: INFO: Pod "pod-2a6d695e-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014111698s
Feb  8 23:20:51.547: INFO: Pod "pod-2a6d695e-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018364624s
STEP: Saw pod success
Feb  8 23:20:51.547: INFO: Pod "pod-2a6d695e-2bf8-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:20:51.551: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-2a6d695e-2bf8-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:20:51.575: INFO: Waiting for pod pod-2a6d695e-2bf8-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:20:51.580: INFO: Pod pod-2a6d695e-2bf8-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:20:51.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qbffn" for this suite.
Feb  8 23:20:57.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:20:57.653: INFO: namespace: e2e-tests-emptydir-qbffn, resource: bindings, ignored listing per whitelist
Feb  8 23:20:57.688: INFO: namespace e2e-tests-emptydir-qbffn deletion completed in 6.103876304s

• [SLOW TEST:10.284 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:20:57.688: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-30879a41-2bf8-11e9-a89e-229fb9a7b2a7
Feb  8 23:20:57.771: INFO: Pod name my-hostname-basic-30879a41-2bf8-11e9-a89e-229fb9a7b2a7: Found 0 pods out of 1
Feb  8 23:21:02.776: INFO: Pod name my-hostname-basic-30879a41-2bf8-11e9-a89e-229fb9a7b2a7: Found 1 pods out of 1
Feb  8 23:21:02.776: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-30879a41-2bf8-11e9-a89e-229fb9a7b2a7" are running
Feb  8 23:21:02.779: INFO: Pod "my-hostname-basic-30879a41-2bf8-11e9-a89e-229fb9a7b2a7-n9bn8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-08 23:20:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-08 23:20:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-08 23:20:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-08 23:20:57 +0000 UTC Reason: Message:}])
Feb  8 23:21:02.779: INFO: Trying to dial the pod
Feb  8 23:21:07.792: INFO: Controller my-hostname-basic-30879a41-2bf8-11e9-a89e-229fb9a7b2a7: Got expected result from replica 1 [my-hostname-basic-30879a41-2bf8-11e9-a89e-229fb9a7b2a7-n9bn8]: "my-hostname-basic-30879a41-2bf8-11e9-a89e-229fb9a7b2a7-n9bn8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:21:07.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-djcbq" for this suite.
Feb  8 23:21:13.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:21:13.892: INFO: namespace: e2e-tests-replication-controller-djcbq, resource: bindings, ignored listing per whitelist
Feb  8 23:21:13.895: INFO: namespace e2e-tests-replication-controller-djcbq deletion completed in 6.098488213s

• [SLOW TEST:16.207 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:21:13.895: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb  8 23:21:13.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 --namespace=e2e-tests-kubectl-mbn8v run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb  8 23:21:17.585: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb  8 23:21:17.585: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:21:19.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mbn8v" for this suite.
Feb  8 23:21:33.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:21:33.642: INFO: namespace: e2e-tests-kubectl-mbn8v, resource: bindings, ignored listing per whitelist
Feb  8 23:21:33.696: INFO: namespace e2e-tests-kubectl-mbn8v deletion completed in 14.100473866s

• [SLOW TEST:19.801 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:21:33.696: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-xhcxc
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-xhcxc to expose endpoints map[]
Feb  8 23:21:33.788: INFO: Get endpoints failed (5.458406ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb  8 23:21:34.792: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-xhcxc exposes endpoints map[] (1.009440869s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-xhcxc
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-xhcxc to expose endpoints map[pod1:[80]]
Feb  8 23:21:37.844: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-xhcxc exposes endpoints map[pod1:[80]] (3.041507176s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-xhcxc
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-xhcxc to expose endpoints map[pod1:[80] pod2:[80]]
Feb  8 23:21:40.896: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-xhcxc exposes endpoints map[pod1:[80] pod2:[80]] (3.043468387s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-xhcxc
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-xhcxc to expose endpoints map[pod2:[80]]
Feb  8 23:21:40.923: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-xhcxc exposes endpoints map[pod2:[80]] (15.762122ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-xhcxc
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-xhcxc to expose endpoints map[]
Feb  8 23:21:41.971: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-xhcxc exposes endpoints map[] (1.015819422s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:21:41.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-xhcxc" for this suite.
Feb  8 23:22:04.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:22:04.056: INFO: namespace: e2e-tests-services-xhcxc, resource: bindings, ignored listing per whitelist
Feb  8 23:22:04.112: INFO: namespace e2e-tests-services-xhcxc deletion completed in 22.11199501s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:30.415 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:22:04.112: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb  8 23:22:04.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:04.368: INFO: stderr: ""
Feb  8 23:22:04.369: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  8 23:22:04.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:04.468: INFO: stderr: ""
Feb  8 23:22:04.468: INFO: stdout: "update-demo-nautilus-flzhh update-demo-nautilus-n45mf "
Feb  8 23:22:04.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-flzhh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:04.548: INFO: stderr: ""
Feb  8 23:22:04.548: INFO: stdout: ""
Feb  8 23:22:04.548: INFO: update-demo-nautilus-flzhh is created but not running
Feb  8 23:22:09.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:09.633: INFO: stderr: ""
Feb  8 23:22:09.633: INFO: stdout: "update-demo-nautilus-flzhh update-demo-nautilus-n45mf "
Feb  8 23:22:09.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-flzhh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:09.709: INFO: stderr: ""
Feb  8 23:22:09.709: INFO: stdout: "true"
Feb  8 23:22:09.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-flzhh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:09.786: INFO: stderr: ""
Feb  8 23:22:09.786: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  8 23:22:09.786: INFO: validating pod update-demo-nautilus-flzhh
Feb  8 23:22:09.794: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  8 23:22:09.794: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  8 23:22:09.794: INFO: update-demo-nautilus-flzhh is verified up and running
Feb  8 23:22:09.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-n45mf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:09.867: INFO: stderr: ""
Feb  8 23:22:09.867: INFO: stdout: "true"
Feb  8 23:22:09.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-n45mf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:09.943: INFO: stderr: ""
Feb  8 23:22:09.943: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  8 23:22:09.943: INFO: validating pod update-demo-nautilus-n45mf
Feb  8 23:22:09.949: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  8 23:22:09.949: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  8 23:22:09.949: INFO: update-demo-nautilus-n45mf is verified up and running
STEP: scaling down the replication controller
Feb  8 23:22:09.951: INFO: scanned /root for discovery docs: <nil>
Feb  8 23:22:09.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:11.062: INFO: stderr: ""
Feb  8 23:22:11.062: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  8 23:22:11.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:11.159: INFO: stderr: ""
Feb  8 23:22:11.159: INFO: stdout: "update-demo-nautilus-flzhh update-demo-nautilus-n45mf "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  8 23:22:16.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:16.244: INFO: stderr: ""
Feb  8 23:22:16.244: INFO: stdout: "update-demo-nautilus-flzhh update-demo-nautilus-n45mf "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  8 23:22:21.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:21.324: INFO: stderr: ""
Feb  8 23:22:21.324: INFO: stdout: "update-demo-nautilus-flzhh "
Feb  8 23:22:21.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-flzhh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:21.401: INFO: stderr: ""
Feb  8 23:22:21.401: INFO: stdout: "true"
Feb  8 23:22:21.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-flzhh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:21.484: INFO: stderr: ""
Feb  8 23:22:21.484: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  8 23:22:21.484: INFO: validating pod update-demo-nautilus-flzhh
Feb  8 23:22:21.489: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  8 23:22:21.489: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  8 23:22:21.489: INFO: update-demo-nautilus-flzhh is verified up and running
STEP: scaling up the replication controller
Feb  8 23:22:21.491: INFO: scanned /root for discovery docs: <nil>
Feb  8 23:22:21.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:22.602: INFO: stderr: ""
Feb  8 23:22:22.602: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  8 23:22:22.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:22.686: INFO: stderr: ""
Feb  8 23:22:22.686: INFO: stdout: "update-demo-nautilus-flzhh update-demo-nautilus-jmstm "
Feb  8 23:22:22.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-flzhh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:22.766: INFO: stderr: ""
Feb  8 23:22:22.766: INFO: stdout: "true"
Feb  8 23:22:22.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-flzhh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:22.849: INFO: stderr: ""
Feb  8 23:22:22.849: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  8 23:22:22.849: INFO: validating pod update-demo-nautilus-flzhh
Feb  8 23:22:22.854: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  8 23:22:22.854: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  8 23:22:22.854: INFO: update-demo-nautilus-flzhh is verified up and running
Feb  8 23:22:22.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-jmstm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:22.932: INFO: stderr: ""
Feb  8 23:22:22.932: INFO: stdout: ""
Feb  8 23:22:22.932: INFO: update-demo-nautilus-jmstm is created but not running
Feb  8 23:22:27.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:28.011: INFO: stderr: ""
Feb  8 23:22:28.011: INFO: stdout: "update-demo-nautilus-flzhh update-demo-nautilus-jmstm "
Feb  8 23:22:28.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-flzhh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:28.090: INFO: stderr: ""
Feb  8 23:22:28.090: INFO: stdout: "true"
Feb  8 23:22:28.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-flzhh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:28.172: INFO: stderr: ""
Feb  8 23:22:28.172: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  8 23:22:28.172: INFO: validating pod update-demo-nautilus-flzhh
Feb  8 23:22:28.176: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  8 23:22:28.176: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  8 23:22:28.176: INFO: update-demo-nautilus-flzhh is verified up and running
Feb  8 23:22:28.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-jmstm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:28.260: INFO: stderr: ""
Feb  8 23:22:28.260: INFO: stdout: "true"
Feb  8 23:22:28.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods update-demo-nautilus-jmstm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:28.335: INFO: stderr: ""
Feb  8 23:22:28.335: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  8 23:22:28.335: INFO: validating pod update-demo-nautilus-jmstm
Feb  8 23:22:28.341: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  8 23:22:28.341: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  8 23:22:28.341: INFO: update-demo-nautilus-jmstm is verified up and running
STEP: using delete to clean up resources
Feb  8 23:22:28.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:28.422: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  8 23:22:28.422: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb  8 23:22:28.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-5sscq'
Feb  8 23:22:28.511: INFO: stderr: "No resources found.\n"
Feb  8 23:22:28.511: INFO: stdout: ""
Feb  8 23:22:28.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pods -l name=update-demo --namespace=e2e-tests-kubectl-5sscq -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  8 23:22:28.591: INFO: stderr: ""
Feb  8 23:22:28.591: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:22:28.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5sscq" for this suite.
Feb  8 23:22:50.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:22:50.622: INFO: namespace: e2e-tests-kubectl-5sscq, resource: bindings, ignored listing per whitelist
Feb  8 23:22:50.698: INFO: namespace e2e-tests-kubectl-5sscq deletion completed in 22.102588507s

• [SLOW TEST:46.586 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:22:50.698: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-73e58721-2bf8-11e9-a89e-229fb9a7b2a7
STEP: Creating secret with name s-test-opt-upd-73e5876c-2bf8-11e9-a89e-229fb9a7b2a7
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-73e58721-2bf8-11e9-a89e-229fb9a7b2a7
STEP: Updating secret s-test-opt-upd-73e5876c-2bf8-11e9-a89e-229fb9a7b2a7
STEP: Creating secret with name s-test-opt-create-73e5877f-2bf8-11e9-a89e-229fb9a7b2a7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:24:13.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cnzkm" for this suite.
Feb  8 23:24:35.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:24:35.386: INFO: namespace: e2e-tests-projected-cnzkm, resource: bindings, ignored listing per whitelist
Feb  8 23:24:35.417: INFO: namespace e2e-tests-projected-cnzkm deletion completed in 22.11265732s

• [SLOW TEST:104.718 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:24:35.417: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:24:35.499: INFO: (0) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.04071ms)
Feb  8 23:24:35.503: INFO: (1) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.883309ms)
Feb  8 23:24:35.507: INFO: (2) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.332318ms)
Feb  8 23:24:35.511: INFO: (3) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.70002ms)
Feb  8 23:24:35.515: INFO: (4) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.934756ms)
Feb  8 23:24:35.518: INFO: (5) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.627425ms)
Feb  8 23:24:35.522: INFO: (6) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.446622ms)
Feb  8 23:24:35.526: INFO: (7) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.552699ms)
Feb  8 23:24:35.529: INFO: (8) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.424025ms)
Feb  8 23:24:35.533: INFO: (9) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.490748ms)
Feb  8 23:24:35.537: INFO: (10) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.421786ms)
Feb  8 23:24:35.541: INFO: (11) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.471412ms)
Feb  8 23:24:35.544: INFO: (12) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.317253ms)
Feb  8 23:24:35.547: INFO: (13) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.392183ms)
Feb  8 23:24:35.551: INFO: (14) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.79099ms)
Feb  8 23:24:35.554: INFO: (15) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.114706ms)
Feb  8 23:24:35.558: INFO: (16) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.080902ms)
Feb  8 23:24:35.562: INFO: (17) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.438227ms)
Feb  8 23:24:35.565: INFO: (18) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.345567ms)
Feb  8 23:24:35.569: INFO: (19) /api/v1/nodes/netztbred3-worker-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.409104ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:24:35.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-gzql9" for this suite.
Feb  8 23:24:41.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:24:41.628: INFO: namespace: e2e-tests-proxy-gzql9, resource: bindings, ignored listing per whitelist
Feb  8 23:24:41.679: INFO: namespace e2e-tests-proxy-gzql9 deletion completed in 6.106659477s

• [SLOW TEST:6.262 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:24:41.679: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-b60a1b56-2bf8-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 23:24:41.806: INFO: Waiting up to 5m0s for pod "pod-secrets-b610dae1-2bf8-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-secrets-49kdn" to be "success or failure"
Feb  8 23:24:41.816: INFO: Pod "pod-secrets-b610dae1-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.183581ms
Feb  8 23:24:43.821: INFO: Pod "pod-secrets-b610dae1-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01453798s
STEP: Saw pod success
Feb  8 23:24:43.821: INFO: Pod "pod-secrets-b610dae1-2bf8-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:24:43.824: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-secrets-b610dae1-2bf8-11e9-a89e-229fb9a7b2a7 container secret-volume-test: <nil>
STEP: delete the pod
Feb  8 23:24:43.859: INFO: Waiting for pod pod-secrets-b610dae1-2bf8-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:24:43.864: INFO: Pod pod-secrets-b610dae1-2bf8-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:24:43.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-49kdn" for this suite.
Feb  8 23:24:49.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:24:49.907: INFO: namespace: e2e-tests-secrets-49kdn, resource: bindings, ignored listing per whitelist
Feb  8 23:24:49.968: INFO: namespace e2e-tests-secrets-49kdn deletion completed in 6.099862384s
STEP: Destroying namespace "e2e-tests-secret-namespace-4r6qt" for this suite.
Feb  8 23:24:55.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:24:56.059: INFO: namespace: e2e-tests-secret-namespace-4r6qt, resource: bindings, ignored listing per whitelist
Feb  8 23:24:56.071: INFO: namespace e2e-tests-secret-namespace-4r6qt deletion completed in 6.103427845s

• [SLOW TEST:14.392 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:24:56.072: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb  8 23:24:56.151: INFO: Waiting up to 5m0s for pod "pod-be9de794-2bf8-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-nv8h4" to be "success or failure"
Feb  8 23:24:56.160: INFO: Pod "pod-be9de794-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.34664ms
Feb  8 23:24:58.164: INFO: Pod "pod-be9de794-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012950522s
STEP: Saw pod success
Feb  8 23:24:58.164: INFO: Pod "pod-be9de794-2bf8-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:24:58.168: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-be9de794-2bf8-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:24:58.192: INFO: Waiting for pod pod-be9de794-2bf8-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:24:58.196: INFO: Pod pod-be9de794-2bf8-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:24:58.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nv8h4" for this suite.
Feb  8 23:25:04.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:25:04.249: INFO: namespace: e2e-tests-emptydir-nv8h4, resource: bindings, ignored listing per whitelist
Feb  8 23:25:04.295: INFO: namespace e2e-tests-emptydir-nv8h4 deletion completed in 6.094846658s

• [SLOW TEST:8.224 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:25:04.295: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 23:25:04.427: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c38ca2e4-2bf8-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-pvpf7" to be "success or failure"
Feb  8 23:25:04.437: INFO: Pod "downwardapi-volume-c38ca2e4-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.576745ms
Feb  8 23:25:06.441: INFO: Pod "downwardapi-volume-c38ca2e4-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013324414s
STEP: Saw pod success
Feb  8 23:25:06.441: INFO: Pod "downwardapi-volume-c38ca2e4-2bf8-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:25:06.443: INFO: Trying to get logs from node netztbred3-worker-1 pod downwardapi-volume-c38ca2e4-2bf8-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 23:25:06.490: INFO: Waiting for pod downwardapi-volume-c38ca2e4-2bf8-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:25:06.495: INFO: Pod downwardapi-volume-c38ca2e4-2bf8-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:25:06.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pvpf7" for this suite.
Feb  8 23:25:12.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:25:12.584: INFO: namespace: e2e-tests-downward-api-pvpf7, resource: bindings, ignored listing per whitelist
Feb  8 23:25:12.598: INFO: namespace e2e-tests-downward-api-pvpf7 deletion completed in 6.099760887s

• [SLOW TEST:8.303 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:25:12.599: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb  8 23:25:12.675: INFO: Waiting up to 5m0s for pod "pod-c877159b-2bf8-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-rpkpv" to be "success or failure"
Feb  8 23:25:12.681: INFO: Pod "pod-c877159b-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.939717ms
Feb  8 23:25:14.684: INFO: Pod "pod-c877159b-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009732214s
STEP: Saw pod success
Feb  8 23:25:14.684: INFO: Pod "pod-c877159b-2bf8-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:25:14.687: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-c877159b-2bf8-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:25:14.715: INFO: Waiting for pod pod-c877159b-2bf8-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:25:14.718: INFO: Pod pod-c877159b-2bf8-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:25:14.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rpkpv" for this suite.
Feb  8 23:25:20.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:25:20.746: INFO: namespace: e2e-tests-emptydir-rpkpv, resource: bindings, ignored listing per whitelist
Feb  8 23:25:20.829: INFO: namespace e2e-tests-emptydir-rpkpv deletion completed in 6.107296376s

• [SLOW TEST:8.230 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:25:20.829: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-cd684a19-2bf8-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 23:25:20.977: INFO: Waiting up to 5m0s for pod "pod-secrets-cd69eac7-2bf8-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-secrets-69qp4" to be "success or failure"
Feb  8 23:25:20.987: INFO: Pod "pod-secrets-cd69eac7-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013457ms
Feb  8 23:25:22.992: INFO: Pod "pod-secrets-cd69eac7-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014168922s
STEP: Saw pod success
Feb  8 23:25:22.992: INFO: Pod "pod-secrets-cd69eac7-2bf8-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:25:22.994: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-secrets-cd69eac7-2bf8-11e9-a89e-229fb9a7b2a7 container secret-volume-test: <nil>
STEP: delete the pod
Feb  8 23:25:23.033: INFO: Waiting for pod pod-secrets-cd69eac7-2bf8-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:25:23.047: INFO: Pod pod-secrets-cd69eac7-2bf8-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:25:23.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-69qp4" for this suite.
Feb  8 23:25:29.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:25:29.153: INFO: namespace: e2e-tests-secrets-69qp4, resource: bindings, ignored listing per whitelist
Feb  8 23:25:29.229: INFO: namespace e2e-tests-secrets-69qp4 deletion completed in 6.177769664s

• [SLOW TEST:8.400 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:25:29.229: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb  8 23:25:29.381: INFO: Waiting up to 5m0s for pod "pod-d26a71f1-2bf8-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-lg84d" to be "success or failure"
Feb  8 23:25:29.386: INFO: Pod "pod-d26a71f1-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.820932ms
Feb  8 23:25:31.390: INFO: Pod "pod-d26a71f1-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009002175s
Feb  8 23:25:33.394: INFO: Pod "pod-d26a71f1-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012904613s
STEP: Saw pod success
Feb  8 23:25:33.394: INFO: Pod "pod-d26a71f1-2bf8-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:25:33.398: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-d26a71f1-2bf8-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:25:33.426: INFO: Waiting for pod pod-d26a71f1-2bf8-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:25:33.430: INFO: Pod pod-d26a71f1-2bf8-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:25:33.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lg84d" for this suite.
Feb  8 23:25:39.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:25:39.466: INFO: namespace: e2e-tests-emptydir-lg84d, resource: bindings, ignored listing per whitelist
Feb  8 23:25:39.535: INFO: namespace e2e-tests-emptydir-lg84d deletion completed in 6.101565738s

• [SLOW TEST:10.306 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:25:39.535: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  8 23:25:39.610: INFO: Waiting up to 5m0s for pod "downward-api-d8852c9d-2bf8-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-m6ss2" to be "success or failure"
Feb  8 23:25:39.620: INFO: Pod "downward-api-d8852c9d-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.295672ms
Feb  8 23:25:41.638: INFO: Pod "downward-api-d8852c9d-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027934861s
Feb  8 23:25:43.643: INFO: Pod "downward-api-d8852c9d-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032004153s
STEP: Saw pod success
Feb  8 23:25:43.643: INFO: Pod "downward-api-d8852c9d-2bf8-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:25:43.646: INFO: Trying to get logs from node netztbred3-worker-1 pod downward-api-d8852c9d-2bf8-11e9-a89e-229fb9a7b2a7 container dapi-container: <nil>
STEP: delete the pod
Feb  8 23:25:43.674: INFO: Waiting for pod downward-api-d8852c9d-2bf8-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:25:43.679: INFO: Pod downward-api-d8852c9d-2bf8-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:25:43.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-m6ss2" for this suite.
Feb  8 23:25:49.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:25:49.754: INFO: namespace: e2e-tests-downward-api-m6ss2, resource: bindings, ignored listing per whitelist
Feb  8 23:25:49.814: INFO: namespace e2e-tests-downward-api-m6ss2 deletion completed in 6.131092363s

• [SLOW TEST:10.279 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:25:49.814: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 23:25:49.894: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dea675bd-2bf8-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-gbpc9" to be "success or failure"
Feb  8 23:25:49.903: INFO: Pod "downwardapi-volume-dea675bd-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.720137ms
Feb  8 23:25:51.906: INFO: Pod "downwardapi-volume-dea675bd-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012167256s
Feb  8 23:25:53.910: INFO: Pod "downwardapi-volume-dea675bd-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016204822s
STEP: Saw pod success
Feb  8 23:25:53.910: INFO: Pod "downwardapi-volume-dea675bd-2bf8-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:25:53.913: INFO: Trying to get logs from node netztbred3-worker-3 pod downwardapi-volume-dea675bd-2bf8-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 23:25:53.940: INFO: Waiting for pod downwardapi-volume-dea675bd-2bf8-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:25:53.943: INFO: Pod downwardapi-volume-dea675bd-2bf8-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:25:53.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gbpc9" for this suite.
Feb  8 23:25:59.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:26:00.019: INFO: namespace: e2e-tests-projected-gbpc9, resource: bindings, ignored listing per whitelist
Feb  8 23:26:00.054: INFO: namespace e2e-tests-projected-gbpc9 deletion completed in 6.107805775s

• [SLOW TEST:10.240 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:26:00.054: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb  8 23:26:00.138: INFO: Waiting up to 5m0s for pod "pod-e4c12ad8-2bf8-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-xrfzz" to be "success or failure"
Feb  8 23:26:00.148: INFO: Pod "pod-e4c12ad8-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.04372ms
Feb  8 23:26:02.152: INFO: Pod "pod-e4c12ad8-2bf8-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014229334s
STEP: Saw pod success
Feb  8 23:26:02.152: INFO: Pod "pod-e4c12ad8-2bf8-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:26:02.157: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-e4c12ad8-2bf8-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:26:02.192: INFO: Waiting for pod pod-e4c12ad8-2bf8-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:26:02.200: INFO: Pod pod-e4c12ad8-2bf8-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:26:02.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xrfzz" for this suite.
Feb  8 23:26:08.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:26:08.275: INFO: namespace: e2e-tests-emptydir-xrfzz, resource: bindings, ignored listing per whitelist
Feb  8 23:26:08.304: INFO: namespace e2e-tests-emptydir-xrfzz deletion completed in 6.10059158s

• [SLOW TEST:8.250 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:26:08.305: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-c72mv
Feb  8 23:26:10.437: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-c72mv
STEP: checking the pod's current state and verifying that restartCount is present
Feb  8 23:26:10.440: INFO: Initial restart count of pod liveness-exec is 0
Feb  8 23:27:00.545: INFO: Restart count of pod e2e-tests-container-probe-c72mv/liveness-exec is now 1 (50.104575233s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:27:00.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-c72mv" for this suite.
Feb  8 23:27:06.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:27:06.662: INFO: namespace: e2e-tests-container-probe-c72mv, resource: bindings, ignored listing per whitelist
Feb  8 23:27:06.666: INFO: namespace e2e-tests-container-probe-c72mv deletion completed in 6.099023898s

• [SLOW TEST:58.362 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:27:06.667: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 23:27:06.747: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c751d77-2bf9-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-gpsgd" to be "success or failure"
Feb  8 23:27:06.754: INFO: Pod "downwardapi-volume-0c751d77-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.437911ms
Feb  8 23:27:08.758: INFO: Pod "downwardapi-volume-0c751d77-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010336374s
STEP: Saw pod success
Feb  8 23:27:08.758: INFO: Pod "downwardapi-volume-0c751d77-2bf9-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:27:08.761: INFO: Trying to get logs from node netztbred3-worker-1 pod downwardapi-volume-0c751d77-2bf9-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 23:27:08.792: INFO: Waiting for pod downwardapi-volume-0c751d77-2bf9-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:27:08.795: INFO: Pod downwardapi-volume-0c751d77-2bf9-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:27:08.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gpsgd" for this suite.
Feb  8 23:27:14.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:27:14.828: INFO: namespace: e2e-tests-projected-gpsgd, resource: bindings, ignored listing per whitelist
Feb  8 23:27:14.984: INFO: namespace e2e-tests-projected-gpsgd deletion completed in 6.184648363s

• [SLOW TEST:8.318 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:27:14.985: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-116f390e-2bf9-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 23:27:15.119: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1172adaf-2bf9-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-f4cg4" to be "success or failure"
Feb  8 23:27:15.124: INFO: Pod "pod-projected-configmaps-1172adaf-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.589176ms
Feb  8 23:27:17.132: INFO: Pod "pod-projected-configmaps-1172adaf-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012446329s
STEP: Saw pod success
Feb  8 23:27:17.132: INFO: Pod "pod-projected-configmaps-1172adaf-2bf9-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:27:17.135: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-projected-configmaps-1172adaf-2bf9-11e9-a89e-229fb9a7b2a7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 23:27:17.162: INFO: Waiting for pod pod-projected-configmaps-1172adaf-2bf9-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:27:17.164: INFO: Pod pod-projected-configmaps-1172adaf-2bf9-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:27:17.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-f4cg4" for this suite.
Feb  8 23:27:23.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:27:23.286: INFO: namespace: e2e-tests-projected-f4cg4, resource: bindings, ignored listing per whitelist
Feb  8 23:27:23.292: INFO: namespace e2e-tests-projected-f4cg4 deletion completed in 6.123511731s

• [SLOW TEST:8.307 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:27:23.292: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0208 23:28:03.414705      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  8 23:28:03.414: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:28:03.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-22d8l" for this suite.
Feb  8 23:28:09.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:28:09.508: INFO: namespace: e2e-tests-gc-22d8l, resource: bindings, ignored listing per whitelist
Feb  8 23:28:09.535: INFO: namespace e2e-tests-gc-22d8l deletion completed in 6.117607307s

• [SLOW TEST:46.243 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:28:09.536: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb  8 23:28:09.619: INFO: Waiting up to 5m0s for pod "client-containers-31ee9618-2bf9-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-containers-zmjxq" to be "success or failure"
Feb  8 23:28:09.622: INFO: Pod "client-containers-31ee9618-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.311514ms
Feb  8 23:28:11.626: INFO: Pod "client-containers-31ee9618-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007300681s
Feb  8 23:28:13.630: INFO: Pod "client-containers-31ee9618-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011472734s
STEP: Saw pod success
Feb  8 23:28:13.630: INFO: Pod "client-containers-31ee9618-2bf9-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:28:13.633: INFO: Trying to get logs from node netztbred3-worker-1 pod client-containers-31ee9618-2bf9-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:28:13.664: INFO: Waiting for pod client-containers-31ee9618-2bf9-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:28:13.670: INFO: Pod client-containers-31ee9618-2bf9-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:28:13.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-zmjxq" for this suite.
Feb  8 23:28:19.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:28:19.747: INFO: namespace: e2e-tests-containers-zmjxq, resource: bindings, ignored listing per whitelist
Feb  8 23:28:19.812: INFO: namespace e2e-tests-containers-zmjxq deletion completed in 6.137805335s

• [SLOW TEST:10.277 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:28:19.813: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:28:19.911: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb  8 23:28:19.924: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:19.928: INFO: Number of nodes with available pods: 0
Feb  8 23:28:19.928: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 23:28:20.933: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:20.941: INFO: Number of nodes with available pods: 0
Feb  8 23:28:20.941: INFO: Node netztbred3-worker-1 is running more than one daemon pod
Feb  8 23:28:21.933: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:21.937: INFO: Number of nodes with available pods: 1
Feb  8 23:28:21.937: INFO: Node netztbred3-worker-2 is running more than one daemon pod
Feb  8 23:28:22.933: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:22.937: INFO: Number of nodes with available pods: 3
Feb  8 23:28:22.937: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb  8 23:28:22.972: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:22.972: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:22.972: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:22.981: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:23.985: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:23.985: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:23.985: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:23.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:24.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:24.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:24.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:24.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:25.994: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:25.994: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:25.994: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:25.998: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:26.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:26.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:26.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:26.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:27.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:27.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:27.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:27.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:28.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:28.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:28.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:28.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:29.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:29.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:29.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:29.989: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:30.987: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:30.987: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:30.987: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:30.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:31.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:31.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:31.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:31.989: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:32.987: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:32.987: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:32.987: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:32.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:33.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:33.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:33.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:33.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:34.988: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:34.988: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:34.988: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:34.997: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:35.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:35.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:35.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:35.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:36.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:36.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:36.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:36.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:37.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:37.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:37.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:37.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:38.985: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:38.985: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:38.985: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:38.989: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:39.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:39.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:39.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:39.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:40.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:40.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:40.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:40.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:41.985: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:41.985: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:41.985: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:41.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:42.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:42.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:42.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:42.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:43.985: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:43.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:43.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:43.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:44.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:44.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:44.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:44.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:45.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:45.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:45.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:45.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:46.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:46.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:46.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:46.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:47.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:47.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:47.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:47.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:48.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:48.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:48.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:48.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:49.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:49.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:49.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:49.989: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:50.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:50.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:50.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:50.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:51.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:51.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:51.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:51.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:52.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:52.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:52.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:52.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:53.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:53.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:53.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:53.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:54.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:54.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:54.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:54.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:55.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:55.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:55.986: INFO: Pod daemon-set-rz5z9 is not available
Feb  8 23:28:55.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:55.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:56.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:56.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:56.986: INFO: Pod daemon-set-rz5z9 is not available
Feb  8 23:28:56.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:56.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:57.985: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:57.985: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:57.985: INFO: Pod daemon-set-rz5z9 is not available
Feb  8 23:28:57.985: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:57.989: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:58.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:58.986: INFO: Wrong image for pod: daemon-set-rz5z9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:58.986: INFO: Pod daemon-set-rz5z9 is not available
Feb  8 23:28:58.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:58.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:28:59.986: INFO: Pod daemon-set-jpq25 is not available
Feb  8 23:28:59.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:59.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:28:59.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:00.987: INFO: Pod daemon-set-jpq25 is not available
Feb  8 23:29:00.987: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:00.987: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:00.992: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:01.987: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:01.987: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:01.994: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:02.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:02.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:02.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:03.987: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:03.987: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:03.992: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:04.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:04.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:04.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:05.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:05.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:05.993: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:06.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:06.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:06.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:07.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:07.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:07.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:08.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:08.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:08.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:09.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:09.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:09.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:10.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:10.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:10.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:11.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:11.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:11.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:12.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:12.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:12.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:13.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:13.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:13.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:14.987: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:14.987: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:14.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:15.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:15.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:15.989: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:16.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:16.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:16.989: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:17.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:17.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:17.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:18.987: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:18.987: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:18.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:19.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:19.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:19.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:20.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:20.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:20.993: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:21.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:21.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:21.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:22.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:22.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:22.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:23.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:23.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:23.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:24.985: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:24.985: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:24.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:25.985: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:25.985: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:25.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:26.985: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:26.985: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:26.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:27.987: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:27.987: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:27.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:28.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:28.988: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:28.993: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:29.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:29.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:29.989: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:30.985: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:30.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:30.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:31.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:31.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:31.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:32.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:32.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:32.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:33.985: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:33.985: INFO: Pod daemon-set-p5kh2 is not available
Feb  8 23:29:33.985: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:33.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:34.987: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:34.987: INFO: Pod daemon-set-p5kh2 is not available
Feb  8 23:29:34.987: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:34.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:35.986: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:35.986: INFO: Pod daemon-set-p5kh2 is not available
Feb  8 23:29:35.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:35.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:36.985: INFO: Wrong image for pod: daemon-set-p5kh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:36.986: INFO: Pod daemon-set-p5kh2 is not available
Feb  8 23:29:36.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:36.989: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:37.986: INFO: Pod daemon-set-9cct2 is not available
Feb  8 23:29:37.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:37.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:38.986: INFO: Pod daemon-set-9cct2 is not available
Feb  8 23:29:38.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:38.995: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:39.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:39.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:40.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:40.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:41.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:41.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:42.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:42.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:43.985: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:43.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:44.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:44.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:45.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:45.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:46.985: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:46.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:47.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:47.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:48.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:48.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:49.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:49.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:50.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:50.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:51.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:51.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:52.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:52.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:53.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:53.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:54.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:54.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:55.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:55.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:56.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:56.991: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:57.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:57.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:58.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:29:58.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:29:59.998: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:00.003: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:00.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:00.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:01.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:01.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:02.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:02.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:03.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:03.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:04.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:04.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:05.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:05.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:06.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:06.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:07.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:07.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:08.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:08.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:09.985: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:09.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:10.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:10.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:11.986: INFO: Wrong image for pod: daemon-set-thxmb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  8 23:30:11.986: INFO: Pod daemon-set-thxmb is not available
Feb  8 23:30:11.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:12.986: INFO: Pod daemon-set-r2jhm is not available
Feb  8 23:30:12.990: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb  8 23:30:12.994: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:12.997: INFO: Number of nodes with available pods: 2
Feb  8 23:30:12.998: INFO: Node netztbred3-worker-2 is running more than one daemon pod
Feb  8 23:30:14.002: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:14.006: INFO: Number of nodes with available pods: 2
Feb  8 23:30:14.006: INFO: Node netztbred3-worker-2 is running more than one daemon pod
Feb  8 23:30:15.002: INFO: DaemonSet pods can't tolerate node netztbred3-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  8 23:30:15.006: INFO: Number of nodes with available pods: 3
Feb  8 23:30:15.006: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-7wh9f, will wait for the garbage collector to delete the pods
Feb  8 23:30:15.085: INFO: Deleting DaemonSet.extensions daemon-set took: 10.898914ms
Feb  8 23:30:15.186: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.149624ms
Feb  8 23:30:27.890: INFO: Number of nodes with available pods: 0
Feb  8 23:30:27.890: INFO: Number of running nodes: 0, number of available pods: 0
Feb  8 23:30:27.893: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-7wh9f/daemonsets","resourceVersion":"16631"},"items":null}

Feb  8 23:30:27.897: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-7wh9f/pods","resourceVersion":"16631"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:30:27.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-7wh9f" for this suite.
Feb  8 23:30:33.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:30:33.952: INFO: namespace: e2e-tests-daemonsets-7wh9f, resource: bindings, ignored listing per whitelist
Feb  8 23:30:34.037: INFO: namespace e2e-tests-daemonsets-7wh9f deletion completed in 6.121538946s

• [SLOW TEST:134.224 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:30:34.037: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb  8 23:30:40.163: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  8 23:30:40.170: INFO: Pod pod-with-prestop-http-hook still exists
Feb  8 23:30:42.170: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  8 23:30:42.174: INFO: Pod pod-with-prestop-http-hook still exists
Feb  8 23:30:44.170: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  8 23:30:44.175: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:30:44.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-bl68z" for this suite.
Feb  8 23:31:06.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:31:06.242: INFO: namespace: e2e-tests-container-lifecycle-hook-bl68z, resource: bindings, ignored listing per whitelist
Feb  8 23:31:06.292: INFO: namespace e2e-tests-container-lifecycle-hook-bl68z deletion completed in 22.100866279s

• [SLOW TEST:32.255 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:31:06.293: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-kxz7x/configmap-test-9b485209-2bf9-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 23:31:06.368: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b48f99d-2bf9-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-configmap-kxz7x" to be "success or failure"
Feb  8 23:31:06.373: INFO: Pod "pod-configmaps-9b48f99d-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.723223ms
Feb  8 23:31:08.377: INFO: Pod "pod-configmaps-9b48f99d-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008637545s
STEP: Saw pod success
Feb  8 23:31:08.377: INFO: Pod "pod-configmaps-9b48f99d-2bf9-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:31:08.380: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-configmaps-9b48f99d-2bf9-11e9-a89e-229fb9a7b2a7 container env-test: <nil>
STEP: delete the pod
Feb  8 23:31:08.409: INFO: Waiting for pod pod-configmaps-9b48f99d-2bf9-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:31:08.415: INFO: Pod pod-configmaps-9b48f99d-2bf9-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:31:08.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-kxz7x" for this suite.
Feb  8 23:31:14.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:31:14.474: INFO: namespace: e2e-tests-configmap-kxz7x, resource: bindings, ignored listing per whitelist
Feb  8 23:31:14.521: INFO: namespace e2e-tests-configmap-kxz7x deletion completed in 6.101076509s

• [SLOW TEST:8.228 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:31:14.521: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-a02fef1a-2bf9-11e9-a89e-229fb9a7b2a7
STEP: Creating secret with name s-test-opt-upd-a02fef78-2bf9-11e9-a89e-229fb9a7b2a7
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a02fef1a-2bf9-11e9-a89e-229fb9a7b2a7
STEP: Updating secret s-test-opt-upd-a02fef78-2bf9-11e9-a89e-229fb9a7b2a7
STEP: Creating secret with name s-test-opt-create-a02fef91-2bf9-11e9-a89e-229fb9a7b2a7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:31:22.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4qv42" for this suite.
Feb  8 23:31:44.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:31:44.823: INFO: namespace: e2e-tests-secrets-4qv42, resource: bindings, ignored listing per whitelist
Feb  8 23:31:44.828: INFO: namespace e2e-tests-secrets-4qv42 deletion completed in 22.10240228s

• [SLOW TEST:30.307 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:31:44.828: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:31:44.907: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb  8 23:31:49.911: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  8 23:31:49.911: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb  8 23:31:51.915: INFO: Creating deployment "test-rollover-deployment"
Feb  8 23:31:51.926: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb  8 23:31:53.933: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb  8 23:31:53.939: INFO: Ensure that both replica sets have 1 created replica
Feb  8 23:31:53.944: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb  8 23:31:53.955: INFO: Updating deployment test-rollover-deployment
Feb  8 23:31:53.955: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb  8 23:31:55.967: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb  8 23:31:55.975: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb  8 23:31:55.981: INFO: all replica sets need to contain the pod-template-hash label
Feb  8 23:31:55.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265514, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  8 23:31:57.988: INFO: all replica sets need to contain the pod-template-hash label
Feb  8 23:31:57.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265516, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  8 23:31:59.989: INFO: all replica sets need to contain the pod-template-hash label
Feb  8 23:31:59.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265516, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  8 23:32:01.989: INFO: all replica sets need to contain the pod-template-hash label
Feb  8 23:32:01.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265516, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  8 23:32:03.987: INFO: all replica sets need to contain the pod-template-hash label
Feb  8 23:32:03.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265516, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  8 23:32:05.989: INFO: all replica sets need to contain the pod-template-hash label
Feb  8 23:32:05.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265516, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685265511, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  8 23:32:07.988: INFO: 
Feb  8 23:32:07.988: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  8 23:32:07.996: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-f4jf5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-f4jf5/deployments/test-rollover-deployment,UID:b67014ef-2bf9-11e9-a0ab-42010a8a0033,ResourceVersion:17031,Generation:2,CreationTimestamp:2019-02-08 23:31:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-08 23:31:51 +0000 UTC 2019-02-08 23:31:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-08 23:32:06 +0000 UTC 2019-02-08 23:31:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb  8 23:32:07.999: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-f4jf5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-f4jf5/replicasets/test-rollover-deployment-6b7f9d6597,UID:b7a76c89-2bf9-11e9-a0ab-42010a8a0033,ResourceVersion:17022,Generation:2,CreationTimestamp:2019-02-08 23:31:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b67014ef-2bf9-11e9-a0ab-42010a8a0033 0xc00208c857 0xc00208c858}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb  8 23:32:07.999: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb  8 23:32:07.999: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-f4jf5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-f4jf5/replicasets/test-rollover-controller,UID:b240f860-2bf9-11e9-a0ab-42010a8a0033,ResourceVersion:17030,Generation:2,CreationTimestamp:2019-02-08 23:31:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b67014ef-2bf9-11e9-a0ab-42010a8a0033 0xc00208c657 0xc00208c658}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  8 23:32:07.999: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-f4jf5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-f4jf5/replicasets/test-rollover-deployment-6586df867b,UID:b673c057-2bf9-11e9-a0ab-42010a8a0033,ResourceVersion:16989,Generation:2,CreationTimestamp:2019-02-08 23:31:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b67014ef-2bf9-11e9-a0ab-42010a8a0033 0xc00208c787 0xc00208c788}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  8 23:32:08.002: INFO: Pod "test-rollover-deployment-6b7f9d6597-wq6q8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-wq6q8,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-f4jf5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-f4jf5/pods/test-rollover-deployment-6b7f9d6597-wq6q8,UID:b7ae25ad-2bf9-11e9-a0ab-42010a8a0033,ResourceVersion:17002,Generation:0,CreationTimestamp:2019-02-08 23:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.119/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 b7a76c89-2bf9-11e9-a0ab-42010a8a0033 0xc00208d6d7 0xc00208d6d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-mfnw4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mfnw4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-mfnw4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00208d740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00208d760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:31:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:31:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:31:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:31:54 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.52,PodIP:10.2.1.119,StartTime:2019-02-08 23:31:54 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-08 23:31:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://2d1a093335524e9a8091659ee60d6736c14b1da1c08c8a0c850f65adc73bcaa5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:32:08.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-f4jf5" for this suite.
Feb  8 23:32:14.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:32:14.086: INFO: namespace: e2e-tests-deployment-f4jf5, resource: bindings, ignored listing per whitelist
Feb  8 23:32:14.102: INFO: namespace e2e-tests-deployment-f4jf5 deletion completed in 6.095631685s

• [SLOW TEST:29.274 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:32:14.102: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 23:32:14.178: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3b3c405-2bf9-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-8qpsp" to be "success or failure"
Feb  8 23:32:14.185: INFO: Pod "downwardapi-volume-c3b3c405-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.042276ms
Feb  8 23:32:16.189: INFO: Pod "downwardapi-volume-c3b3c405-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010870244s
Feb  8 23:32:18.193: INFO: Pod "downwardapi-volume-c3b3c405-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014956071s
STEP: Saw pod success
Feb  8 23:32:18.194: INFO: Pod "downwardapi-volume-c3b3c405-2bf9-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:32:18.196: INFO: Trying to get logs from node netztbred3-worker-1 pod downwardapi-volume-c3b3c405-2bf9-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 23:32:18.225: INFO: Waiting for pod downwardapi-volume-c3b3c405-2bf9-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:32:18.235: INFO: Pod downwardapi-volume-c3b3c405-2bf9-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:32:18.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8qpsp" for this suite.
Feb  8 23:32:24.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:32:24.334: INFO: namespace: e2e-tests-projected-8qpsp, resource: bindings, ignored listing per whitelist
Feb  8 23:32:24.355: INFO: namespace e2e-tests-projected-8qpsp deletion completed in 6.115970623s

• [SLOW TEST:10.253 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:32:24.355: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-c9d203fe-2bf9-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 23:32:24.453: INFO: Waiting up to 5m0s for pod "pod-secrets-c9d345bb-2bf9-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-secrets-pxrqt" to be "success or failure"
Feb  8 23:32:24.468: INFO: Pod "pod-secrets-c9d345bb-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.317207ms
Feb  8 23:32:26.472: INFO: Pod "pod-secrets-c9d345bb-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018216065s
Feb  8 23:32:28.476: INFO: Pod "pod-secrets-c9d345bb-2bf9-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022525686s
STEP: Saw pod success
Feb  8 23:32:28.476: INFO: Pod "pod-secrets-c9d345bb-2bf9-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:32:28.479: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-secrets-c9d345bb-2bf9-11e9-a89e-229fb9a7b2a7 container secret-volume-test: <nil>
STEP: delete the pod
Feb  8 23:32:28.509: INFO: Waiting for pod pod-secrets-c9d345bb-2bf9-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:32:28.512: INFO: Pod pod-secrets-c9d345bb-2bf9-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:32:28.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pxrqt" for this suite.
Feb  8 23:32:34.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:32:34.550: INFO: namespace: e2e-tests-secrets-pxrqt, resource: bindings, ignored listing per whitelist
Feb  8 23:32:34.636: INFO: namespace e2e-tests-secrets-pxrqt deletion completed in 6.119651505s

• [SLOW TEST:10.280 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:32:34.636: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-cff3eb69-2bf9-11e9-a89e-229fb9a7b2a7
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-cff3eb69-2bf9-11e9-a89e-229fb9a7b2a7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:34:09.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7l5zm" for this suite.
Feb  8 23:34:31.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:34:31.269: INFO: namespace: e2e-tests-configmap-7l5zm, resource: bindings, ignored listing per whitelist
Feb  8 23:34:31.361: INFO: namespace e2e-tests-configmap-7l5zm deletion completed in 22.115907008s

• [SLOW TEST:116.725 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:34:31.361: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-7xrh8
Feb  8 23:34:35.444: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-7xrh8
STEP: checking the pod's current state and verifying that restartCount is present
Feb  8 23:34:35.448: INFO: Initial restart count of pod liveness-http is 0
Feb  8 23:34:51.484: INFO: Restart count of pod e2e-tests-container-probe-7xrh8/liveness-http is now 1 (16.036337286s elapsed)
Feb  8 23:35:11.526: INFO: Restart count of pod e2e-tests-container-probe-7xrh8/liveness-http is now 2 (36.078257365s elapsed)
Feb  8 23:35:31.570: INFO: Restart count of pod e2e-tests-container-probe-7xrh8/liveness-http is now 3 (56.122347215s elapsed)
Feb  8 23:35:51.610: INFO: Restart count of pod e2e-tests-container-probe-7xrh8/liveness-http is now 4 (1m16.162355782s elapsed)
Feb  8 23:37:05.767: INFO: Restart count of pod e2e-tests-container-probe-7xrh8/liveness-http is now 5 (2m30.319213579s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:37:05.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-7xrh8" for this suite.
Feb  8 23:37:11.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:37:11.865: INFO: namespace: e2e-tests-container-probe-7xrh8, resource: bindings, ignored listing per whitelist
Feb  8 23:37:11.900: INFO: namespace e2e-tests-container-probe-7xrh8 deletion completed in 6.105571133s

• [SLOW TEST:160.539 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:37:11.900: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb  8 23:37:11.978: INFO: Waiting up to 5m0s for pod "pod-753419aa-2bfa-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-gbpb5" to be "success or failure"
Feb  8 23:37:11.986: INFO: Pod "pod-753419aa-2bfa-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.130438ms
Feb  8 23:37:13.990: INFO: Pod "pod-753419aa-2bfa-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011906905s
Feb  8 23:37:15.993: INFO: Pod "pod-753419aa-2bfa-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015117774s
STEP: Saw pod success
Feb  8 23:37:15.993: INFO: Pod "pod-753419aa-2bfa-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:37:15.996: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-753419aa-2bfa-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:37:16.027: INFO: Waiting for pod pod-753419aa-2bfa-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:37:16.041: INFO: Pod pod-753419aa-2bfa-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:37:16.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gbpb5" for this suite.
Feb  8 23:37:22.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:37:22.145: INFO: namespace: e2e-tests-emptydir-gbpb5, resource: bindings, ignored listing per whitelist
Feb  8 23:37:22.155: INFO: namespace e2e-tests-emptydir-gbpb5 deletion completed in 6.109705003s

• [SLOW TEST:10.255 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:37:22.155: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-7b51baa2-2bfa-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 23:37:22.241: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b526541-2bfa-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-v4kqd" to be "success or failure"
Feb  8 23:37:22.248: INFO: Pod "pod-projected-configmaps-7b526541-2bfa-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.914845ms
Feb  8 23:37:24.252: INFO: Pod "pod-projected-configmaps-7b526541-2bfa-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010850841s
Feb  8 23:37:26.255: INFO: Pod "pod-projected-configmaps-7b526541-2bfa-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014746974s
STEP: Saw pod success
Feb  8 23:37:26.256: INFO: Pod "pod-projected-configmaps-7b526541-2bfa-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:37:26.258: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-projected-configmaps-7b526541-2bfa-11e9-a89e-229fb9a7b2a7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 23:37:26.287: INFO: Waiting for pod pod-projected-configmaps-7b526541-2bfa-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:37:26.290: INFO: Pod pod-projected-configmaps-7b526541-2bfa-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:37:26.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v4kqd" for this suite.
Feb  8 23:37:32.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:37:32.392: INFO: namespace: e2e-tests-projected-v4kqd, resource: bindings, ignored listing per whitelist
Feb  8 23:37:32.394: INFO: namespace e2e-tests-projected-v4kqd deletion completed in 6.100370824s

• [SLOW TEST:10.239 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:37:32.395: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:37:32.465: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:37:33.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-zqxj5" for this suite.
Feb  8 23:37:39.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:37:39.628: INFO: namespace: e2e-tests-custom-resource-definition-zqxj5, resource: bindings, ignored listing per whitelist
Feb  8 23:37:39.638: INFO: namespace e2e-tests-custom-resource-definition-zqxj5 deletion completed in 6.114928674s

• [SLOW TEST:7.244 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:37:39.638: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:37:39.717: INFO: (0) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.523001ms)
Feb  8 23:37:39.721: INFO: (1) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.025164ms)
Feb  8 23:37:39.725: INFO: (2) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.915231ms)
Feb  8 23:37:39.729: INFO: (3) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.996548ms)
Feb  8 23:37:39.733: INFO: (4) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.020599ms)
Feb  8 23:37:39.737: INFO: (5) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.932525ms)
Feb  8 23:37:39.741: INFO: (6) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.952893ms)
Feb  8 23:37:39.745: INFO: (7) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.875332ms)
Feb  8 23:37:39.749: INFO: (8) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.462736ms)
Feb  8 23:37:39.753: INFO: (9) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.706017ms)
Feb  8 23:37:39.756: INFO: (10) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.844539ms)
Feb  8 23:37:39.760: INFO: (11) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.612311ms)
Feb  8 23:37:39.764: INFO: (12) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.054526ms)
Feb  8 23:37:39.772: INFO: (13) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 8.121915ms)
Feb  8 23:37:39.779: INFO: (14) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.423621ms)
Feb  8 23:37:39.785: INFO: (15) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.432787ms)
Feb  8 23:37:39.789: INFO: (16) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.253058ms)
Feb  8 23:37:39.792: INFO: (17) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.525509ms)
Feb  8 23:37:39.796: INFO: (18) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.623519ms)
Feb  8 23:37:39.800: INFO: (19) /api/v1/nodes/netztbred3-worker-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.657246ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:37:39.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-ph7mp" for this suite.
Feb  8 23:37:45.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:37:45.911: INFO: namespace: e2e-tests-proxy-ph7mp, resource: bindings, ignored listing per whitelist
Feb  8 23:37:45.915: INFO: namespace e2e-tests-proxy-ph7mp deletion completed in 6.11173763s

• [SLOW TEST:6.277 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:37:45.916: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-897aa53a-2bfa-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 23:37:45.996: INFO: Waiting up to 5m0s for pod "pod-secrets-897b4fcc-2bfa-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-secrets-255xm" to be "success or failure"
Feb  8 23:37:46.010: INFO: Pod "pod-secrets-897b4fcc-2bfa-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.480878ms
Feb  8 23:37:48.015: INFO: Pod "pod-secrets-897b4fcc-2bfa-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018908829s
STEP: Saw pod success
Feb  8 23:37:48.015: INFO: Pod "pod-secrets-897b4fcc-2bfa-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:37:48.018: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-secrets-897b4fcc-2bfa-11e9-a89e-229fb9a7b2a7 container secret-volume-test: <nil>
STEP: delete the pod
Feb  8 23:37:48.045: INFO: Waiting for pod pod-secrets-897b4fcc-2bfa-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:37:48.054: INFO: Pod pod-secrets-897b4fcc-2bfa-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:37:48.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-255xm" for this suite.
Feb  8 23:37:54.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:37:54.096: INFO: namespace: e2e-tests-secrets-255xm, resource: bindings, ignored listing per whitelist
Feb  8 23:37:54.165: INFO: namespace e2e-tests-secrets-255xm deletion completed in 6.105888781s

• [SLOW TEST:8.249 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:37:54.165: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-d4w44
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb  8 23:37:54.262: INFO: Found 0 stateful pods, waiting for 3
Feb  8 23:38:04.271: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  8 23:38:04.271: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  8 23:38:04.271: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb  8 23:38:04.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-d4w44 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  8 23:38:04.489: INFO: stderr: ""
Feb  8 23:38:04.489: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  8 23:38:04.489: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb  8 23:38:14.525: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb  8 23:38:24.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-d4w44 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  8 23:38:24.763: INFO: stderr: ""
Feb  8 23:38:24.763: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  8 23:38:24.763: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  8 23:38:44.782: INFO: Waiting for StatefulSet e2e-tests-statefulset-d4w44/ss2 to complete update
Feb  8 23:38:44.783: INFO: Waiting for Pod e2e-tests-statefulset-d4w44/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Feb  8 23:38:54.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-d4w44 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  8 23:38:55.004: INFO: stderr: ""
Feb  8 23:38:55.004: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  8 23:38:55.004: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  8 23:39:05.049: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb  8 23:39:15.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-d4w44 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  8 23:39:15.261: INFO: stderr: ""
Feb  8 23:39:15.261: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  8 23:39:15.261: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  8 23:39:35.282: INFO: Waiting for StatefulSet e2e-tests-statefulset-d4w44/ss2 to complete update
Feb  8 23:39:35.282: INFO: Waiting for Pod e2e-tests-statefulset-d4w44/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  8 23:39:45.290: INFO: Deleting all statefulset in ns e2e-tests-statefulset-d4w44
Feb  8 23:39:45.292: INFO: Scaling statefulset ss2 to 0
Feb  8 23:39:55.315: INFO: Waiting for statefulset status.replicas updated to 0
Feb  8 23:39:55.318: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:39:55.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-d4w44" for this suite.
Feb  8 23:40:01.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:40:01.378: INFO: namespace: e2e-tests-statefulset-d4w44, resource: bindings, ignored listing per whitelist
Feb  8 23:40:01.455: INFO: namespace e2e-tests-statefulset-d4w44 deletion completed in 6.109941079s

• [SLOW TEST:127.290 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:40:01.455: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-6b5p2
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  8 23:40:01.526: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  8 23:40:23.656: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.2.3.43 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-6b5p2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:40:23.656: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:40:24.779: INFO: Found all expected endpoints: [netserver-0]
Feb  8 23:40:24.782: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.2.2.121 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-6b5p2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:40:24.782: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:40:25.908: INFO: Found all expected endpoints: [netserver-1]
Feb  8 23:40:25.911: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.2.1.127 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-6b5p2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:40:25.911: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:40:27.045: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:40:27.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-6b5p2" for this suite.
Feb  8 23:40:49.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:40:49.106: INFO: namespace: e2e-tests-pod-network-test-6b5p2, resource: bindings, ignored listing per whitelist
Feb  8 23:40:49.160: INFO: namespace e2e-tests-pod-network-test-6b5p2 deletion completed in 22.108194805s

• [SLOW TEST:47.705 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:40:49.160: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  8 23:40:49.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-jl7hp'
Feb  8 23:40:49.481: INFO: stderr: ""
Feb  8 23:40:49.481: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb  8 23:40:54.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-jl7hp -o json'
Feb  8 23:40:54.609: INFO: stderr: ""
Feb  8 23:40:54.609: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.2.1.129/32\"\n        },\n        \"creationTimestamp\": \"2019-02-08T23:40:49Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-jl7hp\",\n        \"resourceVersion\": \"18619\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-jl7hp/pods/e2e-test-nginx-pod\",\n        \"uid\": \"f6d7ea89-2bfa-11e9-a0ab-42010a8a0033\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-59csr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"netztbred3-worker-1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-59csr\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-59csr\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-08T23:40:49Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-08T23:40:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-08T23:40:51Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-08T23:40:49Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://65a0b9469d41131349427a3d42277b134ed6b119af7195df0998f56c6b35054e\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-08T23:40:50Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.138.0.52\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.2.1.129\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-08T23:40:49Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb  8 23:40:54.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 replace -f - --namespace=e2e-tests-kubectl-jl7hp'
Feb  8 23:40:54.785: INFO: stderr: ""
Feb  8 23:40:54.785: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Feb  8 23:40:54.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-jl7hp'
Feb  8 23:40:56.571: INFO: stderr: ""
Feb  8 23:40:56.571: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:40:56.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jl7hp" for this suite.
Feb  8 23:41:02.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:41:02.612: INFO: namespace: e2e-tests-kubectl-jl7hp, resource: bindings, ignored listing per whitelist
Feb  8 23:41:02.677: INFO: namespace e2e-tests-kubectl-jl7hp deletion completed in 6.099407778s

• [SLOW TEST:13.516 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:41:02.677: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-w27c7
Feb  8 23:41:06.777: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-w27c7
STEP: checking the pod's current state and verifying that restartCount is present
Feb  8 23:41:06.779: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:45:07.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-w27c7" for this suite.
Feb  8 23:45:13.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:45:13.480: INFO: namespace: e2e-tests-container-probe-w27c7, resource: bindings, ignored listing per whitelist
Feb  8 23:45:13.503: INFO: namespace e2e-tests-container-probe-w27c7 deletion completed in 6.108073407s

• [SLOW TEST:250.826 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:45:13.503: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-9443d2d0-2bfb-11e9-a89e-229fb9a7b2a7
STEP: Creating configMap with name cm-test-opt-upd-9443d30c-2bfb-11e9-a89e-229fb9a7b2a7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9443d2d0-2bfb-11e9-a89e-229fb9a7b2a7
STEP: Updating configmap cm-test-opt-upd-9443d30c-2bfb-11e9-a89e-229fb9a7b2a7
STEP: Creating configMap with name cm-test-opt-create-9443d322-2bfb-11e9-a89e-229fb9a7b2a7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:45:21.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k5254" for this suite.
Feb  8 23:45:43.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:45:43.766: INFO: namespace: e2e-tests-projected-k5254, resource: bindings, ignored listing per whitelist
Feb  8 23:45:43.810: INFO: namespace e2e-tests-projected-k5254 deletion completed in 22.116256116s

• [SLOW TEST:30.307 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:45:43.811: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-dhqv
STEP: Creating a pod to test atomic-volume-subpath
Feb  8 23:45:43.894: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dhqv" in namespace "e2e-tests-subpath-77rfp" to be "success or failure"
Feb  8 23:45:43.901: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037219ms
Feb  8 23:45:45.905: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010406747s
Feb  8 23:45:47.909: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Running", Reason="", readiness=false. Elapsed: 4.014229563s
Feb  8 23:45:49.913: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Running", Reason="", readiness=false. Elapsed: 6.01869289s
Feb  8 23:45:51.917: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Running", Reason="", readiness=false. Elapsed: 8.022828901s
Feb  8 23:45:53.921: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Running", Reason="", readiness=false. Elapsed: 10.026507421s
Feb  8 23:45:55.925: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Running", Reason="", readiness=false. Elapsed: 12.030474662s
Feb  8 23:45:57.929: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Running", Reason="", readiness=false. Elapsed: 14.034753364s
Feb  8 23:45:59.937: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Running", Reason="", readiness=false. Elapsed: 16.042490919s
Feb  8 23:46:01.941: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Running", Reason="", readiness=false. Elapsed: 18.046968145s
Feb  8 23:46:03.946: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Running", Reason="", readiness=false. Elapsed: 20.051388416s
Feb  8 23:46:05.950: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Running", Reason="", readiness=false. Elapsed: 22.055426656s
Feb  8 23:46:07.955: INFO: Pod "pod-subpath-test-configmap-dhqv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.06026209s
STEP: Saw pod success
Feb  8 23:46:07.955: INFO: Pod "pod-subpath-test-configmap-dhqv" satisfied condition "success or failure"
Feb  8 23:46:07.958: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-subpath-test-configmap-dhqv container test-container-subpath-configmap-dhqv: <nil>
STEP: delete the pod
Feb  8 23:46:07.987: INFO: Waiting for pod pod-subpath-test-configmap-dhqv to disappear
Feb  8 23:46:07.991: INFO: Pod pod-subpath-test-configmap-dhqv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dhqv
Feb  8 23:46:07.991: INFO: Deleting pod "pod-subpath-test-configmap-dhqv" in namespace "e2e-tests-subpath-77rfp"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:46:07.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-77rfp" for this suite.
Feb  8 23:46:14.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:46:14.048: INFO: namespace: e2e-tests-subpath-77rfp, resource: bindings, ignored listing per whitelist
Feb  8 23:46:14.147: INFO: namespace e2e-tests-subpath-77rfp deletion completed in 6.14964855s

• [SLOW TEST:30.336 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:46:14.147: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0208 23:46:15.268466      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  8 23:46:15.268: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:46:15.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-s2mb4" for this suite.
Feb  8 23:46:21.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:46:21.334: INFO: namespace: e2e-tests-gc-s2mb4, resource: bindings, ignored listing per whitelist
Feb  8 23:46:21.379: INFO: namespace e2e-tests-gc-s2mb4 deletion completed in 6.107354041s

• [SLOW TEST:7.233 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:46:21.380: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0208 23:46:51.992950      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  8 23:46:51.992: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:46:51.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-xxb9n" for this suite.
Feb  8 23:46:58.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:46:58.084: INFO: namespace: e2e-tests-gc-xxb9n, resource: bindings, ignored listing per whitelist
Feb  8 23:46:58.095: INFO: namespace e2e-tests-gc-xxb9n deletion completed in 6.099086701s

• [SLOW TEST:36.715 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:46:58.095: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-d29a897f-2bfb-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 23:46:58.179: INFO: Waiting up to 5m0s for pod "pod-configmaps-d29b2bdf-2bfb-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-configmap-hwrqg" to be "success or failure"
Feb  8 23:46:58.183: INFO: Pod "pod-configmaps-d29b2bdf-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.249596ms
Feb  8 23:47:00.187: INFO: Pod "pod-configmaps-d29b2bdf-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007761955s
STEP: Saw pod success
Feb  8 23:47:00.187: INFO: Pod "pod-configmaps-d29b2bdf-2bfb-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:47:00.189: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-configmaps-d29b2bdf-2bfb-11e9-a89e-229fb9a7b2a7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 23:47:00.234: INFO: Waiting for pod pod-configmaps-d29b2bdf-2bfb-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:47:00.249: INFO: Pod pod-configmaps-d29b2bdf-2bfb-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:47:00.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hwrqg" for this suite.
Feb  8 23:47:06.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:47:06.317: INFO: namespace: e2e-tests-configmap-hwrqg, resource: bindings, ignored listing per whitelist
Feb  8 23:47:06.361: INFO: namespace e2e-tests-configmap-hwrqg deletion completed in 6.106707682s

• [SLOW TEST:8.266 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:47:06.362: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 23:47:06.429: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7867ce8-2bfb-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-twcdk" to be "success or failure"
Feb  8 23:47:06.437: INFO: Pod "downwardapi-volume-d7867ce8-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.238216ms
Feb  8 23:47:08.441: INFO: Pod "downwardapi-volume-d7867ce8-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011825709s
STEP: Saw pod success
Feb  8 23:47:08.441: INFO: Pod "downwardapi-volume-d7867ce8-2bfb-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:47:08.444: INFO: Trying to get logs from node netztbred3-worker-3 pod downwardapi-volume-d7867ce8-2bfb-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 23:47:08.467: INFO: Waiting for pod downwardapi-volume-d7867ce8-2bfb-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:47:08.472: INFO: Pod downwardapi-volume-d7867ce8-2bfb-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:47:08.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-twcdk" for this suite.
Feb  8 23:47:14.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:47:14.514: INFO: namespace: e2e-tests-downward-api-twcdk, resource: bindings, ignored listing per whitelist
Feb  8 23:47:14.578: INFO: namespace e2e-tests-downward-api-twcdk deletion completed in 6.102052701s

• [SLOW TEST:8.216 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:47:14.578: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-dc7111f1-2bfb-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 23:47:14.691: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dc7241ec-2bfb-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-css4z" to be "success or failure"
Feb  8 23:47:14.697: INFO: Pod "pod-projected-secrets-dc7241ec-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.344767ms
Feb  8 23:47:16.701: INFO: Pod "pod-projected-secrets-dc7241ec-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009845228s
STEP: Saw pod success
Feb  8 23:47:16.701: INFO: Pod "pod-projected-secrets-dc7241ec-2bfb-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:47:16.703: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-projected-secrets-dc7241ec-2bfb-11e9-a89e-229fb9a7b2a7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  8 23:47:16.732: INFO: Waiting for pod pod-projected-secrets-dc7241ec-2bfb-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:47:16.736: INFO: Pod pod-projected-secrets-dc7241ec-2bfb-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:47:16.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-css4z" for this suite.
Feb  8 23:47:22.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:47:22.829: INFO: namespace: e2e-tests-projected-css4z, resource: bindings, ignored listing per whitelist
Feb  8 23:47:22.842: INFO: namespace e2e-tests-projected-css4z deletion completed in 6.102209746s

• [SLOW TEST:8.264 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:47:22.843: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 23:47:22.914: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1593a51-2bfb-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-rnd6q" to be "success or failure"
Feb  8 23:47:22.919: INFO: Pod "downwardapi-volume-e1593a51-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.725785ms
Feb  8 23:47:24.923: INFO: Pod "downwardapi-volume-e1593a51-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009448403s
STEP: Saw pod success
Feb  8 23:47:24.923: INFO: Pod "downwardapi-volume-e1593a51-2bfb-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:47:24.926: INFO: Trying to get logs from node netztbred3-worker-3 pod downwardapi-volume-e1593a51-2bfb-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 23:47:24.966: INFO: Waiting for pod downwardapi-volume-e1593a51-2bfb-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:47:24.990: INFO: Pod downwardapi-volume-e1593a51-2bfb-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:47:24.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rnd6q" for this suite.
Feb  8 23:47:31.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:47:31.076: INFO: namespace: e2e-tests-projected-rnd6q, resource: bindings, ignored listing per whitelist
Feb  8 23:47:31.121: INFO: namespace e2e-tests-projected-rnd6q deletion completed in 6.117776147s

• [SLOW TEST:8.278 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:47:31.121: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  8 23:47:31.216: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e64bf327-2bfb-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-55fqs" to be "success or failure"
Feb  8 23:47:31.223: INFO: Pod "downwardapi-volume-e64bf327-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.615782ms
Feb  8 23:47:33.227: INFO: Pod "downwardapi-volume-e64bf327-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010943955s
Feb  8 23:47:35.231: INFO: Pod "downwardapi-volume-e64bf327-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014963546s
STEP: Saw pod success
Feb  8 23:47:35.231: INFO: Pod "downwardapi-volume-e64bf327-2bfb-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:47:35.234: INFO: Trying to get logs from node netztbred3-worker-1 pod downwardapi-volume-e64bf327-2bfb-11e9-a89e-229fb9a7b2a7 container client-container: <nil>
STEP: delete the pod
Feb  8 23:47:35.267: INFO: Waiting for pod downwardapi-volume-e64bf327-2bfb-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:47:35.276: INFO: Pod downwardapi-volume-e64bf327-2bfb-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:47:35.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-55fqs" for this suite.
Feb  8 23:47:41.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:47:41.349: INFO: namespace: e2e-tests-downward-api-55fqs, resource: bindings, ignored listing per whitelist
Feb  8 23:47:41.375: INFO: namespace e2e-tests-downward-api-55fqs deletion completed in 6.09481226s

• [SLOW TEST:10.255 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:47:41.376: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  8 23:47:43.987: INFO: Successfully updated pod "labelsupdateec65a6c0-2bfb-11e9-a89e-229fb9a7b2a7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:47:46.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7plhz" for this suite.
Feb  8 23:48:08.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:48:08.074: INFO: namespace: e2e-tests-projected-7plhz, resource: bindings, ignored listing per whitelist
Feb  8 23:48:08.159: INFO: namespace e2e-tests-projected-7plhz deletion completed in 22.148486573s

• [SLOW TEST:26.784 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:48:08.160: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  8 23:48:08.259: INFO: Waiting up to 5m0s for pod "downward-api-fc60c1e3-2bfb-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-downward-api-qg8bc" to be "success or failure"
Feb  8 23:48:08.273: INFO: Pod "downward-api-fc60c1e3-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.532003ms
Feb  8 23:48:10.278: INFO: Pod "downward-api-fc60c1e3-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017997364s
Feb  8 23:48:12.282: INFO: Pod "downward-api-fc60c1e3-2bfb-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022027614s
STEP: Saw pod success
Feb  8 23:48:12.282: INFO: Pod "downward-api-fc60c1e3-2bfb-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:48:12.284: INFO: Trying to get logs from node netztbred3-worker-1 pod downward-api-fc60c1e3-2bfb-11e9-a89e-229fb9a7b2a7 container dapi-container: <nil>
STEP: delete the pod
Feb  8 23:48:12.311: INFO: Waiting for pod downward-api-fc60c1e3-2bfb-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:48:12.324: INFO: Pod downward-api-fc60c1e3-2bfb-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:48:12.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qg8bc" for this suite.
Feb  8 23:48:18.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:48:18.384: INFO: namespace: e2e-tests-downward-api-qg8bc, resource: bindings, ignored listing per whitelist
Feb  8 23:48:18.426: INFO: namespace e2e-tests-downward-api-qg8bc deletion completed in 6.097655129s

• [SLOW TEST:10.267 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:48:18.427: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-clj9m
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-clj9m
STEP: Deleting pre-stop pod
Feb  8 23:48:31.554: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:48:31.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-clj9m" for this suite.
Feb  8 23:49:09.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:49:09.665: INFO: namespace: e2e-tests-prestop-clj9m, resource: bindings, ignored listing per whitelist
Feb  8 23:49:09.683: INFO: namespace e2e-tests-prestop-clj9m deletion completed in 38.112664232s

• [SLOW TEST:51.256 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:49:09.683: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-210b047f-2bfc-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 23:49:09.781: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-210bd7f7-2bfc-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-projected-ld7sw" to be "success or failure"
Feb  8 23:49:09.788: INFO: Pod "pod-projected-configmaps-210bd7f7-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.098666ms
Feb  8 23:49:11.793: INFO: Pod "pod-projected-configmaps-210bd7f7-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01223148s
Feb  8 23:49:13.797: INFO: Pod "pod-projected-configmaps-210bd7f7-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016339071s
STEP: Saw pod success
Feb  8 23:49:13.797: INFO: Pod "pod-projected-configmaps-210bd7f7-2bfc-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:49:13.800: INFO: Trying to get logs from node netztbred3-worker-3 pod pod-projected-configmaps-210bd7f7-2bfc-11e9-a89e-229fb9a7b2a7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 23:49:13.824: INFO: Waiting for pod pod-projected-configmaps-210bd7f7-2bfc-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:49:13.827: INFO: Pod pod-projected-configmaps-210bd7f7-2bfc-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:49:13.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ld7sw" for this suite.
Feb  8 23:49:19.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:49:19.863: INFO: namespace: e2e-tests-projected-ld7sw, resource: bindings, ignored listing per whitelist
Feb  8 23:49:19.933: INFO: namespace e2e-tests-projected-ld7sw deletion completed in 6.102637826s

• [SLOW TEST:10.250 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:49:19.934: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb  8 23:49:26.053: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-66sj2 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:49:26.053: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:49:26.183: INFO: Exec stderr: ""
Feb  8 23:49:26.183: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-66sj2 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:49:26.183: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:49:26.337: INFO: Exec stderr: ""
Feb  8 23:49:26.337: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-66sj2 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:49:26.337: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:49:26.484: INFO: Exec stderr: ""
Feb  8 23:49:26.484: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-66sj2 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:49:26.484: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:49:26.631: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb  8 23:49:26.631: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-66sj2 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:49:26.631: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:49:26.771: INFO: Exec stderr: ""
Feb  8 23:49:26.771: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-66sj2 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:49:26.771: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:49:26.917: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb  8 23:49:26.917: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-66sj2 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:49:26.917: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:49:27.059: INFO: Exec stderr: ""
Feb  8 23:49:27.059: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-66sj2 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:49:27.059: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:49:27.210: INFO: Exec stderr: ""
Feb  8 23:49:27.210: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-66sj2 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:49:27.210: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:49:27.352: INFO: Exec stderr: ""
Feb  8 23:49:27.352: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-66sj2 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  8 23:49:27.352: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
Feb  8 23:49:27.492: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:49:27.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-66sj2" for this suite.
Feb  8 23:50:13.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:50:13.661: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-66sj2, resource: bindings, ignored listing per whitelist
Feb  8 23:50:13.700: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-66sj2 deletion completed in 46.202354252s

• [SLOW TEST:53.766 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:50:13.700: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-47399c4a-2bfc-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume secrets
Feb  8 23:50:13.858: INFO: Waiting up to 5m0s for pod "pod-secrets-473a7dbb-2bfc-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-secrets-tht7g" to be "success or failure"
Feb  8 23:50:13.870: INFO: Pod "pod-secrets-473a7dbb-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.639117ms
Feb  8 23:50:15.875: INFO: Pod "pod-secrets-473a7dbb-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016714684s
STEP: Saw pod success
Feb  8 23:50:15.875: INFO: Pod "pod-secrets-473a7dbb-2bfc-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:50:15.878: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-secrets-473a7dbb-2bfc-11e9-a89e-229fb9a7b2a7 container secret-env-test: <nil>
STEP: delete the pod
Feb  8 23:50:15.924: INFO: Waiting for pod pod-secrets-473a7dbb-2bfc-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:50:15.933: INFO: Pod pod-secrets-473a7dbb-2bfc-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:50:15.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-tht7g" for this suite.
Feb  8 23:50:21.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:50:21.986: INFO: namespace: e2e-tests-secrets-tht7g, resource: bindings, ignored listing per whitelist
Feb  8 23:50:22.046: INFO: namespace e2e-tests-secrets-tht7g deletion completed in 6.108090638s

• [SLOW TEST:8.346 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:50:22.046: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb  8 23:50:22.165: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  8 23:50:22.172: INFO: Waiting for terminating namespaces to be deleted...
Feb  8 23:50:22.174: INFO: 
Logging pods the kubelet thinks is on node netztbred3-worker-1 before test
Feb  8 23:50:22.181: INFO: calico-node-xctdj from kube-system started at 2019-02-08 22:16:28 +0000 UTC (2 container statuses recorded)
Feb  8 23:50:22.181: INFO: 	Container calico-node ready: true, restart count 0
Feb  8 23:50:22.181: INFO: 	Container install-cni ready: true, restart count 0
Feb  8 23:50:22.181: INFO: dashboard-proxy-846644569f-6szfm from kube-system started at 2019-02-08 22:17:44 +0000 UTC (1 container statuses recorded)
Feb  8 23:50:22.181: INFO: 	Container nginx ready: true, restart count 0
Feb  8 23:50:22.181: INFO: kube-proxy-q2w28 from kube-system started at 2019-02-08 22:16:28 +0000 UTC (1 container statuses recorded)
Feb  8 23:50:22.181: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  8 23:50:22.181: INFO: tiller-deploy-6f8d4f6c9c-lqj4h from kube-system started at 2019-02-08 22:17:15 +0000 UTC (1 container statuses recorded)
Feb  8 23:50:22.181: INFO: 	Container tiller ready: true, restart count 0
Feb  8 23:50:22.181: INFO: sonobuoy-systemd-logs-daemon-set-173572836c7f431d-lz98h from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 23:50:22.181: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb  8 23:50:22.181: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb  8 23:50:22.181: INFO: 
Logging pods the kubelet thinks is on node netztbred3-worker-2 before test
Feb  8 23:50:22.188: INFO: sonobuoy-e2e-job-ae557681a2834e81 from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 23:50:22.188: INFO: 	Container e2e ready: true, restart count 0
Feb  8 23:50:22.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  8 23:50:22.188: INFO: calico-node-625hk from kube-system started at 2019-02-08 22:16:30 +0000 UTC (2 container statuses recorded)
Feb  8 23:50:22.188: INFO: 	Container calico-node ready: true, restart count 0
Feb  8 23:50:22.188: INFO: 	Container install-cni ready: true, restart count 0
Feb  8 23:50:22.188: INFO: kubernetes-dashboard-5d8785cf74-69q9c from kube-system started at 2019-02-08 22:17:44 +0000 UTC (1 container statuses recorded)
Feb  8 23:50:22.188: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb  8 23:50:22.188: INFO: kube-proxy-mcn8h from kube-system started at 2019-02-08 22:16:30 +0000 UTC (1 container statuses recorded)
Feb  8 23:50:22.188: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  8 23:50:22.188: INFO: sonobuoy-systemd-logs-daemon-set-173572836c7f431d-cpmxt from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 23:50:22.188: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb  8 23:50:22.188: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb  8 23:50:22.188: INFO: 
Logging pods the kubelet thinks is on node netztbred3-worker-3 before test
Feb  8 23:50:22.194: INFO: heapster-6cb8b844bb-ggjhz from kube-system started at 2019-02-08 22:17:44 +0000 UTC (1 container statuses recorded)
Feb  8 23:50:22.194: INFO: 	Container heapster ready: true, restart count 0
Feb  8 23:50:22.194: INFO: calico-node-9srcf from kube-system started at 2019-02-08 22:16:30 +0000 UTC (2 container statuses recorded)
Feb  8 23:50:22.194: INFO: 	Container calico-node ready: true, restart count 0
Feb  8 23:50:22.194: INFO: 	Container install-cni ready: true, restart count 0
Feb  8 23:50:22.194: INFO: kube-proxy-587fv from kube-system started at 2019-02-08 22:16:29 +0000 UTC (1 container statuses recorded)
Feb  8 23:50:22.194: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  8 23:50:22.194: INFO: sonobuoy-systemd-logs-daemon-set-173572836c7f431d-zncqp from heptio-sonobuoy started at 2019-02-08 22:19:52 +0000 UTC (2 container statuses recorded)
Feb  8 23:50:22.194: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb  8 23:50:22.194: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb  8 23:50:22.194: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-08 22:19:48 +0000 UTC (1 container statuses recorded)
Feb  8 23:50:22.194: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node netztbred3-worker-1
STEP: verifying the node has the label node netztbred3-worker-2
STEP: verifying the node has the label node netztbred3-worker-3
Feb  8 23:50:22.276: INFO: Pod sonobuoy requesting resource cpu=0m on Node netztbred3-worker-3
Feb  8 23:50:22.277: INFO: Pod sonobuoy-e2e-job-ae557681a2834e81 requesting resource cpu=0m on Node netztbred3-worker-2
Feb  8 23:50:22.277: INFO: Pod sonobuoy-systemd-logs-daemon-set-173572836c7f431d-cpmxt requesting resource cpu=0m on Node netztbred3-worker-2
Feb  8 23:50:22.277: INFO: Pod sonobuoy-systemd-logs-daemon-set-173572836c7f431d-lz98h requesting resource cpu=0m on Node netztbred3-worker-1
Feb  8 23:50:22.277: INFO: Pod sonobuoy-systemd-logs-daemon-set-173572836c7f431d-zncqp requesting resource cpu=0m on Node netztbred3-worker-3
Feb  8 23:50:22.277: INFO: Pod calico-node-625hk requesting resource cpu=250m on Node netztbred3-worker-2
Feb  8 23:50:22.277: INFO: Pod calico-node-9srcf requesting resource cpu=250m on Node netztbred3-worker-3
Feb  8 23:50:22.277: INFO: Pod calico-node-xctdj requesting resource cpu=250m on Node netztbred3-worker-1
Feb  8 23:50:22.277: INFO: Pod dashboard-proxy-846644569f-6szfm requesting resource cpu=0m on Node netztbred3-worker-1
Feb  8 23:50:22.277: INFO: Pod heapster-6cb8b844bb-ggjhz requesting resource cpu=0m on Node netztbred3-worker-3
Feb  8 23:50:22.277: INFO: Pod kube-proxy-587fv requesting resource cpu=0m on Node netztbred3-worker-3
Feb  8 23:50:22.277: INFO: Pod kube-proxy-mcn8h requesting resource cpu=0m on Node netztbred3-worker-2
Feb  8 23:50:22.277: INFO: Pod kube-proxy-q2w28 requesting resource cpu=0m on Node netztbred3-worker-1
Feb  8 23:50:22.277: INFO: Pod kubernetes-dashboard-5d8785cf74-69q9c requesting resource cpu=0m on Node netztbred3-worker-2
Feb  8 23:50:22.277: INFO: Pod tiller-deploy-6f8d4f6c9c-lqj4h requesting resource cpu=0m on Node netztbred3-worker-1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c43baa7-2bfc-11e9-a89e-229fb9a7b2a7.1581887e2d39c211], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-g8j7n/filler-pod-4c43baa7-2bfc-11e9-a89e-229fb9a7b2a7 to netztbred3-worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c43baa7-2bfc-11e9-a89e-229fb9a7b2a7.1581887e70a79dd6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c43baa7-2bfc-11e9-a89e-229fb9a7b2a7.1581887e74b4e24f], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c43baa7-2bfc-11e9-a89e-229fb9a7b2a7.1581887e82167b3a], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c451555-2bfc-11e9-a89e-229fb9a7b2a7.1581887e2e2bd766], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-g8j7n/filler-pod-4c451555-2bfc-11e9-a89e-229fb9a7b2a7 to netztbred3-worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c451555-2bfc-11e9-a89e-229fb9a7b2a7.1581887e7a262606], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c451555-2bfc-11e9-a89e-229fb9a7b2a7.1581887e7e0230af], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c451555-2bfc-11e9-a89e-229fb9a7b2a7.1581887e908f75f2], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c47435c-2bfc-11e9-a89e-229fb9a7b2a7.1581887e2f473500], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-g8j7n/filler-pod-4c47435c-2bfc-11e9-a89e-229fb9a7b2a7 to netztbred3-worker-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c47435c-2bfc-11e9-a89e-229fb9a7b2a7.1581887e711636da], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c47435c-2bfc-11e9-a89e-229fb9a7b2a7.1581887e750808d3], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c47435c-2bfc-11e9-a89e-229fb9a7b2a7.1581887e849649d4], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1581887f1f1d1967], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node netztbred3-worker-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node netztbred3-worker-3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node netztbred3-worker-1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:50:27.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-g8j7n" for this suite.
Feb  8 23:50:33.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:50:33.581: INFO: namespace: e2e-tests-sched-pred-g8j7n, resource: bindings, ignored listing per whitelist
Feb  8 23:50:33.590: INFO: namespace e2e-tests-sched-pred-g8j7n deletion completed in 6.138754074s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.544 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:50:33.590: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb  8 23:50:41.719: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:50:41.723: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:50:43.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:50:43.727: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:50:45.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:50:45.728: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:50:47.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:50:47.727: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:50:49.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:50:49.728: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:50:51.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:50:51.728: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:50:53.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:50:53.728: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:50:55.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:50:55.728: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:50:57.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:50:57.728: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:50:59.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:50:59.728: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:51:01.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:51:01.728: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:51:03.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:51:03.728: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:51:05.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:51:05.727: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:51:07.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:51:07.727: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  8 23:51:09.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  8 23:51:09.728: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:51:09.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-q9p82" for this suite.
Feb  8 23:51:31.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:51:31.781: INFO: namespace: e2e-tests-container-lifecycle-hook-q9p82, resource: bindings, ignored listing per whitelist
Feb  8 23:51:31.833: INFO: namespace e2e-tests-container-lifecycle-hook-q9p82 deletion completed in 22.101314333s

• [SLOW TEST:58.243 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:51:31.833: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:51:31.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 version --client'
Feb  8 23:51:31.958: INFO: stderr: ""
Feb  8 23:51:31.958: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb  8 23:51:31.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-n7cls'
Feb  8 23:51:32.323: INFO: stderr: ""
Feb  8 23:51:32.323: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb  8 23:51:32.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 create -f - --namespace=e2e-tests-kubectl-n7cls'
Feb  8 23:51:32.548: INFO: stderr: ""
Feb  8 23:51:32.548: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb  8 23:51:33.552: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 23:51:33.552: INFO: Found 0 / 1
Feb  8 23:51:34.552: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 23:51:34.553: INFO: Found 0 / 1
Feb  8 23:51:35.554: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 23:51:35.554: INFO: Found 1 / 1
Feb  8 23:51:35.554: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  8 23:51:35.560: INFO: Selector matched 1 pods for map[app:redis]
Feb  8 23:51:35.560: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  8 23:51:35.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 describe pod redis-master-7dvkw --namespace=e2e-tests-kubectl-n7cls'
Feb  8 23:51:35.688: INFO: stderr: ""
Feb  8 23:51:35.688: INFO: stdout: "Name:               redis-master-7dvkw\nNamespace:          e2e-tests-kubectl-n7cls\nPriority:           0\nPriorityClassName:  <none>\nNode:               netztbred3-worker-3/10.138.0.54\nStart Time:         Fri, 08 Feb 2019 23:51:32 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 10.2.2.133/32\nStatus:             Running\nIP:                 10.2.2.133\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://c4ebd8ccdf051d57383d8734a3689d2706f3f48c98c0a4c023d679b0a8d01e75\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 08 Feb 2019 23:51:33 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2bmrg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-2bmrg:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-2bmrg\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                          Message\n  ----    ------     ----  ----                          -------\n  Normal  Scheduled  3s    default-scheduler             Successfully assigned e2e-tests-kubectl-n7cls/redis-master-7dvkw to netztbred3-worker-3\n  Normal  Pulled     2s    kubelet, netztbred3-worker-3  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, netztbred3-worker-3  Created container\n  Normal  Started    2s    kubelet, netztbred3-worker-3  Started container\n"
Feb  8 23:51:35.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 describe rc redis-master --namespace=e2e-tests-kubectl-n7cls'
Feb  8 23:51:35.801: INFO: stderr: ""
Feb  8 23:51:35.801: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-n7cls\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-7dvkw\n"
Feb  8 23:51:35.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 describe service redis-master --namespace=e2e-tests-kubectl-n7cls'
Feb  8 23:51:35.888: INFO: stderr: ""
Feb  8 23:51:35.888: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-n7cls\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.3.0.32\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.2.2.133:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb  8 23:51:35.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 describe node netztbred3-master-1'
Feb  8 23:51:35.997: INFO: stderr: ""
Feb  8 23:51:35.997: INFO: stdout: "Name:               netztbred3-master-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n1-standard-1\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west1\n                    failure-domain.beta.kubernetes.io/zone=us-west1-a\n                    kubernetes.io/hostname=netztbred3-master-1\n                    node-role.kubernetes.io/master=\n                    stackpoint.io/cluster_id=5705\n                    stackpoint.io/instance_id=netztbred3-master-1\n                    stackpoint.io/node_group=\n                    stackpoint.io/node_id=18214\n                    stackpoint.io/node_pool=\n                    stackpoint.io/private_ip=10.138.0.51\n                    stackpoint.io/role=master\n                    stackpoint.io/size=n1-standard-1\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.138.0.51/32\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 08 Feb 2019 22:15:03 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 08 Feb 2019 23:51:18 +0000   Fri, 08 Feb 2019 23:51:18 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Fri, 08 Feb 2019 23:51:33 +0000   Fri, 08 Feb 2019 22:15:03 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 08 Feb 2019 23:51:33 +0000   Fri, 08 Feb 2019 22:15:03 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 08 Feb 2019 23:51:33 +0000   Fri, 08 Feb 2019 22:15:03 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 08 Feb 2019 23:51:33 +0000   Fri, 08 Feb 2019 22:16:05 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.138.0.51\n  ExternalIP:   35.197.124.115\n  InternalDNS:  netztbred3-master-1.c.hello-reykjavik.internal\n  Hostname:     netztbred3-master-1\nCapacity:\n attachable-volumes-gce-pd:  32\n cpu:                        1\n ephemeral-storage:          50758760Ki\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     3781804Ki\n pods:                       110\nAllocatable:\n attachable-volumes-gce-pd:  32\n cpu:                        1\n ephemeral-storage:          46779273139\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     3679404Ki\n pods:                       110\nSystem Info:\n Machine ID:                 d269c97354b1cca478d4caf88714a421\n System UUID:                D269C973-54B1-CCA4-78D4-CAF88714A421\n Boot ID:                    a0f2b302-9eb8-4947-b68b-ef1894f7442a\n Kernel Version:             4.15.0-1027-gcp\n OS Image:                   Ubuntu 16.04.5 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.0\n Kubelet Version:            v1.13.2\n Kube-Proxy Version:         v1.13.2\nPodCIDR:                     10.2.0.0/24\nProviderID:                  gce://hello-reykjavik/us-west1-a/netztbred3-master-1\nNon-terminated Pods:         (8 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-173572836c7f431d-hw7hs    0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                calico-node-r4xhd                                          250m (25%)    0 (0%)      0 (0%)           0 (0%)         96m\n  kube-system                coredns-86c58d9df4-9mghs                                   100m (10%)    0 (0%)      70Mi (1%)        170Mi (4%)     96m\n  kube-system                coredns-86c58d9df4-t2vmz                                   100m (10%)    0 (0%)      70Mi (1%)        170Mi (4%)     96m\n  kube-system                kube-apiserver-netztbred3-master-1                         250m (25%)    0 (0%)      0 (0%)           0 (0%)         95m\n  kube-system                kube-controller-manager-netztbred3-master-1                200m (20%)    0 (0%)      0 (0%)           0 (0%)         95m\n  kube-system                kube-proxy-2h7dq                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         96m\n  kube-system                kube-scheduler-netztbred3-master-1                         100m (10%)    0 (0%)      0 (0%)           0 (0%)         95m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        1 (100%)    0 (0%)\n  memory                     140Mi (3%)  340Mi (9%)\n  ephemeral-storage          0 (0%)      0 (0%)\n  attachable-volumes-gce-pd  0           0\nEvents:                      <none>\n"
Feb  8 23:51:35.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 describe namespace e2e-tests-kubectl-n7cls'
Feb  8 23:51:36.105: INFO: stderr: ""
Feb  8 23:51:36.105: INFO: stdout: "Name:         e2e-tests-kubectl-n7cls\nLabels:       e2e-framework=kubectl\n              e2e-run=b51f581c-2bef-11e9-a89e-229fb9a7b2a7\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:51:36.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-n7cls" for this suite.
Feb  8 23:51:58.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:51:58.161: INFO: namespace: e2e-tests-kubectl-n7cls, resource: bindings, ignored listing per whitelist
Feb  8 23:51:58.212: INFO: namespace e2e-tests-kubectl-n7cls deletion completed in 22.102094833s

• [SLOW TEST:26.379 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:51:58.213: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  8 23:51:58.286: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb  8 23:52:03.291: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  8 23:52:03.291: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  8 23:52:03.333: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-d4n4l,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d4n4l/deployments/test-cleanup-deployment,UID:887a08c3-2bfc-11e9-a0ab-42010a8a0033,ResourceVersion:20472,Generation:1,CreationTimestamp:2019-02-08 23:52:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Feb  8 23:52:03.339: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Feb  8 23:52:03.339: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb  8 23:52:03.340: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-d4n4l,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d4n4l/replicasets/test-cleanup-controller,UID:857c0fca-2bfc-11e9-a0ab-42010a8a0033,ResourceVersion:20473,Generation:1,CreationTimestamp:2019-02-08 23:51:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 887a08c3-2bfc-11e9-a0ab-42010a8a0033 0xc0028bca87 0xc0028bca88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb  8 23:52:03.356: INFO: Pod "test-cleanup-controller-mdzx7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-mdzx7,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-d4n4l,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d4n4l/pods/test-cleanup-controller-mdzx7,UID:857e4a21-2bfc-11e9-a0ab-42010a8a0033,ResourceVersion:20465,Generation:0,CreationTimestamp:2019-02-08 23:51:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.140/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 857c0fca-2bfc-11e9-a0ab-42010a8a0033 0xc0028bd2c7 0xc0028bd2c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-stdc6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stdc6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stdc6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:netztbred3-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028bd330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028bd350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:51:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:52:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:52:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:51:58 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.52,PodIP:10.2.1.140,StartTime:2019-02-08 23:51:58 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-08 23:51:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://192374110ecac99dd78e1d9480dea2e22b0d8669e4e029b795f1a51b913c89ca}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:52:03.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-d4n4l" for this suite.
Feb  8 23:52:09.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:52:09.463: INFO: namespace: e2e-tests-deployment-d4n4l, resource: bindings, ignored listing per whitelist
Feb  8 23:52:09.497: INFO: namespace e2e-tests-deployment-d4n4l deletion completed in 6.112849347s

• [SLOW TEST:11.285 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:52:09.497: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-8c37d516-2bfc-11e9-a89e-229fb9a7b2a7
STEP: Creating a pod to test consume configMaps
Feb  8 23:52:09.587: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c387988-2bfc-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-configmap-pgzlv" to be "success or failure"
Feb  8 23:52:09.593: INFO: Pod "pod-configmaps-8c387988-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.499736ms
Feb  8 23:52:11.597: INFO: Pod "pod-configmaps-8c387988-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009579436s
STEP: Saw pod success
Feb  8 23:52:11.597: INFO: Pod "pod-configmaps-8c387988-2bfc-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:52:11.599: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-configmaps-8c387988-2bfc-11e9-a89e-229fb9a7b2a7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  8 23:52:11.634: INFO: Waiting for pod pod-configmaps-8c387988-2bfc-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:52:11.637: INFO: Pod pod-configmaps-8c387988-2bfc-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:52:11.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-pgzlv" for this suite.
Feb  8 23:52:17.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:52:17.665: INFO: namespace: e2e-tests-configmap-pgzlv, resource: bindings, ignored listing per whitelist
Feb  8 23:52:17.739: INFO: namespace e2e-tests-configmap-pgzlv deletion completed in 6.098617079s

• [SLOW TEST:8.242 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:52:17.740: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb  8 23:52:17.809: INFO: Waiting up to 5m0s for pod "client-containers-911f1c19-2bfc-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-containers-7lrzx" to be "success or failure"
Feb  8 23:52:17.818: INFO: Pod "client-containers-911f1c19-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.034587ms
Feb  8 23:52:19.822: INFO: Pod "client-containers-911f1c19-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012778038s
Feb  8 23:52:21.825: INFO: Pod "client-containers-911f1c19-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016069835s
STEP: Saw pod success
Feb  8 23:52:21.825: INFO: Pod "client-containers-911f1c19-2bfc-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:52:21.827: INFO: Trying to get logs from node netztbred3-worker-3 pod client-containers-911f1c19-2bfc-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:52:21.855: INFO: Waiting for pod client-containers-911f1c19-2bfc-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:52:21.858: INFO: Pod client-containers-911f1c19-2bfc-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:52:21.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-7lrzx" for this suite.
Feb  8 23:52:27.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:52:27.934: INFO: namespace: e2e-tests-containers-7lrzx, resource: bindings, ignored listing per whitelist
Feb  8 23:52:27.964: INFO: namespace e2e-tests-containers-7lrzx deletion completed in 6.101931947s

• [SLOW TEST:10.225 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:52:27.964: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb  8 23:52:28.035: INFO: Waiting up to 5m0s for pod "pod-9737e197-2bfc-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-kdv2g" to be "success or failure"
Feb  8 23:52:28.043: INFO: Pod "pod-9737e197-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.848613ms
Feb  8 23:52:30.048: INFO: Pod "pod-9737e197-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012627161s
Feb  8 23:52:32.052: INFO: Pod "pod-9737e197-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017205234s
STEP: Saw pod success
Feb  8 23:52:32.052: INFO: Pod "pod-9737e197-2bfc-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:52:32.057: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-9737e197-2bfc-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:52:32.090: INFO: Waiting for pod pod-9737e197-2bfc-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:52:32.102: INFO: Pod pod-9737e197-2bfc-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:52:32.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kdv2g" for this suite.
Feb  8 23:52:38.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:52:38.168: INFO: namespace: e2e-tests-emptydir-kdv2g, resource: bindings, ignored listing per whitelist
Feb  8 23:52:38.206: INFO: namespace e2e-tests-emptydir-kdv2g deletion completed in 6.099661792s

• [SLOW TEST:10.242 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:52:38.206: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb  8 23:52:38.280: INFO: Waiting up to 5m0s for pod "var-expansion-9d52d5de-2bfc-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-var-expansion-mmn5z" to be "success or failure"
Feb  8 23:52:38.288: INFO: Pod "var-expansion-9d52d5de-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.479851ms
Feb  8 23:52:40.292: INFO: Pod "var-expansion-9d52d5de-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011953578s
STEP: Saw pod success
Feb  8 23:52:40.292: INFO: Pod "var-expansion-9d52d5de-2bfc-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:52:40.294: INFO: Trying to get logs from node netztbred3-worker-3 pod var-expansion-9d52d5de-2bfc-11e9-a89e-229fb9a7b2a7 container dapi-container: <nil>
STEP: delete the pod
Feb  8 23:52:40.320: INFO: Waiting for pod var-expansion-9d52d5de-2bfc-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:52:40.325: INFO: Pod var-expansion-9d52d5de-2bfc-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:52:40.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-mmn5z" for this suite.
Feb  8 23:52:46.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:52:46.405: INFO: namespace: e2e-tests-var-expansion-mmn5z, resource: bindings, ignored listing per whitelist
Feb  8 23:52:46.424: INFO: namespace e2e-tests-var-expansion-mmn5z deletion completed in 6.094627997s

• [SLOW TEST:8.218 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:52:46.424: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  8 23:52:46.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-89fz4'
Feb  8 23:52:46.567: INFO: stderr: ""
Feb  8 23:52:46.567: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Feb  8 23:52:46.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-89fz4'
Feb  8 23:52:57.794: INFO: stderr: ""
Feb  8 23:52:57.794: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:52:57.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-89fz4" for this suite.
Feb  8 23:53:03.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:53:03.881: INFO: namespace: e2e-tests-kubectl-89fz4, resource: bindings, ignored listing per whitelist
Feb  8 23:53:03.903: INFO: namespace e2e-tests-kubectl-89fz4 deletion completed in 6.104600549s

• [SLOW TEST:17.479 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:53:03.903: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:53:03.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-t2n4n" for this suite.
Feb  8 23:53:09.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:53:10.080: INFO: namespace: e2e-tests-services-t2n4n, resource: bindings, ignored listing per whitelist
Feb  8 23:53:10.089: INFO: namespace e2e-tests-services-t2n4n deletion completed in 6.113268448s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.186 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:53:10.089: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0208 23:53:20.292756      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  8 23:53:20.292: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:53:20.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-788kd" for this suite.
Feb  8 23:53:26.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:53:26.409: INFO: namespace: e2e-tests-gc-788kd, resource: bindings, ignored listing per whitelist
Feb  8 23:53:26.411: INFO: namespace e2e-tests-gc-788kd deletion completed in 6.1136863s

• [SLOW TEST:16.322 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:53:26.411: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb  8 23:53:26.490: INFO: Waiting up to 5m0s for pod "client-containers-ba0eb285-2bfc-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-containers-zkq5x" to be "success or failure"
Feb  8 23:53:26.502: INFO: Pod "client-containers-ba0eb285-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.280023ms
Feb  8 23:53:28.506: INFO: Pod "client-containers-ba0eb285-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015828229s
STEP: Saw pod success
Feb  8 23:53:28.506: INFO: Pod "client-containers-ba0eb285-2bfc-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:53:28.508: INFO: Trying to get logs from node netztbred3-worker-3 pod client-containers-ba0eb285-2bfc-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:53:28.534: INFO: Waiting for pod client-containers-ba0eb285-2bfc-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:53:28.538: INFO: Pod client-containers-ba0eb285-2bfc-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:53:28.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-zkq5x" for this suite.
Feb  8 23:53:34.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:53:34.615: INFO: namespace: e2e-tests-containers-zkq5x, resource: bindings, ignored listing per whitelist
Feb  8 23:53:34.638: INFO: namespace e2e-tests-containers-zkq5x deletion completed in 6.09525705s

• [SLOW TEST:8.227 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:53:34.638: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb  8 23:53:34.704: INFO: Waiting up to 5m0s for pod "pod-bef47520-2bfc-11e9-a89e-229fb9a7b2a7" in namespace "e2e-tests-emptydir-nf7b4" to be "success or failure"
Feb  8 23:53:34.713: INFO: Pod "pod-bef47520-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.999319ms
Feb  8 23:53:36.716: INFO: Pod "pod-bef47520-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012784831s
Feb  8 23:53:38.720: INFO: Pod "pod-bef47520-2bfc-11e9-a89e-229fb9a7b2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016779222s
STEP: Saw pod success
Feb  8 23:53:38.720: INFO: Pod "pod-bef47520-2bfc-11e9-a89e-229fb9a7b2a7" satisfied condition "success or failure"
Feb  8 23:53:38.723: INFO: Trying to get logs from node netztbred3-worker-1 pod pod-bef47520-2bfc-11e9-a89e-229fb9a7b2a7 container test-container: <nil>
STEP: delete the pod
Feb  8 23:53:38.751: INFO: Waiting for pod pod-bef47520-2bfc-11e9-a89e-229fb9a7b2a7 to disappear
Feb  8 23:53:38.761: INFO: Pod pod-bef47520-2bfc-11e9-a89e-229fb9a7b2a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:53:38.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nf7b4" for this suite.
Feb  8 23:53:44.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:53:44.822: INFO: namespace: e2e-tests-emptydir-nf7b4, resource: bindings, ignored listing per whitelist
Feb  8 23:53:44.879: INFO: namespace e2e-tests-emptydir-nf7b4 deletion completed in 6.11336344s

• [SLOW TEST:10.241 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:53:44.879: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb  8 23:53:45.206: INFO: Pod name wrapped-volume-race-c53552ff-2bfc-11e9-a89e-229fb9a7b2a7: Found 0 pods out of 5
Feb  8 23:53:50.212: INFO: Pod name wrapped-volume-race-c53552ff-2bfc-11e9-a89e-229fb9a7b2a7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c53552ff-2bfc-11e9-a89e-229fb9a7b2a7 in namespace e2e-tests-emptydir-wrapper-f76cj, will wait for the garbage collector to delete the pods
Feb  8 23:54:02.309: INFO: Deleting ReplicationController wrapped-volume-race-c53552ff-2bfc-11e9-a89e-229fb9a7b2a7 took: 10.606789ms
Feb  8 23:54:02.409: INFO: Terminating ReplicationController wrapped-volume-race-c53552ff-2bfc-11e9-a89e-229fb9a7b2a7 pods took: 100.223236ms
STEP: Creating RC which spawns configmap-volume pods
Feb  8 23:54:47.936: INFO: Pod name wrapped-volume-race-ea981fcd-2bfc-11e9-a89e-229fb9a7b2a7: Found 0 pods out of 5
Feb  8 23:54:52.947: INFO: Pod name wrapped-volume-race-ea981fcd-2bfc-11e9-a89e-229fb9a7b2a7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ea981fcd-2bfc-11e9-a89e-229fb9a7b2a7 in namespace e2e-tests-emptydir-wrapper-f76cj, will wait for the garbage collector to delete the pods
Feb  8 23:55:05.066: INFO: Deleting ReplicationController wrapped-volume-race-ea981fcd-2bfc-11e9-a89e-229fb9a7b2a7 took: 42.689711ms
Feb  8 23:55:05.267: INFO: Terminating ReplicationController wrapped-volume-race-ea981fcd-2bfc-11e9-a89e-229fb9a7b2a7 pods took: 200.360227ms
STEP: Creating RC which spawns configmap-volume pods
Feb  8 23:55:47.889: INFO: Pod name wrapped-volume-race-0e54d9ad-2bfd-11e9-a89e-229fb9a7b2a7: Found 0 pods out of 5
Feb  8 23:55:52.895: INFO: Pod name wrapped-volume-race-0e54d9ad-2bfd-11e9-a89e-229fb9a7b2a7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0e54d9ad-2bfd-11e9-a89e-229fb9a7b2a7 in namespace e2e-tests-emptydir-wrapper-f76cj, will wait for the garbage collector to delete the pods
Feb  8 23:56:04.989: INFO: Deleting ReplicationController wrapped-volume-race-0e54d9ad-2bfd-11e9-a89e-229fb9a7b2a7 took: 18.474646ms
Feb  8 23:56:05.089: INFO: Terminating ReplicationController wrapped-volume-race-0e54d9ad-2bfd-11e9-a89e-229fb9a7b2a7 pods took: 100.274713ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:56:48.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-f76cj" for this suite.
Feb  8 23:56:56.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:56:56.424: INFO: namespace: e2e-tests-emptydir-wrapper-f76cj, resource: bindings, ignored listing per whitelist
Feb  8 23:56:56.458: INFO: namespace e2e-tests-emptydir-wrapper-f76cj deletion completed in 8.115180175s

• [SLOW TEST:191.579 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  8 23:56:56.458: INFO: >>> kubeConfig: /tmp/kubeconfig-762798943
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-bqfpk
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-bqfpk
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-bqfpk
Feb  8 23:56:56.575: INFO: Found 0 stateful pods, waiting for 1
Feb  8 23:57:06.586: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb  8 23:57:06.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-bqfpk ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  8 23:57:06.818: INFO: stderr: ""
Feb  8 23:57:06.818: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  8 23:57:06.818: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  8 23:57:06.822: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb  8 23:57:16.826: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  8 23:57:16.826: INFO: Waiting for statefulset status.replicas updated to 0
Feb  8 23:57:16.847: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Feb  8 23:57:16.847: INFO: ss-0  netztbred3-worker-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:56:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:56:56 +0000 UTC  }]
Feb  8 23:57:16.847: INFO: 
Feb  8 23:57:16.847: INFO: StatefulSet ss has not reached scale 3, at 1
Feb  8 23:57:17.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993959038s
Feb  8 23:57:18.858: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988090659s
Feb  8 23:57:19.863: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982954209s
Feb  8 23:57:20.869: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977984149s
Feb  8 23:57:21.874: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971970069s
Feb  8 23:57:22.879: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967260152s
Feb  8 23:57:23.883: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962395128s
Feb  8 23:57:24.888: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957699816s
Feb  8 23:57:25.893: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.782216ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-bqfpk
Feb  8 23:57:26.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-bqfpk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  8 23:57:27.115: INFO: stderr: ""
Feb  8 23:57:27.115: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  8 23:57:27.115: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  8 23:57:27.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-bqfpk ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  8 23:57:27.323: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb  8 23:57:27.323: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  8 23:57:27.323: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  8 23:57:27.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-bqfpk ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  8 23:57:27.544: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb  8 23:57:27.544: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  8 23:57:27.544: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  8 23:57:27.548: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb  8 23:57:37.553: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  8 23:57:37.553: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  8 23:57:37.553: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb  8 23:57:37.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-bqfpk ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  8 23:57:37.752: INFO: stderr: ""
Feb  8 23:57:37.752: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  8 23:57:37.752: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  8 23:57:37.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-bqfpk ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  8 23:57:37.980: INFO: stderr: ""
Feb  8 23:57:37.980: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  8 23:57:37.980: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  8 23:57:37.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-762798943 exec --namespace=e2e-tests-statefulset-bqfpk ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  8 23:57:38.208: INFO: stderr: ""
Feb  8 23:57:38.208: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  8 23:57:38.208: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  8 23:57:38.208: INFO: Waiting for statefulset status.replicas updated to 0
Feb  8 23:57:38.211: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb  8 23:57:48.219: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  8 23:57:48.219: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb  8 23:57:48.219: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb  8 23:57:48.236: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Feb  8 23:57:48.236: INFO: ss-0  netztbred3-worker-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:56:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:56:56 +0000 UTC  }]
Feb  8 23:57:48.236: INFO: ss-1  netztbred3-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:48.236: INFO: ss-2  netztbred3-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:48.236: INFO: 
Feb  8 23:57:48.236: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  8 23:57:49.241: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Feb  8 23:57:49.241: INFO: ss-0  netztbred3-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:56:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:56:56 +0000 UTC  }]
Feb  8 23:57:49.241: INFO: ss-1  netztbred3-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:49.241: INFO: ss-2  netztbred3-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:49.241: INFO: 
Feb  8 23:57:49.241: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  8 23:57:50.246: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Feb  8 23:57:50.246: INFO: ss-0  netztbred3-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:56:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:56:56 +0000 UTC  }]
Feb  8 23:57:50.246: INFO: ss-1  netztbred3-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:50.246: INFO: ss-2  netztbred3-worker-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:50.246: INFO: 
Feb  8 23:57:50.246: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  8 23:57:51.250: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Feb  8 23:57:51.250: INFO: ss-1  netztbred3-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:51.251: INFO: 
Feb  8 23:57:51.251: INFO: StatefulSet ss has not reached scale 0, at 1
Feb  8 23:57:52.255: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Feb  8 23:57:52.255: INFO: ss-1  netztbred3-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:52.255: INFO: 
Feb  8 23:57:52.255: INFO: StatefulSet ss has not reached scale 0, at 1
Feb  8 23:57:53.260: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Feb  8 23:57:53.260: INFO: ss-1  netztbred3-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:53.260: INFO: 
Feb  8 23:57:53.260: INFO: StatefulSet ss has not reached scale 0, at 1
Feb  8 23:57:54.264: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Feb  8 23:57:54.264: INFO: ss-1  netztbred3-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:54.264: INFO: 
Feb  8 23:57:54.264: INFO: StatefulSet ss has not reached scale 0, at 1
Feb  8 23:57:55.269: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Feb  8 23:57:55.269: INFO: ss-1  netztbred3-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:55.269: INFO: 
Feb  8 23:57:55.269: INFO: StatefulSet ss has not reached scale 0, at 1
Feb  8 23:57:56.273: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Feb  8 23:57:56.273: INFO: ss-1  netztbred3-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:56.273: INFO: 
Feb  8 23:57:56.273: INFO: StatefulSet ss has not reached scale 0, at 1
Feb  8 23:57:57.278: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Feb  8 23:57:57.278: INFO: ss-1  netztbred3-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-08 23:57:16 +0000 UTC  }]
Feb  8 23:57:57.278: INFO: 
Feb  8 23:57:57.278: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-bqfpk
Feb  8 23:57:58.282: INFO: Scaling statefulset ss to 0
Feb  8 23:57:58.291: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  8 23:57:58.294: INFO: Deleting all statefulset in ns e2e-tests-statefulset-bqfpk
Feb  8 23:57:58.296: INFO: Scaling statefulset ss to 0
Feb  8 23:57:58.305: INFO: Waiting for statefulset status.replicas updated to 0
Feb  8 23:57:58.308: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  8 23:57:58.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-bqfpk" for this suite.
Feb  8 23:58:04.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  8 23:58:04.412: INFO: namespace: e2e-tests-statefulset-bqfpk, resource: bindings, ignored listing per whitelist
Feb  8 23:58:04.456: INFO: namespace e2e-tests-statefulset-bqfpk deletion completed in 6.12867585s

• [SLOW TEST:67.998 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSFeb  8 23:58:04.457: INFO: Running AfterSuite actions on all nodes
Feb  8 23:58:04.457: INFO: Running AfterSuite actions on node 1
Feb  8 23:58:04.457: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5868.447 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h37m49.86763527s
Test Suite Passed
