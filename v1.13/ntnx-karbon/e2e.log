I0313 22:08:23.949653      19 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-703856963
I0313 22:08:23.949842      19 e2e.go:224] Starting e2e run "84c7db97-45dc-11e9-8b9c-0a58ac140107" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1552514903 - Will randomize all specs
Will run 201 of 1946 specs

Mar 13 22:08:24.100: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:08:24.103: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 13 22:08:24.113: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 13 22:08:24.139: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 13 22:08:24.139: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Mar 13 22:08:24.139: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 13 22:08:24.153: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-node-ntnx-plugin' (0 seconds elapsed)
Mar 13 22:08:24.153: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'fluent-bit' (0 seconds elapsed)
Mar 13 22:08:24.153: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Mar 13 22:08:24.153: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-ds' (0 seconds elapsed)
Mar 13 22:08:24.153: INFO: e2e test version: v1.13.0
Mar 13 22:08:24.154: INFO: kube-apiserver version: v1.13.4
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:08:24.154: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pod-network-test
Mar 13 22:08:24.199: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-njqxp
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 13 22:08:24.200: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 13 22:08:52.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.3.8:8080/dial?request=hostName&protocol=http&host=172.20.1.8&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-njqxp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:08:52.270: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:08:52.353: INFO: Waiting for endpoints: map[]
Mar 13 22:08:52.356: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.3.8:8080/dial?request=hostName&protocol=http&host=172.20.3.7&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-njqxp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:08:52.356: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:08:52.434: INFO: Waiting for endpoints: map[]
Mar 13 22:08:52.437: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.3.8:8080/dial?request=hostName&protocol=http&host=172.20.2.8&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-njqxp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:08:52.437: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:08:52.516: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:08:52.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-njqxp" for this suite.
Mar 13 22:09:14.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:09:14.552: INFO: namespace: e2e-tests-pod-network-test-njqxp, resource: bindings, ignored listing per whitelist
Mar 13 22:09:14.597: INFO: namespace e2e-tests-pod-network-test-njqxp deletion completed in 22.077219976s

• [SLOW TEST:50.443 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:09:14.597: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a34d9a1f-45dc-11e9-8b9c-0a58ac140107
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a34d9a1f-45dc-11e9-8b9c-0a58ac140107
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:10:34.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bkfs4" for this suite.
Mar 13 22:10:49.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:10:49.042: INFO: namespace: e2e-tests-projected-bkfs4, resource: bindings, ignored listing per whitelist
Mar 13 22:10:49.075: INFO: namespace e2e-tests-projected-bkfs4 deletion completed in 14.079089321s

• [SLOW TEST:94.478 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:10:49.075: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-db9e04c0-45dc-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 22:10:49.125: INFO: Waiting up to 5m0s for pod "pod-configmaps-db9e4f91-45dc-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-configmap-78lvt" to be "success or failure"
Mar 13 22:10:49.127: INFO: Pod "pod-configmaps-db9e4f91-45dc-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.873612ms
Mar 13 22:10:51.130: INFO: Pod "pod-configmaps-db9e4f91-45dc-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005265722s
Mar 13 22:10:53.134: INFO: Pod "pod-configmaps-db9e4f91-45dc-11e9-8b9c-0a58ac140107": Phase="Running", Reason="", readiness=true. Elapsed: 4.008901189s
Mar 13 22:10:55.137: INFO: Pod "pod-configmaps-db9e4f91-45dc-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012185214s
STEP: Saw pod success
Mar 13 22:10:55.137: INFO: Pod "pod-configmaps-db9e4f91-45dc-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:10:55.139: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-configmaps-db9e4f91-45dc-11e9-8b9c-0a58ac140107 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 22:10:55.156: INFO: Waiting for pod pod-configmaps-db9e4f91-45dc-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:10:55.158: INFO: Pod pod-configmaps-db9e4f91-45dc-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:10:55.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-78lvt" for this suite.
Mar 13 22:11:01.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:11:01.192: INFO: namespace: e2e-tests-configmap-78lvt, resource: bindings, ignored listing per whitelist
Mar 13 22:11:01.241: INFO: namespace e2e-tests-configmap-78lvt deletion completed in 6.079215624s

• [SLOW TEST:12.165 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:11:01.241: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 13 22:11:01.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-qsb9x'
Mar 13 22:11:01.565: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 13 22:11:01.565: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Mar 13 22:11:03.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-qsb9x'
Mar 13 22:11:03.661: INFO: stderr: ""
Mar 13 22:11:03.661: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:11:03.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qsb9x" for this suite.
Mar 13 22:11:09.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:11:09.723: INFO: namespace: e2e-tests-kubectl-qsb9x, resource: bindings, ignored listing per whitelist
Mar 13 22:11:09.751: INFO: namespace e2e-tests-kubectl-qsb9x deletion completed in 6.085343273s

• [SLOW TEST:8.511 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:11:09.752: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 13 22:11:09.811: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 13 22:11:14.816: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:11:15.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-9vscd" for this suite.
Mar 13 22:11:21.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:11:21.883: INFO: namespace: e2e-tests-replication-controller-9vscd, resource: bindings, ignored listing per whitelist
Mar 13 22:11:21.906: INFO: namespace e2e-tests-replication-controller-9vscd deletion completed in 6.077540396s

• [SLOW TEST:12.155 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:11:21.907: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 22:11:21.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef2f8a26-45dc-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-cqkbs" to be "success or failure"
Mar 13 22:11:21.955: INFO: Pod "downwardapi-volume-ef2f8a26-45dc-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.717196ms
Mar 13 22:11:23.959: INFO: Pod "downwardapi-volume-ef2f8a26-45dc-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00497653s
Mar 13 22:11:25.962: INFO: Pod "downwardapi-volume-ef2f8a26-45dc-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008733105s
STEP: Saw pod success
Mar 13 22:11:25.962: INFO: Pod "downwardapi-volume-ef2f8a26-45dc-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:11:25.964: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod downwardapi-volume-ef2f8a26-45dc-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 22:11:25.976: INFO: Waiting for pod downwardapi-volume-ef2f8a26-45dc-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:11:25.978: INFO: Pod downwardapi-volume-ef2f8a26-45dc-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:11:25.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cqkbs" for this suite.
Mar 13 22:11:31.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:11:32.033: INFO: namespace: e2e-tests-downward-api-cqkbs, resource: bindings, ignored listing per whitelist
Mar 13 22:11:32.049: INFO: namespace e2e-tests-downward-api-cqkbs deletion completed in 6.068626837s

• [SLOW TEST:10.143 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:11:32.049: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-f543993a-45dc-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 22:11:32.153: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f543e3f8-45dc-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-d6w97" to be "success or failure"
Mar 13 22:11:32.160: INFO: Pod "pod-projected-secrets-f543e3f8-45dc-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 6.882194ms
Mar 13 22:11:34.164: INFO: Pod "pod-projected-secrets-f543e3f8-45dc-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010658273s
Mar 13 22:11:36.167: INFO: Pod "pod-projected-secrets-f543e3f8-45dc-11e9-8b9c-0a58ac140107": Phase="Running", Reason="", readiness=true. Elapsed: 4.014242445s
Mar 13 22:11:38.171: INFO: Pod "pod-projected-secrets-f543e3f8-45dc-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018187999s
STEP: Saw pod success
Mar 13 22:11:38.171: INFO: Pod "pod-projected-secrets-f543e3f8-45dc-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:11:38.174: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-projected-secrets-f543e3f8-45dc-11e9-8b9c-0a58ac140107 container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 22:11:38.194: INFO: Waiting for pod pod-projected-secrets-f543e3f8-45dc-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:11:38.196: INFO: Pod pod-projected-secrets-f543e3f8-45dc-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:11:38.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-d6w97" for this suite.
Mar 13 22:11:44.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:11:44.260: INFO: namespace: e2e-tests-projected-d6w97, resource: bindings, ignored listing per whitelist
Mar 13 22:11:44.267: INFO: namespace e2e-tests-projected-d6w97 deletion completed in 6.068635683s

• [SLOW TEST:12.218 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:11:44.268: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Mar 13 22:11:44.311: INFO: Waiting up to 5m0s for pod "var-expansion-fc830a8b-45dc-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-var-expansion-92thk" to be "success or failure"
Mar 13 22:11:44.313: INFO: Pod "var-expansion-fc830a8b-45dc-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020535ms
Mar 13 22:11:46.316: INFO: Pod "var-expansion-fc830a8b-45dc-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005399239s
Mar 13 22:11:48.320: INFO: Pod "var-expansion-fc830a8b-45dc-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00879786s
Mar 13 22:11:50.323: INFO: Pod "var-expansion-fc830a8b-45dc-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011779777s
STEP: Saw pod success
Mar 13 22:11:50.323: INFO: Pod "var-expansion-fc830a8b-45dc-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:11:50.324: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod var-expansion-fc830a8b-45dc-11e9-8b9c-0a58ac140107 container dapi-container: <nil>
STEP: delete the pod
Mar 13 22:11:50.335: INFO: Waiting for pod var-expansion-fc830a8b-45dc-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:11:50.336: INFO: Pod var-expansion-fc830a8b-45dc-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:11:50.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-92thk" for this suite.
Mar 13 22:11:56.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:11:56.406: INFO: namespace: e2e-tests-var-expansion-92thk, resource: bindings, ignored listing per whitelist
Mar 13 22:11:56.414: INFO: namespace e2e-tests-var-expansion-92thk deletion completed in 6.075561014s

• [SLOW TEST:12.146 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:11:56.414: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-v7gps
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-v7gps
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-v7gps
Mar 13 22:11:56.521: INFO: Found 0 stateful pods, waiting for 1
Mar 13 22:12:06.525: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 13 22:12:06.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-v7gps ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 13 22:12:06.673: INFO: stderr: ""
Mar 13 22:12:06.674: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 13 22:12:06.674: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 13 22:12:06.677: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 13 22:12:16.680: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 22:12:16.680: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 22:12:16.689: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999438s
Mar 13 22:12:17.692: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997979079s
Mar 13 22:12:18.696: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994384632s
Mar 13 22:12:19.699: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990650066s
Mar 13 22:12:20.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98739549s
Mar 13 22:12:21.706: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.983626559s
Mar 13 22:12:22.710: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.980566657s
Mar 13 22:12:23.714: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.976379603s
Mar 13 22:12:24.717: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.973225462s
Mar 13 22:12:25.721: INFO: Verifying statefulset ss doesn't scale past 1 for another 969.536812ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-v7gps
Mar 13 22:12:26.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-v7gps ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 13 22:12:26.875: INFO: stderr: ""
Mar 13 22:12:26.875: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 13 22:12:26.875: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 13 22:12:26.877: INFO: Found 1 stateful pods, waiting for 3
Mar 13 22:12:36.880: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 22:12:36.880: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 22:12:36.880: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 13 22:12:46.881: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 22:12:46.881: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 22:12:46.881: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 13 22:12:46.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-v7gps ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 13 22:12:47.045: INFO: stderr: ""
Mar 13 22:12:47.045: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 13 22:12:47.045: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 13 22:12:47.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-v7gps ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 13 22:12:47.201: INFO: stderr: ""
Mar 13 22:12:47.201: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 13 22:12:47.201: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 13 22:12:47.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-v7gps ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 13 22:12:47.372: INFO: stderr: ""
Mar 13 22:12:47.372: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 13 22:12:47.372: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 13 22:12:47.372: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 22:12:47.375: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 13 22:12:57.381: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 22:12:57.381: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 22:12:57.381: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 22:12:57.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999602s
Mar 13 22:12:58.394: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996656288s
Mar 13 22:12:59.398: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992678559s
Mar 13 22:13:00.402: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988602307s
Mar 13 22:13:01.406: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984731851s
Mar 13 22:13:02.409: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980761601s
Mar 13 22:13:03.413: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977121223s
Mar 13 22:13:04.417: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973134897s
Mar 13 22:13:05.421: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969087516s
Mar 13 22:13:06.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.159169ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-v7gps
Mar 13 22:13:07.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-v7gps ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 13 22:13:07.593: INFO: stderr: ""
Mar 13 22:13:07.593: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 13 22:13:07.593: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 13 22:13:07.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-v7gps ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 13 22:13:07.746: INFO: stderr: ""
Mar 13 22:13:07.746: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 13 22:13:07.746: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 13 22:13:07.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-v7gps ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 13 22:13:07.913: INFO: stderr: ""
Mar 13 22:13:07.913: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 13 22:13:07.913: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 13 22:13:07.913: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 13 22:13:27.925: INFO: Deleting all statefulset in ns e2e-tests-statefulset-v7gps
Mar 13 22:13:27.927: INFO: Scaling statefulset ss to 0
Mar 13 22:13:27.933: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 22:13:27.935: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:13:27.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-v7gps" for this suite.
Mar 13 22:13:33.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:13:34.000: INFO: namespace: e2e-tests-statefulset-v7gps, resource: bindings, ignored listing per whitelist
Mar 13 22:13:34.024: INFO: namespace e2e-tests-statefulset-v7gps deletion completed in 6.077749276s

• [SLOW TEST:97.610 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:13:34.024: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-3deeb975-45dd-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 22:13:34.071: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3def0aad-45dd-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-bzbzm" to be "success or failure"
Mar 13 22:13:34.072: INFO: Pod "pod-projected-secrets-3def0aad-45dd-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.599649ms
Mar 13 22:13:36.075: INFO: Pod "pod-projected-secrets-3def0aad-45dd-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004759167s
Mar 13 22:13:38.079: INFO: Pod "pod-projected-secrets-3def0aad-45dd-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008655273s
STEP: Saw pod success
Mar 13 22:13:38.079: INFO: Pod "pod-projected-secrets-3def0aad-45dd-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:13:38.082: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-projected-secrets-3def0aad-45dd-11e9-8b9c-0a58ac140107 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 13 22:13:38.096: INFO: Waiting for pod pod-projected-secrets-3def0aad-45dd-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:13:38.098: INFO: Pod pod-projected-secrets-3def0aad-45dd-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:13:38.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bzbzm" for this suite.
Mar 13 22:13:44.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:13:44.134: INFO: namespace: e2e-tests-projected-bzbzm, resource: bindings, ignored listing per whitelist
Mar 13 22:13:44.181: INFO: namespace e2e-tests-projected-bzbzm deletion completed in 6.080360657s

• [SLOW TEST:10.157 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:13:44.181: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Mar 13 22:13:48.241: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-43fd0ea2-45dd-11e9-8b9c-0a58ac140107", GenerateName:"", Namespace:"e2e-tests-pods-nwrvt", SelfLink:"/api/v1/namespaces/e2e-tests-pods-nwrvt/pods/pod-submit-remove-43fd0ea2-45dd-11e9-8b9c-0a58ac140107", UID:"43cb553a-45dd-11e9-b6d3-506b8df34f96", ResourceVersion:"26790", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63688112023, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"225763254"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-vvgmq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000c48a80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vvgmq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00150a2f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"karbon-test-3a0ff6-k8s-worker-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0019d36e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00150a31e)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688112024, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688112026, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688112026, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688112023, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.40.156.31", PodIP:"172.20.2.12", StartTime:(*v1.Time)(0xc00137ba60), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc00137ba80), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"docker.io/nginx:1.14-alpine", ImageID:"docker-pullable://docker.io/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d", ContainerID:"docker://522ce8b7a8ae8234554dc494984bcb1572f33f48e465e032b1e788819acfc52d"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:13:57.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-nwrvt" for this suite.
Mar 13 22:14:03.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:14:03.669: INFO: namespace: e2e-tests-pods-nwrvt, resource: bindings, ignored listing per whitelist
Mar 13 22:14:03.683: INFO: namespace e2e-tests-pods-nwrvt deletion completed in 6.074062799s

• [SLOW TEST:19.501 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:14:03.683: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4f9cbd9f-45dd-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 22:14:03.733: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4f9d2f0e-45dd-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-p8bjg" to be "success or failure"
Mar 13 22:14:03.735: INFO: Pod "pod-projected-configmaps-4f9d2f0e-45dd-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.396253ms
Mar 13 22:14:05.738: INFO: Pod "pod-projected-configmaps-4f9d2f0e-45dd-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00464369s
Mar 13 22:14:07.741: INFO: Pod "pod-projected-configmaps-4f9d2f0e-45dd-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00813398s
STEP: Saw pod success
Mar 13 22:14:07.741: INFO: Pod "pod-projected-configmaps-4f9d2f0e-45dd-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:14:07.743: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-projected-configmaps-4f9d2f0e-45dd-11e9-8b9c-0a58ac140107 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 22:14:07.755: INFO: Waiting for pod pod-projected-configmaps-4f9d2f0e-45dd-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:14:07.757: INFO: Pod pod-projected-configmaps-4f9d2f0e-45dd-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:14:07.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p8bjg" for this suite.
Mar 13 22:14:13.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:14:13.779: INFO: namespace: e2e-tests-projected-p8bjg, resource: bindings, ignored listing per whitelist
Mar 13 22:14:13.842: INFO: namespace e2e-tests-projected-p8bjg deletion completed in 6.082215814s

• [SLOW TEST:10.159 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:14:13.842: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 22:14:13.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 version --client'
Mar 13 22:14:13.962: INFO: stderr: ""
Mar 13 22:14:13.962: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Mar 13 22:14:13.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-vd575'
Mar 13 22:14:14.175: INFO: stderr: ""
Mar 13 22:14:14.175: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar 13 22:14:14.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-vd575'
Mar 13 22:14:14.356: INFO: stderr: ""
Mar 13 22:14:14.357: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 13 22:14:15.360: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 22:14:15.360: INFO: Found 0 / 1
Mar 13 22:14:16.360: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 22:14:16.360: INFO: Found 0 / 1
Mar 13 22:14:17.360: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 22:14:17.360: INFO: Found 0 / 1
Mar 13 22:14:18.360: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 22:14:18.361: INFO: Found 0 / 1
Mar 13 22:14:19.361: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 22:14:19.361: INFO: Found 1 / 1
Mar 13 22:14:19.361: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 13 22:14:19.364: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 22:14:19.364: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 13 22:14:19.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 describe pod redis-master-289gs --namespace=e2e-tests-kubectl-vd575'
Mar 13 22:14:19.457: INFO: stderr: ""
Mar 13 22:14:19.457: INFO: stdout: "Name:           redis-master-289gs\nNamespace:      e2e-tests-kubectl-vd575\nNode:           karbon-test-3a0ff6-k8s-worker-0/10.40.155.216\nStart Time:     Wed, 13 Mar 2019 22:14:14 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             172.20.1.14\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://4abfb2cfe2df2063965d5189597d9d3fb041a8082bb9fe509a61ef5d0c803d6f\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 13 Mar 2019 22:14:18 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qwdpz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-qwdpz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-qwdpz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     <none>\nEvents:\n  Type    Reason     Age   From                                      Message\n  ----    ------     ----  ----                                      -------\n  Normal  Scheduled  6s    default-scheduler                         Successfully assigned e2e-tests-kubectl-vd575/redis-master-289gs to karbon-test-3a0ff6-k8s-worker-0\n  Normal  Pulling    3s    kubelet, karbon-test-3a0ff6-k8s-worker-0  pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     2s    kubelet, karbon-test-3a0ff6-k8s-worker-0  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    2s    kubelet, karbon-test-3a0ff6-k8s-worker-0  Created container\n  Normal  Started    1s    kubelet, karbon-test-3a0ff6-k8s-worker-0  Started container\n"
Mar 13 22:14:19.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 describe rc redis-master --namespace=e2e-tests-kubectl-vd575'
Mar 13 22:14:19.551: INFO: stderr: ""
Mar 13 22:14:19.551: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-vd575\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  6s    replication-controller  Created pod: redis-master-289gs\n"
Mar 13 22:14:19.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 describe service redis-master --namespace=e2e-tests-kubectl-vd575'
Mar 13 22:14:19.650: INFO: stderr: ""
Mar 13 22:14:19.650: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-vd575\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.19.58.4\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.20.1.14:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 13 22:14:19.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 describe node karbon-test-3a0ff6-k8s-master-0'
Mar 13 22:14:19.754: INFO: stderr: ""
Mar 13 22:14:19.754: INFO: stdout: "Name:               karbon-test-3a0ff6-k8s-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=karbon-test-3a0ff6-k8s-master-0\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"ae:8a:9c:c3:0c:a3\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.40.156.144\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 13 Mar 2019 17:35:29 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 13 Mar 2019 22:14:13 +0000   Wed, 13 Mar 2019 17:35:20 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 13 Mar 2019 22:14:13 +0000   Wed, 13 Mar 2019 17:35:20 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 13 Mar 2019 22:14:13 +0000   Wed, 13 Mar 2019 17:35:20 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 13 Mar 2019 22:14:13 +0000   Wed, 13 Mar 2019 17:35:20 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.40.156.144\n  Hostname:    karbon-test-3a0ff6-k8s-master-0\nCapacity:\n cpu:                2\n ephemeral-storage:  40883180Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3844208Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  37677938626\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3741808Ki\n pods:               110\nSystem Info:\n Machine ID:                 66a5ab8865ab4cc7af66da2cc9f730d0\n System UUID:                F1282F5E-8E40-4E16-9548-2032F3F9475D\n Boot ID:                    b421316c-3c51-43b7-bbf0-54d8bbcdd2ed\n Kernel Version:             3.10.0-957.5.1.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.13.1\n Kubelet Version:            v1.13.4\n Kube-Proxy Version:         v1.13.4\nPodCIDR:                     172.20.0.0/24\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-cj2d5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m23s\n  kube-system                fluent-bit-g62b8                                           100m (5%)     100m (5%)   200Mi (5%)       200Mi (5%)     4h35m\n  kube-system                kube-apiserver-karbon-test-3a0ff6-k8s-master-0             300m (15%)    0 (0%)      0 (0%)           0 (0%)         4h37m\n  kube-system                kube-dns-5db657c5d6-whkpt                                  260m (13%)    200m (10%)  110Mi (3%)       170Mi (4%)     4h36m\n  kube-system                kube-flannel-ds-bfkfp                                      100m (5%)     500m (25%)  50Mi (1%)        50Mi (1%)      4h36m\n  kube-system                kube-proxy-ds-lgd94                                        100m (5%)     100m (5%)   70Mi (1%)        70Mi (1%)      4h36m\n  monitoring                 node-exporter-sdl4z                                        112m (5%)     600m (30%)  200Mi (5%)       220Mi (6%)     4h33m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                972m (48%)   1500m (75%)\n  memory             630Mi (17%)  710Mi (19%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:              <none>\n"
Mar 13 22:14:19.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 describe namespace e2e-tests-kubectl-vd575'
Mar 13 22:14:19.835: INFO: stderr: ""
Mar 13 22:14:19.835: INFO: stdout: "Name:         e2e-tests-kubectl-vd575\nLabels:       e2e-framework=kubectl\n              e2e-run=84c7db97-45dc-11e9-8b9c-0a58ac140107\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:14:19.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vd575" for this suite.
Mar 13 22:14:41.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:14:41.900: INFO: namespace: e2e-tests-kubectl-vd575, resource: bindings, ignored listing per whitelist
Mar 13 22:14:41.906: INFO: namespace e2e-tests-kubectl-vd575 deletion completed in 22.067690714s

• [SLOW TEST:28.064 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:14:41.906: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-n2xs
STEP: Creating a pod to test atomic-volume-subpath
Mar 13 22:14:41.952: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-n2xs" in namespace "e2e-tests-subpath-8bv4j" to be "success or failure"
Mar 13 22:14:41.954: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.372382ms
Mar 13 22:14:43.958: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005995279s
Mar 13 22:14:45.960: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008158928s
Mar 13 22:14:47.964: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Running", Reason="", readiness=false. Elapsed: 6.011744662s
Mar 13 22:14:49.967: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Running", Reason="", readiness=false. Elapsed: 8.015160956s
Mar 13 22:14:51.971: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Running", Reason="", readiness=false. Elapsed: 10.018837051s
Mar 13 22:14:53.974: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Running", Reason="", readiness=false. Elapsed: 12.021949266s
Mar 13 22:14:55.977: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Running", Reason="", readiness=false. Elapsed: 14.025494041s
Mar 13 22:14:57.981: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Running", Reason="", readiness=false. Elapsed: 16.029017578s
Mar 13 22:14:59.985: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Running", Reason="", readiness=false. Elapsed: 18.03284122s
Mar 13 22:15:01.988: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Running", Reason="", readiness=false. Elapsed: 20.036440293s
Mar 13 22:15:03.992: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Running", Reason="", readiness=false. Elapsed: 22.039940048s
Mar 13 22:15:05.996: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Running", Reason="", readiness=false. Elapsed: 24.043650799s
Mar 13 22:15:07.999: INFO: Pod "pod-subpath-test-downwardapi-n2xs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.04727282s
STEP: Saw pod success
Mar 13 22:15:07.999: INFO: Pod "pod-subpath-test-downwardapi-n2xs" satisfied condition "success or failure"
Mar 13 22:15:08.001: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-subpath-test-downwardapi-n2xs container test-container-subpath-downwardapi-n2xs: <nil>
STEP: delete the pod
Mar 13 22:15:08.016: INFO: Waiting for pod pod-subpath-test-downwardapi-n2xs to disappear
Mar 13 22:15:08.018: INFO: Pod pod-subpath-test-downwardapi-n2xs no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-n2xs
Mar 13 22:15:08.018: INFO: Deleting pod "pod-subpath-test-downwardapi-n2xs" in namespace "e2e-tests-subpath-8bv4j"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:15:08.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-8bv4j" for this suite.
Mar 13 22:15:14.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:15:14.083: INFO: namespace: e2e-tests-subpath-8bv4j, resource: bindings, ignored listing per whitelist
Mar 13 22:15:14.088: INFO: namespace e2e-tests-subpath-8bv4j deletion completed in 6.066965442s

• [SLOW TEST:32.182 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:15:14.088: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-799394f3-45dd-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 22:15:14.137: INFO: Waiting up to 5m0s for pod "pod-configmaps-7993ebee-45dd-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-configmap-rz5rb" to be "success or failure"
Mar 13 22:15:14.139: INFO: Pod "pod-configmaps-7993ebee-45dd-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.139584ms
Mar 13 22:15:16.143: INFO: Pod "pod-configmaps-7993ebee-45dd-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00611604s
Mar 13 22:15:18.147: INFO: Pod "pod-configmaps-7993ebee-45dd-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009495101s
STEP: Saw pod success
Mar 13 22:15:18.147: INFO: Pod "pod-configmaps-7993ebee-45dd-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:15:18.148: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-configmaps-7993ebee-45dd-11e9-8b9c-0a58ac140107 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 22:15:18.161: INFO: Waiting for pod pod-configmaps-7993ebee-45dd-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:15:18.163: INFO: Pod pod-configmaps-7993ebee-45dd-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:15:18.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rz5rb" for this suite.
Mar 13 22:15:24.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:15:24.191: INFO: namespace: e2e-tests-configmap-rz5rb, resource: bindings, ignored listing per whitelist
Mar 13 22:15:24.237: INFO: namespace e2e-tests-configmap-rz5rb deletion completed in 6.070634895s

• [SLOW TEST:10.148 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:15:24.237: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-x5gg2
Mar 13 22:15:30.293: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-x5gg2
STEP: checking the pod's current state and verifying that restartCount is present
Mar 13 22:15:30.295: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:19:30.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-x5gg2" for this suite.
Mar 13 22:19:36.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:19:36.765: INFO: namespace: e2e-tests-container-probe-x5gg2, resource: bindings, ignored listing per whitelist
Mar 13 22:19:36.797: INFO: namespace e2e-tests-container-probe-x5gg2 deletion completed in 6.06996027s

• [SLOW TEST:252.560 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:19:36.797: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 13 22:19:36.856: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:36.858: INFO: Number of nodes with available pods: 0
Mar 13 22:19:36.858: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 22:19:37.863: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:37.865: INFO: Number of nodes with available pods: 0
Mar 13 22:19:37.865: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 22:19:38.862: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:38.865: INFO: Number of nodes with available pods: 0
Mar 13 22:19:38.865: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 22:19:39.862: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:39.865: INFO: Number of nodes with available pods: 0
Mar 13 22:19:39.865: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 22:19:40.863: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:40.866: INFO: Number of nodes with available pods: 1
Mar 13 22:19:40.866: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:41.863: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:41.865: INFO: Number of nodes with available pods: 2
Mar 13 22:19:41.865: INFO: Node karbon-test-3a0ff6-k8s-worker-2 is running more than one daemon pod
Mar 13 22:19:42.862: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:42.864: INFO: Number of nodes with available pods: 3
Mar 13 22:19:42.864: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 13 22:19:42.875: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:42.877: INFO: Number of nodes with available pods: 2
Mar 13 22:19:42.877: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:43.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:43.883: INFO: Number of nodes with available pods: 2
Mar 13 22:19:43.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:44.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:44.884: INFO: Number of nodes with available pods: 2
Mar 13 22:19:44.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:45.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:45.884: INFO: Number of nodes with available pods: 2
Mar 13 22:19:45.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:46.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:46.883: INFO: Number of nodes with available pods: 2
Mar 13 22:19:46.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:47.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:47.883: INFO: Number of nodes with available pods: 2
Mar 13 22:19:47.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:48.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:48.883: INFO: Number of nodes with available pods: 2
Mar 13 22:19:48.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:49.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:49.884: INFO: Number of nodes with available pods: 2
Mar 13 22:19:49.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:50.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:50.883: INFO: Number of nodes with available pods: 2
Mar 13 22:19:50.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:51.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:51.884: INFO: Number of nodes with available pods: 2
Mar 13 22:19:51.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:52.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:52.884: INFO: Number of nodes with available pods: 2
Mar 13 22:19:52.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:53.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:53.885: INFO: Number of nodes with available pods: 2
Mar 13 22:19:53.885: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:54.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:54.883: INFO: Number of nodes with available pods: 2
Mar 13 22:19:54.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:55.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:55.883: INFO: Number of nodes with available pods: 2
Mar 13 22:19:55.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:56.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:56.883: INFO: Number of nodes with available pods: 2
Mar 13 22:19:56.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:57.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:57.883: INFO: Number of nodes with available pods: 2
Mar 13 22:19:57.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:58.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:58.883: INFO: Number of nodes with available pods: 2
Mar 13 22:19:58.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:19:59.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:19:59.884: INFO: Number of nodes with available pods: 2
Mar 13 22:19:59.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:00.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:00.884: INFO: Number of nodes with available pods: 2
Mar 13 22:20:00.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:01.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:01.884: INFO: Number of nodes with available pods: 2
Mar 13 22:20:01.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:02.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:02.884: INFO: Number of nodes with available pods: 2
Mar 13 22:20:02.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:03.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:03.884: INFO: Number of nodes with available pods: 2
Mar 13 22:20:03.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:04.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:04.883: INFO: Number of nodes with available pods: 2
Mar 13 22:20:04.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:05.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:05.884: INFO: Number of nodes with available pods: 2
Mar 13 22:20:05.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:06.880: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:06.882: INFO: Number of nodes with available pods: 2
Mar 13 22:20:06.882: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:07.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:07.883: INFO: Number of nodes with available pods: 2
Mar 13 22:20:07.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:08.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:08.883: INFO: Number of nodes with available pods: 2
Mar 13 22:20:08.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:09.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:09.884: INFO: Number of nodes with available pods: 2
Mar 13 22:20:09.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:10.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:10.884: INFO: Number of nodes with available pods: 2
Mar 13 22:20:10.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:11.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:11.884: INFO: Number of nodes with available pods: 2
Mar 13 22:20:11.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:12.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:12.885: INFO: Number of nodes with available pods: 2
Mar 13 22:20:12.885: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:13.880: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:13.882: INFO: Number of nodes with available pods: 2
Mar 13 22:20:13.882: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:14.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:14.883: INFO: Number of nodes with available pods: 2
Mar 13 22:20:14.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:15.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:15.883: INFO: Number of nodes with available pods: 2
Mar 13 22:20:15.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:16.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:16.883: INFO: Number of nodes with available pods: 2
Mar 13 22:20:16.883: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:17.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:17.885: INFO: Number of nodes with available pods: 2
Mar 13 22:20:17.885: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:18.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:18.884: INFO: Number of nodes with available pods: 2
Mar 13 22:20:18.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:19.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:19.884: INFO: Number of nodes with available pods: 2
Mar 13 22:20:19.884: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:20:20.881: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:20:20.884: INFO: Number of nodes with available pods: 3
Mar 13 22:20:20.884: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-46dwd, will wait for the garbage collector to delete the pods
Mar 13 22:20:20.942: INFO: Deleting DaemonSet.extensions daemon-set took: 4.942488ms
Mar 13 22:20:21.043: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.325216ms
Mar 13 22:20:57.845: INFO: Number of nodes with available pods: 0
Mar 13 22:20:57.846: INFO: Number of running nodes: 0, number of available pods: 0
Mar 13 22:20:57.849: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-46dwd/daemonsets","resourceVersion":"27690"},"items":null}

Mar 13 22:20:57.851: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-46dwd/pods","resourceVersion":"27690"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:20:57.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-46dwd" for this suite.
Mar 13 22:21:03.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:21:03.890: INFO: namespace: e2e-tests-daemonsets-46dwd, resource: bindings, ignored listing per whitelist
Mar 13 22:21:03.938: INFO: namespace e2e-tests-daemonsets-46dwd deletion completed in 6.076531914s

• [SLOW TEST:87.140 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:21:03.938: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 13 22:21:11.000: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:21:12.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-kpqrh" for this suite.
Mar 13 22:21:34.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:21:34.079: INFO: namespace: e2e-tests-replicaset-kpqrh, resource: bindings, ignored listing per whitelist
Mar 13 22:21:34.098: INFO: namespace e2e-tests-replicaset-kpqrh deletion completed in 22.083022203s

• [SLOW TEST:30.160 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:21:34.098: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Mar 13 22:21:34.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-d4brb'
Mar 13 22:21:34.585: INFO: stderr: ""
Mar 13 22:21:34.586: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Mar 13 22:21:35.589: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 22:21:35.589: INFO: Found 0 / 1
Mar 13 22:21:36.589: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 22:21:36.589: INFO: Found 0 / 1
Mar 13 22:21:37.588: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 22:21:37.588: INFO: Found 1 / 1
Mar 13 22:21:37.588: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 13 22:21:37.590: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 22:21:37.590: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Mar 13 22:21:37.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 logs redis-master-57fkp redis-master --namespace=e2e-tests-kubectl-d4brb'
Mar 13 22:21:37.675: INFO: stderr: ""
Mar 13 22:21:37.676: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Mar 22:21:36.471 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Mar 22:21:36.471 # Server started, Redis version 3.2.12\n1:M 13 Mar 22:21:36.471 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Mar 22:21:36.471 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Mar 13 22:21:37.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 log redis-master-57fkp redis-master --namespace=e2e-tests-kubectl-d4brb --tail=1'
Mar 13 22:21:37.765: INFO: stderr: ""
Mar 13 22:21:37.765: INFO: stdout: "1:M 13 Mar 22:21:36.471 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Mar 13 22:21:37.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 log redis-master-57fkp redis-master --namespace=e2e-tests-kubectl-d4brb --limit-bytes=1'
Mar 13 22:21:37.856: INFO: stderr: ""
Mar 13 22:21:37.856: INFO: stdout: " "
STEP: exposing timestamps
Mar 13 22:21:37.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 log redis-master-57fkp redis-master --namespace=e2e-tests-kubectl-d4brb --tail=1 --timestamps'
Mar 13 22:21:37.951: INFO: stderr: ""
Mar 13 22:21:37.951: INFO: stdout: "2019-03-13T22:21:36.47130116Z 1:M 13 Mar 22:21:36.471 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Mar 13 22:21:40.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 log redis-master-57fkp redis-master --namespace=e2e-tests-kubectl-d4brb --since=1s'
Mar 13 22:21:40.540: INFO: stderr: ""
Mar 13 22:21:40.540: INFO: stdout: ""
Mar 13 22:21:40.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 log redis-master-57fkp redis-master --namespace=e2e-tests-kubectl-d4brb --since=24h'
Mar 13 22:21:40.629: INFO: stderr: ""
Mar 13 22:21:40.629: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Mar 22:21:36.471 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Mar 22:21:36.471 # Server started, Redis version 3.2.12\n1:M 13 Mar 22:21:36.471 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Mar 22:21:36.471 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Mar 13 22:21:40.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-d4brb'
Mar 13 22:21:40.705: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 22:21:40.705: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar 13 22:21:40.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-d4brb'
Mar 13 22:21:40.780: INFO: stderr: "No resources found.\n"
Mar 13 22:21:40.780: INFO: stdout: ""
Mar 13 22:21:40.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -l name=nginx --namespace=e2e-tests-kubectl-d4brb -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 13 22:21:40.855: INFO: stderr: ""
Mar 13 22:21:40.855: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:21:40.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d4brb" for this suite.
Mar 13 22:21:46.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:21:46.878: INFO: namespace: e2e-tests-kubectl-d4brb, resource: bindings, ignored listing per whitelist
Mar 13 22:21:46.931: INFO: namespace e2e-tests-kubectl-d4brb deletion completed in 6.072968417s

• [SLOW TEST:12.833 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:21:46.931: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-mrgg
STEP: Creating a pod to test atomic-volume-subpath
Mar 13 22:21:46.983: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mrgg" in namespace "e2e-tests-subpath-6r2jj" to be "success or failure"
Mar 13 22:21:46.986: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.634239ms
Mar 13 22:21:48.989: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005940885s
Mar 13 22:21:50.992: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Running", Reason="", readiness=false. Elapsed: 4.009297173s
Mar 13 22:21:52.995: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Running", Reason="", readiness=false. Elapsed: 6.012522567s
Mar 13 22:21:54.999: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Running", Reason="", readiness=false. Elapsed: 8.016537089s
Mar 13 22:21:57.003: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Running", Reason="", readiness=false. Elapsed: 10.019810741s
Mar 13 22:21:59.007: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Running", Reason="", readiness=false. Elapsed: 12.023645628s
Mar 13 22:22:01.012: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Running", Reason="", readiness=false. Elapsed: 14.028620813s
Mar 13 22:22:03.015: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Running", Reason="", readiness=false. Elapsed: 16.032316953s
Mar 13 22:22:05.019: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Running", Reason="", readiness=false. Elapsed: 18.036107992s
Mar 13 22:22:07.023: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Running", Reason="", readiness=false. Elapsed: 20.039641638s
Mar 13 22:22:09.026: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Running", Reason="", readiness=false. Elapsed: 22.043245261s
Mar 13 22:22:11.030: INFO: Pod "pod-subpath-test-projected-mrgg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.047486763s
STEP: Saw pod success
Mar 13 22:22:11.031: INFO: Pod "pod-subpath-test-projected-mrgg" satisfied condition "success or failure"
Mar 13 22:22:11.033: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-subpath-test-projected-mrgg container test-container-subpath-projected-mrgg: <nil>
STEP: delete the pod
Mar 13 22:22:11.047: INFO: Waiting for pod pod-subpath-test-projected-mrgg to disappear
Mar 13 22:22:11.049: INFO: Pod pod-subpath-test-projected-mrgg no longer exists
STEP: Deleting pod pod-subpath-test-projected-mrgg
Mar 13 22:22:11.049: INFO: Deleting pod "pod-subpath-test-projected-mrgg" in namespace "e2e-tests-subpath-6r2jj"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:22:11.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-6r2jj" for this suite.
Mar 13 22:22:17.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:22:17.088: INFO: namespace: e2e-tests-subpath-6r2jj, resource: bindings, ignored listing per whitelist
Mar 13 22:22:17.123: INFO: namespace e2e-tests-subpath-6r2jj deletion completed in 6.069135018s

• [SLOW TEST:30.192 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:22:17.123: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 13 22:22:17.181: INFO: Waiting up to 5m0s for pod "pod-75bb25d6-45de-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-s5rbp" to be "success or failure"
Mar 13 22:22:17.183: INFO: Pod "pod-75bb25d6-45de-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.298678ms
Mar 13 22:22:19.187: INFO: Pod "pod-75bb25d6-45de-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005991054s
Mar 13 22:22:21.190: INFO: Pod "pod-75bb25d6-45de-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009399235s
Mar 13 22:22:23.193: INFO: Pod "pod-75bb25d6-45de-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012380345s
STEP: Saw pod success
Mar 13 22:22:23.193: INFO: Pod "pod-75bb25d6-45de-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:22:23.195: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-75bb25d6-45de-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 22:22:23.209: INFO: Waiting for pod pod-75bb25d6-45de-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:22:23.211: INFO: Pod pod-75bb25d6-45de-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:22:23.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-s5rbp" for this suite.
Mar 13 22:22:29.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:22:29.240: INFO: namespace: e2e-tests-emptydir-s5rbp, resource: bindings, ignored listing per whitelist
Mar 13 22:22:29.288: INFO: namespace e2e-tests-emptydir-s5rbp deletion completed in 6.075281551s

• [SLOW TEST:12.165 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:22:29.289: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-v8xxm
Mar 13 22:22:35.344: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-v8xxm
STEP: checking the pod's current state and verifying that restartCount is present
Mar 13 22:22:35.346: INFO: Initial restart count of pod liveness-http is 0
Mar 13 22:22:47.369: INFO: Restart count of pod e2e-tests-container-probe-v8xxm/liveness-http is now 1 (12.023264082s elapsed)
Mar 13 22:23:07.403: INFO: Restart count of pod e2e-tests-container-probe-v8xxm/liveness-http is now 2 (32.057550667s elapsed)
Mar 13 22:23:27.438: INFO: Restart count of pod e2e-tests-container-probe-v8xxm/liveness-http is now 3 (52.091677968s elapsed)
Mar 13 22:23:47.472: INFO: Restart count of pod e2e-tests-container-probe-v8xxm/liveness-http is now 4 (1m12.126188572s elapsed)
Mar 13 22:24:47.575: INFO: Restart count of pod e2e-tests-container-probe-v8xxm/liveness-http is now 5 (2m12.228974291s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:24:47.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-v8xxm" for this suite.
Mar 13 22:24:53.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:24:53.625: INFO: namespace: e2e-tests-container-probe-v8xxm, resource: bindings, ignored listing per whitelist
Mar 13 22:24:53.662: INFO: namespace e2e-tests-container-probe-v8xxm deletion completed in 6.076155214s

• [SLOW TEST:144.373 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:24:53.662: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 13 22:24:53.717: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25ww4,SelfLink:/api/v1/namespaces/e2e-tests-watch-25ww4/configmaps/e2e-watch-test-label-changed,UID:d2d36f4e-45de-11e9-b6d3-506b8df34f96,ResourceVersion:28288,Generation:0,CreationTimestamp:2019-03-13 22:24:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 13 22:24:53.717: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25ww4,SelfLink:/api/v1/namespaces/e2e-tests-watch-25ww4/configmaps/e2e-watch-test-label-changed,UID:d2d36f4e-45de-11e9-b6d3-506b8df34f96,ResourceVersion:28289,Generation:0,CreationTimestamp:2019-03-13 22:24:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 13 22:24:53.717: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25ww4,SelfLink:/api/v1/namespaces/e2e-tests-watch-25ww4/configmaps/e2e-watch-test-label-changed,UID:d2d36f4e-45de-11e9-b6d3-506b8df34f96,ResourceVersion:28290,Generation:0,CreationTimestamp:2019-03-13 22:24:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 13 22:25:03.737: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25ww4,SelfLink:/api/v1/namespaces/e2e-tests-watch-25ww4/configmaps/e2e-watch-test-label-changed,UID:d2d36f4e-45de-11e9-b6d3-506b8df34f96,ResourceVersion:28307,Generation:0,CreationTimestamp:2019-03-13 22:24:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 13 22:25:03.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25ww4,SelfLink:/api/v1/namespaces/e2e-tests-watch-25ww4/configmaps/e2e-watch-test-label-changed,UID:d2d36f4e-45de-11e9-b6d3-506b8df34f96,ResourceVersion:28308,Generation:0,CreationTimestamp:2019-03-13 22:24:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar 13 22:25:03.737: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25ww4,SelfLink:/api/v1/namespaces/e2e-tests-watch-25ww4/configmaps/e2e-watch-test-label-changed,UID:d2d36f4e-45de-11e9-b6d3-506b8df34f96,ResourceVersion:28309,Generation:0,CreationTimestamp:2019-03-13 22:24:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:25:03.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-25ww4" for this suite.
Mar 13 22:25:09.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:25:09.797: INFO: namespace: e2e-tests-watch-25ww4, resource: bindings, ignored listing per whitelist
Mar 13 22:25:09.814: INFO: namespace e2e-tests-watch-25ww4 deletion completed in 6.073843313s

• [SLOW TEST:16.152 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:25:09.814: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 13 22:25:13.873: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-dca88387-45de-11e9-8b9c-0a58ac140107,GenerateName:,Namespace:e2e-tests-events-l9qqc,SelfLink:/api/v1/namespaces/e2e-tests-events-l9qqc/pods/send-events-dca88387-45de-11e9-8b9c-0a58ac140107,UID:dc745545-45de-11e9-b6d3-506b8df34f96,ResourceVersion:28339,Generation:0,CreationTimestamp:2019-03-13 22:25:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 859834035,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fn5dx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fn5dx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-fn5dx true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:25:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:25:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:25:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:25:09 +0000 UTC  }],Message:,Reason:,HostIP:10.40.156.31,PodIP:172.20.2.18,StartTime:2019-03-13 22:25:09 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-03-13 22:25:12 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://1558b3d423e84952408537016a84467fcd660c5ee17fa13034380da46b21ee29}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Mar 13 22:25:15.877: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 13 22:25:17.880: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:25:17.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-l9qqc" for this suite.
Mar 13 22:26:01.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:26:01.923: INFO: namespace: e2e-tests-events-l9qqc, resource: bindings, ignored listing per whitelist
Mar 13 22:26:01.955: INFO: namespace e2e-tests-events-l9qqc deletion completed in 44.068029325s

• [SLOW TEST:52.141 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:26:01.955: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 22:26:01.994: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fbbb2000-45de-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-hqvtq" to be "success or failure"
Mar 13 22:26:01.996: INFO: Pod "downwardapi-volume-fbbb2000-45de-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.540721ms
Mar 13 22:26:03.999: INFO: Pod "downwardapi-volume-fbbb2000-45de-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004994977s
Mar 13 22:26:06.003: INFO: Pod "downwardapi-volume-fbbb2000-45de-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00871405s
STEP: Saw pod success
Mar 13 22:26:06.003: INFO: Pod "downwardapi-volume-fbbb2000-45de-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:26:06.005: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downwardapi-volume-fbbb2000-45de-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 22:26:06.017: INFO: Waiting for pod downwardapi-volume-fbbb2000-45de-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:26:06.019: INFO: Pod downwardapi-volume-fbbb2000-45de-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:26:06.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hqvtq" for this suite.
Mar 13 22:26:12.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:26:12.040: INFO: namespace: e2e-tests-downward-api-hqvtq, resource: bindings, ignored listing per whitelist
Mar 13 22:26:12.092: INFO: namespace e2e-tests-downward-api-hqvtq deletion completed in 6.070100697s

• [SLOW TEST:10.137 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:26:12.092: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-01c798c1-45df-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 22:26:12.145: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-01c7e5ee-45df-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-tj7fw" to be "success or failure"
Mar 13 22:26:12.147: INFO: Pod "pod-projected-configmaps-01c7e5ee-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.117177ms
Mar 13 22:26:14.150: INFO: Pod "pod-projected-configmaps-01c7e5ee-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005724815s
Mar 13 22:26:16.154: INFO: Pod "pod-projected-configmaps-01c7e5ee-45df-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009060708s
STEP: Saw pod success
Mar 13 22:26:16.154: INFO: Pod "pod-projected-configmaps-01c7e5ee-45df-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:26:16.156: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-projected-configmaps-01c7e5ee-45df-11e9-8b9c-0a58ac140107 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 22:26:16.169: INFO: Waiting for pod pod-projected-configmaps-01c7e5ee-45df-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:26:16.171: INFO: Pod pod-projected-configmaps-01c7e5ee-45df-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:26:16.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tj7fw" for this suite.
Mar 13 22:26:22.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:26:22.196: INFO: namespace: e2e-tests-projected-tj7fw, resource: bindings, ignored listing per whitelist
Mar 13 22:26:22.250: INFO: namespace e2e-tests-projected-tj7fw deletion completed in 6.075645237s

• [SLOW TEST:10.158 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:26:22.250: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar 13 22:26:22.303: INFO: Waiting up to 5m0s for pod "downward-api-07d5a72f-45df-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-jrf52" to be "success or failure"
Mar 13 22:26:22.305: INFO: Pod "downward-api-07d5a72f-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181386ms
Mar 13 22:26:24.308: INFO: Pod "downward-api-07d5a72f-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005532391s
Mar 13 22:26:26.312: INFO: Pod "downward-api-07d5a72f-45df-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00930538s
STEP: Saw pod success
Mar 13 22:26:26.312: INFO: Pod "downward-api-07d5a72f-45df-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:26:26.314: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downward-api-07d5a72f-45df-11e9-8b9c-0a58ac140107 container dapi-container: <nil>
STEP: delete the pod
Mar 13 22:26:26.332: INFO: Waiting for pod downward-api-07d5a72f-45df-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:26:26.334: INFO: Pod downward-api-07d5a72f-45df-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:26:26.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jrf52" for this suite.
Mar 13 22:26:32.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:26:32.374: INFO: namespace: e2e-tests-downward-api-jrf52, resource: bindings, ignored listing per whitelist
Mar 13 22:26:32.406: INFO: namespace e2e-tests-downward-api-jrf52 deletion completed in 6.068760795s

• [SLOW TEST:10.156 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:26:32.406: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 22:26:32.457: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 13 22:26:37.461: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 13 22:26:37.461: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 13 22:26:37.477: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-96k9v,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-96k9v/deployments/test-cleanup-deployment,UID:10ab39a4-45df-11e9-b6d3-506b8df34f96,ResourceVersion:28574,Generation:1,CreationTimestamp:2019-03-13 22:26:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Mar 13 22:26:37.480: INFO: New ReplicaSet "test-cleanup-deployment-7dbbfcf846" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846,GenerateName:,Namespace:e2e-tests-deployment-96k9v,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-96k9v/replicasets/test-cleanup-deployment-7dbbfcf846,UID:10accc98-45df-11e9-b6d3-506b8df34f96,ResourceVersion:28576,Generation:1,CreationTimestamp:2019-03-13 22:26:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 10ab39a4-45df-11e9-b6d3-506b8df34f96 0xc002241637 0xc002241638}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 13 22:26:37.480: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 13 22:26:37.480: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-96k9v,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-96k9v/replicasets/test-cleanup-controller,UID:0daea649-45df-11e9-b6d3-506b8df34f96,ResourceVersion:28575,Generation:1,CreationTimestamp:2019-03-13 22:26:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 10ab39a4-45df-11e9-b6d3-506b8df34f96 0xc002241577 0xc002241578}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar 13 22:26:37.485: INFO: Pod "test-cleanup-controller-2cvlc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-2cvlc,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-96k9v,SelfLink:/api/v1/namespaces/e2e-tests-deployment-96k9v/pods/test-cleanup-controller-2cvlc,UID:0daf4e36-45df-11e9-b6d3-506b8df34f96,ResourceVersion:28568,Generation:0,CreationTimestamp:2019-03-13 22:26:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 0daea649-45df-11e9-b6d3-506b8df34f96 0xc001ec81c7 0xc001ec81c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gcgq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gcgq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gcgq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:26:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:26:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:26:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:26:32 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.213,PodIP:172.20.3.18,StartTime:2019-03-13 22:26:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-13 22:26:34 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://959170ea840d3902c29e1356e42e6165ca24060f09bdc254905956ade8935236}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:26:37.485: INFO: Pod "test-cleanup-deployment-7dbbfcf846-nz66g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846-nz66g,GenerateName:test-cleanup-deployment-7dbbfcf846-,Namespace:e2e-tests-deployment-96k9v,SelfLink:/api/v1/namespaces/e2e-tests-deployment-96k9v/pods/test-cleanup-deployment-7dbbfcf846-nz66g,UID:10ad54ab-45df-11e9-b6d3-506b8df34f96,ResourceVersion:28577,Generation:0,CreationTimestamp:2019-03-13 22:26:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-7dbbfcf846 10accc98-45df-11e9-b6d3-506b8df34f96 0xc001ec8407 0xc001ec8408}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gcgq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gcgq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-gcgq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:26:37.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-96k9v" for this suite.
Mar 13 22:26:43.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:26:43.523: INFO: namespace: e2e-tests-deployment-96k9v, resource: bindings, ignored listing per whitelist
Mar 13 22:26:43.573: INFO: namespace e2e-tests-deployment-96k9v deletion completed in 6.079534488s

• [SLOW TEST:11.167 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:26:43.573: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-hd56t
Mar 13 22:26:49.624: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-hd56t
STEP: checking the pod's current state and verifying that restartCount is present
Mar 13 22:26:49.626: INFO: Initial restart count of pod liveness-http is 0
Mar 13 22:27:11.665: INFO: Restart count of pod e2e-tests-container-probe-hd56t/liveness-http is now 1 (22.038926027s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:27:11.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-hd56t" for this suite.
Mar 13 22:27:17.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:27:17.711: INFO: namespace: e2e-tests-container-probe-hd56t, resource: bindings, ignored listing per whitelist
Mar 13 22:27:17.749: INFO: namespace e2e-tests-container-probe-hd56t deletion completed in 6.074154947s

• [SLOW TEST:34.176 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:27:17.749: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar 13 22:27:17.846: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:27:23.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-7h78t" for this suite.
Mar 13 22:27:29.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:27:29.670: INFO: namespace: e2e-tests-init-container-7h78t, resource: bindings, ignored listing per whitelist
Mar 13 22:27:29.729: INFO: namespace e2e-tests-init-container-7h78t deletion completed in 6.080710549s

• [SLOW TEST:11.980 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:27:29.729: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 22:27:29.779: INFO: Waiting up to 5m0s for pod "downwardapi-volume-300dd63c-45df-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-6qv8j" to be "success or failure"
Mar 13 22:27:29.781: INFO: Pod "downwardapi-volume-300dd63c-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.70381ms
Mar 13 22:27:31.785: INFO: Pod "downwardapi-volume-300dd63c-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00604956s
Mar 13 22:27:33.788: INFO: Pod "downwardapi-volume-300dd63c-45df-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009462569s
STEP: Saw pod success
Mar 13 22:27:33.788: INFO: Pod "downwardapi-volume-300dd63c-45df-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:27:33.790: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downwardapi-volume-300dd63c-45df-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 22:27:33.804: INFO: Waiting for pod downwardapi-volume-300dd63c-45df-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:27:33.806: INFO: Pod downwardapi-volume-300dd63c-45df-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:27:33.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6qv8j" for this suite.
Mar 13 22:27:39.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:27:39.882: INFO: namespace: e2e-tests-projected-6qv8j, resource: bindings, ignored listing per whitelist
Mar 13 22:27:39.893: INFO: namespace e2e-tests-projected-6qv8j deletion completed in 6.084198883s

• [SLOW TEST:10.164 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:27:39.894: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 22:27:39.950: INFO: Waiting up to 5m0s for pod "downwardapi-volume-361deab9-45df-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-5g45w" to be "success or failure"
Mar 13 22:27:39.952: INFO: Pod "downwardapi-volume-361deab9-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.433007ms
Mar 13 22:27:41.955: INFO: Pod "downwardapi-volume-361deab9-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005263247s
Mar 13 22:27:43.958: INFO: Pod "downwardapi-volume-361deab9-45df-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007937649s
STEP: Saw pod success
Mar 13 22:27:43.958: INFO: Pod "downwardapi-volume-361deab9-45df-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:27:43.960: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downwardapi-volume-361deab9-45df-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 22:27:43.972: INFO: Waiting for pod downwardapi-volume-361deab9-45df-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:27:43.974: INFO: Pod downwardapi-volume-361deab9-45df-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:27:43.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5g45w" for this suite.
Mar 13 22:27:49.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:27:50.000: INFO: namespace: e2e-tests-projected-5g45w, resource: bindings, ignored listing per whitelist
Mar 13 22:27:50.048: INFO: namespace e2e-tests-projected-5g45w deletion completed in 6.071556989s

• [SLOW TEST:10.155 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:27:50.048: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar 13 22:27:50.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:27:50.245: INFO: stderr: ""
Mar 13 22:27:50.245: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 22:27:50.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:27:50.322: INFO: stderr: ""
Mar 13 22:27:50.322: INFO: stdout: "update-demo-nautilus-jdkr4 update-demo-nautilus-xfjdj "
Mar 13 22:27:50.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-jdkr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:27:50.391: INFO: stderr: ""
Mar 13 22:27:50.391: INFO: stdout: ""
Mar 13 22:27:50.391: INFO: update-demo-nautilus-jdkr4 is created but not running
Mar 13 22:27:55.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:27:55.462: INFO: stderr: ""
Mar 13 22:27:55.462: INFO: stdout: "update-demo-nautilus-jdkr4 update-demo-nautilus-xfjdj "
Mar 13 22:27:55.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-jdkr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:27:55.540: INFO: stderr: ""
Mar 13 22:27:55.540: INFO: stdout: "true"
Mar 13 22:27:55.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-jdkr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:27:55.624: INFO: stderr: ""
Mar 13 22:27:55.624: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 22:27:55.624: INFO: validating pod update-demo-nautilus-jdkr4
Mar 13 22:27:55.628: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 22:27:55.628: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 22:27:55.628: INFO: update-demo-nautilus-jdkr4 is verified up and running
Mar 13 22:27:55.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-xfjdj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:27:55.707: INFO: stderr: ""
Mar 13 22:27:55.707: INFO: stdout: ""
Mar 13 22:27:55.707: INFO: update-demo-nautilus-xfjdj is created but not running
Mar 13 22:28:00.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:00.795: INFO: stderr: ""
Mar 13 22:28:00.795: INFO: stdout: "update-demo-nautilus-jdkr4 update-demo-nautilus-xfjdj "
Mar 13 22:28:00.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-jdkr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:00.872: INFO: stderr: ""
Mar 13 22:28:00.872: INFO: stdout: "true"
Mar 13 22:28:00.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-jdkr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:00.948: INFO: stderr: ""
Mar 13 22:28:00.948: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 22:28:00.948: INFO: validating pod update-demo-nautilus-jdkr4
Mar 13 22:28:00.951: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 22:28:00.951: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 22:28:00.951: INFO: update-demo-nautilus-jdkr4 is verified up and running
Mar 13 22:28:00.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-xfjdj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:01.040: INFO: stderr: ""
Mar 13 22:28:01.040: INFO: stdout: "true"
Mar 13 22:28:01.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-xfjdj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:01.118: INFO: stderr: ""
Mar 13 22:28:01.118: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 22:28:01.118: INFO: validating pod update-demo-nautilus-xfjdj
Mar 13 22:28:01.122: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 22:28:01.122: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 22:28:01.122: INFO: update-demo-nautilus-xfjdj is verified up and running
STEP: scaling down the replication controller
Mar 13 22:28:01.124: INFO: scanned /root for discovery docs: <nil>
Mar 13 22:28:01.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:02.222: INFO: stderr: ""
Mar 13 22:28:02.222: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 22:28:02.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:02.304: INFO: stderr: ""
Mar 13 22:28:02.304: INFO: stdout: "update-demo-nautilus-jdkr4 update-demo-nautilus-xfjdj "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 13 22:28:07.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:07.389: INFO: stderr: ""
Mar 13 22:28:07.390: INFO: stdout: "update-demo-nautilus-jdkr4 update-demo-nautilus-xfjdj "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 13 22:28:12.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:12.473: INFO: stderr: ""
Mar 13 22:28:12.473: INFO: stdout: "update-demo-nautilus-jdkr4 "
Mar 13 22:28:12.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-jdkr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:12.557: INFO: stderr: ""
Mar 13 22:28:12.557: INFO: stdout: "true"
Mar 13 22:28:12.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-jdkr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:12.631: INFO: stderr: ""
Mar 13 22:28:12.631: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 22:28:12.631: INFO: validating pod update-demo-nautilus-jdkr4
Mar 13 22:28:12.633: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 22:28:12.633: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 22:28:12.633: INFO: update-demo-nautilus-jdkr4 is verified up and running
STEP: scaling up the replication controller
Mar 13 22:28:12.635: INFO: scanned /root for discovery docs: <nil>
Mar 13 22:28:12.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:13.738: INFO: stderr: ""
Mar 13 22:28:13.738: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 22:28:13.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:13.817: INFO: stderr: ""
Mar 13 22:28:13.817: INFO: stdout: "update-demo-nautilus-jdkr4 update-demo-nautilus-p59cc "
Mar 13 22:28:13.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-jdkr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:13.896: INFO: stderr: ""
Mar 13 22:28:13.896: INFO: stdout: "true"
Mar 13 22:28:13.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-jdkr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:13.965: INFO: stderr: ""
Mar 13 22:28:13.965: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 22:28:13.965: INFO: validating pod update-demo-nautilus-jdkr4
Mar 13 22:28:13.968: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 22:28:13.968: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 22:28:13.968: INFO: update-demo-nautilus-jdkr4 is verified up and running
Mar 13 22:28:13.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-p59cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:14.045: INFO: stderr: ""
Mar 13 22:28:14.045: INFO: stdout: ""
Mar 13 22:28:14.045: INFO: update-demo-nautilus-p59cc is created but not running
Mar 13 22:28:19.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:19.117: INFO: stderr: ""
Mar 13 22:28:19.117: INFO: stdout: "update-demo-nautilus-jdkr4 update-demo-nautilus-p59cc "
Mar 13 22:28:19.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-jdkr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:19.187: INFO: stderr: ""
Mar 13 22:28:19.187: INFO: stdout: "true"
Mar 13 22:28:19.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-jdkr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:19.253: INFO: stderr: ""
Mar 13 22:28:19.253: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 22:28:19.253: INFO: validating pod update-demo-nautilus-jdkr4
Mar 13 22:28:19.255: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 22:28:19.255: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 22:28:19.255: INFO: update-demo-nautilus-jdkr4 is verified up and running
Mar 13 22:28:19.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-p59cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:19.331: INFO: stderr: ""
Mar 13 22:28:19.332: INFO: stdout: "true"
Mar 13 22:28:19.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-p59cc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:19.406: INFO: stderr: ""
Mar 13 22:28:19.406: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 22:28:19.406: INFO: validating pod update-demo-nautilus-p59cc
Mar 13 22:28:19.410: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 22:28:19.410: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 22:28:19.410: INFO: update-demo-nautilus-p59cc is verified up and running
STEP: using delete to clean up resources
Mar 13 22:28:19.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:19.482: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 22:28:19.482: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 13 22:28:19.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-k2npb'
Mar 13 22:28:19.571: INFO: stderr: "No resources found.\n"
Mar 13 22:28:19.571: INFO: stdout: ""
Mar 13 22:28:19.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -l name=update-demo --namespace=e2e-tests-kubectl-k2npb -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 13 22:28:19.653: INFO: stderr: ""
Mar 13 22:28:19.653: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:28:19.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-k2npb" for this suite.
Mar 13 22:28:41.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:28:41.718: INFO: namespace: e2e-tests-kubectl-k2npb, resource: bindings, ignored listing per whitelist
Mar 13 22:28:41.725: INFO: namespace e2e-tests-kubectl-k2npb deletion completed in 22.068944519s

• [SLOW TEST:51.677 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:28:41.725: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 22:28:41.774: INFO: (0) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 4.513604ms)
Mar 13 22:28:41.779: INFO: (1) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 5.053397ms)
Mar 13 22:28:41.782: INFO: (2) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.760318ms)
Mar 13 22:28:41.784: INFO: (3) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.183361ms)
Mar 13 22:28:41.786: INFO: (4) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.168283ms)
Mar 13 22:28:41.790: INFO: (5) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 3.355003ms)
Mar 13 22:28:41.792: INFO: (6) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.692477ms)
Mar 13 22:28:41.794: INFO: (7) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.13656ms)
Mar 13 22:28:41.797: INFO: (8) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.389188ms)
Mar 13 22:28:41.801: INFO: (9) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 4.465176ms)
Mar 13 22:28:41.804: INFO: (10) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.753921ms)
Mar 13 22:28:41.807: INFO: (11) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 3.235978ms)
Mar 13 22:28:41.809: INFO: (12) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.095337ms)
Mar 13 22:28:41.812: INFO: (13) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.17953ms)
Mar 13 22:28:41.814: INFO: (14) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.215844ms)
Mar 13 22:28:41.816: INFO: (15) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.299374ms)
Mar 13 22:28:41.819: INFO: (16) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.335452ms)
Mar 13 22:28:41.821: INFO: (17) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.196714ms)
Mar 13 22:28:41.823: INFO: (18) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.276509ms)
Mar 13 22:28:41.825: INFO: (19) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.177634ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:28:41.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-h4tcm" for this suite.
Mar 13 22:28:47.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:28:47.858: INFO: namespace: e2e-tests-proxy-h4tcm, resource: bindings, ignored listing per whitelist
Mar 13 22:28:47.900: INFO: namespace e2e-tests-proxy-h4tcm deletion completed in 6.072628193s

• [SLOW TEST:6.175 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:28:47.900: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar 13 22:28:47.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-6tf27'
Mar 13 22:28:48.123: INFO: stderr: ""
Mar 13 22:28:48.123: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 22:28:48.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6tf27'
Mar 13 22:28:48.199: INFO: stderr: ""
Mar 13 22:28:48.199: INFO: stdout: "update-demo-nautilus-krpqp update-demo-nautilus-s9z66 "
Mar 13 22:28:48.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-krpqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6tf27'
Mar 13 22:28:48.267: INFO: stderr: ""
Mar 13 22:28:48.267: INFO: stdout: ""
Mar 13 22:28:48.267: INFO: update-demo-nautilus-krpqp is created but not running
Mar 13 22:28:53.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6tf27'
Mar 13 22:28:53.356: INFO: stderr: ""
Mar 13 22:28:53.357: INFO: stdout: "update-demo-nautilus-krpqp update-demo-nautilus-s9z66 "
Mar 13 22:28:53.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-krpqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6tf27'
Mar 13 22:28:53.426: INFO: stderr: ""
Mar 13 22:28:53.426: INFO: stdout: "true"
Mar 13 22:28:53.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-krpqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6tf27'
Mar 13 22:28:53.493: INFO: stderr: ""
Mar 13 22:28:53.493: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 22:28:53.493: INFO: validating pod update-demo-nautilus-krpqp
Mar 13 22:28:53.496: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 22:28:53.496: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 22:28:53.496: INFO: update-demo-nautilus-krpqp is verified up and running
Mar 13 22:28:53.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-s9z66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6tf27'
Mar 13 22:28:53.564: INFO: stderr: ""
Mar 13 22:28:53.564: INFO: stdout: "true"
Mar 13 22:28:53.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-s9z66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6tf27'
Mar 13 22:28:53.641: INFO: stderr: ""
Mar 13 22:28:53.641: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 22:28:53.641: INFO: validating pod update-demo-nautilus-s9z66
Mar 13 22:28:53.645: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 22:28:53.645: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 22:28:53.645: INFO: update-demo-nautilus-s9z66 is verified up and running
STEP: using delete to clean up resources
Mar 13 22:28:53.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6tf27'
Mar 13 22:28:53.721: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 22:28:53.721: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 13 22:28:53.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-6tf27'
Mar 13 22:28:53.807: INFO: stderr: "No resources found.\n"
Mar 13 22:28:53.807: INFO: stdout: ""
Mar 13 22:28:53.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -l name=update-demo --namespace=e2e-tests-kubectl-6tf27 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 13 22:28:53.881: INFO: stderr: ""
Mar 13 22:28:53.881: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:28:53.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6tf27" for this suite.
Mar 13 22:28:59.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:28:59.928: INFO: namespace: e2e-tests-kubectl-6tf27, resource: bindings, ignored listing per whitelist
Mar 13 22:28:59.962: INFO: namespace e2e-tests-kubectl-6tf27 deletion completed in 6.077813442s

• [SLOW TEST:12.062 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:28:59.962: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0313 22:29:10.015843      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 13 22:29:10.015: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:29:10.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-59kfk" for this suite.
Mar 13 22:29:16.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:29:16.085: INFO: namespace: e2e-tests-gc-59kfk, resource: bindings, ignored listing per whitelist
Mar 13 22:29:16.091: INFO: namespace e2e-tests-gc-59kfk deletion completed in 6.073382271s

• [SLOW TEST:16.129 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:29:16.091: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Mar 13 22:29:16.138: INFO: Waiting up to 5m0s for pod "client-containers-6f73386c-45df-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-containers-m9ffr" to be "success or failure"
Mar 13 22:29:16.140: INFO: Pod "client-containers-6f73386c-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.914909ms
Mar 13 22:29:18.144: INFO: Pod "client-containers-6f73386c-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005707327s
Mar 13 22:29:20.148: INFO: Pod "client-containers-6f73386c-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009194089s
Mar 13 22:29:22.151: INFO: Pod "client-containers-6f73386c-45df-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012505284s
STEP: Saw pod success
Mar 13 22:29:22.151: INFO: Pod "client-containers-6f73386c-45df-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:29:22.153: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod client-containers-6f73386c-45df-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 22:29:22.165: INFO: Waiting for pod client-containers-6f73386c-45df-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:29:22.166: INFO: Pod client-containers-6f73386c-45df-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:29:22.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-m9ffr" for this suite.
Mar 13 22:29:28.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:29:28.217: INFO: namespace: e2e-tests-containers-m9ffr, resource: bindings, ignored listing per whitelist
Mar 13 22:29:28.236: INFO: namespace e2e-tests-containers-m9ffr deletion completed in 6.067108063s

• [SLOW TEST:12.144 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:29:28.236: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 13 22:29:36.313: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 22:29:36.317: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 22:29:38.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 22:29:38.320: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 22:29:40.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 22:29:40.320: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 22:29:42.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 22:29:42.321: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 22:29:44.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 22:29:44.320: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 22:29:46.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 22:29:46.320: INFO: Pod pod-with-poststart-http-hook still exists
Mar 13 22:29:48.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 13 22:29:48.320: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:29:48.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-tdj9r" for this suite.
Mar 13 22:30:10.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:30:10.357: INFO: namespace: e2e-tests-container-lifecycle-hook-tdj9r, resource: bindings, ignored listing per whitelist
Mar 13 22:30:10.391: INFO: namespace e2e-tests-container-lifecycle-hook-tdj9r deletion completed in 22.068486791s

• [SLOW TEST:42.156 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:30:10.392: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 13 22:30:10.629: INFO: Pod name wrapped-volume-race-8fe1eab1-45df-11e9-8b9c-0a58ac140107: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8fe1eab1-45df-11e9-8b9c-0a58ac140107 in namespace e2e-tests-emptydir-wrapper-l2z95, will wait for the garbage collector to delete the pods
Mar 13 22:30:26.740: INFO: Deleting ReplicationController wrapped-volume-race-8fe1eab1-45df-11e9-8b9c-0a58ac140107 took: 5.642558ms
Mar 13 22:30:26.840: INFO: Terminating ReplicationController wrapped-volume-race-8fe1eab1-45df-11e9-8b9c-0a58ac140107 pods took: 100.253789ms
STEP: Creating RC which spawns configmap-volume pods
Mar 13 22:31:08.854: INFO: Pod name wrapped-volume-race-b2a0a459-45df-11e9-8b9c-0a58ac140107: Found 0 pods out of 5
Mar 13 22:31:13.860: INFO: Pod name wrapped-volume-race-b2a0a459-45df-11e9-8b9c-0a58ac140107: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b2a0a459-45df-11e9-8b9c-0a58ac140107 in namespace e2e-tests-emptydir-wrapper-l2z95, will wait for the garbage collector to delete the pods
Mar 13 22:31:23.932: INFO: Deleting ReplicationController wrapped-volume-race-b2a0a459-45df-11e9-8b9c-0a58ac140107 took: 5.284079ms
Mar 13 22:31:24.032: INFO: Terminating ReplicationController wrapped-volume-race-b2a0a459-45df-11e9-8b9c-0a58ac140107 pods took: 100.187627ms
STEP: Creating RC which spawns configmap-volume pods
Mar 13 22:32:08.243: INFO: Pod name wrapped-volume-race-d60721a2-45df-11e9-8b9c-0a58ac140107: Found 0 pods out of 5
Mar 13 22:32:13.252: INFO: Pod name wrapped-volume-race-d60721a2-45df-11e9-8b9c-0a58ac140107: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d60721a2-45df-11e9-8b9c-0a58ac140107 in namespace e2e-tests-emptydir-wrapper-l2z95, will wait for the garbage collector to delete the pods
Mar 13 22:32:23.325: INFO: Deleting ReplicationController wrapped-volume-race-d60721a2-45df-11e9-8b9c-0a58ac140107 took: 5.350087ms
Mar 13 22:32:23.425: INFO: Terminating ReplicationController wrapped-volume-race-d60721a2-45df-11e9-8b9c-0a58ac140107 pods took: 100.194578ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:33:08.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-l2z95" for this suite.
Mar 13 22:33:14.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:33:14.020: INFO: namespace: e2e-tests-emptydir-wrapper-l2z95, resource: bindings, ignored listing per whitelist
Mar 13 22:33:14.082: INFO: namespace e2e-tests-emptydir-wrapper-l2z95 deletion completed in 6.078345028s

• [SLOW TEST:183.690 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:33:14.082: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar 13 22:33:14.131: INFO: Waiting up to 5m0s for pod "downward-api-fd4d7e1c-45df-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-b4g8q" to be "success or failure"
Mar 13 22:33:14.136: INFO: Pod "downward-api-fd4d7e1c-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.905385ms
Mar 13 22:33:16.139: INFO: Pod "downward-api-fd4d7e1c-45df-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007866416s
Mar 13 22:33:18.143: INFO: Pod "downward-api-fd4d7e1c-45df-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011917345s
STEP: Saw pod success
Mar 13 22:33:18.143: INFO: Pod "downward-api-fd4d7e1c-45df-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:33:18.146: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod downward-api-fd4d7e1c-45df-11e9-8b9c-0a58ac140107 container dapi-container: <nil>
STEP: delete the pod
Mar 13 22:33:18.161: INFO: Waiting for pod downward-api-fd4d7e1c-45df-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:33:18.164: INFO: Pod downward-api-fd4d7e1c-45df-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:33:18.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-b4g8q" for this suite.
Mar 13 22:33:24.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:33:24.192: INFO: namespace: e2e-tests-downward-api-b4g8q, resource: bindings, ignored listing per whitelist
Mar 13 22:33:24.247: INFO: namespace e2e-tests-downward-api-b4g8q deletion completed in 6.079796934s

• [SLOW TEST:10.165 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:33:24.248: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-035d5ad4-45e0-11e9-8b9c-0a58ac140107
STEP: Creating secret with name s-test-opt-upd-035d5b41-45e0-11e9-8b9c-0a58ac140107
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-035d5ad4-45e0-11e9-8b9c-0a58ac140107
STEP: Updating secret s-test-opt-upd-035d5b41-45e0-11e9-8b9c-0a58ac140107
STEP: Creating secret with name s-test-opt-create-035d5b6c-45e0-11e9-8b9c-0a58ac140107
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:33:30.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-w2szr" for this suite.
Mar 13 22:33:52.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:33:52.411: INFO: namespace: e2e-tests-secrets-w2szr, resource: bindings, ignored listing per whitelist
Mar 13 22:33:52.443: INFO: namespace e2e-tests-secrets-w2szr deletion completed in 22.076886086s

• [SLOW TEST:28.196 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:33:52.444: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 13 22:33:52.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-v47mk'
Mar 13 22:33:52.779: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 13 22:33:52.779: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Mar 13 22:33:52.811: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-mt2pc]
Mar 13 22:33:52.811: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-mt2pc" in namespace "e2e-tests-kubectl-v47mk" to be "running and ready"
Mar 13 22:33:52.818: INFO: Pod "e2e-test-nginx-rc-mt2pc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.931258ms
Mar 13 22:33:54.822: INFO: Pod "e2e-test-nginx-rc-mt2pc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01084247s
Mar 13 22:33:56.825: INFO: Pod "e2e-test-nginx-rc-mt2pc": Phase="Running", Reason="", readiness=true. Elapsed: 4.014052222s
Mar 13 22:33:56.825: INFO: Pod "e2e-test-nginx-rc-mt2pc" satisfied condition "running and ready"
Mar 13 22:33:56.825: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-mt2pc]
Mar 13 22:33:56.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-v47mk'
Mar 13 22:33:56.928: INFO: stderr: ""
Mar 13 22:33:56.928: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Mar 13 22:33:56.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-v47mk'
Mar 13 22:33:57.004: INFO: stderr: ""
Mar 13 22:33:57.004: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:33:57.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-v47mk" for this suite.
Mar 13 22:34:03.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:34:03.049: INFO: namespace: e2e-tests-kubectl-v47mk, resource: bindings, ignored listing per whitelist
Mar 13 22:34:03.089: INFO: namespace e2e-tests-kubectl-v47mk deletion completed in 6.079353517s

• [SLOW TEST:10.645 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:34:03.089: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Mar 13 22:34:03.138: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-585th" to be "success or failure"
Mar 13 22:34:03.142: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.323226ms
Mar 13 22:34:05.145: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007524761s
Mar 13 22:34:07.148: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010664808s
STEP: Saw pod success
Mar 13 22:34:07.149: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar 13 22:34:07.150: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar 13 22:34:07.162: INFO: Waiting for pod pod-host-path-test to disappear
Mar 13 22:34:07.163: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:34:07.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-585th" for this suite.
Mar 13 22:34:13.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:34:13.228: INFO: namespace: e2e-tests-hostpath-585th, resource: bindings, ignored listing per whitelist
Mar 13 22:34:13.250: INFO: namespace e2e-tests-hostpath-585th deletion completed in 6.084132645s

• [SLOW TEST:10.161 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:34:13.250: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 13 22:34:13.309: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-lfvcv,SelfLink:/api/v1/namespaces/e2e-tests-watch-lfvcv/configmaps/e2e-watch-test-resource-version,UID:205c1e90-45e0-11e9-b6d3-506b8df34f96,ResourceVersion:30674,Generation:0,CreationTimestamp:2019-03-13 22:34:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 13 22:34:13.309: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-lfvcv,SelfLink:/api/v1/namespaces/e2e-tests-watch-lfvcv/configmaps/e2e-watch-test-resource-version,UID:205c1e90-45e0-11e9-b6d3-506b8df34f96,ResourceVersion:30675,Generation:0,CreationTimestamp:2019-03-13 22:34:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:34:13.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-lfvcv" for this suite.
Mar 13 22:34:19.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:34:19.345: INFO: namespace: e2e-tests-watch-lfvcv, resource: bindings, ignored listing per whitelist
Mar 13 22:34:19.395: INFO: namespace e2e-tests-watch-lfvcv deletion completed in 6.083082813s

• [SLOW TEST:6.145 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:34:19.396: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 13 22:34:19.442: INFO: Waiting up to 5m0s for pod "pod-243b9cdb-45e0-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-b5msf" to be "success or failure"
Mar 13 22:34:19.446: INFO: Pod "pod-243b9cdb-45e0-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 3.566175ms
Mar 13 22:34:21.449: INFO: Pod "pod-243b9cdb-45e0-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006654172s
Mar 13 22:34:23.453: INFO: Pod "pod-243b9cdb-45e0-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010310394s
Mar 13 22:34:25.456: INFO: Pod "pod-243b9cdb-45e0-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014045156s
STEP: Saw pod success
Mar 13 22:34:25.456: INFO: Pod "pod-243b9cdb-45e0-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:34:25.458: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-243b9cdb-45e0-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 22:34:25.469: INFO: Waiting for pod pod-243b9cdb-45e0-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:34:25.471: INFO: Pod pod-243b9cdb-45e0-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:34:25.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-b5msf" for this suite.
Mar 13 22:34:31.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:34:31.536: INFO: namespace: e2e-tests-emptydir-b5msf, resource: bindings, ignored listing per whitelist
Mar 13 22:34:31.552: INFO: namespace e2e-tests-emptydir-b5msf deletion completed in 6.078879975s

• [SLOW TEST:12.157 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:34:31.552: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar 13 22:34:31.598: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 13 22:34:31.603: INFO: Waiting for terminating namespaces to be deleted...
Mar 13 22:34:31.605: INFO: 
Logging pods the kubelet thinks is on node karbon-test-3a0ff6-k8s-worker-0 before test
Mar 13 22:34:31.611: INFO: kube-flannel-ds-25rft from kube-system started at 2019-03-13 17:38:02 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.611: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 22:34:31.611: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-13 22:07:51 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.611: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 13 22:34:31.611: INFO: sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-8jqgg from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.611: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 13 22:34:31.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 22:34:31.611: INFO: csi-attacher-ntnx-plugin-0 from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.611: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 13 22:34:31.611: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Mar 13 22:34:31.611: INFO: csi-node-ntnx-plugin-v9pcq from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.611: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Mar 13 22:34:31.611: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 13 22:34:31.611: INFO: prometheus-k8s-0 from monitoring started at 2019-03-13 17:41:33 +0000 UTC (3 container statuses recorded)
Mar 13 22:34:31.611: INFO: 	Container prometheus ready: true, restart count 1
Mar 13 22:34:31.611: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 13 22:34:31.611: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 13 22:34:31.611: INFO: kube-proxy-ds-bqvcl from kube-system started at 2019-03-13 17:37:58 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.611: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 13 22:34:31.611: INFO: node-exporter-l8bv5 from monitoring started at 2019-03-13 17:41:11 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.611: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 13 22:34:31.611: INFO: 	Container node-exporter ready: true, restart count 0
Mar 13 22:34:31.611: INFO: sonobuoy-e2e-job-f2ba8fc748ec406a from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.611: INFO: 	Container e2e ready: true, restart count 0
Mar 13 22:34:31.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 22:34:31.611: INFO: fluent-bit-zsqpg from kube-system started at 2019-03-13 17:38:38 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.611: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 13 22:34:31.611: INFO: 
Logging pods the kubelet thinks is on node karbon-test-3a0ff6-k8s-worker-1 before test
Mar 13 22:34:31.616: INFO: kube-proxy-ds-x52pl from kube-system started at 2019-03-13 17:37:58 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.616: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 13 22:34:31.616: INFO: kube-flannel-ds-8xdq9 from kube-system started at 2019-03-13 17:38:02 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.616: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 22:34:31.616: INFO: node-exporter-2prk8 from monitoring started at 2019-03-13 17:41:11 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.616: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 13 22:34:31.616: INFO: 	Container node-exporter ready: true, restart count 0
Mar 13 22:34:31.616: INFO: kube-state-metrics-54996cf64c-gdfqp from monitoring started at 2019-03-13 17:41:22 +0000 UTC (4 container statuses recorded)
Mar 13 22:34:31.616: INFO: 	Container addon-resizer ready: true, restart count 0
Mar 13 22:34:31.616: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 13 22:34:31.616: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 13 22:34:31.616: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 13 22:34:31.616: INFO: sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-c2w76 from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.616: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 13 22:34:31.616: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 22:34:31.616: INFO: csi-node-ntnx-plugin-v29vq from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.616: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Mar 13 22:34:31.616: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 13 22:34:31.616: INFO: fluent-bit-x4t2v from kube-system started at 2019-03-13 17:38:38 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.616: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 13 22:34:31.616: INFO: kubernetes-events-printer-75755d587-rcrrf from ntnx-logging started at 2019-03-13 17:38:39 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.616: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Mar 13 22:34:31.616: INFO: elasticsearch-logging-0 from ntnx-logging started at 2019-03-13 17:38:51 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.616: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Mar 13 22:34:31.616: INFO: prometheus-operator-d58cfc597-rr4rv from monitoring started at 2019-03-13 17:41:11 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.616: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 13 22:34:31.616: INFO: alertmanager-main-0 from monitoring started at 2019-03-13 17:41:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.616: INFO: 	Container alertmanager ready: true, restart count 0
Mar 13 22:34:31.616: INFO: 	Container config-reloader ready: true, restart count 0
Mar 13 22:34:31.616: INFO: 
Logging pods the kubelet thinks is on node karbon-test-3a0ff6-k8s-worker-2 before test
Mar 13 22:34:31.624: INFO: kube-flannel-ds-gcljm from kube-system started at 2019-03-13 17:38:02 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.624: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 22:34:31.624: INFO: csi-node-ntnx-plugin-9kv8f from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.624: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Mar 13 22:34:31.624: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 13 22:34:31.624: INFO: kibana-logging-5d46457978-l2xgl from ntnx-logging started at 2019-03-13 17:38:39 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.624: INFO: 	Container kibana-logging ready: true, restart count 0
Mar 13 22:34:31.624: INFO: 	Container nginxhttp ready: true, restart count 0
Mar 13 22:34:31.624: INFO: alertmanager-main-1 from monitoring started at 2019-03-13 17:41:30 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.624: INFO: 	Container alertmanager ready: true, restart count 0
Mar 13 22:34:31.624: INFO: 	Container config-reloader ready: true, restart count 0
Mar 13 22:34:31.624: INFO: sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-vtjz7 from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.624: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 13 22:34:31.624: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 22:34:31.624: INFO: kube-proxy-ds-64bb6 from kube-system started at 2019-03-13 17:37:58 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.624: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 13 22:34:31.624: INFO: csi-provisioner-ntnx-plugin-0 from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.624: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 13 22:34:31.624: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Mar 13 22:34:31.624: INFO: fluent-bit-q2plf from kube-system started at 2019-03-13 17:38:38 +0000 UTC (1 container statuses recorded)
Mar 13 22:34:31.624: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 13 22:34:31.624: INFO: node-exporter-wnrgc from monitoring started at 2019-03-13 17:41:11 +0000 UTC (2 container statuses recorded)
Mar 13 22:34:31.624: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 13 22:34:31.624: INFO: 	Container node-exporter ready: true, restart count 0
Mar 13 22:34:31.624: INFO: prometheus-k8s-1 from monitoring started at 2019-03-13 17:42:18 +0000 UTC (3 container statuses recorded)
Mar 13 22:34:31.624: INFO: 	Container prometheus ready: true, restart count 1
Mar 13 22:34:31.624: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 13 22:34:31.624: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2de40c30-45e0-11e9-8b9c-0a58ac140107 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-2de40c30-45e0-11e9-8b9c-0a58ac140107 off the node karbon-test-3a0ff6-k8s-worker-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2de40c30-45e0-11e9-8b9c-0a58ac140107
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:34:39.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-vgltv" for this suite.
Mar 13 22:34:47.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:34:47.718: INFO: namespace: e2e-tests-sched-pred-vgltv, resource: bindings, ignored listing per whitelist
Mar 13 22:34:47.745: INFO: namespace e2e-tests-sched-pred-vgltv deletion completed in 8.073004358s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:16.192 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:34:47.745: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-9ftkn
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 13 22:34:47.810: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 13 22:35:13.877: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 172.20.3.28 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-9ftkn PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:35:13.877: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:35:14.940: INFO: Found all expected endpoints: [netserver-0]
Mar 13 22:35:14.943: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 172.20.2.27 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-9ftkn PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:35:14.943: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:35:16.011: INFO: Found all expected endpoints: [netserver-1]
Mar 13 22:35:16.014: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 172.20.1.42 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-9ftkn PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:35:16.014: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:35:17.089: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:35:17.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-9ftkn" for this suite.
Mar 13 22:35:39.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:35:39.119: INFO: namespace: e2e-tests-pod-network-test-9ftkn, resource: bindings, ignored listing per whitelist
Mar 13 22:35:39.171: INFO: namespace e2e-tests-pod-network-test-9ftkn deletion completed in 22.076317741s

• [SLOW TEST:51.426 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:35:39.171: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Mar 13 22:35:39.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 cluster-info'
Mar 13 22:35:39.340: INFO: stderr: ""
Mar 13 22:35:39.340: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:35:39.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bw2cw" for this suite.
Mar 13 22:35:45.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:35:45.393: INFO: namespace: e2e-tests-kubectl-bw2cw, resource: bindings, ignored listing per whitelist
Mar 13 22:35:45.420: INFO: namespace e2e-tests-kubectl-bw2cw deletion completed in 6.077310314s

• [SLOW TEST:6.249 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:35:45.420: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar 13 22:35:45.462: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 13 22:35:45.467: INFO: Waiting for terminating namespaces to be deleted...
Mar 13 22:35:45.468: INFO: 
Logging pods the kubelet thinks is on node karbon-test-3a0ff6-k8s-worker-0 before test
Mar 13 22:35:45.474: INFO: fluent-bit-zsqpg from kube-system started at 2019-03-13 17:38:38 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.474: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 13 22:35:45.474: INFO: kube-flannel-ds-25rft from kube-system started at 2019-03-13 17:38:02 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.474: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 22:35:45.474: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-13 22:07:51 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.474: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 13 22:35:45.474: INFO: sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-8jqgg from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.474: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 13 22:35:45.474: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 22:35:45.474: INFO: csi-attacher-ntnx-plugin-0 from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.474: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 13 22:35:45.474: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Mar 13 22:35:45.474: INFO: csi-node-ntnx-plugin-v9pcq from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.474: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Mar 13 22:35:45.474: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 13 22:35:45.474: INFO: prometheus-k8s-0 from monitoring started at 2019-03-13 17:41:33 +0000 UTC (3 container statuses recorded)
Mar 13 22:35:45.474: INFO: 	Container prometheus ready: true, restart count 1
Mar 13 22:35:45.474: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 13 22:35:45.474: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 13 22:35:45.474: INFO: kube-proxy-ds-bqvcl from kube-system started at 2019-03-13 17:37:58 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.474: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 13 22:35:45.474: INFO: node-exporter-l8bv5 from monitoring started at 2019-03-13 17:41:11 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.474: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 13 22:35:45.474: INFO: 	Container node-exporter ready: true, restart count 0
Mar 13 22:35:45.474: INFO: sonobuoy-e2e-job-f2ba8fc748ec406a from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.474: INFO: 	Container e2e ready: true, restart count 0
Mar 13 22:35:45.474: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 22:35:45.474: INFO: 
Logging pods the kubelet thinks is on node karbon-test-3a0ff6-k8s-worker-1 before test
Mar 13 22:35:45.480: INFO: csi-node-ntnx-plugin-v29vq from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.480: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Mar 13 22:35:45.480: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 13 22:35:45.480: INFO: fluent-bit-x4t2v from kube-system started at 2019-03-13 17:38:38 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.480: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 13 22:35:45.480: INFO: kubernetes-events-printer-75755d587-rcrrf from ntnx-logging started at 2019-03-13 17:38:39 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.480: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Mar 13 22:35:45.480: INFO: elasticsearch-logging-0 from ntnx-logging started at 2019-03-13 17:38:51 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.480: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Mar 13 22:35:45.480: INFO: prometheus-operator-d58cfc597-rr4rv from monitoring started at 2019-03-13 17:41:11 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.480: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 13 22:35:45.480: INFO: alertmanager-main-0 from monitoring started at 2019-03-13 17:41:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.480: INFO: 	Container alertmanager ready: true, restart count 0
Mar 13 22:35:45.480: INFO: 	Container config-reloader ready: true, restart count 0
Mar 13 22:35:45.480: INFO: kube-proxy-ds-x52pl from kube-system started at 2019-03-13 17:37:58 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.480: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 13 22:35:45.480: INFO: kube-flannel-ds-8xdq9 from kube-system started at 2019-03-13 17:38:02 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.480: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 22:35:45.480: INFO: node-exporter-2prk8 from monitoring started at 2019-03-13 17:41:11 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 13 22:35:45.480: INFO: 	Container node-exporter ready: true, restart count 0
Mar 13 22:35:45.480: INFO: kube-state-metrics-54996cf64c-gdfqp from monitoring started at 2019-03-13 17:41:22 +0000 UTC (4 container statuses recorded)
Mar 13 22:35:45.480: INFO: 	Container addon-resizer ready: true, restart count 0
Mar 13 22:35:45.480: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 13 22:35:45.480: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 13 22:35:45.480: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 13 22:35:45.480: INFO: sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-c2w76 from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.480: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 13 22:35:45.480: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 22:35:45.481: INFO: 
Logging pods the kubelet thinks is on node karbon-test-3a0ff6-k8s-worker-2 before test
Mar 13 22:35:45.486: INFO: kube-proxy-ds-64bb6 from kube-system started at 2019-03-13 17:37:58 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.486: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 13 22:35:45.486: INFO: csi-provisioner-ntnx-plugin-0 from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.486: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 13 22:35:45.486: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Mar 13 22:35:45.486: INFO: fluent-bit-q2plf from kube-system started at 2019-03-13 17:38:38 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.486: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 13 22:35:45.486: INFO: node-exporter-wnrgc from monitoring started at 2019-03-13 17:41:11 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.486: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 13 22:35:45.486: INFO: 	Container node-exporter ready: true, restart count 0
Mar 13 22:35:45.486: INFO: prometheus-k8s-1 from monitoring started at 2019-03-13 17:42:18 +0000 UTC (3 container statuses recorded)
Mar 13 22:35:45.486: INFO: 	Container prometheus ready: true, restart count 1
Mar 13 22:35:45.486: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 13 22:35:45.486: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 13 22:35:45.486: INFO: kube-flannel-ds-gcljm from kube-system started at 2019-03-13 17:38:02 +0000 UTC (1 container statuses recorded)
Mar 13 22:35:45.486: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 22:35:45.486: INFO: csi-node-ntnx-plugin-9kv8f from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.486: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Mar 13 22:35:45.486: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 13 22:35:45.486: INFO: kibana-logging-5d46457978-l2xgl from ntnx-logging started at 2019-03-13 17:38:39 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.486: INFO: 	Container kibana-logging ready: true, restart count 0
Mar 13 22:35:45.486: INFO: 	Container nginxhttp ready: true, restart count 0
Mar 13 22:35:45.486: INFO: alertmanager-main-1 from monitoring started at 2019-03-13 17:41:30 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.486: INFO: 	Container alertmanager ready: true, restart count 0
Mar 13 22:35:45.486: INFO: 	Container config-reloader ready: true, restart count 0
Mar 13 22:35:45.486: INFO: sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-vtjz7 from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 22:35:45.486: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 13 22:35:45.486: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node karbon-test-3a0ff6-k8s-worker-0
STEP: verifying the node has the label node karbon-test-3a0ff6-k8s-worker-1
STEP: verifying the node has the label node karbon-test-3a0ff6-k8s-worker-2
Mar 13 22:35:45.550: INFO: Pod sonobuoy requesting resource cpu=0m on Node karbon-test-3a0ff6-k8s-worker-0
Mar 13 22:35:45.550: INFO: Pod sonobuoy-e2e-job-f2ba8fc748ec406a requesting resource cpu=0m on Node karbon-test-3a0ff6-k8s-worker-0
Mar 13 22:35:45.550: INFO: Pod sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-8jqgg requesting resource cpu=0m on Node karbon-test-3a0ff6-k8s-worker-0
Mar 13 22:35:45.550: INFO: Pod sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-c2w76 requesting resource cpu=0m on Node karbon-test-3a0ff6-k8s-worker-1
Mar 13 22:35:45.550: INFO: Pod sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-vtjz7 requesting resource cpu=0m on Node karbon-test-3a0ff6-k8s-worker-2
Mar 13 22:35:45.550: INFO: Pod csi-attacher-ntnx-plugin-0 requesting resource cpu=200m on Node karbon-test-3a0ff6-k8s-worker-0
Mar 13 22:35:45.550: INFO: Pod csi-node-ntnx-plugin-9kv8f requesting resource cpu=200m on Node karbon-test-3a0ff6-k8s-worker-2
Mar 13 22:35:45.550: INFO: Pod csi-node-ntnx-plugin-v29vq requesting resource cpu=200m on Node karbon-test-3a0ff6-k8s-worker-1
Mar 13 22:35:45.550: INFO: Pod csi-node-ntnx-plugin-v9pcq requesting resource cpu=200m on Node karbon-test-3a0ff6-k8s-worker-0
Mar 13 22:35:45.550: INFO: Pod csi-provisioner-ntnx-plugin-0 requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-2
Mar 13 22:35:45.550: INFO: Pod fluent-bit-q2plf requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-2
Mar 13 22:35:45.550: INFO: Pod fluent-bit-x4t2v requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-1
Mar 13 22:35:45.550: INFO: Pod fluent-bit-zsqpg requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-0
Mar 13 22:35:45.550: INFO: Pod kube-flannel-ds-25rft requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-0
Mar 13 22:35:45.550: INFO: Pod kube-flannel-ds-8xdq9 requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-1
Mar 13 22:35:45.550: INFO: Pod kube-flannel-ds-gcljm requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-2
Mar 13 22:35:45.550: INFO: Pod kube-proxy-ds-64bb6 requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-2
Mar 13 22:35:45.550: INFO: Pod kube-proxy-ds-bqvcl requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-0
Mar 13 22:35:45.551: INFO: Pod kube-proxy-ds-x52pl requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-1
Mar 13 22:35:45.551: INFO: Pod alertmanager-main-0 requesting resource cpu=105m on Node karbon-test-3a0ff6-k8s-worker-1
Mar 13 22:35:45.551: INFO: Pod alertmanager-main-1 requesting resource cpu=105m on Node karbon-test-3a0ff6-k8s-worker-2
Mar 13 22:35:45.551: INFO: Pod kube-state-metrics-54996cf64c-gdfqp requesting resource cpu=158m on Node karbon-test-3a0ff6-k8s-worker-1
Mar 13 22:35:45.551: INFO: Pod node-exporter-2prk8 requesting resource cpu=112m on Node karbon-test-3a0ff6-k8s-worker-1
Mar 13 22:35:45.551: INFO: Pod node-exporter-l8bv5 requesting resource cpu=112m on Node karbon-test-3a0ff6-k8s-worker-0
Mar 13 22:35:45.551: INFO: Pod node-exporter-wnrgc requesting resource cpu=112m on Node karbon-test-3a0ff6-k8s-worker-2
Mar 13 22:35:45.551: INFO: Pod prometheus-k8s-0 requesting resource cpu=215m on Node karbon-test-3a0ff6-k8s-worker-0
Mar 13 22:35:45.551: INFO: Pod prometheus-k8s-1 requesting resource cpu=215m on Node karbon-test-3a0ff6-k8s-worker-2
Mar 13 22:35:45.551: INFO: Pod prometheus-operator-d58cfc597-rr4rv requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-1
Mar 13 22:35:45.551: INFO: Pod elasticsearch-logging-0 requesting resource cpu=500m on Node karbon-test-3a0ff6-k8s-worker-1
Mar 13 22:35:45.551: INFO: Pod kibana-logging-5d46457978-l2xgl requesting resource cpu=200m on Node karbon-test-3a0ff6-k8s-worker-2
Mar 13 22:35:45.551: INFO: Pod kubernetes-events-printer-75755d587-rcrrf requesting resource cpu=100m on Node karbon-test-3a0ff6-k8s-worker-1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-578f4bef-45e0-11e9-8b9c-0a58ac140107.158ba59280a5745e], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-p7fts/filler-pod-578f4bef-45e0-11e9-8b9c-0a58ac140107 to karbon-test-3a0ff6-k8s-worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-578f4bef-45e0-11e9-8b9c-0a58ac140107.158ba59324702db3], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-578f4bef-45e0-11e9-8b9c-0a58ac140107.158ba59326c23a9c], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-578f4bef-45e0-11e9-8b9c-0a58ac140107.158ba5932e7bd6a5], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57900598-45e0-11e9-8b9c-0a58ac140107.158ba592811627d1], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-p7fts/filler-pod-57900598-45e0-11e9-8b9c-0a58ac140107 to karbon-test-3a0ff6-k8s-worker-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57900598-45e0-11e9-8b9c-0a58ac140107.158ba5932ff95563], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57900598-45e0-11e9-8b9c-0a58ac140107.158ba593322f2e1f], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57900598-45e0-11e9-8b9c-0a58ac140107.158ba5933a4093ca], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57907a80-45e0-11e9-8b9c-0a58ac140107.158ba59281465ca9], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-p7fts/filler-pod-57907a80-45e0-11e9-8b9c-0a58ac140107 to karbon-test-3a0ff6-k8s-worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57907a80-45e0-11e9-8b9c-0a58ac140107.158ba592fa4124d3], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57907a80-45e0-11e9-8b9c-0a58ac140107.158ba592fcddbe5a], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57907a80-45e0-11e9-8b9c-0a58ac140107.158ba59304aa36b3], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.158ba59370883003], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node karbon-test-3a0ff6-k8s-worker-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node karbon-test-3a0ff6-k8s-worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node karbon-test-3a0ff6-k8s-worker-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:35:50.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-p7fts" for this suite.
Mar 13 22:35:56.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:35:56.685: INFO: namespace: e2e-tests-sched-pred-p7fts, resource: bindings, ignored listing per whitelist
Mar 13 22:35:56.700: INFO: namespace e2e-tests-sched-pred-p7fts deletion completed in 6.07384944s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.280 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:35:56.700: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 22:35:56.747: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e3b10fe-45e0-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-wxwvx" to be "success or failure"
Mar 13 22:35:56.749: INFO: Pod "downwardapi-volume-5e3b10fe-45e0-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.646622ms
Mar 13 22:35:58.752: INFO: Pod "downwardapi-volume-5e3b10fe-45e0-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004554186s
Mar 13 22:36:00.755: INFO: Pod "downwardapi-volume-5e3b10fe-45e0-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008008041s
STEP: Saw pod success
Mar 13 22:36:00.755: INFO: Pod "downwardapi-volume-5e3b10fe-45e0-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:36:00.757: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod downwardapi-volume-5e3b10fe-45e0-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 22:36:00.773: INFO: Waiting for pod downwardapi-volume-5e3b10fe-45e0-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:36:00.775: INFO: Pod downwardapi-volume-5e3b10fe-45e0-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:36:00.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wxwvx" for this suite.
Mar 13 22:36:06.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:36:06.830: INFO: namespace: e2e-tests-downward-api-wxwvx, resource: bindings, ignored listing per whitelist
Mar 13 22:36:06.854: INFO: namespace e2e-tests-downward-api-wxwvx deletion completed in 6.075636913s

• [SLOW TEST:10.154 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:36:06.854: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 13 22:36:06.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-gjrzf'
Mar 13 22:36:06.996: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 13 22:36:06.996: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Mar 13 22:36:07.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-gjrzf'
Mar 13 22:36:07.100: INFO: stderr: ""
Mar 13 22:36:07.100: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:36:07.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gjrzf" for this suite.
Mar 13 22:36:29.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:36:29.176: INFO: namespace: e2e-tests-kubectl-gjrzf, resource: bindings, ignored listing per whitelist
Mar 13 22:36:29.180: INFO: namespace e2e-tests-kubectl-gjrzf deletion completed in 22.077351178s

• [SLOW TEST:22.326 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:36:29.181: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 22:36:29.228: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 13 22:36:34.231: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 13 22:36:34.232: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 13 22:36:36.235: INFO: Creating deployment "test-rollover-deployment"
Mar 13 22:36:36.243: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 13 22:36:38.249: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 13 22:36:38.253: INFO: Ensure that both replica sets have 1 created replica
Mar 13 22:36:38.257: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 13 22:36:38.262: INFO: Updating deployment test-rollover-deployment
Mar 13 22:36:38.262: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 13 22:36:40.267: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 13 22:36:40.270: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 13 22:36:40.274: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 22:36:40.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113397, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 22:36:42.279: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 22:36:42.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 22:36:44.279: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 22:36:44.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 22:36:46.280: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 22:36:46.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 22:36:48.281: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 22:36:48.281: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 22:36:50.280: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 22:36:50.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 22:36:52.280: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 22:36:52.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 22:36:54.280: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 22:36:54.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 22:36:56.280: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 22:36:56.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 22:36:58.280: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 22:36:58.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 22:37:00.279: INFO: all replica sets need to contain the pod-template-hash label
Mar 13 22:37:00.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113400, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688113395, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 22:37:02.280: INFO: 
Mar 13 22:37:02.280: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 13 22:37:02.287: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-qcszg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qcszg/deployments/test-rollover-deployment,UID:758ebef7-45e0-11e9-b6d3-506b8df34f96,ResourceVersion:31295,Generation:2,CreationTimestamp:2019-03-13 22:36:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-13 22:36:35 +0000 UTC 2019-03-13 22:36:35 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-13 22:37:00 +0000 UTC 2019-03-13 22:36:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar 13 22:37:02.289: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-qcszg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qcszg/replicasets/test-rollover-deployment-6b7f9d6597,UID:76c40401-45e0-11e9-b6d3-506b8df34f96,ResourceVersion:31286,Generation:2,CreationTimestamp:2019-03-13 22:36:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 758ebef7-45e0-11e9-b6d3-506b8df34f96 0xc001bbab27 0xc001bbab28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar 13 22:37:02.289: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 13 22:37:02.289: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-qcszg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qcszg/replicasets/test-rollover-controller,UID:7160ce4b-45e0-11e9-b6d3-506b8df34f96,ResourceVersion:31294,Generation:2,CreationTimestamp:2019-03-13 22:36:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 758ebef7-45e0-11e9-b6d3-506b8df34f96 0xc001bba997 0xc001bba998}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 13 22:37:02.290: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-qcszg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qcszg/replicasets/test-rollover-deployment-6586df867b,UID:75906026-45e0-11e9-b6d3-506b8df34f96,ResourceVersion:31241,Generation:2,CreationTimestamp:2019-03-13 22:36:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 758ebef7-45e0-11e9-b6d3-506b8df34f96 0xc001bbaa57 0xc001bbaa58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 13 22:37:02.292: INFO: Pod "test-rollover-deployment-6b7f9d6597-d8wc7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-d8wc7,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-qcszg,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qcszg/pods/test-rollover-deployment-6b7f9d6597-d8wc7,UID:76c670bd-45e0-11e9-b6d3-506b8df34f96,ResourceVersion:31253,Generation:0,CreationTimestamp:2019-03-13 22:36:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 76c40401-45e0-11e9-b6d3-506b8df34f96 0xc001bbb667 0xc001bbb668}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rh5nl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rh5nl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-rh5nl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:36:38 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:36:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:36:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:36:37 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.216,PodIP:172.20.1.45,StartTime:2019-03-13 22:36:38 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-13 22:36:40 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://c9fe07f68d153a7b1c05e744d2c7217c795ee5d92517ff6b360d25d3eb095b1c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:37:02.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-qcszg" for this suite.
Mar 13 22:37:08.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:37:08.324: INFO: namespace: e2e-tests-deployment-qcszg, resource: bindings, ignored listing per whitelist
Mar 13 22:37:08.362: INFO: namespace e2e-tests-deployment-qcszg deletion completed in 6.067578862s

• [SLOW TEST:39.181 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:37:08.362: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-88f159d2-45e0-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 22:37:08.407: INFO: Waiting up to 5m0s for pod "pod-configmaps-88f1a787-45e0-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-configmap-9jc44" to be "success or failure"
Mar 13 22:37:08.409: INFO: Pod "pod-configmaps-88f1a787-45e0-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.690708ms
Mar 13 22:37:10.412: INFO: Pod "pod-configmaps-88f1a787-45e0-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005151213s
STEP: Saw pod success
Mar 13 22:37:10.412: INFO: Pod "pod-configmaps-88f1a787-45e0-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:37:10.414: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-configmaps-88f1a787-45e0-11e9-8b9c-0a58ac140107 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 22:37:10.425: INFO: Waiting for pod pod-configmaps-88f1a787-45e0-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:37:10.427: INFO: Pod pod-configmaps-88f1a787-45e0-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:37:10.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9jc44" for this suite.
Mar 13 22:37:16.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:37:16.468: INFO: namespace: e2e-tests-configmap-9jc44, resource: bindings, ignored listing per whitelist
Mar 13 22:37:16.498: INFO: namespace e2e-tests-configmap-9jc44 deletion completed in 6.068267175s

• [SLOW TEST:8.135 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:37:16.498: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:37:22.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-rw7t6" for this suite.
Mar 13 22:37:28.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:37:28.611: INFO: namespace: e2e-tests-emptydir-wrapper-rw7t6, resource: bindings, ignored listing per whitelist
Mar 13 22:37:28.641: INFO: namespace e2e-tests-emptydir-wrapper-rw7t6 deletion completed in 6.064898515s

• [SLOW TEST:12.143 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:37:28.641: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:37:55.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-4bbbk" for this suite.
Mar 13 22:38:01.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:38:01.893: INFO: namespace: e2e-tests-container-runtime-4bbbk, resource: bindings, ignored listing per whitelist
Mar 13 22:38:01.938: INFO: namespace e2e-tests-container-runtime-4bbbk deletion completed in 6.079626251s

• [SLOW TEST:33.297 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:38:01.938: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-8vx2m/configmap-test-a8e0cbd6-45e0-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 22:38:01.986: INFO: Waiting up to 5m0s for pod "pod-configmaps-a8e11e14-45e0-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-configmap-8vx2m" to be "success or failure"
Mar 13 22:38:01.987: INFO: Pod "pod-configmaps-a8e11e14-45e0-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.596922ms
Mar 13 22:38:03.990: INFO: Pod "pod-configmaps-a8e11e14-45e0-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00441418s
Mar 13 22:38:05.994: INFO: Pod "pod-configmaps-a8e11e14-45e0-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007835617s
STEP: Saw pod success
Mar 13 22:38:05.994: INFO: Pod "pod-configmaps-a8e11e14-45e0-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:38:05.995: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-configmaps-a8e11e14-45e0-11e9-8b9c-0a58ac140107 container env-test: <nil>
STEP: delete the pod
Mar 13 22:38:06.015: INFO: Waiting for pod pod-configmaps-a8e11e14-45e0-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:38:06.019: INFO: Pod pod-configmaps-a8e11e14-45e0-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:38:06.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8vx2m" for this suite.
Mar 13 22:38:12.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:38:12.080: INFO: namespace: e2e-tests-configmap-8vx2m, resource: bindings, ignored listing per whitelist
Mar 13 22:38:12.099: INFO: namespace e2e-tests-configmap-8vx2m deletion completed in 6.077439162s

• [SLOW TEST:10.161 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:38:12.099: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-r46wj
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 13 22:38:12.145: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 13 22:38:38.204: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.33:8080/dial?request=hostName&protocol=udp&host=172.20.2.32&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-r46wj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:38:38.205: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:38:38.282: INFO: Waiting for endpoints: map[]
Mar 13 22:38:38.285: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.33:8080/dial?request=hostName&protocol=udp&host=172.20.3.35&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-r46wj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:38:38.285: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:38:38.362: INFO: Waiting for endpoints: map[]
Mar 13 22:38:38.365: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.33:8080/dial?request=hostName&protocol=udp&host=172.20.1.48&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-r46wj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:38:38.365: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:38:38.440: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:38:38.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-r46wj" for this suite.
Mar 13 22:39:00.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:39:00.483: INFO: namespace: e2e-tests-pod-network-test-r46wj, resource: bindings, ignored listing per whitelist
Mar 13 22:39:00.520: INFO: namespace e2e-tests-pod-network-test-r46wj deletion completed in 22.075116302s

• [SLOW TEST:48.421 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:39:00.520: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 22:39:00.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbcd0ab1-45e0-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-khfw8" to be "success or failure"
Mar 13 22:39:00.578: INFO: Pod "downwardapi-volume-cbcd0ab1-45e0-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.776558ms
Mar 13 22:39:02.581: INFO: Pod "downwardapi-volume-cbcd0ab1-45e0-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004932063s
Mar 13 22:39:04.585: INFO: Pod "downwardapi-volume-cbcd0ab1-45e0-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008373339s
STEP: Saw pod success
Mar 13 22:39:04.585: INFO: Pod "downwardapi-volume-cbcd0ab1-45e0-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:39:04.587: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downwardapi-volume-cbcd0ab1-45e0-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 22:39:04.601: INFO: Waiting for pod downwardapi-volume-cbcd0ab1-45e0-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:39:04.602: INFO: Pod downwardapi-volume-cbcd0ab1-45e0-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:39:04.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-khfw8" for this suite.
Mar 13 22:39:10.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:39:10.642: INFO: namespace: e2e-tests-projected-khfw8, resource: bindings, ignored listing per whitelist
Mar 13 22:39:10.676: INFO: namespace e2e-tests-projected-khfw8 deletion completed in 6.071289914s

• [SLOW TEST:10.157 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:39:10.676: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-d2znl
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 13 22:39:10.721: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 13 22:39:32.779: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.2.35:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-d2znl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:39:32.779: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:39:32.855: INFO: Found all expected endpoints: [netserver-0]
Mar 13 22:39:32.857: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.1.49:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-d2znl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:39:32.857: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:39:32.932: INFO: Found all expected endpoints: [netserver-1]
Mar 13 22:39:32.933: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.3.36:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-d2znl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 22:39:32.933: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 22:39:33.010: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:39:33.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-d2znl" for this suite.
Mar 13 22:39:55.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:39:55.034: INFO: namespace: e2e-tests-pod-network-test-d2znl, resource: bindings, ignored listing per whitelist
Mar 13 22:39:55.091: INFO: namespace e2e-tests-pod-network-test-d2znl deletion completed in 22.077765763s

• [SLOW TEST:44.415 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:39:55.091: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-ec5bc884-45e0-11e9-8b9c-0a58ac140107
STEP: Creating secret with name s-test-opt-upd-ec5bc8cf-45e0-11e9-8b9c-0a58ac140107
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ec5bc884-45e0-11e9-8b9c-0a58ac140107
STEP: Updating secret s-test-opt-upd-ec5bc8cf-45e0-11e9-8b9c-0a58ac140107
STEP: Creating secret with name s-test-opt-create-ec5bc8e5-45e0-11e9-8b9c-0a58ac140107
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:39:59.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rzjhn" for this suite.
Mar 13 22:40:21.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:40:21.356: INFO: namespace: e2e-tests-projected-rzjhn, resource: bindings, ignored listing per whitelist
Mar 13 22:40:21.364: INFO: namespace e2e-tests-projected-rzjhn deletion completed in 22.071499198s

• [SLOW TEST:26.272 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:40:21.364: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar 13 22:40:21.409: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:40:25.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-8jdsz" for this suite.
Mar 13 22:40:31.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:40:31.080: INFO: namespace: e2e-tests-init-container-8jdsz, resource: bindings, ignored listing per whitelist
Mar 13 22:40:31.131: INFO: namespace e2e-tests-init-container-8jdsz deletion completed in 6.068972242s

• [SLOW TEST:9.767 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:40:31.131: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar 13 22:40:35.712: INFO: Successfully updated pod "labelsupdate01cfb191-45e1-11e9-8b9c-0a58ac140107"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:40:39.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xxdx8" for this suite.
Mar 13 22:41:01.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:41:01.802: INFO: namespace: e2e-tests-projected-xxdx8, resource: bindings, ignored listing per whitelist
Mar 13 22:41:01.815: INFO: namespace e2e-tests-projected-xxdx8 deletion completed in 22.074656314s

• [SLOW TEST:30.684 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:41:01.815: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Mar 13 22:41:05.888: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:41:29.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-8wwxf" for this suite.
Mar 13 22:41:35.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:41:35.958: INFO: namespace: e2e-tests-namespaces-8wwxf, resource: bindings, ignored listing per whitelist
Mar 13 22:41:35.992: INFO: namespace e2e-tests-namespaces-8wwxf deletion completed in 6.072113981s
STEP: Destroying namespace "e2e-tests-nsdeletetest-4g9j2" for this suite.
Mar 13 22:41:35.994: INFO: Namespace e2e-tests-nsdeletetest-4g9j2 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-jk6nc" for this suite.
Mar 13 22:41:42.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:41:42.019: INFO: namespace: e2e-tests-nsdeletetest-jk6nc, resource: bindings, ignored listing per whitelist
Mar 13 22:41:42.073: INFO: namespace e2e-tests-nsdeletetest-jk6nc deletion completed in 6.079471494s

• [SLOW TEST:40.258 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:41:42.073: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 13 22:41:46.645: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2c17c1b4-45e1-11e9-8b9c-0a58ac140107"
Mar 13 22:41:46.645: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2c17c1b4-45e1-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-pods-l2xcg" to be "terminated due to deadline exceeded"
Mar 13 22:41:46.647: INFO: Pod "pod-update-activedeadlineseconds-2c17c1b4-45e1-11e9-8b9c-0a58ac140107": Phase="Running", Reason="", readiness=true. Elapsed: 2.159757ms
Mar 13 22:41:48.652: INFO: Pod "pod-update-activedeadlineseconds-2c17c1b4-45e1-11e9-8b9c-0a58ac140107": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.006418305s
Mar 13 22:41:48.652: INFO: Pod "pod-update-activedeadlineseconds-2c17c1b4-45e1-11e9-8b9c-0a58ac140107" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:41:48.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-l2xcg" for this suite.
Mar 13 22:41:54.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:41:54.676: INFO: namespace: e2e-tests-pods-l2xcg, resource: bindings, ignored listing per whitelist
Mar 13 22:41:54.731: INFO: namespace e2e-tests-pods-l2xcg deletion completed in 6.075993211s

• [SLOW TEST:12.658 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:41:54.731: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-j6fkd/secret-test-33a287bb-45e1-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 22:41:54.782: INFO: Waiting up to 5m0s for pod "pod-configmaps-33a2e08e-45e1-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-secrets-j6fkd" to be "success or failure"
Mar 13 22:41:54.784: INFO: Pod "pod-configmaps-33a2e08e-45e1-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.885951ms
Mar 13 22:41:56.787: INFO: Pod "pod-configmaps-33a2e08e-45e1-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005739166s
Mar 13 22:41:58.790: INFO: Pod "pod-configmaps-33a2e08e-45e1-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008278749s
STEP: Saw pod success
Mar 13 22:41:58.790: INFO: Pod "pod-configmaps-33a2e08e-45e1-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:41:58.792: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-configmaps-33a2e08e-45e1-11e9-8b9c-0a58ac140107 container env-test: <nil>
STEP: delete the pod
Mar 13 22:41:58.807: INFO: Waiting for pod pod-configmaps-33a2e08e-45e1-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:41:58.810: INFO: Pod pod-configmaps-33a2e08e-45e1-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:41:58.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-j6fkd" for this suite.
Mar 13 22:42:04.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:42:04.864: INFO: namespace: e2e-tests-secrets-j6fkd, resource: bindings, ignored listing per whitelist
Mar 13 22:42:04.883: INFO: namespace e2e-tests-secrets-j6fkd deletion completed in 6.069859043s

• [SLOW TEST:10.152 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:42:04.883: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-zl4ch/configmap-test-39ae8e09-45e1-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 22:42:04.927: INFO: Waiting up to 5m0s for pod "pod-configmaps-39aeecea-45e1-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-configmap-zl4ch" to be "success or failure"
Mar 13 22:42:04.930: INFO: Pod "pod-configmaps-39aeecea-45e1-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 3.136278ms
Mar 13 22:42:06.933: INFO: Pod "pod-configmaps-39aeecea-45e1-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006095207s
STEP: Saw pod success
Mar 13 22:42:06.933: INFO: Pod "pod-configmaps-39aeecea-45e1-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:42:06.935: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-configmaps-39aeecea-45e1-11e9-8b9c-0a58ac140107 container env-test: <nil>
STEP: delete the pod
Mar 13 22:42:06.954: INFO: Waiting for pod pod-configmaps-39aeecea-45e1-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:42:06.956: INFO: Pod pod-configmaps-39aeecea-45e1-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:42:06.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zl4ch" for this suite.
Mar 13 22:42:12.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:42:13.036: INFO: namespace: e2e-tests-configmap-zl4ch, resource: bindings, ignored listing per whitelist
Mar 13 22:42:13.036: INFO: namespace e2e-tests-configmap-zl4ch deletion completed in 6.076174612s

• [SLOW TEST:8.153 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:42:13.036: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar 13 22:42:13.091: INFO: Waiting up to 5m0s for pod "downward-api-3e8c844d-45e1-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-n6wpv" to be "success or failure"
Mar 13 22:42:13.093: INFO: Pod "downward-api-3e8c844d-45e1-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.985371ms
Mar 13 22:42:15.096: INFO: Pod "downward-api-3e8c844d-45e1-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005373642s
Mar 13 22:42:17.099: INFO: Pod "downward-api-3e8c844d-45e1-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008119248s
STEP: Saw pod success
Mar 13 22:42:17.099: INFO: Pod "downward-api-3e8c844d-45e1-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:42:17.101: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downward-api-3e8c844d-45e1-11e9-8b9c-0a58ac140107 container dapi-container: <nil>
STEP: delete the pod
Mar 13 22:42:17.117: INFO: Waiting for pod downward-api-3e8c844d-45e1-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:42:17.119: INFO: Pod downward-api-3e8c844d-45e1-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:42:17.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-n6wpv" for this suite.
Mar 13 22:42:23.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:42:23.191: INFO: namespace: e2e-tests-downward-api-n6wpv, resource: bindings, ignored listing per whitelist
Mar 13 22:42:23.195: INFO: namespace e2e-tests-downward-api-n6wpv deletion completed in 6.073692794s

• [SLOW TEST:10.160 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:42:23.195: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-449a8fb9-45e1-11e9-8b9c-0a58ac140107
STEP: Creating configMap with name cm-test-opt-upd-449a9011-45e1-11e9-8b9c-0a58ac140107
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-449a8fb9-45e1-11e9-8b9c-0a58ac140107
STEP: Updating configmap cm-test-opt-upd-449a9011-45e1-11e9-8b9c-0a58ac140107
STEP: Creating configMap with name cm-test-opt-create-449a9033-45e1-11e9-8b9c-0a58ac140107
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:43:53.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5brm2" for this suite.
Mar 13 22:44:15.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:44:15.716: INFO: namespace: e2e-tests-projected-5brm2, resource: bindings, ignored listing per whitelist
Mar 13 22:44:15.761: INFO: namespace e2e-tests-projected-5brm2 deletion completed in 22.074689548s

• [SLOW TEST:112.566 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:44:15.761: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-87b1324d-45e1-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 22:44:15.872: INFO: Waiting up to 5m0s for pod "pod-secrets-87bb6a24-45e1-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-secrets-kh2zw" to be "success or failure"
Mar 13 22:44:15.874: INFO: Pod "pod-secrets-87bb6a24-45e1-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.988015ms
Mar 13 22:44:17.877: INFO: Pod "pod-secrets-87bb6a24-45e1-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005361477s
Mar 13 22:44:19.880: INFO: Pod "pod-secrets-87bb6a24-45e1-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00842628s
STEP: Saw pod success
Mar 13 22:44:19.880: INFO: Pod "pod-secrets-87bb6a24-45e1-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:44:19.882: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-secrets-87bb6a24-45e1-11e9-8b9c-0a58ac140107 container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 22:44:19.895: INFO: Waiting for pod pod-secrets-87bb6a24-45e1-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:44:19.897: INFO: Pod pod-secrets-87bb6a24-45e1-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:44:19.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-kh2zw" for this suite.
Mar 13 22:44:25.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:44:25.934: INFO: namespace: e2e-tests-secrets-kh2zw, resource: bindings, ignored listing per whitelist
Mar 13 22:44:25.978: INFO: namespace e2e-tests-secrets-kh2zw deletion completed in 6.077475143s
STEP: Destroying namespace "e2e-tests-secret-namespace-cxqmk" for this suite.
Mar 13 22:44:31.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:44:32.038: INFO: namespace: e2e-tests-secret-namespace-cxqmk, resource: bindings, ignored listing per whitelist
Mar 13 22:44:32.054: INFO: namespace e2e-tests-secret-namespace-cxqmk deletion completed in 6.076617823s

• [SLOW TEST:16.293 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:44:32.054: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 13 22:44:32.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-29cd6'
Mar 13 22:44:32.387: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 13 22:44:32.387: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Mar 13 22:44:32.393: INFO: scanned /root for discovery docs: <nil>
Mar 13 22:44:32.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-29cd6'
Mar 13 22:44:48.151: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 13 22:44:48.151: INFO: stdout: "Created e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503\nScaling up e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar 13 22:44:48.151: INFO: stdout: "Created e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503\nScaling up e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar 13 22:44:48.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-29cd6'
Mar 13 22:44:48.236: INFO: stderr: ""
Mar 13 22:44:48.236: INFO: stdout: "e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503-kzlsx "
Mar 13 22:44:48.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503-kzlsx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-29cd6'
Mar 13 22:44:48.315: INFO: stderr: ""
Mar 13 22:44:48.315: INFO: stdout: "true"
Mar 13 22:44:48.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503-kzlsx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-29cd6'
Mar 13 22:44:48.385: INFO: stderr: ""
Mar 13 22:44:48.385: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Mar 13 22:44:48.385: INFO: e2e-test-nginx-rc-0d8a30ef9df162b74a866353d5d21503-kzlsx is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Mar 13 22:44:48.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-29cd6'
Mar 13 22:44:48.466: INFO: stderr: ""
Mar 13 22:44:48.466: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:44:48.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-29cd6" for this suite.
Mar 13 22:44:54.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:44:54.547: INFO: namespace: e2e-tests-kubectl-29cd6, resource: bindings, ignored listing per whitelist
Mar 13 22:44:54.553: INFO: namespace e2e-tests-kubectl-29cd6 deletion completed in 6.082881743s

• [SLOW TEST:22.499 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:44:54.553: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-522x
STEP: Creating a pod to test atomic-volume-subpath
Mar 13 22:44:54.607: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-522x" in namespace "e2e-tests-subpath-42phx" to be "success or failure"
Mar 13 22:44:54.610: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.62337ms
Mar 13 22:44:56.613: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006193909s
Mar 13 22:44:58.616: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008744798s
Mar 13 22:45:00.619: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Running", Reason="", readiness=false. Elapsed: 6.012001157s
Mar 13 22:45:02.622: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Running", Reason="", readiness=false. Elapsed: 8.015183328s
Mar 13 22:45:04.626: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Running", Reason="", readiness=false. Elapsed: 10.018823649s
Mar 13 22:45:06.629: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Running", Reason="", readiness=false. Elapsed: 12.022287721s
Mar 13 22:45:08.632: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Running", Reason="", readiness=false. Elapsed: 14.025161626s
Mar 13 22:45:10.635: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Running", Reason="", readiness=false. Elapsed: 16.028536154s
Mar 13 22:45:12.639: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Running", Reason="", readiness=false. Elapsed: 18.031859694s
Mar 13 22:45:14.642: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Running", Reason="", readiness=false. Elapsed: 20.035345211s
Mar 13 22:45:16.645: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Running", Reason="", readiness=false. Elapsed: 22.038478644s
Mar 13 22:45:18.649: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Running", Reason="", readiness=false. Elapsed: 24.04191442s
Mar 13 22:45:20.652: INFO: Pod "pod-subpath-test-configmap-522x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.045199609s
STEP: Saw pod success
Mar 13 22:45:20.652: INFO: Pod "pod-subpath-test-configmap-522x" satisfied condition "success or failure"
Mar 13 22:45:20.654: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-subpath-test-configmap-522x container test-container-subpath-configmap-522x: <nil>
STEP: delete the pod
Mar 13 22:45:20.667: INFO: Waiting for pod pod-subpath-test-configmap-522x to disappear
Mar 13 22:45:20.670: INFO: Pod pod-subpath-test-configmap-522x no longer exists
STEP: Deleting pod pod-subpath-test-configmap-522x
Mar 13 22:45:20.670: INFO: Deleting pod "pod-subpath-test-configmap-522x" in namespace "e2e-tests-subpath-42phx"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:45:20.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-42phx" for this suite.
Mar 13 22:45:26.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:45:26.747: INFO: namespace: e2e-tests-subpath-42phx, resource: bindings, ignored listing per whitelist
Mar 13 22:45:26.747: INFO: namespace e2e-tests-subpath-42phx deletion completed in 6.072536599s

• [SLOW TEST:32.193 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:45:26.747: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 22:45:26.790: INFO: Creating ReplicaSet my-hostname-basic-b2015b9c-45e1-11e9-8b9c-0a58ac140107
Mar 13 22:45:26.794: INFO: Pod name my-hostname-basic-b2015b9c-45e1-11e9-8b9c-0a58ac140107: Found 0 pods out of 1
Mar 13 22:45:31.798: INFO: Pod name my-hostname-basic-b2015b9c-45e1-11e9-8b9c-0a58ac140107: Found 1 pods out of 1
Mar 13 22:45:31.798: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b2015b9c-45e1-11e9-8b9c-0a58ac140107" is running
Mar 13 22:45:31.800: INFO: Pod "my-hostname-basic-b2015b9c-45e1-11e9-8b9c-0a58ac140107-tlqr8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-13 22:45:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-13 22:45:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-13 22:45:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-13 22:45:26 +0000 UTC Reason: Message:}])
Mar 13 22:45:31.800: INFO: Trying to dial the pod
Mar 13 22:45:36.808: INFO: Controller my-hostname-basic-b2015b9c-45e1-11e9-8b9c-0a58ac140107: Got expected result from replica 1 [my-hostname-basic-b2015b9c-45e1-11e9-8b9c-0a58ac140107-tlqr8]: "my-hostname-basic-b2015b9c-45e1-11e9-8b9c-0a58ac140107-tlqr8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:45:36.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-g8qrd" for this suite.
Mar 13 22:45:42.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:45:42.834: INFO: namespace: e2e-tests-replicaset-g8qrd, resource: bindings, ignored listing per whitelist
Mar 13 22:45:42.883: INFO: namespace e2e-tests-replicaset-g8qrd deletion completed in 6.071912697s

• [SLOW TEST:16.137 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:45:42.884: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Mar 13 22:45:42.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:45:43.115: INFO: stderr: ""
Mar 13 22:45:43.115: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 22:45:43.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:45:43.196: INFO: stderr: ""
Mar 13 22:45:43.196: INFO: stdout: "update-demo-nautilus-nh9dq update-demo-nautilus-vnv7s "
Mar 13 22:45:43.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-nh9dq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:45:43.267: INFO: stderr: ""
Mar 13 22:45:43.267: INFO: stdout: ""
Mar 13 22:45:43.267: INFO: update-demo-nautilus-nh9dq is created but not running
Mar 13 22:45:48.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:45:48.355: INFO: stderr: ""
Mar 13 22:45:48.355: INFO: stdout: "update-demo-nautilus-nh9dq update-demo-nautilus-vnv7s "
Mar 13 22:45:48.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-nh9dq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:45:48.436: INFO: stderr: ""
Mar 13 22:45:48.436: INFO: stdout: "true"
Mar 13 22:45:48.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-nh9dq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:45:48.514: INFO: stderr: ""
Mar 13 22:45:48.514: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 22:45:48.514: INFO: validating pod update-demo-nautilus-nh9dq
Mar 13 22:45:48.518: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 22:45:48.518: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 22:45:48.518: INFO: update-demo-nautilus-nh9dq is verified up and running
Mar 13 22:45:48.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-vnv7s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:45:48.600: INFO: stderr: ""
Mar 13 22:45:48.600: INFO: stdout: "true"
Mar 13 22:45:48.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-nautilus-vnv7s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:45:48.680: INFO: stderr: ""
Mar 13 22:45:48.680: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 13 22:45:48.680: INFO: validating pod update-demo-nautilus-vnv7s
Mar 13 22:45:48.689: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 13 22:45:48.689: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 13 22:45:48.689: INFO: update-demo-nautilus-vnv7s is verified up and running
STEP: rolling-update to new replication controller
Mar 13 22:45:48.692: INFO: scanned /root for discovery docs: <nil>
Mar 13 22:45:48.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:46:12.048: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 13 22:46:12.048: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 13 22:46:12.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:46:12.136: INFO: stderr: ""
Mar 13 22:46:12.137: INFO: stdout: "update-demo-kitten-2tfs8 update-demo-kitten-6mmf7 "
Mar 13 22:46:12.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-kitten-2tfs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:46:12.218: INFO: stderr: ""
Mar 13 22:46:12.219: INFO: stdout: "true"
Mar 13 22:46:12.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-kitten-2tfs8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:46:12.298: INFO: stderr: ""
Mar 13 22:46:12.298: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 13 22:46:12.298: INFO: validating pod update-demo-kitten-2tfs8
Mar 13 22:46:12.302: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 13 22:46:12.302: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 13 22:46:12.302: INFO: update-demo-kitten-2tfs8 is verified up and running
Mar 13 22:46:12.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-kitten-6mmf7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:46:12.390: INFO: stderr: ""
Mar 13 22:46:12.390: INFO: stdout: "true"
Mar 13 22:46:12.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods update-demo-kitten-6mmf7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tjsnh'
Mar 13 22:46:12.471: INFO: stderr: ""
Mar 13 22:46:12.471: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 13 22:46:12.471: INFO: validating pod update-demo-kitten-6mmf7
Mar 13 22:46:12.475: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 13 22:46:12.475: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 13 22:46:12.475: INFO: update-demo-kitten-6mmf7 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:46:12.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tjsnh" for this suite.
Mar 13 22:46:34.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:46:34.544: INFO: namespace: e2e-tests-kubectl-tjsnh, resource: bindings, ignored listing per whitelist
Mar 13 22:46:34.549: INFO: namespace e2e-tests-kubectl-tjsnh deletion completed in 22.071857429s

• [SLOW TEST:51.666 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:46:34.550: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 13 22:46:34.598: INFO: Waiting up to 5m0s for pod "pod-da6b7682-45e1-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-5xkc2" to be "success or failure"
Mar 13 22:46:34.600: INFO: Pod "pod-da6b7682-45e1-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.665874ms
Mar 13 22:46:36.603: INFO: Pod "pod-da6b7682-45e1-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005207382s
Mar 13 22:46:38.607: INFO: Pod "pod-da6b7682-45e1-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00887235s
STEP: Saw pod success
Mar 13 22:46:38.607: INFO: Pod "pod-da6b7682-45e1-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:46:38.609: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-da6b7682-45e1-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 22:46:38.622: INFO: Waiting for pod pod-da6b7682-45e1-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:46:38.624: INFO: Pod pod-da6b7682-45e1-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:46:38.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5xkc2" for this suite.
Mar 13 22:46:44.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:46:44.672: INFO: namespace: e2e-tests-emptydir-5xkc2, resource: bindings, ignored listing per whitelist
Mar 13 22:46:44.687: INFO: namespace e2e-tests-emptydir-5xkc2 deletion completed in 6.060066479s

• [SLOW TEST:10.137 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:46:44.687: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-vwmkb in namespace e2e-tests-proxy-bcpwq
I0313 22:46:44.749408      19 runners.go:184] Created replication controller with name: proxy-service-vwmkb, namespace: e2e-tests-proxy-bcpwq, replica count: 1
I0313 22:46:45.799862      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0313 22:46:46.800151      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0313 22:46:47.800410      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0313 22:46:48.800611      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0313 22:46:49.800889      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 22:46:50.801124      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 22:46:51.801376      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 22:46:52.801563      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 22:46:53.801763      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 22:46:54.802004      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 22:46:55.802243      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 22:46:56.802497      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0313 22:46:57.802774      19 runners.go:184] proxy-service-vwmkb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 13 22:46:57.806: INFO: setup took 13.068234047s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 13 22:46:57.819: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 12.861494ms)
Mar 13 22:46:57.819: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 12.712023ms)
Mar 13 22:46:57.819: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 13.390325ms)
Mar 13 22:46:57.820: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 13.812505ms)
Mar 13 22:46:57.820: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 13.906625ms)
Mar 13 22:46:57.820: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 13.528595ms)
Mar 13 22:46:57.820: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 13.989699ms)
Mar 13 22:46:57.822: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 15.98181ms)
Mar 13 22:46:57.822: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 16.469707ms)
Mar 13 22:46:57.822: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 16.195452ms)
Mar 13 22:46:57.823: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 16.6241ms)
Mar 13 22:46:57.825: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 18.675445ms)
Mar 13 22:46:57.825: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 19.219009ms)
Mar 13 22:46:57.825: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 19.653993ms)
Mar 13 22:46:57.828: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 21.981062ms)
Mar 13 22:46:57.830: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 23.496759ms)
Mar 13 22:46:57.834: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 4.667573ms)
Mar 13 22:46:57.837: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 7.116743ms)
Mar 13 22:46:57.837: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 7.374795ms)
Mar 13 22:46:57.837: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 7.354152ms)
Mar 13 22:46:57.838: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 7.763317ms)
Mar 13 22:46:57.838: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 7.77658ms)
Mar 13 22:46:57.838: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 7.843133ms)
Mar 13 22:46:57.838: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 8.190346ms)
Mar 13 22:46:57.838: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 8.294907ms)
Mar 13 22:46:57.839: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 8.955408ms)
Mar 13 22:46:57.839: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 9.452656ms)
Mar 13 22:46:57.840: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 9.528295ms)
Mar 13 22:46:57.840: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 9.685073ms)
Mar 13 22:46:57.840: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 9.920847ms)
Mar 13 22:46:57.840: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 9.781156ms)
Mar 13 22:46:57.840: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 10.043732ms)
Mar 13 22:46:57.843: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 3.101918ms)
Mar 13 22:46:57.845: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 4.233184ms)
Mar 13 22:46:57.845: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 5.18962ms)
Mar 13 22:46:57.846: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 5.941745ms)
Mar 13 22:46:57.846: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 5.816339ms)
Mar 13 22:46:57.846: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 5.175449ms)
Mar 13 22:46:57.847: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 6.770693ms)
Mar 13 22:46:57.848: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 6.928417ms)
Mar 13 22:46:57.848: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 7.608059ms)
Mar 13 22:46:57.848: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 7.169376ms)
Mar 13 22:46:57.848: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 6.963082ms)
Mar 13 22:46:57.848: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 7.057171ms)
Mar 13 22:46:57.848: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 7.607986ms)
Mar 13 22:46:57.848: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 7.30784ms)
Mar 13 22:46:57.849: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 8.042346ms)
Mar 13 22:46:57.849: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 8.515534ms)
Mar 13 22:46:57.853: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 3.995922ms)
Mar 13 22:46:57.854: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.605489ms)
Mar 13 22:46:57.855: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 5.438716ms)
Mar 13 22:46:57.855: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 5.663038ms)
Mar 13 22:46:57.856: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 6.187931ms)
Mar 13 22:46:57.856: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 6.285184ms)
Mar 13 22:46:57.857: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 7.321928ms)
Mar 13 22:46:57.858: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 8.212168ms)
Mar 13 22:46:57.858: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 8.089254ms)
Mar 13 22:46:57.858: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 8.270704ms)
Mar 13 22:46:57.858: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 8.167421ms)
Mar 13 22:46:57.858: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 8.155912ms)
Mar 13 22:46:57.858: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 8.491033ms)
Mar 13 22:46:57.858: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 8.464953ms)
Mar 13 22:46:57.858: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 8.646193ms)
Mar 13 22:46:57.858: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 8.551834ms)
Mar 13 22:46:57.864: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 5.18957ms)
Mar 13 22:46:57.864: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 5.461303ms)
Mar 13 22:46:57.864: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 4.854105ms)
Mar 13 22:46:57.865: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 6.088304ms)
Mar 13 22:46:57.865: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 6.139813ms)
Mar 13 22:46:57.865: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 5.921663ms)
Mar 13 22:46:57.865: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 6.192664ms)
Mar 13 22:46:57.865: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 6.250535ms)
Mar 13 22:46:57.866: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 6.596643ms)
Mar 13 22:46:57.866: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 6.02291ms)
Mar 13 22:46:57.866: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 6.937033ms)
Mar 13 22:46:57.866: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 6.174309ms)
Mar 13 22:46:57.866: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 6.529691ms)
Mar 13 22:46:57.866: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 6.333882ms)
Mar 13 22:46:57.866: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 7.207692ms)
Mar 13 22:46:57.866: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 6.583813ms)
Mar 13 22:46:57.869: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 3.285047ms)
Mar 13 22:46:57.870: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 3.109023ms)
Mar 13 22:46:57.870: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 3.474681ms)
Mar 13 22:46:57.870: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 3.464049ms)
Mar 13 22:46:57.870: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 3.674772ms)
Mar 13 22:46:57.871: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 3.945811ms)
Mar 13 22:46:57.871: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 4.129621ms)
Mar 13 22:46:57.871: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 4.029789ms)
Mar 13 22:46:57.872: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 4.449895ms)
Mar 13 22:46:57.872: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 4.579758ms)
Mar 13 22:46:57.872: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.40847ms)
Mar 13 22:46:57.872: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 5.202566ms)
Mar 13 22:46:57.872: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 5.415112ms)
Mar 13 22:46:57.872: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 5.659169ms)
Mar 13 22:46:57.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 5.699874ms)
Mar 13 22:46:57.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 5.611673ms)
Mar 13 22:46:57.875: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 2.241917ms)
Mar 13 22:46:57.876: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 3.340817ms)
Mar 13 22:46:57.876: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 3.471667ms)
Mar 13 22:46:57.877: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 3.880243ms)
Mar 13 22:46:57.877: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.172126ms)
Mar 13 22:46:57.877: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 4.466737ms)
Mar 13 22:46:57.877: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.495937ms)
Mar 13 22:46:57.878: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 4.774814ms)
Mar 13 22:46:57.878: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 4.74127ms)
Mar 13 22:46:57.878: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 4.717169ms)
Mar 13 22:46:57.878: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 5.002754ms)
Mar 13 22:46:57.878: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 5.237128ms)
Mar 13 22:46:57.878: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 5.123157ms)
Mar 13 22:46:57.879: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 5.765356ms)
Mar 13 22:46:57.879: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 5.824311ms)
Mar 13 22:46:57.879: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 5.918252ms)
Mar 13 22:46:57.881: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 2.327724ms)
Mar 13 22:46:57.881: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 2.597514ms)
Mar 13 22:46:57.882: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 3.367338ms)
Mar 13 22:46:57.883: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 3.693317ms)
Mar 13 22:46:57.883: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.081245ms)
Mar 13 22:46:57.883: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 4.314364ms)
Mar 13 22:46:57.883: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 4.51996ms)
Mar 13 22:46:57.884: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 4.632737ms)
Mar 13 22:46:57.884: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 4.90449ms)
Mar 13 22:46:57.885: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 6.052606ms)
Mar 13 22:46:57.886: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 6.62954ms)
Mar 13 22:46:57.886: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 6.537411ms)
Mar 13 22:46:57.886: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 6.773493ms)
Mar 13 22:46:57.886: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 6.868149ms)
Mar 13 22:46:57.886: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 7.306335ms)
Mar 13 22:46:57.886: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 7.518586ms)
Mar 13 22:46:57.889: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 2.829057ms)
Mar 13 22:46:57.890: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 3.281457ms)
Mar 13 22:46:57.890: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 3.09102ms)
Mar 13 22:46:57.890: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 3.594536ms)
Mar 13 22:46:57.891: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.132903ms)
Mar 13 22:46:57.892: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 5.386765ms)
Mar 13 22:46:57.892: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 5.210493ms)
Mar 13 22:46:57.892: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 5.204565ms)
Mar 13 22:46:57.892: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 5.36303ms)
Mar 13 22:46:57.892: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 5.284845ms)
Mar 13 22:46:57.892: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 5.57844ms)
Mar 13 22:46:57.896: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 9.307888ms)
Mar 13 22:46:57.896: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 9.573949ms)
Mar 13 22:46:57.896: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 9.652722ms)
Mar 13 22:46:57.897: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 9.758096ms)
Mar 13 22:46:57.897: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 9.977184ms)
Mar 13 22:46:57.900: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 2.43362ms)
Mar 13 22:46:57.900: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 2.926901ms)
Mar 13 22:46:57.900: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 3.188014ms)
Mar 13 22:46:57.901: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 3.369944ms)
Mar 13 22:46:57.901: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 3.473132ms)
Mar 13 22:46:57.902: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 4.256424ms)
Mar 13 22:46:57.902: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.576789ms)
Mar 13 22:46:57.902: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 4.675328ms)
Mar 13 22:46:57.902: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 5.1285ms)
Mar 13 22:46:57.903: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 4.854801ms)
Mar 13 22:46:57.904: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 6.261905ms)
Mar 13 22:46:57.904: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 6.154735ms)
Mar 13 22:46:57.904: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 6.264586ms)
Mar 13 22:46:57.905: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 6.800474ms)
Mar 13 22:46:57.905: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 7.346057ms)
Mar 13 22:46:57.905: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 6.901541ms)
Mar 13 22:46:57.908: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 2.876019ms)
Mar 13 22:46:57.910: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 4.013183ms)
Mar 13 22:46:57.910: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 5.305608ms)
Mar 13 22:46:57.910: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.511237ms)
Mar 13 22:46:57.910: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 4.373918ms)
Mar 13 22:46:57.911: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 6.132951ms)
Mar 13 22:46:57.913: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 7.234934ms)
Mar 13 22:46:57.913: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 7.247733ms)
Mar 13 22:46:57.913: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 7.684685ms)
Mar 13 22:46:57.913: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 7.037976ms)
Mar 13 22:46:57.913: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 7.932531ms)
Mar 13 22:46:57.913: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 7.885232ms)
Mar 13 22:46:57.914: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 8.879655ms)
Mar 13 22:46:57.914: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 8.546602ms)
Mar 13 22:46:57.914: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 8.723028ms)
Mar 13 22:46:57.914: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 8.626397ms)
Mar 13 22:46:57.919: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 4.85485ms)
Mar 13 22:46:57.920: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 5.127163ms)
Mar 13 22:46:57.920: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 5.1353ms)
Mar 13 22:46:57.920: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 5.090397ms)
Mar 13 22:46:57.920: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 4.773168ms)
Mar 13 22:46:57.921: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 5.580482ms)
Mar 13 22:46:57.921: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 6.166929ms)
Mar 13 22:46:57.921: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 6.596876ms)
Mar 13 22:46:57.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 6.513744ms)
Mar 13 22:46:57.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 6.459245ms)
Mar 13 22:46:57.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 7.572308ms)
Mar 13 22:46:57.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 6.934665ms)
Mar 13 22:46:57.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 6.927293ms)
Mar 13 22:46:57.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 7.039751ms)
Mar 13 22:46:57.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 7.513009ms)
Mar 13 22:46:57.922: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 7.323598ms)
Mar 13 22:46:57.929: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 6.307827ms)
Mar 13 22:46:57.929: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 7.007149ms)
Mar 13 22:46:57.931: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 8.246141ms)
Mar 13 22:46:57.931: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 8.350688ms)
Mar 13 22:46:57.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 8.203546ms)
Mar 13 22:46:57.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 8.360384ms)
Mar 13 22:46:57.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 9.058737ms)
Mar 13 22:46:57.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 8.735313ms)
Mar 13 22:46:57.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 8.91132ms)
Mar 13 22:46:57.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 9.115339ms)
Mar 13 22:46:57.932: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 9.266763ms)
Mar 13 22:46:57.933: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 9.297286ms)
Mar 13 22:46:57.933: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 9.817264ms)
Mar 13 22:46:57.933: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 10.388123ms)
Mar 13 22:46:57.933: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 9.895584ms)
Mar 13 22:46:57.935: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 11.827846ms)
Mar 13 22:46:57.937: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 2.139211ms)
Mar 13 22:46:57.937: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 2.205883ms)
Mar 13 22:46:57.937: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 2.326387ms)
Mar 13 22:46:57.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 3.166611ms)
Mar 13 22:46:57.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 3.324036ms)
Mar 13 22:46:57.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 3.289433ms)
Mar 13 22:46:57.941: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 6.312378ms)
Mar 13 22:46:57.942: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 6.81112ms)
Mar 13 22:46:57.942: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 6.686521ms)
Mar 13 22:46:57.942: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 6.792574ms)
Mar 13 22:46:57.942: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 6.723226ms)
Mar 13 22:46:57.942: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 6.813145ms)
Mar 13 22:46:57.942: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 6.767804ms)
Mar 13 22:46:57.942: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 6.940982ms)
Mar 13 22:46:57.942: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 6.900424ms)
Mar 13 22:46:57.942: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 7.119806ms)
Mar 13 22:46:57.946: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 4.267206ms)
Mar 13 22:46:57.947: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 5.253594ms)
Mar 13 22:46:57.947: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 5.426432ms)
Mar 13 22:46:57.948: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 5.644884ms)
Mar 13 22:46:57.951: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 8.520771ms)
Mar 13 22:46:57.951: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 8.872192ms)
Mar 13 22:46:57.951: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 8.881976ms)
Mar 13 22:46:57.951: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 9.285975ms)
Mar 13 22:46:57.951: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 9.260759ms)
Mar 13 22:46:57.951: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 9.399651ms)
Mar 13 22:46:57.951: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 9.379576ms)
Mar 13 22:46:57.952: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 9.676785ms)
Mar 13 22:46:57.952: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 9.830915ms)
Mar 13 22:46:57.952: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 9.911621ms)
Mar 13 22:46:57.952: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 10.034276ms)
Mar 13 22:46:57.952: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 10.375364ms)
Mar 13 22:46:57.955: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 2.50663ms)
Mar 13 22:46:57.956: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 2.548124ms)
Mar 13 22:46:57.957: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 3.966055ms)
Mar 13 22:46:57.957: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 3.84889ms)
Mar 13 22:46:57.957: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 4.445383ms)
Mar 13 22:46:57.957: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 4.314084ms)
Mar 13 22:46:57.957: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.178602ms)
Mar 13 22:46:57.957: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 4.687691ms)
Mar 13 22:46:57.958: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 4.867588ms)
Mar 13 22:46:57.958: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 4.949669ms)
Mar 13 22:46:57.958: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 4.619928ms)
Mar 13 22:46:57.958: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 5.67673ms)
Mar 13 22:46:57.958: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 5.073525ms)
Mar 13 22:46:57.958: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 4.85499ms)
Mar 13 22:46:57.958: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 5.21516ms)
Mar 13 22:46:57.958: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 5.017024ms)
Mar 13 22:46:57.962: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 3.836124ms)
Mar 13 22:46:57.964: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 4.853774ms)
Mar 13 22:46:57.964: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 5.072883ms)
Mar 13 22:46:57.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 6.065068ms)
Mar 13 22:46:57.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 5.896905ms)
Mar 13 22:46:57.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 5.695225ms)
Mar 13 22:46:57.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 5.733072ms)
Mar 13 22:46:57.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 6.050227ms)
Mar 13 22:46:57.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 6.334541ms)
Mar 13 22:46:57.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 6.549117ms)
Mar 13 22:46:57.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 6.383249ms)
Mar 13 22:46:57.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 6.763873ms)
Mar 13 22:46:57.965: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 6.614095ms)
Mar 13 22:46:57.966: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 6.66815ms)
Mar 13 22:46:57.966: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 6.833321ms)
Mar 13 22:46:57.966: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 6.885786ms)
Mar 13 22:46:57.969: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 3.135999ms)
Mar 13 22:46:57.970: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 3.510707ms)
Mar 13 22:46:57.970: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 4.113453ms)
Mar 13 22:46:57.970: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 3.544986ms)
Mar 13 22:46:57.970: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.528691ms)
Mar 13 22:46:57.970: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 4.107223ms)
Mar 13 22:46:57.971: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 4.218116ms)
Mar 13 22:46:57.971: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 4.568512ms)
Mar 13 22:46:57.971: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 4.840637ms)
Mar 13 22:46:57.971: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 5.226527ms)
Mar 13 22:46:57.972: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 5.052741ms)
Mar 13 22:46:57.972: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 5.642003ms)
Mar 13 22:46:57.972: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 5.290612ms)
Mar 13 22:46:57.972: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 5.546353ms)
Mar 13 22:46:57.972: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 5.523539ms)
Mar 13 22:46:57.973: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 6.25299ms)
Mar 13 22:46:57.975: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 2.725035ms)
Mar 13 22:46:57.977: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.203139ms)
Mar 13 22:46:57.977: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 4.689384ms)
Mar 13 22:46:57.978: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 5.021871ms)
Mar 13 22:46:57.978: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 5.117166ms)
Mar 13 22:46:57.978: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 5.498299ms)
Mar 13 22:46:57.979: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 5.843176ms)
Mar 13 22:46:57.979: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 6.075292ms)
Mar 13 22:46:57.979: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 6.099293ms)
Mar 13 22:46:57.979: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 5.976067ms)
Mar 13 22:46:57.979: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 6.422717ms)
Mar 13 22:46:57.979: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 6.430973ms)
Mar 13 22:46:57.979: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 6.626999ms)
Mar 13 22:46:57.979: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 6.595431ms)
Mar 13 22:46:57.979: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 6.506069ms)
Mar 13 22:46:57.979: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 6.876996ms)
Mar 13 22:46:57.983: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 3.408423ms)
Mar 13 22:46:57.984: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:1080/proxy/... (200; 4.456105ms)
Mar 13 22:46:57.984: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 4.765644ms)
Mar 13 22:46:57.984: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:460/proxy/: tls baz (200; 4.880772ms)
Mar 13 22:46:57.984: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:1080/proxy/rewri... (200; 4.576455ms)
Mar 13 22:46:57.985: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7/proxy/rewriteme"... (200; 4.725051ms)
Mar 13 22:46:57.985: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname1/proxy/: foo (200; 5.686735ms)
Mar 13 22:46:57.985: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/proxy-service-vwmkb-t8rc7:160/proxy/: foo (200; 5.635015ms)
Mar 13 22:46:57.985: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname1/proxy/: foo (200; 5.847964ms)
Mar 13 22:46:57.986: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:462/proxy/: tls qux (200; 5.98148ms)
Mar 13 22:46:57.986: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/proxy-service-vwmkb:portname2/proxy/: bar (200; 6.535508ms)
Mar 13 22:46:57.987: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/https:proxy-service-vwmkb-t8rc7:443/proxy/... (200; 6.98894ms)
Mar 13 22:46:57.987: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname2/proxy/: tls qux (200; 6.841968ms)
Mar 13 22:46:57.987: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/pods/http:proxy-service-vwmkb-t8rc7:162/proxy/: bar (200; 7.044849ms)
Mar 13 22:46:57.987: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/http:proxy-service-vwmkb:portname2/proxy/: bar (200; 7.138209ms)
Mar 13 22:46:57.987: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bcpwq/services/https:proxy-service-vwmkb:tlsportname1/proxy/: tls baz (200; 7.026091ms)
STEP: deleting ReplicationController proxy-service-vwmkb in namespace e2e-tests-proxy-bcpwq, will wait for the garbage collector to delete the pods
Mar 13 22:46:58.043: INFO: Deleting ReplicationController proxy-service-vwmkb took: 4.647184ms
Mar 13 22:46:58.143: INFO: Terminating ReplicationController proxy-service-vwmkb pods took: 100.258272ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:47:08.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-bcpwq" for this suite.
Mar 13 22:47:14.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:47:14.187: INFO: namespace: e2e-tests-proxy-bcpwq, resource: bindings, ignored listing per whitelist
Mar 13 22:47:14.216: INFO: namespace e2e-tests-proxy-bcpwq deletion completed in 6.068844023s

• [SLOW TEST:29.529 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:47:14.216: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-zhwx
STEP: Creating a pod to test atomic-volume-subpath
Mar 13 22:47:14.269: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-zhwx" in namespace "e2e-tests-subpath-qcjt4" to be "success or failure"
Mar 13 22:47:14.273: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.734652ms
Mar 13 22:47:16.276: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007146751s
Mar 13 22:47:18.281: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Running", Reason="", readiness=false. Elapsed: 4.01227143s
Mar 13 22:47:20.284: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Running", Reason="", readiness=false. Elapsed: 6.015480448s
Mar 13 22:47:22.287: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Running", Reason="", readiness=false. Elapsed: 8.018538189s
Mar 13 22:47:24.291: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Running", Reason="", readiness=false. Elapsed: 10.022043667s
Mar 13 22:47:26.295: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Running", Reason="", readiness=false. Elapsed: 12.025926175s
Mar 13 22:47:28.299: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Running", Reason="", readiness=false. Elapsed: 14.029576673s
Mar 13 22:47:30.302: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Running", Reason="", readiness=false. Elapsed: 16.033203676s
Mar 13 22:47:32.306: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Running", Reason="", readiness=false. Elapsed: 18.036887156s
Mar 13 22:47:34.310: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Running", Reason="", readiness=false. Elapsed: 20.040788309s
Mar 13 22:47:36.313: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Running", Reason="", readiness=false. Elapsed: 22.044333437s
Mar 13 22:47:38.317: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Running", Reason="", readiness=false. Elapsed: 24.04794345s
Mar 13 22:47:40.321: INFO: Pod "pod-subpath-test-secret-zhwx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.051750895s
STEP: Saw pod success
Mar 13 22:47:40.321: INFO: Pod "pod-subpath-test-secret-zhwx" satisfied condition "success or failure"
Mar 13 22:47:40.323: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-subpath-test-secret-zhwx container test-container-subpath-secret-zhwx: <nil>
STEP: delete the pod
Mar 13 22:47:40.336: INFO: Waiting for pod pod-subpath-test-secret-zhwx to disappear
Mar 13 22:47:40.338: INFO: Pod pod-subpath-test-secret-zhwx no longer exists
STEP: Deleting pod pod-subpath-test-secret-zhwx
Mar 13 22:47:40.338: INFO: Deleting pod "pod-subpath-test-secret-zhwx" in namespace "e2e-tests-subpath-qcjt4"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:47:40.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-qcjt4" for this suite.
Mar 13 22:47:46.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:47:46.402: INFO: namespace: e2e-tests-subpath-qcjt4, resource: bindings, ignored listing per whitelist
Mar 13 22:47:46.414: INFO: namespace e2e-tests-subpath-qcjt4 deletion completed in 6.071546895s

• [SLOW TEST:32.198 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:47:46.414: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 13 22:47:54.484: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 22:47:54.486: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 22:47:56.490: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 22:47:56.492: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 22:47:58.486: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 22:47:58.490: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 22:48:00.486: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 22:48:00.490: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 22:48:02.486: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 22:48:02.490: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 22:48:04.486: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 22:48:04.490: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 22:48:06.486: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 22:48:06.490: INFO: Pod pod-with-prestop-http-hook still exists
Mar 13 22:48:08.486: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 13 22:48:08.490: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:48:08.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-plfgt" for this suite.
Mar 13 22:48:30.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:48:30.536: INFO: namespace: e2e-tests-container-lifecycle-hook-plfgt, resource: bindings, ignored listing per whitelist
Mar 13 22:48:30.569: INFO: namespace e2e-tests-container-lifecycle-hook-plfgt deletion completed in 22.068681757s

• [SLOW TEST:44.155 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:48:30.570: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Mar 13 22:48:30.615: INFO: Waiting up to 5m0s for pod "client-containers-1f9245be-45e2-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-containers-wzj82" to be "success or failure"
Mar 13 22:48:30.618: INFO: Pod "client-containers-1f9245be-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297459ms
Mar 13 22:48:32.621: INFO: Pod "client-containers-1f9245be-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005469599s
Mar 13 22:48:34.624: INFO: Pod "client-containers-1f9245be-45e2-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008628732s
STEP: Saw pod success
Mar 13 22:48:34.624: INFO: Pod "client-containers-1f9245be-45e2-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:48:34.625: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod client-containers-1f9245be-45e2-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 22:48:34.636: INFO: Waiting for pod client-containers-1f9245be-45e2-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:48:34.638: INFO: Pod client-containers-1f9245be-45e2-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:48:34.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-wzj82" for this suite.
Mar 13 22:48:40.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:48:40.688: INFO: namespace: e2e-tests-containers-wzj82, resource: bindings, ignored listing per whitelist
Mar 13 22:48:40.714: INFO: namespace e2e-tests-containers-wzj82 deletion completed in 6.073761666s

• [SLOW TEST:10.144 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:48:40.714: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 13 22:48:40.812: INFO: Waiting up to 5m0s for pod "pod-25a6412b-45e2-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-xrjlk" to be "success or failure"
Mar 13 22:48:40.813: INFO: Pod "pod-25a6412b-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.563551ms
Mar 13 22:48:42.816: INFO: Pod "pod-25a6412b-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004682059s
Mar 13 22:48:44.819: INFO: Pod "pod-25a6412b-45e2-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007723763s
STEP: Saw pod success
Mar 13 22:48:44.819: INFO: Pod "pod-25a6412b-45e2-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:48:44.821: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-25a6412b-45e2-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 22:48:44.832: INFO: Waiting for pod pod-25a6412b-45e2-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:48:44.834: INFO: Pod pod-25a6412b-45e2-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:48:44.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xrjlk" for this suite.
Mar 13 22:48:50.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:48:50.870: INFO: namespace: e2e-tests-emptydir-xrjlk, resource: bindings, ignored listing per whitelist
Mar 13 22:48:50.897: INFO: namespace e2e-tests-emptydir-xrjlk deletion completed in 6.061082427s

• [SLOW TEST:10.183 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:48:50.897: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-2bb08f3d-45e2-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 22:48:50.949: INFO: Waiting up to 5m0s for pod "pod-secrets-2bb0f54e-45e2-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-secrets-5zghl" to be "success or failure"
Mar 13 22:48:50.955: INFO: Pod "pod-secrets-2bb0f54e-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 5.565478ms
Mar 13 22:48:52.958: INFO: Pod "pod-secrets-2bb0f54e-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008535187s
Mar 13 22:48:54.961: INFO: Pod "pod-secrets-2bb0f54e-45e2-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012051753s
STEP: Saw pod success
Mar 13 22:48:54.961: INFO: Pod "pod-secrets-2bb0f54e-45e2-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:48:54.963: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-secrets-2bb0f54e-45e2-11e9-8b9c-0a58ac140107 container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 22:48:54.976: INFO: Waiting for pod pod-secrets-2bb0f54e-45e2-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:48:54.978: INFO: Pod pod-secrets-2bb0f54e-45e2-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:48:54.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-5zghl" for this suite.
Mar 13 22:49:00.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:49:01.040: INFO: namespace: e2e-tests-secrets-5zghl, resource: bindings, ignored listing per whitelist
Mar 13 22:49:01.048: INFO: namespace e2e-tests-secrets-5zghl deletion completed in 6.067832392s

• [SLOW TEST:10.151 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:49:01.049: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 22:49:01.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31bd2f46-45e2-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-9n4sq" to be "success or failure"
Mar 13 22:49:01.100: INFO: Pod "downwardapi-volume-31bd2f46-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 3.664582ms
Mar 13 22:49:03.105: INFO: Pod "downwardapi-volume-31bd2f46-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008480516s
Mar 13 22:49:05.110: INFO: Pod "downwardapi-volume-31bd2f46-45e2-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013548885s
STEP: Saw pod success
Mar 13 22:49:05.110: INFO: Pod "downwardapi-volume-31bd2f46-45e2-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:49:05.112: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downwardapi-volume-31bd2f46-45e2-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 22:49:05.129: INFO: Waiting for pod downwardapi-volume-31bd2f46-45e2-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:49:05.131: INFO: Pod downwardapi-volume-31bd2f46-45e2-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:49:05.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9n4sq" for this suite.
Mar 13 22:49:11.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:49:11.201: INFO: namespace: e2e-tests-projected-9n4sq, resource: bindings, ignored listing per whitelist
Mar 13 22:49:11.211: INFO: namespace e2e-tests-projected-9n4sq deletion completed in 6.076615216s

• [SLOW TEST:10.162 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:49:11.211: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 13 22:49:11.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-zq64j'
Mar 13 22:49:11.385: INFO: stderr: ""
Mar 13 22:49:11.385: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Mar 13 22:49:11.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-zq64j'
Mar 13 22:49:17.606: INFO: stderr: ""
Mar 13 22:49:17.606: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:49:17.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zq64j" for this suite.
Mar 13 22:49:23.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:49:23.661: INFO: namespace: e2e-tests-kubectl-zq64j, resource: bindings, ignored listing per whitelist
Mar 13 22:49:23.692: INFO: namespace e2e-tests-kubectl-zq64j deletion completed in 6.081704049s

• [SLOW TEST:12.480 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:49:23.692: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 22:49:23.770: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f40d302-45e2-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-lj2bn" to be "success or failure"
Mar 13 22:49:23.773: INFO: Pod "downwardapi-volume-3f40d302-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.984092ms
Mar 13 22:49:25.776: INFO: Pod "downwardapi-volume-3f40d302-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006050866s
Mar 13 22:49:27.779: INFO: Pod "downwardapi-volume-3f40d302-45e2-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009267755s
STEP: Saw pod success
Mar 13 22:49:27.779: INFO: Pod "downwardapi-volume-3f40d302-45e2-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:49:27.781: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod downwardapi-volume-3f40d302-45e2-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 22:49:27.794: INFO: Waiting for pod downwardapi-volume-3f40d302-45e2-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:49:27.796: INFO: Pod downwardapi-volume-3f40d302-45e2-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:49:27.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lj2bn" for this suite.
Mar 13 22:49:33.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:49:33.839: INFO: namespace: e2e-tests-projected-lj2bn, resource: bindings, ignored listing per whitelist
Mar 13 22:49:33.872: INFO: namespace e2e-tests-projected-lj2bn deletion completed in 6.073253878s

• [SLOW TEST:10.181 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:49:33.873: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Mar 13 22:49:33.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-mxtww'
Mar 13 22:49:34.084: INFO: stderr: ""
Mar 13 22:49:34.084: INFO: stdout: "pod/pause created\n"
Mar 13 22:49:34.084: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 13 22:49:34.084: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-mxtww" to be "running and ready"
Mar 13 22:49:34.087: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.856998ms
Mar 13 22:49:36.091: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006493909s
Mar 13 22:49:38.094: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.009570004s
Mar 13 22:49:38.094: INFO: Pod "pause" satisfied condition "running and ready"
Mar 13 22:49:38.094: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 13 22:49:38.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-mxtww'
Mar 13 22:49:38.173: INFO: stderr: ""
Mar 13 22:49:38.173: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 13 22:49:38.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pod pause -L testing-label --namespace=e2e-tests-kubectl-mxtww'
Mar 13 22:49:38.243: INFO: stderr: ""
Mar 13 22:49:38.243: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 13 22:49:38.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 label pods pause testing-label- --namespace=e2e-tests-kubectl-mxtww'
Mar 13 22:49:38.324: INFO: stderr: ""
Mar 13 22:49:38.324: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 13 22:49:38.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pod pause -L testing-label --namespace=e2e-tests-kubectl-mxtww'
Mar 13 22:49:38.403: INFO: stderr: ""
Mar 13 22:49:38.403: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Mar 13 22:49:38.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-mxtww'
Mar 13 22:49:38.483: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 22:49:38.483: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 13 22:49:38.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-mxtww'
Mar 13 22:49:38.563: INFO: stderr: "No resources found.\n"
Mar 13 22:49:38.563: INFO: stdout: ""
Mar 13 22:49:38.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pods -l name=pause --namespace=e2e-tests-kubectl-mxtww -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 13 22:49:38.638: INFO: stderr: ""
Mar 13 22:49:38.638: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:49:38.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mxtww" for this suite.
Mar 13 22:49:44.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:49:44.742: INFO: namespace: e2e-tests-kubectl-mxtww, resource: bindings, ignored listing per whitelist
Mar 13 22:49:44.747: INFO: namespace e2e-tests-kubectl-mxtww deletion completed in 6.106192205s

• [SLOW TEST:10.875 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:49:44.748: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 13 22:49:50.837: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:49:50.839: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:49:52.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:49:52.842: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:49:54.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:49:54.842: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:49:56.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:49:56.842: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:49:58.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:49:58.842: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:50:00.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:50:00.843: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:50:02.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:50:02.842: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:50:04.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:50:04.843: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:50:06.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:50:06.843: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:50:08.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:50:08.843: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:50:10.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:50:10.843: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:50:12.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:50:12.842: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:50:14.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:50:14.843: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:50:16.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:50:16.843: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 13 22:50:18.839: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 13 22:50:18.842: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:50:18.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-lftrt" for this suite.
Mar 13 22:50:40.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:50:40.908: INFO: namespace: e2e-tests-container-lifecycle-hook-lftrt, resource: bindings, ignored listing per whitelist
Mar 13 22:50:40.924: INFO: namespace e2e-tests-container-lifecycle-hook-lftrt deletion completed in 22.072615615s

• [SLOW TEST:56.177 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:50:40.925: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Mar 13 22:50:40.968: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-703856963 proxy --unix-socket=/tmp/kubectl-proxy-unix259806150/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:50:41.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-b5twl" for this suite.
Mar 13 22:50:47.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:50:47.075: INFO: namespace: e2e-tests-kubectl-b5twl, resource: bindings, ignored listing per whitelist
Mar 13 22:50:47.107: INFO: namespace e2e-tests-kubectl-b5twl deletion completed in 6.070367891s

• [SLOW TEST:6.183 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:50:47.108: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-70f41ae0-45e2-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 22:50:47.153: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-70f4623b-45e2-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-f7kg4" to be "success or failure"
Mar 13 22:50:47.156: INFO: Pod "pod-projected-configmaps-70f4623b-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.399729ms
Mar 13 22:50:49.159: INFO: Pod "pod-projected-configmaps-70f4623b-45e2-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005535096s
Mar 13 22:50:51.163: INFO: Pod "pod-projected-configmaps-70f4623b-45e2-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009594221s
STEP: Saw pod success
Mar 13 22:50:51.163: INFO: Pod "pod-projected-configmaps-70f4623b-45e2-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:50:51.165: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-projected-configmaps-70f4623b-45e2-11e9-8b9c-0a58ac140107 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 22:50:51.179: INFO: Waiting for pod pod-projected-configmaps-70f4623b-45e2-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:50:51.181: INFO: Pod pod-projected-configmaps-70f4623b-45e2-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:50:51.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-f7kg4" for this suite.
Mar 13 22:50:57.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:50:57.271: INFO: namespace: e2e-tests-projected-f7kg4, resource: bindings, ignored listing per whitelist
Mar 13 22:50:57.278: INFO: namespace e2e-tests-projected-f7kg4 deletion completed in 6.094073071s

• [SLOW TEST:10.171 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:50:57.278: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 22:50:57.324: INFO: Creating deployment "nginx-deployment"
Mar 13 22:50:57.326: INFO: Waiting for observed generation 1
Mar 13 22:50:59.332: INFO: Waiting for all required pods to come up
Mar 13 22:50:59.336: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 13 22:51:03.344: INFO: Waiting for deployment "nginx-deployment" to complete
Mar 13 22:51:03.348: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar 13 22:51:03.354: INFO: Updating deployment nginx-deployment
Mar 13 22:51:03.354: INFO: Waiting for observed generation 2
Mar 13 22:51:05.360: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 13 22:51:05.362: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 13 22:51:05.364: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar 13 22:51:05.372: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 13 22:51:05.372: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 13 22:51:05.374: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar 13 22:51:05.400: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar 13 22:51:05.400: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar 13 22:51:05.407: INFO: Updating deployment nginx-deployment
Mar 13 22:51:05.407: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar 13 22:51:05.412: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 13 22:51:05.418: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 13 22:51:05.443: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dtwc8/deployments/nginx-deployment,UID:76cbd3ab-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34180,Generation:3,CreationTimestamp:2019-03-13 22:50:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-03-13 22:51:03 +0000 UTC 2019-03-13 22:50:56 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.} {Available False 2019-03-13 22:51:05 +0000 UTC 2019-03-13 22:51:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Mar 13 22:51:05.479: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dtwc8/replicasets/nginx-deployment-65bbdb5f8,UID:7a63dcd5-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34167,Generation:3,CreationTimestamp:2019-03-13 22:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 76cbd3ab-45e2-11e9-b6d3-506b8df34f96 0xc002707627 0xc002707628}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 13 22:51:05.480: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar 13 22:51:05.480: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dtwc8/replicasets/nginx-deployment-555b55d965,UID:76cc4220-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34166,Generation:3,CreationTimestamp:2019-03-13 22:50:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 76cbd3ab-45e2-11e9-b6d3-506b8df34f96 0xc002707567 0xc002707568}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Mar 13 22:51:05.508: INFO: Pod "nginx-deployment-555b55d965-5754h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5754h,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-5754h,UID:7b9f6a79-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34191,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d83d7 0xc0020d83d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.508: INFO: Pod "nginx-deployment-555b55d965-5rclq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5rclq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-5rclq,UID:7ba07fc6-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34206,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d8490 0xc0020d8491}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.509: INFO: Pod "nginx-deployment-555b55d965-62l4m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-62l4m,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-62l4m,UID:7b9f05a1-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34215,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d85b0 0xc0020d85b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:10.40.156.31,PodIP:,StartTime:2019-03-13 22:51:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.509: INFO: Pod "nginx-deployment-555b55d965-7wwnl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-7wwnl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-7wwnl,UID:7ba09534-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34207,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d86d7 0xc0020d86d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.509: INFO: Pod "nginx-deployment-555b55d965-b64d5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-b64d5,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-b64d5,UID:7ba326f6-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34201,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d8890 0xc0020d8891}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.509: INFO: Pod "nginx-deployment-555b55d965-bb6hx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-bb6hx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-bb6hx,UID:76d12e1a-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34066,Generation:0,CreationTimestamp:2019-03-13 22:50:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d8957 0xc0020d8958}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:57 +0000 UTC  }],Message:,Reason:,HostIP:10.40.156.31,PodIP:172.20.2.51,StartTime:2019-03-13 22:50:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-13 22:51:00 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://6937b7d4b4bae87f3d70908c2ceaf15f4c9291ca278525237c7dac4671a98276}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.509: INFO: Pod "nginx-deployment-555b55d965-bxhpc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-bxhpc,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-bxhpc,UID:76ce0dca-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34069,Generation:0,CreationTimestamp:2019-03-13 22:50:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d8b20 0xc0020d8b21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:56 +0000 UTC  }],Message:,Reason:,HostIP:10.40.156.31,PodIP:172.20.2.49,StartTime:2019-03-13 22:50:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-13 22:50:59 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://23f8c3ca80311f0e3b73c5556ca43b1728c421b6da2d2674d094f69ec8b91c67}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.509: INFO: Pod "nginx-deployment-555b55d965-c9nbg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-c9nbg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-c9nbg,UID:7ba34188-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34219,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d8c90 0xc0020d8c91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.510: INFO: Pod "nginx-deployment-555b55d965-dcwql" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dcwql,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-dcwql,UID:76cf234f-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34061,Generation:0,CreationTimestamp:2019-03-13 22:50:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d8d40 0xc0020d8d41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:56 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.216,PodIP:172.20.1.60,StartTime:2019-03-13 22:50:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-13 22:50:59 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://d101233f72aa02786b79d1f9ee5c4376f3a46af7c19ef27687647c612f160d68}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.510: INFO: Pod "nginx-deployment-555b55d965-dvzv4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dvzv4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-dvzv4,UID:7ba36c23-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34222,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d8f80 0xc0020d8f81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.510: INFO: Pod "nginx-deployment-555b55d965-gjk4q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gjk4q,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-gjk4q,UID:76d10a80-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34072,Generation:0,CreationTimestamp:2019-03-13 22:50:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d9170 0xc0020d9171}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:56 +0000 UTC  }],Message:,Reason:,HostIP:10.40.156.31,PodIP:172.20.2.50,StartTime:2019-03-13 22:50:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-13 22:50:59 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://15d387e49e6ed109522a3b6e0499265f1c01f694caf8f41c0dc22a1a52400556}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.510: INFO: Pod "nginx-deployment-555b55d965-hm4vk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-hm4vk,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-hm4vk,UID:76d11bb9-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34039,Generation:0,CreationTimestamp:2019-03-13 22:50:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d9270 0xc0020d9271}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:56 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.216,PodIP:172.20.1.61,StartTime:2019-03-13 22:50:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-13 22:50:58 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://69b0b9bd7c091912bc9d0e6612225f10557b5ef84ea05f5de4e6d366d5edc67e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.510: INFO: Pod "nginx-deployment-555b55d965-kc7nd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-kc7nd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-kc7nd,UID:7ba0b906-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34204,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d9400 0xc0020d9401}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.510: INFO: Pod "nginx-deployment-555b55d965-lqpbq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lqpbq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-lqpbq,UID:7b9e1ac3-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34203,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d94b0 0xc0020d94b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.213,PodIP:,StartTime:2019-03-13 22:51:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.510: INFO: Pod "nginx-deployment-555b55d965-n2wbb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-n2wbb,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-n2wbb,UID:7ba355a3-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34218,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d9617 0xc0020d9618}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.511: INFO: Pod "nginx-deployment-555b55d965-q7dt7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-q7dt7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-q7dt7,UID:7ba2efc8-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34214,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d96d0 0xc0020d96d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.511: INFO: Pod "nginx-deployment-555b55d965-wtsxx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-wtsxx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-wtsxx,UID:7ba0a2f8-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34208,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d97f0 0xc0020d97f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.511: INFO: Pod "nginx-deployment-555b55d965-x9ncj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-x9ncj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-x9ncj,UID:76cef837-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34081,Generation:0,CreationTimestamp:2019-03-13 22:50:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d98b0 0xc0020d98b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:56 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.213,PodIP:172.20.3.50,StartTime:2019-03-13 22:50:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-13 22:51:00 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://254f4e1609a9e652a668a7002f11ad17e8c87156c489893c0d21a9f696704c6a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.511: INFO: Pod "nginx-deployment-555b55d965-z5q26" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-z5q26,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-z5q26,UID:76d2598a-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34058,Generation:0,CreationTimestamp:2019-03-13 22:50:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d9a10 0xc0020d9a11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:57 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.216,PodIP:172.20.1.62,StartTime:2019-03-13 22:50:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-13 22:50:59 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://5cc0049227455419c9b4d85619534a35e94db1e4158fbd5720e5fcc97286587a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.511: INFO: Pod "nginx-deployment-555b55d965-zncgj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zncgj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-555b55d965-zncgj,UID:76d30b67-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34084,Generation:0,CreationTimestamp:2019-03-13 22:50:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 76cc4220-45e2-11e9-b6d3-506b8df34f96 0xc0020d9b20 0xc0020d9b21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:50:57 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.213,PodIP:172.20.3.49,StartTime:2019-03-13 22:50:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-13 22:51:00 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://2d9b626ad7dfe66e58026d8181de9376b7c1f23071dc63d669746a2fcace2e64}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.511: INFO: Pod "nginx-deployment-65bbdb5f8-2n9w2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-2n9w2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-2n9w2,UID:7ba30f8a-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34210,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0020d9c20 0xc0020d9c21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.511: INFO: Pod "nginx-deployment-65bbdb5f8-4pm5f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-4pm5f,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-4pm5f,UID:7ba1b407-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34205,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0020d9ce0 0xc0020d9ce1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.512: INFO: Pod "nginx-deployment-65bbdb5f8-695vd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-695vd,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-695vd,UID:7ba198f2-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34212,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0020d9da0 0xc0020d9da1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.512: INFO: Pod "nginx-deployment-65bbdb5f8-9q2pg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-9q2pg,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-9q2pg,UID:7a64a9f4-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34161,Generation:0,CreationTimestamp:2019-03-13 22:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0020d9e60 0xc0020d9e61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:02 +0000 UTC  }],Message:,Reason:,HostIP:10.40.156.31,PodIP:172.20.2.52,StartTime:2019-03-13 22:51:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = manifest for docker.io/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.512: INFO: Pod "nginx-deployment-65bbdb5f8-dbhbb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-dbhbb,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-dbhbb,UID:7ba1cd1f-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34211,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0020d9fa0 0xc0020d9fa1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.512: INFO: Pod "nginx-deployment-65bbdb5f8-gdstb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-gdstb,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-gdstb,UID:7a667495-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34134,Generation:0,CreationTimestamp:2019-03-13 22:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0022e8400 0xc0022e8401}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.216,PodIP:,StartTime:2019-03-13 22:51:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.512: INFO: Pod "nginx-deployment-65bbdb5f8-h4pfw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-h4pfw,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-h4pfw,UID:7ba126bf-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34209,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0022e8510 0xc0022e8511}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.512: INFO: Pod "nginx-deployment-65bbdb5f8-lnjp8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-lnjp8,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-lnjp8,UID:7b9f9513-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34194,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0022e8a10 0xc0022e8a11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.512: INFO: Pod "nginx-deployment-65bbdb5f8-ltpnz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-ltpnz,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-ltpnz,UID:7a6f7e22-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34150,Generation:0,CreationTimestamp:2019-03-13 22:51:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0022e8b40 0xc0022e8b41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC  }],Message:,Reason:,HostIP:10.40.156.31,PodIP:,StartTime:2019-03-13 22:51:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.512: INFO: Pod "nginx-deployment-65bbdb5f8-m2cfl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-m2cfl,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-m2cfl,UID:7b9e7a2c-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34217,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0022e8ed0 0xc0022e8ed1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.216,PodIP:,StartTime:2019-03-13 22:51:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.512: INFO: Pod "nginx-deployment-65bbdb5f8-q6btz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-q6btz,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-q6btz,UID:7a6eb27f-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34148,Generation:0,CreationTimestamp:2019-03-13 22:51:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0022e9020 0xc0022e9021}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.213,PodIP:,StartTime:2019-03-13 22:51:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.513: INFO: Pod "nginx-deployment-65bbdb5f8-rkxlg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rkxlg,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-rkxlg,UID:7a661f42-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34133,Generation:0,CreationTimestamp:2019-03-13 22:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0022e92f0 0xc0022e92f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:03 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.213,PodIP:,StartTime:2019-03-13 22:51:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 13 22:51:05.513: INFO: Pod "nginx-deployment-65bbdb5f8-rwp8d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rwp8d,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-dtwc8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dtwc8/pods/nginx-deployment-65bbdb5f8-rwp8d,UID:7b9fb494-45e2-11e9-b6d3-506b8df34f96,ResourceVersion:34193,Generation:0,CreationTimestamp:2019-03-13 22:51:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 7a63dcd5-45e2-11e9-b6d3-506b8df34f96 0xc0022e9480 0xc0022e9481}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t5ntc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t5ntc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t5ntc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 22:51:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:51:05.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-dtwc8" for this suite.
Mar 13 22:51:11.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:51:11.595: INFO: namespace: e2e-tests-deployment-dtwc8, resource: bindings, ignored listing per whitelist
Mar 13 22:51:11.610: INFO: namespace e2e-tests-deployment-dtwc8 deletion completed in 6.081443235s

• [SLOW TEST:14.331 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:51:11.610: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-srr7q
Mar 13 22:51:15.667: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-srr7q
STEP: checking the pod's current state and verifying that restartCount is present
Mar 13 22:51:15.669: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:55:16.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-srr7q" for this suite.
Mar 13 22:55:22.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:55:22.131: INFO: namespace: e2e-tests-container-probe-srr7q, resource: bindings, ignored listing per whitelist
Mar 13 22:55:22.178: INFO: namespace e2e-tests-container-probe-srr7q deletion completed in 6.067517871s

• [SLOW TEST:250.568 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:55:22.178: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 22:55:22.277: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 13 22:55:22.282: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:22.285: INFO: Number of nodes with available pods: 0
Mar 13 22:55:22.285: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 22:55:23.289: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:23.291: INFO: Number of nodes with available pods: 0
Mar 13 22:55:23.291: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 22:55:24.289: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:24.291: INFO: Number of nodes with available pods: 0
Mar 13 22:55:24.291: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 22:55:25.290: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:25.301: INFO: Number of nodes with available pods: 1
Mar 13 22:55:25.301: INFO: Node karbon-test-3a0ff6-k8s-worker-1 is running more than one daemon pod
Mar 13 22:55:26.289: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:26.291: INFO: Number of nodes with available pods: 3
Mar 13 22:55:26.291: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 13 22:55:26.313: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:26.313: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:26.313: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:26.316: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:27.319: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:27.319: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:27.319: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:27.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:28.319: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:28.319: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:28.319: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:28.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:29.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:29.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:29.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:29.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:30.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:30.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:30.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:30.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:31.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:31.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:31.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:31.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:32.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:32.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:32.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:32.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:33.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:33.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:33.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:33.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:34.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:34.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:34.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:34.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:35.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:35.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:35.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:35.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:36.319: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:36.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:36.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:36.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:37.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:37.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:37.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:37.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:38.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:38.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:38.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:38.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:39.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:39.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:39.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:39.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:40.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:40.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:40.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:40.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:41.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:41.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:41.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:41.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:42.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:42.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:42.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:42.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:43.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:43.321: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:43.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:43.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:44.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:44.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:44.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:44.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:45.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:45.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:45.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:45.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:46.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:46.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:46.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:46.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:47.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:47.321: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:47.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:47.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:48.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:48.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:48.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:48.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:49.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:49.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:49.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:49.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:50.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:50.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:50.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:50.325: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:51.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:51.321: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:51.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:51.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:52.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:52.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:52.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:52.327: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:53.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:53.321: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:53.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:53.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:54.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:54.321: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:54.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:54.325: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:55.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:55.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:55.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:55.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:56.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:56.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:56.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:56.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:57.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:57.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:57.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:57.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:58.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:58.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:58.320: INFO: Pod daemon-set-9npxv is not available
Mar 13 22:55:58.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:58.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:55:59.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:59.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:59.320: INFO: Pod daemon-set-9npxv is not available
Mar 13 22:55:59.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:55:59.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:00.319: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:00.319: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:00.319: INFO: Pod daemon-set-9npxv is not available
Mar 13 22:56:00.319: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:00.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:01.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:01.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:01.320: INFO: Pod daemon-set-9npxv is not available
Mar 13 22:56:01.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:01.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:02.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:02.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:02.320: INFO: Pod daemon-set-9npxv is not available
Mar 13 22:56:02.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:02.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:03.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:03.321: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:03.321: INFO: Pod daemon-set-9npxv is not available
Mar 13 22:56:03.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:03.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:04.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:04.321: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:04.321: INFO: Pod daemon-set-9npxv is not available
Mar 13 22:56:04.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:04.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:05.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:05.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:05.320: INFO: Pod daemon-set-9npxv is not available
Mar 13 22:56:05.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:05.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:06.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:06.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:06.320: INFO: Pod daemon-set-9npxv is not available
Mar 13 22:56:06.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:06.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:07.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:07.320: INFO: Wrong image for pod: daemon-set-9npxv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:07.320: INFO: Pod daemon-set-9npxv is not available
Mar 13 22:56:07.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:07.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:08.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:08.321: INFO: Pod daemon-set-tvr5j is not available
Mar 13 22:56:08.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:08.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:09.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:09.321: INFO: Pod daemon-set-tvr5j is not available
Mar 13 22:56:09.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:09.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:10.324: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:10.324: INFO: Pod daemon-set-tvr5j is not available
Mar 13 22:56:10.324: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:10.326: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:11.323: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:11.323: INFO: Pod daemon-set-tvr5j is not available
Mar 13 22:56:11.323: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:11.326: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:12.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:12.321: INFO: Pod daemon-set-tvr5j is not available
Mar 13 22:56:12.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:12.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:13.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:13.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:13.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:14.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:14.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:14.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:15.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:15.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:15.325: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:16.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:16.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:16.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:17.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:17.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:17.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:18.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:18.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:18.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:19.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:19.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:19.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:20.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:20.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:20.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:21.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:21.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:21.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:22.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:22.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:22.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:23.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:23.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:23.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:24.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:24.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:24.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:25.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:25.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:25.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:26.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:26.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:26.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:27.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:27.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:27.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:28.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:28.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:28.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:29.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:29.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:29.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:30.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:30.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:30.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:31.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:31.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:31.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:32.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:32.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:32.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:33.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:33.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:33.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:34.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:34.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:34.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:35.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:35.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:35.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:36.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:36.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:36.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:37.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:37.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:37.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:38.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:38.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:38.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:39.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:39.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:39.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:40.319: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:40.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:40.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:41.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:41.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:41.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:42.320: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:42.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:42.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:43.321: INFO: Wrong image for pod: daemon-set-8tl84. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:43.321: INFO: Pod daemon-set-8tl84 is not available
Mar 13 22:56:43.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:43.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:44.320: INFO: Pod daemon-set-knmz6 is not available
Mar 13 22:56:44.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:44.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:45.320: INFO: Pod daemon-set-knmz6 is not available
Mar 13 22:56:45.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:45.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:46.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:46.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:47.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:47.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:48.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:48.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:49.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:49.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:50.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:50.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:51.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:51.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:52.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:52.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:53.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:53.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:54.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:54.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:55.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:55.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:56.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:56.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:57.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:57.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:58.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:58.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:56:59.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:56:59.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:00.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:00.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:01.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:01.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:02.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:02.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:03.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:03.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:04.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:04.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:05.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:05.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:06.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:06.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:07.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:07.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:08.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:08.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:09.321: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:09.325: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:10.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:10.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:11.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:11.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:12.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:12.322: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:13.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:13.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:14.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:14.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:15.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:15.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:16.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:16.324: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:17.320: INFO: Wrong image for pod: daemon-set-v2r72. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 13 22:57:17.320: INFO: Pod daemon-set-v2r72 is not available
Mar 13 22:57:17.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:18.320: INFO: Pod daemon-set-pcrlz is not available
Mar 13 22:57:18.323: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 13 22:57:18.326: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:18.328: INFO: Number of nodes with available pods: 2
Mar 13 22:57:18.328: INFO: Node karbon-test-3a0ff6-k8s-worker-2 is running more than one daemon pod
Mar 13 22:57:19.332: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:19.335: INFO: Number of nodes with available pods: 2
Mar 13 22:57:19.335: INFO: Node karbon-test-3a0ff6-k8s-worker-2 is running more than one daemon pod
Mar 13 22:57:20.331: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:20.334: INFO: Number of nodes with available pods: 2
Mar 13 22:57:20.334: INFO: Node karbon-test-3a0ff6-k8s-worker-2 is running more than one daemon pod
Mar 13 22:57:21.332: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:21.334: INFO: Number of nodes with available pods: 2
Mar 13 22:57:21.334: INFO: Node karbon-test-3a0ff6-k8s-worker-2 is running more than one daemon pod
Mar 13 22:57:22.333: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:22.336: INFO: Number of nodes with available pods: 2
Mar 13 22:57:22.336: INFO: Node karbon-test-3a0ff6-k8s-worker-2 is running more than one daemon pod
Mar 13 22:57:23.333: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:23.335: INFO: Number of nodes with available pods: 3
Mar 13 22:57:23.335: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-l7dsr, will wait for the garbage collector to delete the pods
Mar 13 22:57:23.408: INFO: Deleting DaemonSet.extensions daemon-set took: 5.129381ms
Mar 13 22:57:23.508: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.257714ms
Mar 13 22:57:38.111: INFO: Number of nodes with available pods: 0
Mar 13 22:57:38.111: INFO: Number of running nodes: 0, number of available pods: 0
Mar 13 22:57:38.113: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-l7dsr/daemonsets","resourceVersion":"35233"},"items":null}

Mar 13 22:57:38.114: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-l7dsr/pods","resourceVersion":"35233"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:57:38.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-l7dsr" for this suite.
Mar 13 22:57:44.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:57:44.199: INFO: namespace: e2e-tests-daemonsets-l7dsr, resource: bindings, ignored listing per whitelist
Mar 13 22:57:44.205: INFO: namespace e2e-tests-daemonsets-l7dsr deletion completed in 6.072514391s

• [SLOW TEST:142.027 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:57:44.205: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-6990600e-45e3-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 22:57:44.253: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6990b363-45e3-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-xpv2s" to be "success or failure"
Mar 13 22:57:44.257: INFO: Pod "pod-projected-configmaps-6990b363-45e3-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.348318ms
Mar 13 22:57:46.260: INFO: Pod "pod-projected-configmaps-6990b363-45e3-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007713945s
Mar 13 22:57:48.263: INFO: Pod "pod-projected-configmaps-6990b363-45e3-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009815906s
STEP: Saw pod success
Mar 13 22:57:48.263: INFO: Pod "pod-projected-configmaps-6990b363-45e3-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:57:48.264: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-projected-configmaps-6990b363-45e3-11e9-8b9c-0a58ac140107 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 22:57:48.275: INFO: Waiting for pod pod-projected-configmaps-6990b363-45e3-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:57:48.277: INFO: Pod pod-projected-configmaps-6990b363-45e3-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:57:48.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xpv2s" for this suite.
Mar 13 22:57:54.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:57:54.320: INFO: namespace: e2e-tests-projected-xpv2s, resource: bindings, ignored listing per whitelist
Mar 13 22:57:54.346: INFO: namespace e2e-tests-projected-xpv2s deletion completed in 6.067654928s

• [SLOW TEST:10.141 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:57:54.347: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 13 22:57:54.406: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:54.410: INFO: Number of nodes with available pods: 0
Mar 13 22:57:54.410: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 22:57:55.414: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:55.417: INFO: Number of nodes with available pods: 0
Mar 13 22:57:55.417: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 22:57:56.414: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:56.417: INFO: Number of nodes with available pods: 0
Mar 13 22:57:56.417: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 22:57:57.414: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:57.416: INFO: Number of nodes with available pods: 0
Mar 13 22:57:57.416: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 22:57:58.415: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:58.417: INFO: Number of nodes with available pods: 3
Mar 13 22:57:58.417: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 13 22:57:58.428: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:58.432: INFO: Number of nodes with available pods: 2
Mar 13 22:57:58.432: INFO: Node karbon-test-3a0ff6-k8s-worker-2 is running more than one daemon pod
Mar 13 22:57:59.436: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:57:59.439: INFO: Number of nodes with available pods: 2
Mar 13 22:57:59.439: INFO: Node karbon-test-3a0ff6-k8s-worker-2 is running more than one daemon pod
Mar 13 22:58:00.440: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:58:00.442: INFO: Number of nodes with available pods: 2
Mar 13 22:58:00.442: INFO: Node karbon-test-3a0ff6-k8s-worker-2 is running more than one daemon pod
Mar 13 22:58:01.437: INFO: DaemonSet pods can't tolerate node karbon-test-3a0ff6-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 13 22:58:01.439: INFO: Number of nodes with available pods: 3
Mar 13 22:58:01.439: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-7lmns, will wait for the garbage collector to delete the pods
Mar 13 22:58:01.498: INFO: Deleting DaemonSet.extensions daemon-set took: 4.481051ms
Mar 13 22:58:01.599: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.267426ms
Mar 13 22:58:38.101: INFO: Number of nodes with available pods: 0
Mar 13 22:58:38.101: INFO: Number of running nodes: 0, number of available pods: 0
Mar 13 22:58:38.103: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-7lmns/daemonsets","resourceVersion":"35469"},"items":null}

Mar 13 22:58:38.105: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-7lmns/pods","resourceVersion":"35469"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:58:38.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-7lmns" for this suite.
Mar 13 22:58:44.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:58:44.162: INFO: namespace: e2e-tests-daemonsets-7lmns, resource: bindings, ignored listing per whitelist
Mar 13 22:58:44.188: INFO: namespace e2e-tests-daemonsets-7lmns deletion completed in 6.072217174s

• [SLOW TEST:49.841 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:58:44.188: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 22:58:48.252: INFO: Waiting up to 5m0s for pod "client-envvars-8fb66875-45e3-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-pods-gqsbc" to be "success or failure"
Mar 13 22:58:48.260: INFO: Pod "client-envvars-8fb66875-45e3-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 7.784364ms
Mar 13 22:58:50.263: INFO: Pod "client-envvars-8fb66875-45e3-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011008365s
Mar 13 22:58:52.266: INFO: Pod "client-envvars-8fb66875-45e3-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014283566s
STEP: Saw pod success
Mar 13 22:58:52.266: INFO: Pod "client-envvars-8fb66875-45e3-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 22:58:52.268: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod client-envvars-8fb66875-45e3-11e9-8b9c-0a58ac140107 container env3cont: <nil>
STEP: delete the pod
Mar 13 22:58:52.281: INFO: Waiting for pod client-envvars-8fb66875-45e3-11e9-8b9c-0a58ac140107 to disappear
Mar 13 22:58:52.283: INFO: Pod client-envvars-8fb66875-45e3-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:58:52.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-gqsbc" for this suite.
Mar 13 22:59:42.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 22:59:42.353: INFO: namespace: e2e-tests-pods-gqsbc, resource: bindings, ignored listing per whitelist
Mar 13 22:59:42.353: INFO: namespace e2e-tests-pods-gqsbc deletion completed in 50.067394382s

• [SLOW TEST:58.165 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 22:59:42.353: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 22:59:46.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-kvr2b" for this suite.
Mar 13 23:00:30.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:00:30.462: INFO: namespace: e2e-tests-kubelet-test-kvr2b, resource: bindings, ignored listing per whitelist
Mar 13 23:00:30.485: INFO: namespace e2e-tests-kubelet-test-kvr2b deletion completed in 44.074200044s

• [SLOW TEST:48.132 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:00:30.485: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Mar 13 23:00:30.530: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-703856963 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:00:30.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hbckj" for this suite.
Mar 13 23:00:36.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:00:36.627: INFO: namespace: e2e-tests-kubectl-hbckj, resource: bindings, ignored listing per whitelist
Mar 13 23:00:36.668: INFO: namespace e2e-tests-kubectl-hbckj deletion completed in 6.063499649s

• [SLOW TEST:6.183 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:00:36.668: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:00:40.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-fdt75" for this suite.
Mar 13 23:01:30.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:01:30.747: INFO: namespace: e2e-tests-kubelet-test-fdt75, resource: bindings, ignored listing per whitelist
Mar 13 23:01:30.804: INFO: namespace e2e-tests-kubelet-test-fdt75 deletion completed in 50.073819613s

• [SLOW TEST:54.136 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:01:30.804: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 23:01:30.873: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f067a724-45e3-11e9-b6d3-506b8df34f96", Controller:(*bool)(0xc0022e9f26), BlockOwnerDeletion:(*bool)(0xc0022e9f27)}}
Mar 13 23:01:30.878: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f0662414-45e3-11e9-b6d3-506b8df34f96", Controller:(*bool)(0xc00204415e), BlockOwnerDeletion:(*bool)(0xc00204415f)}}
Mar 13 23:01:30.882: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f066c2df-45e3-11e9-b6d3-506b8df34f96", Controller:(*bool)(0xc0025f3f56), BlockOwnerDeletion:(*bool)(0xc0025f3f57)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:01:35.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-6tq6r" for this suite.
Mar 13 23:01:41.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:01:41.940: INFO: namespace: e2e-tests-gc-6tq6r, resource: bindings, ignored listing per whitelist
Mar 13 23:01:41.976: INFO: namespace e2e-tests-gc-6tq6r deletion completed in 6.081762767s

• [SLOW TEST:11.172 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:01:41.976: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-q4tkk
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-q4tkk
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-q4tkk
Mar 13 23:01:42.031: INFO: Found 0 stateful pods, waiting for 1
Mar 13 23:01:52.035: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 13 23:01:52.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-q4tkk ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 13 23:01:52.195: INFO: stderr: ""
Mar 13 23:01:52.195: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 13 23:01:52.195: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 13 23:01:52.198: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 13 23:02:02.201: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 23:02:02.201: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 23:02:02.210: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar 13 23:02:02.210: INFO: ss-0  karbon-test-3a0ff6-k8s-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:01:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:01:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:01:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:01:41 +0000 UTC  }]
Mar 13 23:02:02.210: INFO: 
Mar 13 23:02:02.210: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 13 23:02:03.214: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997820931s
Mar 13 23:02:04.218: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993892817s
Mar 13 23:02:05.222: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989571984s
Mar 13 23:02:06.225: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986050806s
Mar 13 23:02:07.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982441198s
Mar 13 23:02:08.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97857642s
Mar 13 23:02:09.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974461695s
Mar 13 23:02:10.242: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970311661s
Mar 13 23:02:11.246: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.001331ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-q4tkk
Mar 13 23:02:12.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-q4tkk ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 13 23:02:12.401: INFO: stderr: ""
Mar 13 23:02:12.401: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 13 23:02:12.401: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 13 23:02:12.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-q4tkk ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 13 23:02:12.553: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar 13 23:02:12.553: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 13 23:02:12.553: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 13 23:02:12.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-q4tkk ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 13 23:02:12.715: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar 13 23:02:12.715: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 13 23:02:12.715: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 13 23:02:12.720: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar 13 23:02:22.723: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 23:02:22.723: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 23:02:22.723: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 13 23:02:22.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-q4tkk ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 13 23:02:22.871: INFO: stderr: ""
Mar 13 23:02:22.871: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 13 23:02:22.871: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 13 23:02:22.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-q4tkk ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 13 23:02:23.023: INFO: stderr: ""
Mar 13 23:02:23.023: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 13 23:02:23.023: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 13 23:02:23.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-q4tkk ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 13 23:02:23.180: INFO: stderr: ""
Mar 13 23:02:23.180: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 13 23:02:23.181: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 13 23:02:23.181: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 23:02:23.183: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 13 23:02:33.189: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 23:02:33.189: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 23:02:33.189: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 13 23:02:33.197: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar 13 23:02:33.197: INFO: ss-0  karbon-test-3a0ff6-k8s-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:01:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:01:41 +0000 UTC  }]
Mar 13 23:02:33.197: INFO: ss-1  karbon-test-3a0ff6-k8s-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:01 +0000 UTC  }]
Mar 13 23:02:33.197: INFO: ss-2  karbon-test-3a0ff6-k8s-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:01 +0000 UTC  }]
Mar 13 23:02:33.197: INFO: 
Mar 13 23:02:33.197: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 13 23:02:34.201: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar 13 23:02:34.201: INFO: ss-0  karbon-test-3a0ff6-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:01:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:01:41 +0000 UTC  }]
Mar 13 23:02:34.201: INFO: ss-1  karbon-test-3a0ff6-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:01 +0000 UTC  }]
Mar 13 23:02:34.201: INFO: ss-2  karbon-test-3a0ff6-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:02:01 +0000 UTC  }]
Mar 13 23:02:34.201: INFO: 
Mar 13 23:02:34.201: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 13 23:02:35.205: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993180456s
Mar 13 23:02:36.208: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989721687s
Mar 13 23:02:37.211: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.986184293s
Mar 13 23:02:38.215: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983147942s
Mar 13 23:02:39.219: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.978943981s
Mar 13 23:02:40.222: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.975660305s
Mar 13 23:02:41.226: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.972326189s
Mar 13 23:02:42.230: INFO: Verifying statefulset ss doesn't scale past 0 for another 968.525718ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-q4tkk
Mar 13 23:02:43.232: INFO: Scaling statefulset ss to 0
Mar 13 23:02:43.239: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 13 23:02:43.240: INFO: Deleting all statefulset in ns e2e-tests-statefulset-q4tkk
Mar 13 23:02:43.242: INFO: Scaling statefulset ss to 0
Mar 13 23:02:43.248: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 23:02:43.250: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:02:43.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-q4tkk" for this suite.
Mar 13 23:02:49.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:02:49.279: INFO: namespace: e2e-tests-statefulset-q4tkk, resource: bindings, ignored listing per whitelist
Mar 13 23:02:49.336: INFO: namespace e2e-tests-statefulset-q4tkk deletion completed in 6.074630767s

• [SLOW TEST:67.360 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:02:49.336: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar 13 23:02:49.379: INFO: PodSpec: initContainers in spec.initContainers
Mar 13 23:03:38.190: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1f6fe288-45e4-11e9-8b9c-0a58ac140107", GenerateName:"", Namespace:"e2e-tests-init-container-dfvpx", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-dfvpx/pods/pod-init-1f6fe288-45e4-11e9-8b9c-0a58ac140107", UID:"1f34edd3-45e4-11e9-b6d3-506b8df34f96", ResourceVersion:"36293", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63688114968, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"379705013"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-g8pdm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001eefb00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g8pdm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g8pdm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g8pdm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0022f0308), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"karbon-test-3a0ff6-k8s-worker-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0022166c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0022f0370)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688114969, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688114969, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688114969, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688114968, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.40.155.216", PodIP:"172.20.1.81", StartTime:(*v1.Time)(0xc000979f20), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00149d500)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00149d650)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://82d48abfe4a3103f94a347af04154d5858a62083a7e71b2cdb7be03ca389aa39"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000979f60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000979f40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:03:38.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-dfvpx" for this suite.
Mar 13 23:04:00.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:04:00.227: INFO: namespace: e2e-tests-init-container-dfvpx, resource: bindings, ignored listing per whitelist
Mar 13 23:04:00.267: INFO: namespace e2e-tests-init-container-dfvpx deletion completed in 22.072464557s

• [SLOW TEST:70.931 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:04:00.268: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-49b73c92-45e4-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 23:04:00.317: INFO: Waiting up to 5m0s for pod "pod-secrets-49b7a18b-45e4-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-secrets-bssmj" to be "success or failure"
Mar 13 23:04:00.320: INFO: Pod "pod-secrets-49b7a18b-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.350248ms
Mar 13 23:04:02.323: INFO: Pod "pod-secrets-49b7a18b-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005743118s
Mar 13 23:04:04.326: INFO: Pod "pod-secrets-49b7a18b-45e4-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00916024s
STEP: Saw pod success
Mar 13 23:04:04.326: INFO: Pod "pod-secrets-49b7a18b-45e4-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:04:04.328: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-secrets-49b7a18b-45e4-11e9-8b9c-0a58ac140107 container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 23:04:04.343: INFO: Waiting for pod pod-secrets-49b7a18b-45e4-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:04:04.346: INFO: Pod pod-secrets-49b7a18b-45e4-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:04:04.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-bssmj" for this suite.
Mar 13 23:04:10.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:04:10.406: INFO: namespace: e2e-tests-secrets-bssmj, resource: bindings, ignored listing per whitelist
Mar 13 23:04:10.421: INFO: namespace e2e-tests-secrets-bssmj deletion completed in 6.072246714s

• [SLOW TEST:10.153 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:04:10.421: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar 13 23:04:10.481: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 13 23:04:10.498: INFO: Waiting for terminating namespaces to be deleted...
Mar 13 23:04:10.500: INFO: 
Logging pods the kubelet thinks is on node karbon-test-3a0ff6-k8s-worker-0 before test
Mar 13 23:04:10.506: INFO: sonobuoy-e2e-job-f2ba8fc748ec406a from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.506: INFO: 	Container e2e ready: true, restart count 0
Mar 13 23:04:10.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 23:04:10.506: INFO: kube-proxy-ds-bqvcl from kube-system started at 2019-03-13 17:37:58 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.506: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 13 23:04:10.506: INFO: node-exporter-l8bv5 from monitoring started at 2019-03-13 17:41:11 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.506: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 13 23:04:10.506: INFO: 	Container node-exporter ready: true, restart count 0
Mar 13 23:04:10.506: INFO: fluent-bit-zsqpg from kube-system started at 2019-03-13 17:38:38 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.506: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 13 23:04:10.506: INFO: sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-8jqgg from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.506: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 13 23:04:10.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 23:04:10.506: INFO: kube-flannel-ds-25rft from kube-system started at 2019-03-13 17:38:02 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.506: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 23:04:10.506: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-13 22:07:51 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.506: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 13 23:04:10.506: INFO: prometheus-k8s-0 from monitoring started at 2019-03-13 17:41:33 +0000 UTC (3 container statuses recorded)
Mar 13 23:04:10.506: INFO: 	Container prometheus ready: true, restart count 1
Mar 13 23:04:10.506: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 13 23:04:10.506: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 13 23:04:10.506: INFO: csi-attacher-ntnx-plugin-0 from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.506: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 13 23:04:10.506: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Mar 13 23:04:10.506: INFO: csi-node-ntnx-plugin-v9pcq from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.506: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Mar 13 23:04:10.506: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 13 23:04:10.506: INFO: 
Logging pods the kubelet thinks is on node karbon-test-3a0ff6-k8s-worker-1 before test
Mar 13 23:04:10.512: INFO: elasticsearch-logging-0 from ntnx-logging started at 2019-03-13 17:38:51 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.512: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Mar 13 23:04:10.512: INFO: prometheus-operator-d58cfc597-rr4rv from monitoring started at 2019-03-13 17:41:11 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.512: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 13 23:04:10.512: INFO: alertmanager-main-0 from monitoring started at 2019-03-13 17:41:20 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.512: INFO: 	Container alertmanager ready: true, restart count 0
Mar 13 23:04:10.512: INFO: 	Container config-reloader ready: true, restart count 0
Mar 13 23:04:10.512: INFO: kube-flannel-ds-8xdq9 from kube-system started at 2019-03-13 17:38:02 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.512: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 23:04:10.512: INFO: node-exporter-2prk8 from monitoring started at 2019-03-13 17:41:11 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 13 23:04:10.512: INFO: 	Container node-exporter ready: true, restart count 0
Mar 13 23:04:10.512: INFO: kube-state-metrics-54996cf64c-gdfqp from monitoring started at 2019-03-13 17:41:22 +0000 UTC (4 container statuses recorded)
Mar 13 23:04:10.512: INFO: 	Container addon-resizer ready: true, restart count 0
Mar 13 23:04:10.512: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 13 23:04:10.512: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 13 23:04:10.512: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 13 23:04:10.512: INFO: csi-node-ntnx-plugin-v29vq from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.512: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Mar 13 23:04:10.512: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 13 23:04:10.512: INFO: fluent-bit-x4t2v from kube-system started at 2019-03-13 17:38:38 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.512: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 13 23:04:10.512: INFO: kubernetes-events-printer-75755d587-rcrrf from ntnx-logging started at 2019-03-13 17:38:39 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.512: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Mar 13 23:04:10.512: INFO: kube-proxy-ds-x52pl from kube-system started at 2019-03-13 17:37:58 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.512: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 13 23:04:10.512: INFO: sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-c2w76 from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.512: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 13 23:04:10.512: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 23:04:10.512: INFO: 
Logging pods the kubelet thinks is on node karbon-test-3a0ff6-k8s-worker-2 before test
Mar 13 23:04:10.520: INFO: alertmanager-main-1 from monitoring started at 2019-03-13 17:41:30 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.520: INFO: 	Container alertmanager ready: true, restart count 0
Mar 13 23:04:10.520: INFO: 	Container config-reloader ready: true, restart count 0
Mar 13 23:04:10.520: INFO: sonobuoy-systemd-logs-daemon-set-dd4db6732dba4c6c-vtjz7 from heptio-sonobuoy started at 2019-03-13 22:07:57 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.520: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 13 23:04:10.520: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 13 23:04:10.520: INFO: kube-proxy-ds-64bb6 from kube-system started at 2019-03-13 17:37:58 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.520: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 13 23:04:10.520: INFO: csi-provisioner-ntnx-plugin-0 from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.520: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 13 23:04:10.520: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Mar 13 23:04:10.520: INFO: node-exporter-wnrgc from monitoring started at 2019-03-13 17:41:11 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.520: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 13 23:04:10.520: INFO: 	Container node-exporter ready: true, restart count 0
Mar 13 23:04:10.520: INFO: prometheus-k8s-1 from monitoring started at 2019-03-13 17:42:18 +0000 UTC (3 container statuses recorded)
Mar 13 23:04:10.520: INFO: 	Container prometheus ready: true, restart count 1
Mar 13 23:04:10.520: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 13 23:04:10.520: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 13 23:04:10.520: INFO: kube-flannel-ds-gcljm from kube-system started at 2019-03-13 17:38:02 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.520: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 13 23:04:10.520: INFO: csi-node-ntnx-plugin-9kv8f from kube-system started at 2019-03-13 17:38:20 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.520: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Mar 13 23:04:10.520: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 13 23:04:10.520: INFO: kibana-logging-5d46457978-l2xgl from ntnx-logging started at 2019-03-13 17:38:39 +0000 UTC (2 container statuses recorded)
Mar 13 23:04:10.520: INFO: 	Container kibana-logging ready: true, restart count 0
Mar 13 23:04:10.520: INFO: 	Container nginxhttp ready: true, restart count 0
Mar 13 23:04:10.520: INFO: fluent-bit-q2plf from kube-system started at 2019-03-13 17:38:38 +0000 UTC (1 container statuses recorded)
Mar 13 23:04:10.520: INFO: 	Container fluent-bit ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.158ba71f774a7325], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:04:11.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-d5zw9" for this suite.
Mar 13 23:04:17.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:04:17.565: INFO: namespace: e2e-tests-sched-pred-d5zw9, resource: bindings, ignored listing per whitelist
Mar 13 23:04:17.616: INFO: namespace e2e-tests-sched-pred-d5zw9 deletion completed in 6.07342413s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.195 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:04:17.616: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-540e7c3b-45e4-11e9-8b9c-0a58ac140107
Mar 13 23:04:17.667: INFO: Pod name my-hostname-basic-540e7c3b-45e4-11e9-8b9c-0a58ac140107: Found 0 pods out of 1
Mar 13 23:04:22.670: INFO: Pod name my-hostname-basic-540e7c3b-45e4-11e9-8b9c-0a58ac140107: Found 1 pods out of 1
Mar 13 23:04:22.670: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-540e7c3b-45e4-11e9-8b9c-0a58ac140107" are running
Mar 13 23:04:22.672: INFO: Pod "my-hostname-basic-540e7c3b-45e4-11e9-8b9c-0a58ac140107-fk4vm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-13 23:04:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-13 23:04:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-13 23:04:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-13 23:04:17 +0000 UTC Reason: Message:}])
Mar 13 23:04:22.672: INFO: Trying to dial the pod
Mar 13 23:04:27.681: INFO: Controller my-hostname-basic-540e7c3b-45e4-11e9-8b9c-0a58ac140107: Got expected result from replica 1 [my-hostname-basic-540e7c3b-45e4-11e9-8b9c-0a58ac140107-fk4vm]: "my-hostname-basic-540e7c3b-45e4-11e9-8b9c-0a58ac140107-fk4vm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:04:27.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-lv7qh" for this suite.
Mar 13 23:04:33.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:04:33.730: INFO: namespace: e2e-tests-replication-controller-lv7qh, resource: bindings, ignored listing per whitelist
Mar 13 23:04:33.766: INFO: namespace e2e-tests-replication-controller-lv7qh deletion completed in 6.082108143s

• [SLOW TEST:16.150 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:04:33.766: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:04:37.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-7wpkr" for this suite.
Mar 13 23:04:43.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:04:43.869: INFO: namespace: e2e-tests-kubelet-test-7wpkr, resource: bindings, ignored listing per whitelist
Mar 13 23:04:43.894: INFO: namespace e2e-tests-kubelet-test-7wpkr deletion completed in 6.059155628s

• [SLOW TEST:10.128 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:04:43.894: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-rm97c
I0313 23:04:43.949894      19 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-rm97c, replica count: 1
I0313 23:04:45.000350      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0313 23:04:46.000543      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0313 23:04:47.000799      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 13 23:04:47.108: INFO: Created: latency-svc-wp45x
Mar 13 23:04:47.112: INFO: Got endpoints: latency-svc-wp45x [11.626727ms]
Mar 13 23:04:47.126: INFO: Created: latency-svc-q9fl8
Mar 13 23:04:47.129: INFO: Created: latency-svc-sh95s
Mar 13 23:04:47.130: INFO: Got endpoints: latency-svc-q9fl8 [18.03492ms]
Mar 13 23:04:47.134: INFO: Got endpoints: latency-svc-sh95s [21.204112ms]
Mar 13 23:04:47.135: INFO: Created: latency-svc-jld8n
Mar 13 23:04:47.140: INFO: Created: latency-svc-tflq6
Mar 13 23:04:47.143: INFO: Got endpoints: latency-svc-jld8n [30.082477ms]
Mar 13 23:04:47.148: INFO: Created: latency-svc-gwlb9
Mar 13 23:04:47.148: INFO: Got endpoints: latency-svc-tflq6 [35.749169ms]
Mar 13 23:04:47.153: INFO: Created: latency-svc-krwnp
Mar 13 23:04:47.158: INFO: Got endpoints: latency-svc-gwlb9 [45.218295ms]
Mar 13 23:04:47.162: INFO: Created: latency-svc-bsnxl
Mar 13 23:04:47.162: INFO: Got endpoints: latency-svc-krwnp [49.317251ms]
Mar 13 23:04:47.167: INFO: Got endpoints: latency-svc-bsnxl [54.16801ms]
Mar 13 23:04:47.169: INFO: Created: latency-svc-rb66j
Mar 13 23:04:47.169: INFO: Got endpoints: latency-svc-rb66j [56.775491ms]
Mar 13 23:04:47.171: INFO: Created: latency-svc-cjt4h
Mar 13 23:04:47.174: INFO: Created: latency-svc-gbrcn
Mar 13 23:04:47.174: INFO: Got endpoints: latency-svc-cjt4h [16.707435ms]
Mar 13 23:04:47.180: INFO: Created: latency-svc-s4jqk
Mar 13 23:04:47.180: INFO: Got endpoints: latency-svc-gbrcn [67.156157ms]
Mar 13 23:04:47.184: INFO: Got endpoints: latency-svc-s4jqk [71.147157ms]
Mar 13 23:04:47.184: INFO: Created: latency-svc-fg6c5
Mar 13 23:04:47.189: INFO: Created: latency-svc-qth52
Mar 13 23:04:47.192: INFO: Got endpoints: latency-svc-fg6c5 [79.409055ms]
Mar 13 23:04:47.197: INFO: Got endpoints: latency-svc-qth52 [84.475661ms]
Mar 13 23:04:47.202: INFO: Created: latency-svc-552gv
Mar 13 23:04:47.202: INFO: Got endpoints: latency-svc-552gv [89.892061ms]
Mar 13 23:04:47.204: INFO: Created: latency-svc-8b7gf
Mar 13 23:04:47.207: INFO: Got endpoints: latency-svc-8b7gf [94.455573ms]
Mar 13 23:04:47.210: INFO: Created: latency-svc-55dv8
Mar 13 23:04:47.214: INFO: Got endpoints: latency-svc-55dv8 [101.307691ms]
Mar 13 23:04:47.215: INFO: Created: latency-svc-pgsh8
Mar 13 23:04:47.218: INFO: Created: latency-svc-jhn6p
Mar 13 23:04:47.222: INFO: Got endpoints: latency-svc-pgsh8 [91.198124ms]
Mar 13 23:04:47.224: INFO: Created: latency-svc-p4fvd
Mar 13 23:04:47.229: INFO: Got endpoints: latency-svc-jhn6p [94.861532ms]
Mar 13 23:04:47.231: INFO: Created: latency-svc-q27nq
Mar 13 23:04:47.232: INFO: Got endpoints: latency-svc-p4fvd [89.27592ms]
Mar 13 23:04:47.240: INFO: Created: latency-svc-vzs6s
Mar 13 23:04:47.240: INFO: Got endpoints: latency-svc-q27nq [91.963534ms]
Mar 13 23:04:47.244: INFO: Got endpoints: latency-svc-vzs6s [82.331617ms]
Mar 13 23:04:47.245: INFO: Created: latency-svc-5fzz7
Mar 13 23:04:47.253: INFO: Created: latency-svc-68czz
Mar 13 23:04:47.254: INFO: Got endpoints: latency-svc-5fzz7 [87.430804ms]
Mar 13 23:04:47.256: INFO: Got endpoints: latency-svc-68czz [86.519796ms]
Mar 13 23:04:47.262: INFO: Created: latency-svc-z4cjr
Mar 13 23:04:47.267: INFO: Created: latency-svc-kxpvw
Mar 13 23:04:47.268: INFO: Got endpoints: latency-svc-z4cjr [93.773705ms]
Mar 13 23:04:47.271: INFO: Got endpoints: latency-svc-kxpvw [90.697095ms]
Mar 13 23:04:47.275: INFO: Created: latency-svc-g8kdj
Mar 13 23:04:47.277: INFO: Got endpoints: latency-svc-g8kdj [92.791523ms]
Mar 13 23:04:47.277: INFO: Created: latency-svc-zjj5w
Mar 13 23:04:47.279: INFO: Got endpoints: latency-svc-zjj5w [86.91632ms]
Mar 13 23:04:47.282: INFO: Created: latency-svc-nj6f7
Mar 13 23:04:47.286: INFO: Created: latency-svc-rjsfb
Mar 13 23:04:47.286: INFO: Got endpoints: latency-svc-nj6f7 [88.81942ms]
Mar 13 23:04:47.291: INFO: Created: latency-svc-jftp9
Mar 13 23:04:47.292: INFO: Got endpoints: latency-svc-rjsfb [89.008002ms]
Mar 13 23:04:47.297: INFO: Got endpoints: latency-svc-jftp9 [89.645994ms]
Mar 13 23:04:47.298: INFO: Created: latency-svc-mdrss
Mar 13 23:04:47.302: INFO: Created: latency-svc-sb6vs
Mar 13 23:04:47.305: INFO: Got endpoints: latency-svc-mdrss [91.417053ms]
Mar 13 23:04:47.308: INFO: Got endpoints: latency-svc-sb6vs [86.173739ms]
Mar 13 23:04:47.309: INFO: Created: latency-svc-g2ljr
Mar 13 23:04:47.312: INFO: Got endpoints: latency-svc-g2ljr [83.732238ms]
Mar 13 23:04:47.313: INFO: Created: latency-svc-p2xnb
Mar 13 23:04:47.316: INFO: Created: latency-svc-nr9gr
Mar 13 23:04:47.321: INFO: Created: latency-svc-pmqng
Mar 13 23:04:47.323: INFO: Created: latency-svc-zjvp2
Mar 13 23:04:47.326: INFO: Created: latency-svc-dhfmb
Mar 13 23:04:47.331: INFO: Created: latency-svc-ww456
Mar 13 23:04:47.333: INFO: Created: latency-svc-nw569
Mar 13 23:04:47.338: INFO: Created: latency-svc-bddmm
Mar 13 23:04:47.343: INFO: Created: latency-svc-8bd8z
Mar 13 23:04:47.346: INFO: Created: latency-svc-bnsmq
Mar 13 23:04:47.349: INFO: Created: latency-svc-rfp8z
Mar 13 23:04:47.352: INFO: Created: latency-svc-7qfft
Mar 13 23:04:47.357: INFO: Created: latency-svc-qxcmb
Mar 13 23:04:47.359: INFO: Created: latency-svc-twvnx
Mar 13 23:04:47.361: INFO: Got endpoints: latency-svc-p2xnb [129.308209ms]
Mar 13 23:04:47.368: INFO: Created: latency-svc-bhtc2
Mar 13 23:04:47.372: INFO: Created: latency-svc-mv9gm
Mar 13 23:04:47.410: INFO: Got endpoints: latency-svc-nr9gr [169.841048ms]
Mar 13 23:04:47.417: INFO: Created: latency-svc-txwnn
Mar 13 23:04:47.461: INFO: Got endpoints: latency-svc-pmqng [216.35081ms]
Mar 13 23:04:47.468: INFO: Created: latency-svc-j7hvp
Mar 13 23:04:47.511: INFO: Got endpoints: latency-svc-zjvp2 [257.291879ms]
Mar 13 23:04:47.517: INFO: Created: latency-svc-4q2t6
Mar 13 23:04:47.562: INFO: Got endpoints: latency-svc-dhfmb [306.381357ms]
Mar 13 23:04:47.570: INFO: Created: latency-svc-dmnhf
Mar 13 23:04:47.611: INFO: Got endpoints: latency-svc-ww456 [342.614967ms]
Mar 13 23:04:47.617: INFO: Created: latency-svc-xwvnr
Mar 13 23:04:47.661: INFO: Got endpoints: latency-svc-nw569 [390.177849ms]
Mar 13 23:04:47.666: INFO: Created: latency-svc-xv2sx
Mar 13 23:04:47.711: INFO: Got endpoints: latency-svc-bddmm [433.980586ms]
Mar 13 23:04:47.733: INFO: Created: latency-svc-48xls
Mar 13 23:04:47.762: INFO: Got endpoints: latency-svc-8bd8z [482.48201ms]
Mar 13 23:04:47.768: INFO: Created: latency-svc-x5hjx
Mar 13 23:04:47.811: INFO: Got endpoints: latency-svc-bnsmq [525.40815ms]
Mar 13 23:04:47.818: INFO: Created: latency-svc-k67g9
Mar 13 23:04:47.861: INFO: Got endpoints: latency-svc-rfp8z [569.52101ms]
Mar 13 23:04:47.868: INFO: Created: latency-svc-njrh5
Mar 13 23:04:47.911: INFO: Got endpoints: latency-svc-7qfft [614.279439ms]
Mar 13 23:04:47.917: INFO: Created: latency-svc-7vx7m
Mar 13 23:04:47.960: INFO: Got endpoints: latency-svc-qxcmb [655.041112ms]
Mar 13 23:04:47.966: INFO: Created: latency-svc-4wdt5
Mar 13 23:04:48.011: INFO: Got endpoints: latency-svc-twvnx [703.646557ms]
Mar 13 23:04:48.018: INFO: Created: latency-svc-6pgr8
Mar 13 23:04:48.062: INFO: Got endpoints: latency-svc-bhtc2 [749.516976ms]
Mar 13 23:04:48.068: INFO: Created: latency-svc-7mkdd
Mar 13 23:04:48.111: INFO: Got endpoints: latency-svc-mv9gm [749.755751ms]
Mar 13 23:04:48.118: INFO: Created: latency-svc-2pv6p
Mar 13 23:04:48.161: INFO: Got endpoints: latency-svc-txwnn [751.040348ms]
Mar 13 23:04:48.169: INFO: Created: latency-svc-c2x4p
Mar 13 23:04:48.211: INFO: Got endpoints: latency-svc-j7hvp [750.38461ms]
Mar 13 23:04:48.219: INFO: Created: latency-svc-8lf2j
Mar 13 23:04:48.262: INFO: Got endpoints: latency-svc-4q2t6 [750.283283ms]
Mar 13 23:04:48.269: INFO: Created: latency-svc-7g9pb
Mar 13 23:04:48.312: INFO: Got endpoints: latency-svc-dmnhf [749.692527ms]
Mar 13 23:04:48.320: INFO: Created: latency-svc-qfr78
Mar 13 23:04:48.361: INFO: Got endpoints: latency-svc-xwvnr [749.932864ms]
Mar 13 23:04:48.367: INFO: Created: latency-svc-8795n
Mar 13 23:04:48.413: INFO: Got endpoints: latency-svc-xv2sx [751.670485ms]
Mar 13 23:04:48.419: INFO: Created: latency-svc-mqnpm
Mar 13 23:04:48.461: INFO: Got endpoints: latency-svc-48xls [750.825854ms]
Mar 13 23:04:48.477: INFO: Created: latency-svc-vqx9n
Mar 13 23:04:48.512: INFO: Got endpoints: latency-svc-x5hjx [749.950765ms]
Mar 13 23:04:48.519: INFO: Created: latency-svc-rllxl
Mar 13 23:04:48.561: INFO: Got endpoints: latency-svc-k67g9 [749.684227ms]
Mar 13 23:04:48.569: INFO: Created: latency-svc-hzm9v
Mar 13 23:04:48.613: INFO: Got endpoints: latency-svc-njrh5 [751.505585ms]
Mar 13 23:04:48.619: INFO: Created: latency-svc-fsdz9
Mar 13 23:04:48.661: INFO: Got endpoints: latency-svc-7vx7m [750.291778ms]
Mar 13 23:04:48.668: INFO: Created: latency-svc-fcp9d
Mar 13 23:04:48.712: INFO: Got endpoints: latency-svc-4wdt5 [751.251579ms]
Mar 13 23:04:48.720: INFO: Created: latency-svc-5jwlm
Mar 13 23:04:48.761: INFO: Got endpoints: latency-svc-6pgr8 [749.877688ms]
Mar 13 23:04:48.770: INFO: Created: latency-svc-bksh9
Mar 13 23:04:48.812: INFO: Got endpoints: latency-svc-7mkdd [749.664319ms]
Mar 13 23:04:48.819: INFO: Created: latency-svc-hv8bm
Mar 13 23:04:48.862: INFO: Got endpoints: latency-svc-2pv6p [751.052932ms]
Mar 13 23:04:48.871: INFO: Created: latency-svc-gvm8z
Mar 13 23:04:48.912: INFO: Got endpoints: latency-svc-c2x4p [750.312445ms]
Mar 13 23:04:48.919: INFO: Created: latency-svc-shcwk
Mar 13 23:04:48.961: INFO: Got endpoints: latency-svc-8lf2j [750.130302ms]
Mar 13 23:04:48.970: INFO: Created: latency-svc-q628w
Mar 13 23:04:49.013: INFO: Got endpoints: latency-svc-7g9pb [750.700404ms]
Mar 13 23:04:49.020: INFO: Created: latency-svc-j6mct
Mar 13 23:04:49.061: INFO: Got endpoints: latency-svc-qfr78 [748.974622ms]
Mar 13 23:04:49.069: INFO: Created: latency-svc-w9626
Mar 13 23:04:49.111: INFO: Got endpoints: latency-svc-8795n [750.334621ms]
Mar 13 23:04:49.120: INFO: Created: latency-svc-h6t97
Mar 13 23:04:49.161: INFO: Got endpoints: latency-svc-mqnpm [748.660489ms]
Mar 13 23:04:49.168: INFO: Created: latency-svc-5sxjf
Mar 13 23:04:49.213: INFO: Got endpoints: latency-svc-vqx9n [751.017448ms]
Mar 13 23:04:49.220: INFO: Created: latency-svc-gmkp4
Mar 13 23:04:49.262: INFO: Got endpoints: latency-svc-rllxl [750.164303ms]
Mar 13 23:04:49.268: INFO: Created: latency-svc-94lhv
Mar 13 23:04:49.312: INFO: Got endpoints: latency-svc-hzm9v [750.827842ms]
Mar 13 23:04:49.318: INFO: Created: latency-svc-t9t8v
Mar 13 23:04:49.361: INFO: Got endpoints: latency-svc-fsdz9 [748.445646ms]
Mar 13 23:04:49.368: INFO: Created: latency-svc-b9nm7
Mar 13 23:04:49.412: INFO: Got endpoints: latency-svc-fcp9d [750.611339ms]
Mar 13 23:04:49.417: INFO: Created: latency-svc-d9b8v
Mar 13 23:04:49.462: INFO: Got endpoints: latency-svc-5jwlm [750.250197ms]
Mar 13 23:04:49.468: INFO: Created: latency-svc-bg7z4
Mar 13 23:04:49.511: INFO: Got endpoints: latency-svc-bksh9 [749.927626ms]
Mar 13 23:04:49.518: INFO: Created: latency-svc-mf7g4
Mar 13 23:04:49.564: INFO: Got endpoints: latency-svc-hv8bm [752.013248ms]
Mar 13 23:04:49.575: INFO: Created: latency-svc-xfvfb
Mar 13 23:04:49.612: INFO: Got endpoints: latency-svc-gvm8z [749.257785ms]
Mar 13 23:04:49.622: INFO: Created: latency-svc-kp2fm
Mar 13 23:04:49.662: INFO: Got endpoints: latency-svc-shcwk [749.883876ms]
Mar 13 23:04:49.669: INFO: Created: latency-svc-9znz8
Mar 13 23:04:49.712: INFO: Got endpoints: latency-svc-q628w [750.594597ms]
Mar 13 23:04:49.718: INFO: Created: latency-svc-h4rw5
Mar 13 23:04:49.762: INFO: Got endpoints: latency-svc-j6mct [749.079501ms]
Mar 13 23:04:49.769: INFO: Created: latency-svc-pbckf
Mar 13 23:04:49.811: INFO: Got endpoints: latency-svc-w9626 [750.277143ms]
Mar 13 23:04:49.819: INFO: Created: latency-svc-f4mvt
Mar 13 23:04:49.861: INFO: Got endpoints: latency-svc-h6t97 [749.383736ms]
Mar 13 23:04:49.867: INFO: Created: latency-svc-fntsb
Mar 13 23:04:49.911: INFO: Got endpoints: latency-svc-5sxjf [749.801847ms]
Mar 13 23:04:49.917: INFO: Created: latency-svc-bgfvv
Mar 13 23:04:49.961: INFO: Got endpoints: latency-svc-gmkp4 [748.811611ms]
Mar 13 23:04:49.967: INFO: Created: latency-svc-jhn68
Mar 13 23:04:50.012: INFO: Got endpoints: latency-svc-94lhv [750.584034ms]
Mar 13 23:04:50.019: INFO: Created: latency-svc-5l7wl
Mar 13 23:04:50.061: INFO: Got endpoints: latency-svc-t9t8v [749.518426ms]
Mar 13 23:04:50.067: INFO: Created: latency-svc-k268n
Mar 13 23:04:50.111: INFO: Got endpoints: latency-svc-b9nm7 [749.783689ms]
Mar 13 23:04:50.117: INFO: Created: latency-svc-59zqp
Mar 13 23:04:50.161: INFO: Got endpoints: latency-svc-d9b8v [749.31715ms]
Mar 13 23:04:50.168: INFO: Created: latency-svc-vbnxp
Mar 13 23:04:50.212: INFO: Got endpoints: latency-svc-bg7z4 [749.554378ms]
Mar 13 23:04:50.218: INFO: Created: latency-svc-42sfc
Mar 13 23:04:50.262: INFO: Got endpoints: latency-svc-mf7g4 [750.77045ms]
Mar 13 23:04:50.268: INFO: Created: latency-svc-ckvh2
Mar 13 23:04:50.311: INFO: Got endpoints: latency-svc-xfvfb [747.526433ms]
Mar 13 23:04:50.317: INFO: Created: latency-svc-ffpnr
Mar 13 23:04:50.362: INFO: Got endpoints: latency-svc-kp2fm [750.055387ms]
Mar 13 23:04:50.369: INFO: Created: latency-svc-6jmhj
Mar 13 23:04:50.412: INFO: Got endpoints: latency-svc-9znz8 [750.711547ms]
Mar 13 23:04:50.421: INFO: Created: latency-svc-82q2b
Mar 13 23:04:50.462: INFO: Got endpoints: latency-svc-h4rw5 [749.753676ms]
Mar 13 23:04:50.468: INFO: Created: latency-svc-q9xpj
Mar 13 23:04:50.511: INFO: Got endpoints: latency-svc-pbckf [749.533366ms]
Mar 13 23:04:50.518: INFO: Created: latency-svc-f68sv
Mar 13 23:04:50.566: INFO: Got endpoints: latency-svc-f4mvt [754.554993ms]
Mar 13 23:04:50.574: INFO: Created: latency-svc-5l8z9
Mar 13 23:04:50.612: INFO: Got endpoints: latency-svc-fntsb [750.962851ms]
Mar 13 23:04:50.620: INFO: Created: latency-svc-tzqrw
Mar 13 23:04:50.662: INFO: Got endpoints: latency-svc-bgfvv [750.647836ms]
Mar 13 23:04:50.669: INFO: Created: latency-svc-wrxjp
Mar 13 23:04:50.711: INFO: Got endpoints: latency-svc-jhn68 [749.789098ms]
Mar 13 23:04:50.717: INFO: Created: latency-svc-9n982
Mar 13 23:04:50.763: INFO: Got endpoints: latency-svc-5l7wl [750.835501ms]
Mar 13 23:04:50.771: INFO: Created: latency-svc-7pjv7
Mar 13 23:04:50.812: INFO: Got endpoints: latency-svc-k268n [751.042324ms]
Mar 13 23:04:50.820: INFO: Created: latency-svc-7z6f8
Mar 13 23:04:50.862: INFO: Got endpoints: latency-svc-59zqp [751.287413ms]
Mar 13 23:04:50.869: INFO: Created: latency-svc-nhzfr
Mar 13 23:04:50.911: INFO: Got endpoints: latency-svc-vbnxp [749.849775ms]
Mar 13 23:04:50.918: INFO: Created: latency-svc-28m44
Mar 13 23:04:50.961: INFO: Got endpoints: latency-svc-42sfc [749.243431ms]
Mar 13 23:04:50.970: INFO: Created: latency-svc-nf7bw
Mar 13 23:04:51.012: INFO: Got endpoints: latency-svc-ckvh2 [749.293327ms]
Mar 13 23:04:51.020: INFO: Created: latency-svc-phv2k
Mar 13 23:04:51.061: INFO: Got endpoints: latency-svc-ffpnr [750.140355ms]
Mar 13 23:04:51.067: INFO: Created: latency-svc-sqll2
Mar 13 23:04:51.112: INFO: Got endpoints: latency-svc-6jmhj [750.200773ms]
Mar 13 23:04:51.121: INFO: Created: latency-svc-bdvbn
Mar 13 23:04:51.161: INFO: Got endpoints: latency-svc-82q2b [748.784956ms]
Mar 13 23:04:51.168: INFO: Created: latency-svc-mggmw
Mar 13 23:04:51.212: INFO: Got endpoints: latency-svc-q9xpj [749.804307ms]
Mar 13 23:04:51.217: INFO: Created: latency-svc-pfk2r
Mar 13 23:04:51.261: INFO: Got endpoints: latency-svc-f68sv [749.82838ms]
Mar 13 23:04:51.268: INFO: Created: latency-svc-z4xzj
Mar 13 23:04:51.311: INFO: Got endpoints: latency-svc-5l8z9 [744.693439ms]
Mar 13 23:04:51.318: INFO: Created: latency-svc-ncc2s
Mar 13 23:04:51.361: INFO: Got endpoints: latency-svc-tzqrw [749.502181ms]
Mar 13 23:04:51.368: INFO: Created: latency-svc-f2mrd
Mar 13 23:04:51.411: INFO: Got endpoints: latency-svc-wrxjp [749.263716ms]
Mar 13 23:04:51.418: INFO: Created: latency-svc-f58qc
Mar 13 23:04:51.462: INFO: Got endpoints: latency-svc-9n982 [750.297754ms]
Mar 13 23:04:51.467: INFO: Created: latency-svc-599jb
Mar 13 23:04:51.512: INFO: Got endpoints: latency-svc-7pjv7 [748.267216ms]
Mar 13 23:04:51.518: INFO: Created: latency-svc-sj7h4
Mar 13 23:04:51.562: INFO: Got endpoints: latency-svc-7z6f8 [749.053694ms]
Mar 13 23:04:51.567: INFO: Created: latency-svc-ltcw6
Mar 13 23:04:51.611: INFO: Got endpoints: latency-svc-nhzfr [749.16396ms]
Mar 13 23:04:51.618: INFO: Created: latency-svc-tw6gm
Mar 13 23:04:51.661: INFO: Got endpoints: latency-svc-28m44 [749.904892ms]
Mar 13 23:04:51.666: INFO: Created: latency-svc-dv4gj
Mar 13 23:04:51.712: INFO: Got endpoints: latency-svc-nf7bw [750.846355ms]
Mar 13 23:04:51.719: INFO: Created: latency-svc-t27cc
Mar 13 23:04:51.762: INFO: Got endpoints: latency-svc-phv2k [750.048149ms]
Mar 13 23:04:51.769: INFO: Created: latency-svc-9vkf7
Mar 13 23:04:51.811: INFO: Got endpoints: latency-svc-sqll2 [749.871524ms]
Mar 13 23:04:51.819: INFO: Created: latency-svc-m9ktt
Mar 13 23:04:51.862: INFO: Got endpoints: latency-svc-bdvbn [749.597949ms]
Mar 13 23:04:51.868: INFO: Created: latency-svc-r4bjk
Mar 13 23:04:51.911: INFO: Got endpoints: latency-svc-mggmw [749.552702ms]
Mar 13 23:04:51.920: INFO: Created: latency-svc-bwmsd
Mar 13 23:04:51.962: INFO: Got endpoints: latency-svc-pfk2r [750.158241ms]
Mar 13 23:04:51.967: INFO: Created: latency-svc-kvq7b
Mar 13 23:04:52.012: INFO: Got endpoints: latency-svc-z4xzj [750.642145ms]
Mar 13 23:04:52.020: INFO: Created: latency-svc-wr8xc
Mar 13 23:04:52.061: INFO: Got endpoints: latency-svc-ncc2s [750.596111ms]
Mar 13 23:04:52.068: INFO: Created: latency-svc-nhprp
Mar 13 23:04:52.111: INFO: Got endpoints: latency-svc-f2mrd [749.706172ms]
Mar 13 23:04:52.117: INFO: Created: latency-svc-vclrz
Mar 13 23:04:52.161: INFO: Got endpoints: latency-svc-f58qc [749.66947ms]
Mar 13 23:04:52.167: INFO: Created: latency-svc-szqdv
Mar 13 23:04:52.211: INFO: Got endpoints: latency-svc-599jb [749.118984ms]
Mar 13 23:04:52.216: INFO: Created: latency-svc-t65jv
Mar 13 23:04:52.261: INFO: Got endpoints: latency-svc-sj7h4 [749.768654ms]
Mar 13 23:04:52.268: INFO: Created: latency-svc-9tf5s
Mar 13 23:04:52.312: INFO: Got endpoints: latency-svc-ltcw6 [749.881203ms]
Mar 13 23:04:52.318: INFO: Created: latency-svc-h675d
Mar 13 23:04:52.361: INFO: Got endpoints: latency-svc-tw6gm [749.754746ms]
Mar 13 23:04:52.369: INFO: Created: latency-svc-zs7mt
Mar 13 23:04:52.412: INFO: Got endpoints: latency-svc-dv4gj [750.502394ms]
Mar 13 23:04:52.417: INFO: Created: latency-svc-jxhsh
Mar 13 23:04:52.462: INFO: Got endpoints: latency-svc-t27cc [749.785951ms]
Mar 13 23:04:52.469: INFO: Created: latency-svc-c4h74
Mar 13 23:04:52.514: INFO: Got endpoints: latency-svc-9vkf7 [752.181193ms]
Mar 13 23:04:52.521: INFO: Created: latency-svc-dj74z
Mar 13 23:04:52.561: INFO: Got endpoints: latency-svc-m9ktt [749.80415ms]
Mar 13 23:04:52.567: INFO: Created: latency-svc-nzp6v
Mar 13 23:04:52.611: INFO: Got endpoints: latency-svc-r4bjk [749.494812ms]
Mar 13 23:04:52.623: INFO: Created: latency-svc-ffqf5
Mar 13 23:04:52.662: INFO: Got endpoints: latency-svc-bwmsd [750.747918ms]
Mar 13 23:04:52.667: INFO: Created: latency-svc-nbk7d
Mar 13 23:04:52.711: INFO: Got endpoints: latency-svc-kvq7b [749.316348ms]
Mar 13 23:04:52.717: INFO: Created: latency-svc-86vc5
Mar 13 23:04:52.762: INFO: Got endpoints: latency-svc-wr8xc [749.843276ms]
Mar 13 23:04:52.768: INFO: Created: latency-svc-f5qq5
Mar 13 23:04:52.811: INFO: Got endpoints: latency-svc-nhprp [749.800635ms]
Mar 13 23:04:52.817: INFO: Created: latency-svc-cdqtg
Mar 13 23:04:52.861: INFO: Got endpoints: latency-svc-vclrz [750.293315ms]
Mar 13 23:04:52.868: INFO: Created: latency-svc-j2w2k
Mar 13 23:04:52.911: INFO: Got endpoints: latency-svc-szqdv [750.03253ms]
Mar 13 23:04:52.918: INFO: Created: latency-svc-vmwcj
Mar 13 23:04:52.961: INFO: Got endpoints: latency-svc-t65jv [749.865454ms]
Mar 13 23:04:52.967: INFO: Created: latency-svc-s8pnb
Mar 13 23:04:53.013: INFO: Got endpoints: latency-svc-9tf5s [751.76368ms]
Mar 13 23:04:53.022: INFO: Created: latency-svc-p85d9
Mar 13 23:04:53.062: INFO: Got endpoints: latency-svc-h675d [750.029021ms]
Mar 13 23:04:53.068: INFO: Created: latency-svc-qvl9j
Mar 13 23:04:53.112: INFO: Got endpoints: latency-svc-zs7mt [750.332838ms]
Mar 13 23:04:53.118: INFO: Created: latency-svc-gplp2
Mar 13 23:04:53.162: INFO: Got endpoints: latency-svc-jxhsh [750.169338ms]
Mar 13 23:04:53.168: INFO: Created: latency-svc-9bxn2
Mar 13 23:04:53.212: INFO: Got endpoints: latency-svc-c4h74 [750.727171ms]
Mar 13 23:04:53.219: INFO: Created: latency-svc-br64g
Mar 13 23:04:53.261: INFO: Got endpoints: latency-svc-dj74z [747.312664ms]
Mar 13 23:04:53.268: INFO: Created: latency-svc-28bm9
Mar 13 23:04:53.312: INFO: Got endpoints: latency-svc-nzp6v [750.340831ms]
Mar 13 23:04:53.317: INFO: Created: latency-svc-rlwzc
Mar 13 23:04:53.361: INFO: Got endpoints: latency-svc-ffqf5 [750.071764ms]
Mar 13 23:04:53.369: INFO: Created: latency-svc-g47fz
Mar 13 23:04:53.411: INFO: Got endpoints: latency-svc-nbk7d [749.659848ms]
Mar 13 23:04:53.418: INFO: Created: latency-svc-vlcdk
Mar 13 23:04:53.461: INFO: Got endpoints: latency-svc-86vc5 [749.645605ms]
Mar 13 23:04:53.468: INFO: Created: latency-svc-bnp4g
Mar 13 23:04:53.512: INFO: Got endpoints: latency-svc-f5qq5 [749.798258ms]
Mar 13 23:04:53.519: INFO: Created: latency-svc-5wfhw
Mar 13 23:04:53.561: INFO: Got endpoints: latency-svc-cdqtg [749.713102ms]
Mar 13 23:04:53.567: INFO: Created: latency-svc-lxf57
Mar 13 23:04:53.612: INFO: Got endpoints: latency-svc-j2w2k [750.177847ms]
Mar 13 23:04:53.617: INFO: Created: latency-svc-hr72j
Mar 13 23:04:53.661: INFO: Got endpoints: latency-svc-vmwcj [750.441199ms]
Mar 13 23:04:53.667: INFO: Created: latency-svc-fnq9q
Mar 13 23:04:53.711: INFO: Got endpoints: latency-svc-s8pnb [750.136677ms]
Mar 13 23:04:53.717: INFO: Created: latency-svc-d494p
Mar 13 23:04:53.762: INFO: Got endpoints: latency-svc-p85d9 [748.441638ms]
Mar 13 23:04:53.780: INFO: Created: latency-svc-96wcj
Mar 13 23:04:53.811: INFO: Got endpoints: latency-svc-qvl9j [748.746457ms]
Mar 13 23:04:53.817: INFO: Created: latency-svc-b52rg
Mar 13 23:04:53.860: INFO: Got endpoints: latency-svc-gplp2 [748.707998ms]
Mar 13 23:04:53.867: INFO: Created: latency-svc-5wc94
Mar 13 23:04:53.912: INFO: Got endpoints: latency-svc-9bxn2 [749.877721ms]
Mar 13 23:04:53.921: INFO: Created: latency-svc-j85vx
Mar 13 23:04:53.961: INFO: Got endpoints: latency-svc-br64g [748.668557ms]
Mar 13 23:04:53.970: INFO: Created: latency-svc-pq5bg
Mar 13 23:04:54.012: INFO: Got endpoints: latency-svc-28bm9 [750.5934ms]
Mar 13 23:04:54.019: INFO: Created: latency-svc-xkmbf
Mar 13 23:04:54.062: INFO: Got endpoints: latency-svc-rlwzc [750.740351ms]
Mar 13 23:04:54.071: INFO: Created: latency-svc-4qkvw
Mar 13 23:04:54.112: INFO: Got endpoints: latency-svc-g47fz [750.262947ms]
Mar 13 23:04:54.117: INFO: Created: latency-svc-t4hq5
Mar 13 23:04:54.162: INFO: Got endpoints: latency-svc-vlcdk [750.271891ms]
Mar 13 23:04:54.171: INFO: Created: latency-svc-9xfrg
Mar 13 23:04:54.215: INFO: Got endpoints: latency-svc-bnp4g [753.967355ms]
Mar 13 23:04:54.222: INFO: Created: latency-svc-kmvdf
Mar 13 23:04:54.261: INFO: Got endpoints: latency-svc-5wfhw [749.390161ms]
Mar 13 23:04:54.267: INFO: Created: latency-svc-sld8g
Mar 13 23:04:54.312: INFO: Got endpoints: latency-svc-lxf57 [750.399682ms]
Mar 13 23:04:54.318: INFO: Created: latency-svc-xfndm
Mar 13 23:04:54.363: INFO: Got endpoints: latency-svc-hr72j [750.941194ms]
Mar 13 23:04:54.370: INFO: Created: latency-svc-lbptk
Mar 13 23:04:54.413: INFO: Got endpoints: latency-svc-fnq9q [751.921633ms]
Mar 13 23:04:54.419: INFO: Created: latency-svc-c99xb
Mar 13 23:04:54.463: INFO: Got endpoints: latency-svc-d494p [751.940137ms]
Mar 13 23:04:54.469: INFO: Created: latency-svc-49qsk
Mar 13 23:04:54.512: INFO: Got endpoints: latency-svc-96wcj [749.816606ms]
Mar 13 23:04:54.520: INFO: Created: latency-svc-scx6p
Mar 13 23:04:54.561: INFO: Got endpoints: latency-svc-b52rg [750.331896ms]
Mar 13 23:04:54.568: INFO: Created: latency-svc-4mcjj
Mar 13 23:04:54.611: INFO: Got endpoints: latency-svc-5wc94 [750.827398ms]
Mar 13 23:04:54.618: INFO: Created: latency-svc-nqmkv
Mar 13 23:04:54.661: INFO: Got endpoints: latency-svc-j85vx [749.089982ms]
Mar 13 23:04:54.666: INFO: Created: latency-svc-w9twd
Mar 13 23:04:54.714: INFO: Got endpoints: latency-svc-pq5bg [752.800157ms]
Mar 13 23:04:54.720: INFO: Created: latency-svc-svnsn
Mar 13 23:04:54.761: INFO: Got endpoints: latency-svc-xkmbf [749.005583ms]
Mar 13 23:04:54.767: INFO: Created: latency-svc-tdzzt
Mar 13 23:04:54.811: INFO: Got endpoints: latency-svc-4qkvw [748.553794ms]
Mar 13 23:04:54.819: INFO: Created: latency-svc-cg6bx
Mar 13 23:04:54.861: INFO: Got endpoints: latency-svc-t4hq5 [749.228501ms]
Mar 13 23:04:54.869: INFO: Created: latency-svc-tlnc2
Mar 13 23:04:54.913: INFO: Got endpoints: latency-svc-9xfrg [751.156757ms]
Mar 13 23:04:54.924: INFO: Created: latency-svc-8bz8d
Mar 13 23:04:54.962: INFO: Got endpoints: latency-svc-kmvdf [747.094206ms]
Mar 13 23:04:55.013: INFO: Got endpoints: latency-svc-sld8g [751.937522ms]
Mar 13 23:04:55.062: INFO: Got endpoints: latency-svc-xfndm [750.293348ms]
Mar 13 23:04:55.111: INFO: Got endpoints: latency-svc-lbptk [748.396604ms]
Mar 13 23:04:55.161: INFO: Got endpoints: latency-svc-c99xb [748.067897ms]
Mar 13 23:04:55.212: INFO: Got endpoints: latency-svc-49qsk [748.924869ms]
Mar 13 23:04:55.261: INFO: Got endpoints: latency-svc-scx6p [749.411092ms]
Mar 13 23:04:55.311: INFO: Got endpoints: latency-svc-4mcjj [750.526315ms]
Mar 13 23:04:55.361: INFO: Got endpoints: latency-svc-nqmkv [749.757859ms]
Mar 13 23:04:55.412: INFO: Got endpoints: latency-svc-w9twd [750.960447ms]
Mar 13 23:04:55.462: INFO: Got endpoints: latency-svc-svnsn [747.724033ms]
Mar 13 23:04:55.512: INFO: Got endpoints: latency-svc-tdzzt [750.811042ms]
Mar 13 23:04:55.561: INFO: Got endpoints: latency-svc-cg6bx [750.368978ms]
Mar 13 23:04:55.612: INFO: Got endpoints: latency-svc-tlnc2 [750.505196ms]
Mar 13 23:04:55.662: INFO: Got endpoints: latency-svc-8bz8d [749.076254ms]
Mar 13 23:04:55.662: INFO: Latencies: [16.707435ms 18.03492ms 21.204112ms 30.082477ms 35.749169ms 45.218295ms 49.317251ms 54.16801ms 56.775491ms 67.156157ms 71.147157ms 79.409055ms 82.331617ms 83.732238ms 84.475661ms 86.173739ms 86.519796ms 86.91632ms 87.430804ms 88.81942ms 89.008002ms 89.27592ms 89.645994ms 89.892061ms 90.697095ms 91.198124ms 91.417053ms 91.963534ms 92.791523ms 93.773705ms 94.455573ms 94.861532ms 101.307691ms 129.308209ms 169.841048ms 216.35081ms 257.291879ms 306.381357ms 342.614967ms 390.177849ms 433.980586ms 482.48201ms 525.40815ms 569.52101ms 614.279439ms 655.041112ms 703.646557ms 744.693439ms 747.094206ms 747.312664ms 747.526433ms 747.724033ms 748.067897ms 748.267216ms 748.396604ms 748.441638ms 748.445646ms 748.553794ms 748.660489ms 748.668557ms 748.707998ms 748.746457ms 748.784956ms 748.811611ms 748.924869ms 748.974622ms 749.005583ms 749.053694ms 749.076254ms 749.079501ms 749.089982ms 749.118984ms 749.16396ms 749.228501ms 749.243431ms 749.257785ms 749.263716ms 749.293327ms 749.316348ms 749.31715ms 749.383736ms 749.390161ms 749.411092ms 749.494812ms 749.502181ms 749.516976ms 749.518426ms 749.533366ms 749.552702ms 749.554378ms 749.597949ms 749.645605ms 749.659848ms 749.664319ms 749.66947ms 749.684227ms 749.692527ms 749.706172ms 749.713102ms 749.753676ms 749.754746ms 749.755751ms 749.757859ms 749.768654ms 749.783689ms 749.785951ms 749.789098ms 749.798258ms 749.800635ms 749.801847ms 749.80415ms 749.804307ms 749.816606ms 749.82838ms 749.843276ms 749.849775ms 749.865454ms 749.871524ms 749.877688ms 749.877721ms 749.881203ms 749.883876ms 749.904892ms 749.927626ms 749.932864ms 749.950765ms 750.029021ms 750.03253ms 750.048149ms 750.055387ms 750.071764ms 750.130302ms 750.136677ms 750.140355ms 750.158241ms 750.164303ms 750.169338ms 750.177847ms 750.200773ms 750.250197ms 750.262947ms 750.271891ms 750.277143ms 750.283283ms 750.291778ms 750.293315ms 750.293348ms 750.297754ms 750.312445ms 750.331896ms 750.332838ms 750.334621ms 750.340831ms 750.368978ms 750.38461ms 750.399682ms 750.441199ms 750.502394ms 750.505196ms 750.526315ms 750.584034ms 750.5934ms 750.594597ms 750.596111ms 750.611339ms 750.642145ms 750.647836ms 750.700404ms 750.711547ms 750.727171ms 750.740351ms 750.747918ms 750.77045ms 750.811042ms 750.825854ms 750.827398ms 750.827842ms 750.835501ms 750.846355ms 750.941194ms 750.960447ms 750.962851ms 751.017448ms 751.040348ms 751.042324ms 751.052932ms 751.156757ms 751.251579ms 751.287413ms 751.505585ms 751.670485ms 751.76368ms 751.921633ms 751.937522ms 751.940137ms 752.013248ms 752.181193ms 752.800157ms 753.967355ms 754.554993ms]
Mar 13 23:04:55.662: INFO: 50 %ile: 749.754746ms
Mar 13 23:04:55.662: INFO: 90 %ile: 750.960447ms
Mar 13 23:04:55.662: INFO: 99 %ile: 753.967355ms
Mar 13 23:04:55.662: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:04:55.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-rm97c" for this suite.
Mar 13 23:05:09.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:05:09.704: INFO: namespace: e2e-tests-svc-latency-rm97c, resource: bindings, ignored listing per whitelist
Mar 13 23:05:09.740: INFO: namespace e2e-tests-svc-latency-rm97c deletion completed in 14.074360213s

• [SLOW TEST:25.845 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:05:09.740: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Mar 13 23:05:09.796: INFO: Waiting up to 5m0s for pod "client-containers-73211992-45e4-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-containers-52g95" to be "success or failure"
Mar 13 23:05:09.799: INFO: Pod "client-containers-73211992-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179846ms
Mar 13 23:05:11.802: INFO: Pod "client-containers-73211992-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005921648s
Mar 13 23:05:13.806: INFO: Pod "client-containers-73211992-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009666802s
Mar 13 23:05:15.809: INFO: Pod "client-containers-73211992-45e4-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012784687s
STEP: Saw pod success
Mar 13 23:05:15.809: INFO: Pod "client-containers-73211992-45e4-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:05:15.811: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod client-containers-73211992-45e4-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:05:15.823: INFO: Waiting for pod client-containers-73211992-45e4-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:05:15.824: INFO: Pod client-containers-73211992-45e4-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:05:15.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-52g95" for this suite.
Mar 13 23:05:21.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:05:21.850: INFO: namespace: e2e-tests-containers-52g95, resource: bindings, ignored listing per whitelist
Mar 13 23:05:21.899: INFO: namespace e2e-tests-containers-52g95 deletion completed in 6.072609368s

• [SLOW TEST:12.159 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:05:21.900: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:05:21.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-4wzgh" for this suite.
Mar 13 23:05:27.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:05:27.998: INFO: namespace: e2e-tests-kubelet-test-4wzgh, resource: bindings, ignored listing per whitelist
Mar 13 23:05:28.061: INFO: namespace e2e-tests-kubelet-test-4wzgh deletion completed in 6.079411368s

• [SLOW TEST:6.162 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:05:28.061: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-2dzgc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-2dzgc to expose endpoints map[]
Mar 13 23:05:28.114: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-2dzgc exposes endpoints map[] (3.821063ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-2dzgc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-2dzgc to expose endpoints map[pod1:[100]]
Mar 13 23:05:31.147: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-2dzgc exposes endpoints map[pod1:[100]] (3.026300411s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-2dzgc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-2dzgc to expose endpoints map[pod1:[100] pod2:[101]]
Mar 13 23:05:34.181: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-2dzgc exposes endpoints map[pod2:[101] pod1:[100]] (3.030256277s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-2dzgc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-2dzgc to expose endpoints map[pod2:[101]]
Mar 13 23:05:35.193: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-2dzgc exposes endpoints map[pod2:[101]] (1.008906073s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-2dzgc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-2dzgc to expose endpoints map[]
Mar 13 23:05:35.199: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-2dzgc exposes endpoints map[] (2.354588ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:05:35.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-2dzgc" for this suite.
Mar 13 23:05:49.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:05:49.268: INFO: namespace: e2e-tests-services-2dzgc, resource: bindings, ignored listing per whitelist
Mar 13 23:05:49.297: INFO: namespace e2e-tests-services-2dzgc deletion completed in 14.080244466s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:21.235 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:05:49.297: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 13 23:05:49.349: INFO: Waiting up to 5m0s for pod "pod-8ab4460b-45e4-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-dthvn" to be "success or failure"
Mar 13 23:05:49.351: INFO: Pod "pod-8ab4460b-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.611811ms
Mar 13 23:05:51.354: INFO: Pod "pod-8ab4460b-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005103985s
Mar 13 23:05:53.358: INFO: Pod "pod-8ab4460b-45e4-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008513672s
STEP: Saw pod success
Mar 13 23:05:53.358: INFO: Pod "pod-8ab4460b-45e4-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:05:53.359: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-8ab4460b-45e4-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:05:53.372: INFO: Waiting for pod pod-8ab4460b-45e4-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:05:53.374: INFO: Pod pod-8ab4460b-45e4-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:05:53.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dthvn" for this suite.
Mar 13 23:05:59.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:05:59.423: INFO: namespace: e2e-tests-emptydir-dthvn, resource: bindings, ignored listing per whitelist
Mar 13 23:05:59.449: INFO: namespace e2e-tests-emptydir-dthvn deletion completed in 6.071786262s

• [SLOW TEST:10.152 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:05:59.449: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-tc2vf
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Mar 13 23:05:59.505: INFO: Found 0 stateful pods, waiting for 3
Mar 13 23:06:09.509: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 23:06:09.509: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 23:06:09.509: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 23:06:09.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-tc2vf ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 13 23:06:09.675: INFO: stderr: ""
Mar 13 23:06:09.675: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 13 23:06:09.675: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar 13 23:06:19.702: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 13 23:06:29.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-tc2vf ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 13 23:06:29.876: INFO: stderr: ""
Mar 13 23:06:29.876: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 13 23:06:29.876: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 13 23:06:59.891: INFO: Waiting for StatefulSet e2e-tests-statefulset-tc2vf/ss2 to complete update
STEP: Rolling back to a previous revision
Mar 13 23:07:09.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-tc2vf ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 13 23:07:10.070: INFO: stderr: ""
Mar 13 23:07:10.070: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 13 23:07:10.070: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 13 23:07:20.096: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 13 23:07:30.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 exec --namespace=e2e-tests-statefulset-tc2vf ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 13 23:07:30.254: INFO: stderr: ""
Mar 13 23:07:30.254: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 13 23:07:30.254: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 13 23:07:40.274: INFO: Waiting for StatefulSet e2e-tests-statefulset-tc2vf/ss2 to complete update
Mar 13 23:07:40.274: INFO: Waiting for Pod e2e-tests-statefulset-tc2vf/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar 13 23:07:40.274: INFO: Waiting for Pod e2e-tests-statefulset-tc2vf/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar 13 23:07:50.280: INFO: Waiting for StatefulSet e2e-tests-statefulset-tc2vf/ss2 to complete update
Mar 13 23:07:50.280: INFO: Waiting for Pod e2e-tests-statefulset-tc2vf/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 13 23:08:00.281: INFO: Deleting all statefulset in ns e2e-tests-statefulset-tc2vf
Mar 13 23:08:00.284: INFO: Scaling statefulset ss2 to 0
Mar 13 23:08:10.298: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 23:08:10.300: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:08:10.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-tc2vf" for this suite.
Mar 13 23:08:16.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:08:16.394: INFO: namespace: e2e-tests-statefulset-tc2vf, resource: bindings, ignored listing per whitelist
Mar 13 23:08:16.426: INFO: namespace e2e-tests-statefulset-tc2vf deletion completed in 6.114779852s

• [SLOW TEST:136.977 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:08:16.426: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 13 23:08:16.488: INFO: Waiting up to 5m0s for pod "pod-e267b983-45e4-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-vpg65" to be "success or failure"
Mar 13 23:08:16.491: INFO: Pod "pod-e267b983-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 3.286357ms
Mar 13 23:08:18.496: INFO: Pod "pod-e267b983-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007734847s
Mar 13 23:08:20.499: INFO: Pod "pod-e267b983-45e4-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011108408s
STEP: Saw pod success
Mar 13 23:08:20.499: INFO: Pod "pod-e267b983-45e4-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:08:20.501: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-e267b983-45e4-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:08:20.517: INFO: Waiting for pod pod-e267b983-45e4-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:08:20.519: INFO: Pod pod-e267b983-45e4-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:08:20.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vpg65" for this suite.
Mar 13 23:08:26.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:08:26.559: INFO: namespace: e2e-tests-emptydir-vpg65, resource: bindings, ignored listing per whitelist
Mar 13 23:08:26.606: INFO: namespace e2e-tests-emptydir-vpg65 deletion completed in 6.083592295s

• [SLOW TEST:10.180 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:08:26.606: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-e876cf60-45e4-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 23:08:26.652: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e877207e-45e4-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-7fczd" to be "success or failure"
Mar 13 23:08:26.654: INFO: Pod "pod-projected-secrets-e877207e-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.811729ms
Mar 13 23:08:28.657: INFO: Pod "pod-projected-secrets-e877207e-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005034921s
Mar 13 23:08:30.661: INFO: Pod "pod-projected-secrets-e877207e-45e4-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008477244s
STEP: Saw pod success
Mar 13 23:08:30.661: INFO: Pod "pod-projected-secrets-e877207e-45e4-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:08:30.663: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-projected-secrets-e877207e-45e4-11e9-8b9c-0a58ac140107 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 13 23:08:30.676: INFO: Waiting for pod pod-projected-secrets-e877207e-45e4-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:08:30.678: INFO: Pod pod-projected-secrets-e877207e-45e4-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:08:30.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7fczd" for this suite.
Mar 13 23:08:36.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:08:36.724: INFO: namespace: e2e-tests-projected-7fczd, resource: bindings, ignored listing per whitelist
Mar 13 23:08:36.763: INFO: namespace e2e-tests-projected-7fczd deletion completed in 6.081607009s

• [SLOW TEST:10.157 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:08:36.763: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Mar 13 23:08:36.812: INFO: Waiting up to 5m0s for pod "client-containers-ee84e69e-45e4-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-containers-268sc" to be "success or failure"
Mar 13 23:08:36.814: INFO: Pod "client-containers-ee84e69e-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.854658ms
Mar 13 23:08:38.816: INFO: Pod "client-containers-ee84e69e-45e4-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003912703s
Mar 13 23:08:40.819: INFO: Pod "client-containers-ee84e69e-45e4-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007296888s
STEP: Saw pod success
Mar 13 23:08:40.819: INFO: Pod "client-containers-ee84e69e-45e4-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:08:40.821: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod client-containers-ee84e69e-45e4-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:08:40.835: INFO: Waiting for pod client-containers-ee84e69e-45e4-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:08:40.837: INFO: Pod client-containers-ee84e69e-45e4-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:08:40.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-268sc" for this suite.
Mar 13 23:08:46.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:08:46.859: INFO: namespace: e2e-tests-containers-268sc, resource: bindings, ignored listing per whitelist
Mar 13 23:08:46.906: INFO: namespace e2e-tests-containers-268sc deletion completed in 6.066740845s

• [SLOW TEST:10.143 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:08:46.906: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:09:46.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-25d2l" for this suite.
Mar 13 23:10:08.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:10:08.974: INFO: namespace: e2e-tests-container-probe-25d2l, resource: bindings, ignored listing per whitelist
Mar 13 23:10:09.029: INFO: namespace e2e-tests-container-probe-25d2l deletion completed in 22.067625566s

• [SLOW TEST:82.122 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:10:09.029: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-258346d2-45e5-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 23:10:09.075: INFO: Waiting up to 5m0s for pod "pod-configmaps-2583990a-45e5-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-configmap-nr5zr" to be "success or failure"
Mar 13 23:10:09.080: INFO: Pod "pod-configmaps-2583990a-45e5-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.577134ms
Mar 13 23:10:11.083: INFO: Pod "pod-configmaps-2583990a-45e5-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007960802s
Mar 13 23:10:13.087: INFO: Pod "pod-configmaps-2583990a-45e5-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011414172s
STEP: Saw pod success
Mar 13 23:10:13.087: INFO: Pod "pod-configmaps-2583990a-45e5-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:10:13.089: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-configmaps-2583990a-45e5-11e9-8b9c-0a58ac140107 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 23:10:13.104: INFO: Waiting for pod pod-configmaps-2583990a-45e5-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:10:13.106: INFO: Pod pod-configmaps-2583990a-45e5-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:10:13.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-nr5zr" for this suite.
Mar 13 23:10:19.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:10:19.137: INFO: namespace: e2e-tests-configmap-nr5zr, resource: bindings, ignored listing per whitelist
Mar 13 23:10:19.181: INFO: namespace e2e-tests-configmap-nr5zr deletion completed in 6.072504839s

• [SLOW TEST:10.152 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:10:19.181: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 23:10:19.221: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:10:20.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-bgqvf" for this suite.
Mar 13 23:10:26.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:10:26.361: INFO: namespace: e2e-tests-custom-resource-definition-bgqvf, resource: bindings, ignored listing per whitelist
Mar 13 23:10:26.405: INFO: namespace e2e-tests-custom-resource-definition-bgqvf deletion completed in 6.073041886s

• [SLOW TEST:7.224 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:10:26.405: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-2fdf2d5d-45e5-11e9-8b9c-0a58ac140107
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:10:30.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-2sjgb" for this suite.
Mar 13 23:10:50.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:10:50.526: INFO: namespace: e2e-tests-configmap-2sjgb, resource: bindings, ignored listing per whitelist
Mar 13 23:10:50.568: INFO: namespace e2e-tests-configmap-2sjgb deletion completed in 20.089182137s

• [SLOW TEST:24.162 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:10:50.568: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 23:10:50.626: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e47996b-45e5-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-tm8wq" to be "success or failure"
Mar 13 23:10:50.632: INFO: Pod "downwardapi-volume-3e47996b-45e5-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 5.101264ms
Mar 13 23:10:52.635: INFO: Pod "downwardapi-volume-3e47996b-45e5-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008433151s
Mar 13 23:10:54.638: INFO: Pod "downwardapi-volume-3e47996b-45e5-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012005323s
STEP: Saw pod success
Mar 13 23:10:54.639: INFO: Pod "downwardapi-volume-3e47996b-45e5-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:10:54.640: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downwardapi-volume-3e47996b-45e5-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 23:10:54.654: INFO: Waiting for pod downwardapi-volume-3e47996b-45e5-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:10:54.656: INFO: Pod downwardapi-volume-3e47996b-45e5-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:10:54.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tm8wq" for this suite.
Mar 13 23:11:00.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:11:00.684: INFO: namespace: e2e-tests-downward-api-tm8wq, resource: bindings, ignored listing per whitelist
Mar 13 23:11:00.731: INFO: namespace e2e-tests-downward-api-tm8wq deletion completed in 6.072003257s

• [SLOW TEST:10.163 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:11:00.731: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-44542704-45e5-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 23:11:00.776: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-44547952-45e5-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-hmktl" to be "success or failure"
Mar 13 23:11:00.779: INFO: Pod "pod-projected-configmaps-44547952-45e5-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.700987ms
Mar 13 23:11:02.782: INFO: Pod "pod-projected-configmaps-44547952-45e5-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00640498s
Mar 13 23:11:04.786: INFO: Pod "pod-projected-configmaps-44547952-45e5-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009852042s
STEP: Saw pod success
Mar 13 23:11:04.786: INFO: Pod "pod-projected-configmaps-44547952-45e5-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:11:04.788: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-projected-configmaps-44547952-45e5-11e9-8b9c-0a58ac140107 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 23:11:04.804: INFO: Waiting for pod pod-projected-configmaps-44547952-45e5-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:11:04.805: INFO: Pod pod-projected-configmaps-44547952-45e5-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:11:04.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hmktl" for this suite.
Mar 13 23:11:10.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:11:10.841: INFO: namespace: e2e-tests-projected-hmktl, resource: bindings, ignored listing per whitelist
Mar 13 23:11:10.881: INFO: namespace e2e-tests-projected-hmktl deletion completed in 6.072491884s

• [SLOW TEST:10.150 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:11:10.881: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-dw7x4
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-dw7x4
STEP: Deleting pre-stop pod
Mar 13 23:11:25.962: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:11:25.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-dw7x4" for this suite.
Mar 13 23:12:03.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:12:04.046: INFO: namespace: e2e-tests-prestop-dw7x4, resource: bindings, ignored listing per whitelist
Mar 13 23:12:04.051: INFO: namespace e2e-tests-prestop-dw7x4 deletion completed in 38.082318157s

• [SLOW TEST:53.170 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:12:04.051: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 23:12:04.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 version'
Mar 13 23:12:04.179: INFO: stderr: ""
Mar 13 23:12:04.179: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.4\", GitCommit:\"c27b913fddd1a6c480c229191a087698aa92f0b1\", GitTreeState:\"clean\", BuildDate:\"2019-02-28T13:30:26Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:12:04.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vxn5q" for this suite.
Mar 13 23:12:10.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:12:10.214: INFO: namespace: e2e-tests-kubectl-vxn5q, resource: bindings, ignored listing per whitelist
Mar 13 23:12:10.250: INFO: namespace e2e-tests-kubectl-vxn5q deletion completed in 6.067355631s

• [SLOW TEST:6.199 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:12:10.250: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar 13 23:12:10.298: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:12:14.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-hgh86" for this suite.
Mar 13 23:12:36.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:12:36.328: INFO: namespace: e2e-tests-init-container-hgh86, resource: bindings, ignored listing per whitelist
Mar 13 23:12:36.370: INFO: namespace e2e-tests-init-container-hgh86 deletion completed in 22.073352938s

• [SLOW TEST:26.120 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:12:36.370: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-mwpj
STEP: Creating a pod to test atomic-volume-subpath
Mar 13 23:12:36.419: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mwpj" in namespace "e2e-tests-subpath-pmxql" to be "success or failure"
Mar 13 23:12:36.421: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Pending", Reason="", readiness=false. Elapsed: 1.975093ms
Mar 13 23:12:38.424: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005029507s
Mar 13 23:12:40.427: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Running", Reason="", readiness=false. Elapsed: 4.008148974s
Mar 13 23:12:42.430: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Running", Reason="", readiness=false. Elapsed: 6.011755029s
Mar 13 23:12:44.434: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Running", Reason="", readiness=false. Elapsed: 8.015460234s
Mar 13 23:12:46.438: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Running", Reason="", readiness=false. Elapsed: 10.018863577s
Mar 13 23:12:48.441: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Running", Reason="", readiness=false. Elapsed: 12.022464662s
Mar 13 23:12:50.445: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Running", Reason="", readiness=false. Elapsed: 14.026315943s
Mar 13 23:12:52.448: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Running", Reason="", readiness=false. Elapsed: 16.029641679s
Mar 13 23:12:54.452: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Running", Reason="", readiness=false. Elapsed: 18.032986407s
Mar 13 23:12:56.455: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Running", Reason="", readiness=false. Elapsed: 20.036155863s
Mar 13 23:12:58.458: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Running", Reason="", readiness=false. Elapsed: 22.039686186s
Mar 13 23:13:00.462: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Running", Reason="", readiness=false. Elapsed: 24.043201242s
Mar 13 23:13:02.465: INFO: Pod "pod-subpath-test-configmap-mwpj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.046598877s
STEP: Saw pod success
Mar 13 23:13:02.465: INFO: Pod "pod-subpath-test-configmap-mwpj" satisfied condition "success or failure"
Mar 13 23:13:02.467: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-subpath-test-configmap-mwpj container test-container-subpath-configmap-mwpj: <nil>
STEP: delete the pod
Mar 13 23:13:02.482: INFO: Waiting for pod pod-subpath-test-configmap-mwpj to disappear
Mar 13 23:13:02.485: INFO: Pod pod-subpath-test-configmap-mwpj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mwpj
Mar 13 23:13:02.485: INFO: Deleting pod "pod-subpath-test-configmap-mwpj" in namespace "e2e-tests-subpath-pmxql"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:13:02.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-pmxql" for this suite.
Mar 13 23:13:08.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:13:08.532: INFO: namespace: e2e-tests-subpath-pmxql, resource: bindings, ignored listing per whitelist
Mar 13 23:13:08.565: INFO: namespace e2e-tests-subpath-pmxql deletion completed in 6.075326058s

• [SLOW TEST:32.195 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:13:08.565: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 23:13:08.625: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 13 23:13:08.631: INFO: Number of nodes with available pods: 0
Mar 13 23:13:08.631: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 13 23:13:08.648: INFO: Number of nodes with available pods: 0
Mar 13 23:13:08.648: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:09.652: INFO: Number of nodes with available pods: 0
Mar 13 23:13:09.652: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:10.652: INFO: Number of nodes with available pods: 0
Mar 13 23:13:10.653: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:11.657: INFO: Number of nodes with available pods: 1
Mar 13 23:13:11.657: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 13 23:13:11.669: INFO: Number of nodes with available pods: 0
Mar 13 23:13:11.669: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 13 23:13:11.674: INFO: Number of nodes with available pods: 0
Mar 13 23:13:11.674: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:12.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:12.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:13.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:13.677: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:14.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:14.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:15.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:15.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:16.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:16.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:17.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:17.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:18.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:18.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:19.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:19.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:20.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:20.677: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:21.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:21.677: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:22.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:22.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:23.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:23.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:24.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:24.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:25.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:25.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:26.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:26.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:27.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:27.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:28.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:28.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:29.681: INFO: Number of nodes with available pods: 0
Mar 13 23:13:29.681: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:30.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:30.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:31.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:31.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:32.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:32.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:33.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:33.677: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:34.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:34.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:35.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:35.677: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:36.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:36.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:37.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:37.677: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:38.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:38.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:39.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:39.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:40.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:40.677: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:41.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:41.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:42.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:42.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:43.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:43.677: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:44.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:44.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:45.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:45.677: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:46.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:46.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:47.678: INFO: Number of nodes with available pods: 0
Mar 13 23:13:47.678: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:48.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:48.677: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:49.677: INFO: Number of nodes with available pods: 0
Mar 13 23:13:49.677: INFO: Node karbon-test-3a0ff6-k8s-worker-0 is running more than one daemon pod
Mar 13 23:13:50.678: INFO: Number of nodes with available pods: 1
Mar 13 23:13:50.678: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-555wr, will wait for the garbage collector to delete the pods
Mar 13 23:13:50.744: INFO: Deleting DaemonSet.extensions daemon-set took: 7.167391ms
Mar 13 23:13:50.844: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.306264ms
Mar 13 23:14:27.847: INFO: Number of nodes with available pods: 0
Mar 13 23:14:27.847: INFO: Number of running nodes: 0, number of available pods: 0
Mar 13 23:14:27.849: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-555wr/daemonsets","resourceVersion":"39560"},"items":null}

Mar 13 23:14:27.851: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-555wr/pods","resourceVersion":"39560"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:14:27.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-555wr" for this suite.
Mar 13 23:14:33.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:14:33.901: INFO: namespace: e2e-tests-daemonsets-555wr, resource: bindings, ignored listing per whitelist
Mar 13 23:14:33.941: INFO: namespace e2e-tests-daemonsets-555wr deletion completed in 6.075333863s

• [SLOW TEST:85.376 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:14:33.941: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Mar 13 23:14:33.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 --namespace=e2e-tests-kubectl-t58jg run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar 13 23:14:37.225: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar 13 23:14:37.225: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:14:39.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-t58jg" for this suite.
Mar 13 23:14:49.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:14:49.256: INFO: namespace: e2e-tests-kubectl-t58jg, resource: bindings, ignored listing per whitelist
Mar 13 23:14:49.317: INFO: namespace e2e-tests-kubectl-t58jg deletion completed in 10.083525641s

• [SLOW TEST:15.375 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:14:49.317: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-gds76
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-gds76 to expose endpoints map[]
Mar 13 23:14:49.375: INFO: Get endpoints failed (1.968297ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar 13 23:14:50.379: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-gds76 exposes endpoints map[] (1.005591933s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-gds76
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-gds76 to expose endpoints map[pod1:[80]]
Mar 13 23:14:53.426: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-gds76 exposes endpoints map[pod1:[80]] (3.04165341s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-gds76
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-gds76 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 13 23:14:56.459: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-gds76 exposes endpoints map[pod1:[80] pod2:[80]] (3.030238462s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-gds76
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-gds76 to expose endpoints map[pod2:[80]]
Mar 13 23:14:56.470: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-gds76 exposes endpoints map[pod2:[80]] (6.603759ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-gds76
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-gds76 to expose endpoints map[]
Mar 13 23:14:57.482: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-gds76 exposes endpoints map[] (1.005144316s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:14:57.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-gds76" for this suite.
Mar 13 23:15:19.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:15:19.561: INFO: namespace: e2e-tests-services-gds76, resource: bindings, ignored listing per whitelist
Mar 13 23:15:19.571: INFO: namespace e2e-tests-services-gds76 deletion completed in 22.072916029s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:30.255 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:15:19.572: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 23:15:19.668: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:15:23.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-nz9d9" for this suite.
Mar 13 23:16:13.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:16:13.773: INFO: namespace: e2e-tests-pods-nz9d9, resource: bindings, ignored listing per whitelist
Mar 13 23:16:13.775: INFO: namespace e2e-tests-pods-nz9d9 deletion completed in 50.077540455s

• [SLOW TEST:54.203 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:16:13.775: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0313 23:16:19.842190      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 13 23:16:19.842: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:16:19.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-bmhq5" for this suite.
Mar 13 23:16:25.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:16:25.894: INFO: namespace: e2e-tests-gc-bmhq5, resource: bindings, ignored listing per whitelist
Mar 13 23:16:25.922: INFO: namespace e2e-tests-gc-bmhq5 deletion completed in 6.077860512s

• [SLOW TEST:12.147 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:16:25.922: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar 13 23:16:30.496: INFO: Successfully updated pod "annotationupdate0629a7a1-45e6-11e9-8b9c-0a58ac140107"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:16:32.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-m46jn" for this suite.
Mar 13 23:16:54.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:16:54.561: INFO: namespace: e2e-tests-projected-m46jn, resource: bindings, ignored listing per whitelist
Mar 13 23:16:54.597: INFO: namespace e2e-tests-projected-m46jn deletion completed in 22.080624815s

• [SLOW TEST:28.675 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:16:54.598: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Mar 13 23:16:55.164: INFO: created pod pod-service-account-defaultsa
Mar 13 23:16:55.164: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 13 23:16:55.169: INFO: created pod pod-service-account-mountsa
Mar 13 23:16:55.169: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 13 23:16:55.176: INFO: created pod pod-service-account-nomountsa
Mar 13 23:16:55.176: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 13 23:16:55.188: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 13 23:16:55.188: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 13 23:16:55.192: INFO: created pod pod-service-account-mountsa-mountspec
Mar 13 23:16:55.192: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 13 23:16:55.195: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 13 23:16:55.195: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 13 23:16:55.204: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 13 23:16:55.204: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 13 23:16:55.211: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 13 23:16:55.211: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 13 23:16:55.218: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 13 23:16:55.218: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:16:55.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-4t8pw" for this suite.
Mar 13 23:17:01.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:17:01.259: INFO: namespace: e2e-tests-svcaccounts-4t8pw, resource: bindings, ignored listing per whitelist
Mar 13 23:17:01.306: INFO: namespace e2e-tests-svcaccounts-4t8pw deletion completed in 6.080403386s

• [SLOW TEST:6.708 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:17:01.307: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 13 23:17:01.356: INFO: Waiting up to 5m0s for pod "pod-1b405e2b-45e6-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-7llf7" to be "success or failure"
Mar 13 23:17:01.359: INFO: Pod "pod-1b405e2b-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 3.319019ms
Mar 13 23:17:03.363: INFO: Pod "pod-1b405e2b-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00690193s
Mar 13 23:17:05.366: INFO: Pod "pod-1b405e2b-45e6-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010592813s
STEP: Saw pod success
Mar 13 23:17:05.366: INFO: Pod "pod-1b405e2b-45e6-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:17:05.368: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-1b405e2b-45e6-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:17:05.382: INFO: Waiting for pod pod-1b405e2b-45e6-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:17:05.384: INFO: Pod pod-1b405e2b-45e6-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:17:05.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7llf7" for this suite.
Mar 13 23:17:11.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:17:11.439: INFO: namespace: e2e-tests-emptydir-7llf7, resource: bindings, ignored listing per whitelist
Mar 13 23:17:11.465: INFO: namespace e2e-tests-emptydir-7llf7 deletion completed in 6.078257582s

• [SLOW TEST:10.158 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:17:11.465: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-214f9322-45e6-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 23:17:11.524: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-214fe226-45e6-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-mtx2x" to be "success or failure"
Mar 13 23:17:11.526: INFO: Pod "pod-projected-secrets-214fe226-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002558ms
Mar 13 23:17:13.529: INFO: Pod "pod-projected-secrets-214fe226-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005718269s
Mar 13 23:17:15.533: INFO: Pod "pod-projected-secrets-214fe226-45e6-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008941469s
STEP: Saw pod success
Mar 13 23:17:15.533: INFO: Pod "pod-projected-secrets-214fe226-45e6-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:17:15.535: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-projected-secrets-214fe226-45e6-11e9-8b9c-0a58ac140107 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 13 23:17:15.545: INFO: Waiting for pod pod-projected-secrets-214fe226-45e6-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:17:15.548: INFO: Pod pod-projected-secrets-214fe226-45e6-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:17:15.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mtx2x" for this suite.
Mar 13 23:17:21.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:17:21.603: INFO: namespace: e2e-tests-projected-mtx2x, resource: bindings, ignored listing per whitelist
Mar 13 23:17:21.622: INFO: namespace e2e-tests-projected-mtx2x deletion completed in 6.071141097s

• [SLOW TEST:10.157 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:17:21.622: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 23:17:21.681: INFO: Waiting up to 5m0s for pod "downwardapi-volume-275d954e-45e6-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-pptv2" to be "success or failure"
Mar 13 23:17:21.701: INFO: Pod "downwardapi-volume-275d954e-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 19.721515ms
Mar 13 23:17:23.704: INFO: Pod "downwardapi-volume-275d954e-45e6-11e9-8b9c-0a58ac140107": Phase="Running", Reason="", readiness=true. Elapsed: 2.023312975s
Mar 13 23:17:25.708: INFO: Pod "downwardapi-volume-275d954e-45e6-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027082358s
STEP: Saw pod success
Mar 13 23:17:25.708: INFO: Pod "downwardapi-volume-275d954e-45e6-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:17:25.710: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downwardapi-volume-275d954e-45e6-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 23:17:25.727: INFO: Waiting for pod downwardapi-volume-275d954e-45e6-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:17:25.729: INFO: Pod downwardapi-volume-275d954e-45e6-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:17:25.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pptv2" for this suite.
Mar 13 23:17:31.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:17:31.787: INFO: namespace: e2e-tests-projected-pptv2, resource: bindings, ignored listing per whitelist
Mar 13 23:17:31.801: INFO: namespace e2e-tests-projected-pptv2 deletion completed in 6.069052539s

• [SLOW TEST:10.179 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:17:31.801: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 13 23:17:39.876: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:17:39.878: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:17:41.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:17:41.881: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:17:43.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:17:43.881: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:17:45.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:17:45.882: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:17:47.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:17:47.882: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:17:49.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:17:49.882: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:17:51.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:17:51.882: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:17:53.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:17:53.882: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:17:55.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:17:55.882: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:17:57.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:17:57.881: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:17:59.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:17:59.881: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:18:01.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:18:01.881: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:18:03.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:18:03.881: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:18:05.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:18:05.881: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:18:07.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:18:07.881: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 13 23:18:09.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 13 23:18:09.881: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:18:09.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-86dfg" for this suite.
Mar 13 23:18:31.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:18:31.914: INFO: namespace: e2e-tests-container-lifecycle-hook-86dfg, resource: bindings, ignored listing per whitelist
Mar 13 23:18:31.951: INFO: namespace e2e-tests-container-lifecycle-hook-86dfg deletion completed in 22.067415869s

• [SLOW TEST:60.151 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:18:31.952: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Mar 13 23:18:36.010: INFO: Pod pod-hostip-5147b628-45e6-11e9-8b9c-0a58ac140107 has hostIP: 10.40.155.216
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:18:36.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bl7bg" for this suite.
Mar 13 23:18:58.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:18:58.042: INFO: namespace: e2e-tests-pods-bl7bg, resource: bindings, ignored listing per whitelist
Mar 13 23:18:58.092: INFO: namespace e2e-tests-pods-bl7bg deletion completed in 22.0787856s

• [SLOW TEST:26.141 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:18:58.093: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 13 23:18:58.137: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-a,UID:609e4cec-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40587,Generation:0,CreationTimestamp:2019-03-13 23:18:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 13 23:18:58.138: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-a,UID:609e4cec-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40587,Generation:0,CreationTimestamp:2019-03-13 23:18:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 13 23:19:08.144: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-a,UID:609e4cec-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40603,Generation:0,CreationTimestamp:2019-03-13 23:18:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 13 23:19:08.144: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-a,UID:609e4cec-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40603,Generation:0,CreationTimestamp:2019-03-13 23:18:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 13 23:19:18.149: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-a,UID:609e4cec-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40619,Generation:0,CreationTimestamp:2019-03-13 23:18:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 13 23:19:18.149: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-a,UID:609e4cec-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40619,Generation:0,CreationTimestamp:2019-03-13 23:18:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 13 23:19:28.154: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-a,UID:609e4cec-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40635,Generation:0,CreationTimestamp:2019-03-13 23:18:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 13 23:19:28.154: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-a,UID:609e4cec-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40635,Generation:0,CreationTimestamp:2019-03-13 23:18:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 13 23:19:38.159: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-b,UID:7878bba4-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40651,Generation:0,CreationTimestamp:2019-03-13 23:19:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 13 23:19:38.159: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-b,UID:7878bba4-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40651,Generation:0,CreationTimestamp:2019-03-13 23:19:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 13 23:19:48.165: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-b,UID:7878bba4-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40667,Generation:0,CreationTimestamp:2019-03-13 23:19:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 13 23:19:48.165: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-d7pjn,SelfLink:/api/v1/namespaces/e2e-tests-watch-d7pjn/configmaps/e2e-watch-test-configmap-b,UID:7878bba4-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:40667,Generation:0,CreationTimestamp:2019-03-13 23:19:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:19:58.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-d7pjn" for this suite.
Mar 13 23:20:04.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:20:04.219: INFO: namespace: e2e-tests-watch-d7pjn, resource: bindings, ignored listing per whitelist
Mar 13 23:20:04.255: INFO: namespace e2e-tests-watch-d7pjn deletion completed in 6.085115736s

• [SLOW TEST:66.163 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:20:04.255: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-884b5d34-45e6-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 23:20:04.300: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-884bafcc-45e6-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-n2945" to be "success or failure"
Mar 13 23:20:04.302: INFO: Pod "pod-projected-configmaps-884bafcc-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.99901ms
Mar 13 23:20:06.306: INFO: Pod "pod-projected-configmaps-884bafcc-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006179821s
Mar 13 23:20:08.309: INFO: Pod "pod-projected-configmaps-884bafcc-45e6-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009425334s
STEP: Saw pod success
Mar 13 23:20:08.309: INFO: Pod "pod-projected-configmaps-884bafcc-45e6-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:20:08.311: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-projected-configmaps-884bafcc-45e6-11e9-8b9c-0a58ac140107 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 23:20:08.325: INFO: Waiting for pod pod-projected-configmaps-884bafcc-45e6-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:20:08.327: INFO: Pod pod-projected-configmaps-884bafcc-45e6-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:20:08.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n2945" for this suite.
Mar 13 23:20:14.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:20:14.372: INFO: namespace: e2e-tests-projected-n2945, resource: bindings, ignored listing per whitelist
Mar 13 23:20:14.403: INFO: namespace e2e-tests-projected-n2945 deletion completed in 6.073305429s

• [SLOW TEST:10.148 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:20:14.403: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8pm4x A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-8pm4x;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8pm4x A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-8pm4x;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8pm4x.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-8pm4x.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8pm4x.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-8pm4x.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8pm4x.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8pm4x.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8pm4x.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8pm4x.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8pm4x.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-8pm4x.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8pm4x.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-8pm4x.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8pm4x.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 199.80.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.80.199_udp@PTR;check="$$(dig +tcp +noall +answer +search 199.80.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.80.199_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8pm4x A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-8pm4x;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8pm4x A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-8pm4x;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8pm4x.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-8pm4x.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8pm4x.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-8pm4x.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8pm4x.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8pm4x.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8pm4x.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8pm4x.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8pm4x.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-8pm4x.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8pm4x.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-8pm4x.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8pm4x.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 199.80.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.80.199_udp@PTR;check="$$(dig +tcp +noall +answer +search 199.80.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.80.199_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 13 23:20:28.536: INFO: DNS probes using e2e-tests-dns-8pm4x/dns-test-8e5b9a49-45e6-11e9-8b9c-0a58ac140107 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:20:28.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-8pm4x" for this suite.
Mar 13 23:20:34.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:20:34.610: INFO: namespace: e2e-tests-dns-8pm4x, resource: bindings, ignored listing per whitelist
Mar 13 23:20:34.641: INFO: namespace e2e-tests-dns-8pm4x deletion completed in 6.072539501s

• [SLOW TEST:20.237 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:20:34.641: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-9a708c2e-45e6-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 23:20:34.743: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a70deb3-45e6-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-mkwsm" to be "success or failure"
Mar 13 23:20:34.745: INFO: Pod "pod-projected-configmaps-9a70deb3-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.937765ms
Mar 13 23:20:36.748: INFO: Pod "pod-projected-configmaps-9a70deb3-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00504143s
Mar 13 23:20:38.752: INFO: Pod "pod-projected-configmaps-9a70deb3-45e6-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008526221s
STEP: Saw pod success
Mar 13 23:20:38.752: INFO: Pod "pod-projected-configmaps-9a70deb3-45e6-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:20:38.754: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-projected-configmaps-9a70deb3-45e6-11e9-8b9c-0a58ac140107 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 23:20:38.765: INFO: Waiting for pod pod-projected-configmaps-9a70deb3-45e6-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:20:38.767: INFO: Pod pod-projected-configmaps-9a70deb3-45e6-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:20:38.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mkwsm" for this suite.
Mar 13 23:20:44.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:20:44.846: INFO: namespace: e2e-tests-projected-mkwsm, resource: bindings, ignored listing per whitelist
Mar 13 23:20:44.846: INFO: namespace e2e-tests-projected-mkwsm deletion completed in 6.074682079s

• [SLOW TEST:10.205 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:20:44.846: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 23:20:44.889: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a07cd447-45e6-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-rnm2d" to be "success or failure"
Mar 13 23:20:44.891: INFO: Pod "downwardapi-volume-a07cd447-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004921ms
Mar 13 23:20:46.894: INFO: Pod "downwardapi-volume-a07cd447-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005101028s
Mar 13 23:20:48.897: INFO: Pod "downwardapi-volume-a07cd447-45e6-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008085317s
STEP: Saw pod success
Mar 13 23:20:48.897: INFO: Pod "downwardapi-volume-a07cd447-45e6-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:20:48.899: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downwardapi-volume-a07cd447-45e6-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 23:20:48.909: INFO: Waiting for pod downwardapi-volume-a07cd447-45e6-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:20:48.911: INFO: Pod downwardapi-volume-a07cd447-45e6-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:20:48.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rnm2d" for this suite.
Mar 13 23:20:54.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:20:54.978: INFO: namespace: e2e-tests-downward-api-rnm2d, resource: bindings, ignored listing per whitelist
Mar 13 23:20:54.981: INFO: namespace e2e-tests-downward-api-rnm2d deletion completed in 6.06682602s

• [SLOW TEST:10.135 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:20:54.981: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-wqr5w
Mar 13 23:20:59.034: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-wqr5w
STEP: checking the pod's current state and verifying that restartCount is present
Mar 13 23:20:59.036: INFO: Initial restart count of pod liveness-exec is 0
Mar 13 23:21:51.139: INFO: Restart count of pod e2e-tests-container-probe-wqr5w/liveness-exec is now 1 (52.103126441s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:21:51.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-wqr5w" for this suite.
Mar 13 23:21:57.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:21:57.225: INFO: namespace: e2e-tests-container-probe-wqr5w, resource: bindings, ignored listing per whitelist
Mar 13 23:21:57.230: INFO: namespace e2e-tests-container-probe-wqr5w deletion completed in 6.080174534s

• [SLOW TEST:62.248 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:21:57.230: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 23:21:57.276: INFO: Creating deployment "test-recreate-deployment"
Mar 13 23:21:57.278: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 13 23:21:57.284: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 13 23:21:59.290: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 13 23:21:59.292: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688116116, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688116116, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688116116, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688116116, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 23:22:01.295: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 13 23:22:01.300: INFO: Updating deployment test-recreate-deployment
Mar 13 23:22:01.300: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 13 23:22:01.340: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-g42t2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-g42t2/deployments/test-recreate-deployment,UID:cb6471e1-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:41083,Generation:2,CreationTimestamp:2019-03-13 23:21:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-03-13 23:22:00 +0000 UTC 2019-03-13 23:22:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-13 23:22:00 +0000 UTC 2019-03-13 23:21:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar 13 23:22:01.342: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-g42t2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-g42t2/replicasets/test-recreate-deployment-697fbf54bf,UID:cdcd28f6-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:41080,Generation:1,CreationTimestamp:2019-03-13 23:22:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment cb6471e1-45e6-11e9-b6d3-506b8df34f96 0xc001d2ca17 0xc001d2ca18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 13 23:22:01.342: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 13 23:22:01.342: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-g42t2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-g42t2/replicasets/test-recreate-deployment-5dfdcc846d,UID:cb64c3a5-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:41072,Generation:2,CreationTimestamp:2019-03-13 23:21:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment cb6471e1-45e6-11e9-b6d3-506b8df34f96 0xc001d2c3c7 0xc001d2c3c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 13 23:22:01.344: INFO: Pod "test-recreate-deployment-697fbf54bf-ffghm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-ffghm,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-g42t2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-g42t2/pods/test-recreate-deployment-697fbf54bf-ffghm,UID:cdcda9e2-45e6-11e9-b6d3-506b8df34f96,ResourceVersion:41084,Generation:0,CreationTimestamp:2019-03-13 23:22:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf cdcd28f6-45e6-11e9-b6d3-506b8df34f96 0xc001e47b77 0xc001e47b78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-69zb7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-69zb7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-69zb7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:22:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:22:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:22:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:22:00 +0000 UTC  }],Message:,Reason:,HostIP:10.40.156.31,PodIP:,StartTime:2019-03-13 23:22:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:22:01.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-g42t2" for this suite.
Mar 13 23:22:07.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:22:07.379: INFO: namespace: e2e-tests-deployment-g42t2, resource: bindings, ignored listing per whitelist
Mar 13 23:22:07.414: INFO: namespace e2e-tests-deployment-g42t2 deletion completed in 6.067029681s

• [SLOW TEST:10.184 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:22:07.414: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 13 23:22:07.459: INFO: Waiting up to 5m0s for pod "pod-d1b4434e-45e6-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-qf4wh" to be "success or failure"
Mar 13 23:22:07.462: INFO: Pod "pod-d1b4434e-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.515526ms
Mar 13 23:22:09.466: INFO: Pod "pod-d1b4434e-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006343394s
Mar 13 23:22:11.469: INFO: Pod "pod-d1b4434e-45e6-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009793045s
STEP: Saw pod success
Mar 13 23:22:11.469: INFO: Pod "pod-d1b4434e-45e6-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:22:11.471: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-d1b4434e-45e6-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:22:11.483: INFO: Waiting for pod pod-d1b4434e-45e6-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:22:11.485: INFO: Pod pod-d1b4434e-45e6-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:22:11.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qf4wh" for this suite.
Mar 13 23:22:17.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:22:17.550: INFO: namespace: e2e-tests-emptydir-qf4wh, resource: bindings, ignored listing per whitelist
Mar 13 23:22:17.563: INFO: namespace e2e-tests-emptydir-qf4wh deletion completed in 6.074955645s

• [SLOW TEST:10.148 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:22:17.563: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-d7c2ef15-45e6-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 23:22:17.624: INFO: Waiting up to 5m0s for pod "pod-configmaps-d7c344d8-45e6-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-configmap-bzsdm" to be "success or failure"
Mar 13 23:22:17.626: INFO: Pod "pod-configmaps-d7c344d8-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.880726ms
Mar 13 23:22:19.630: INFO: Pod "pod-configmaps-d7c344d8-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00568565s
Mar 13 23:22:21.634: INFO: Pod "pod-configmaps-d7c344d8-45e6-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010234441s
STEP: Saw pod success
Mar 13 23:22:21.634: INFO: Pod "pod-configmaps-d7c344d8-45e6-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:22:21.637: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-configmaps-d7c344d8-45e6-11e9-8b9c-0a58ac140107 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 23:22:21.657: INFO: Waiting for pod pod-configmaps-d7c344d8-45e6-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:22:21.660: INFO: Pod pod-configmaps-d7c344d8-45e6-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:22:21.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bzsdm" for this suite.
Mar 13 23:22:27.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:22:27.713: INFO: namespace: e2e-tests-configmap-bzsdm, resource: bindings, ignored listing per whitelist
Mar 13 23:22:27.750: INFO: namespace e2e-tests-configmap-bzsdm deletion completed in 6.085664962s

• [SLOW TEST:10.188 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:22:27.751: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 13 23:22:32.317: INFO: Successfully updated pod "pod-update-ddd4299a-45e6-11e9-8b9c-0a58ac140107"
STEP: verifying the updated pod is in kubernetes
Mar 13 23:22:32.321: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:22:32.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-hflcj" for this suite.
Mar 13 23:22:54.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:22:54.387: INFO: namespace: e2e-tests-pods-hflcj, resource: bindings, ignored listing per whitelist
Mar 13 23:22:54.403: INFO: namespace e2e-tests-pods-hflcj deletion completed in 22.078038135s

• [SLOW TEST:26.652 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:22:54.403: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Mar 13 23:22:54.966: INFO: Waiting up to 5m0s for pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-bmwhb" in namespace "e2e-tests-svcaccounts-7jvvr" to be "success or failure"
Mar 13 23:22:54.973: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-bmwhb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.172495ms
Mar 13 23:22:56.976: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-bmwhb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00962229s
Mar 13 23:22:58.979: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-bmwhb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012804892s
STEP: Saw pod success
Mar 13 23:22:58.979: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-bmwhb" satisfied condition "success or failure"
Mar 13 23:22:58.981: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-bmwhb container token-test: <nil>
STEP: delete the pod
Mar 13 23:22:58.995: INFO: Waiting for pod pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-bmwhb to disappear
Mar 13 23:22:58.997: INFO: Pod pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-bmwhb no longer exists
STEP: Creating a pod to test consume service account root CA
Mar 13 23:22:58.999: INFO: Waiting up to 5m0s for pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-v6xr6" in namespace "e2e-tests-svcaccounts-7jvvr" to be "success or failure"
Mar 13 23:22:59.001: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-v6xr6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.904095ms
Mar 13 23:23:01.004: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-v6xr6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005210572s
Mar 13 23:23:03.008: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-v6xr6": Phase="Running", Reason="", readiness=false. Elapsed: 4.008778301s
Mar 13 23:23:05.012: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-v6xr6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012572492s
STEP: Saw pod success
Mar 13 23:23:05.012: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-v6xr6" satisfied condition "success or failure"
Mar 13 23:23:05.013: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-v6xr6 container root-ca-test: <nil>
STEP: delete the pod
Mar 13 23:23:05.028: INFO: Waiting for pod pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-v6xr6 to disappear
Mar 13 23:23:05.031: INFO: Pod pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-v6xr6 no longer exists
STEP: Creating a pod to test consume service account namespace
Mar 13 23:23:05.034: INFO: Waiting up to 5m0s for pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-958vd" in namespace "e2e-tests-svcaccounts-7jvvr" to be "success or failure"
Mar 13 23:23:05.038: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-958vd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.77166ms
Mar 13 23:23:07.041: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-958vd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006815064s
Mar 13 23:23:09.045: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-958vd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010129018s
STEP: Saw pod success
Mar 13 23:23:09.045: INFO: Pod "pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-958vd" satisfied condition "success or failure"
Mar 13 23:23:09.046: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-958vd container namespace-test: <nil>
STEP: delete the pod
Mar 13 23:23:09.061: INFO: Waiting for pod pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-958vd to disappear
Mar 13 23:23:09.063: INFO: Pod pod-service-account-ee0474b9-45e6-11e9-8b9c-0a58ac140107-958vd no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:23:09.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-7jvvr" for this suite.
Mar 13 23:23:15.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:23:15.129: INFO: namespace: e2e-tests-svcaccounts-7jvvr, resource: bindings, ignored listing per whitelist
Mar 13 23:23:15.154: INFO: namespace e2e-tests-svcaccounts-7jvvr deletion completed in 6.08343751s

• [SLOW TEST:20.751 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:23:15.154: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-fa14bf35-45e6-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 23:23:15.203: INFO: Waiting up to 5m0s for pod "pod-secrets-fa150f2b-45e6-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-secrets-ngs9l" to be "success or failure"
Mar 13 23:23:15.205: INFO: Pod "pod-secrets-fa150f2b-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.757284ms
Mar 13 23:23:17.209: INFO: Pod "pod-secrets-fa150f2b-45e6-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00529781s
Mar 13 23:23:19.212: INFO: Pod "pod-secrets-fa150f2b-45e6-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008561262s
STEP: Saw pod success
Mar 13 23:23:19.212: INFO: Pod "pod-secrets-fa150f2b-45e6-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:23:19.214: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-secrets-fa150f2b-45e6-11e9-8b9c-0a58ac140107 container secret-env-test: <nil>
STEP: delete the pod
Mar 13 23:23:19.226: INFO: Waiting for pod pod-secrets-fa150f2b-45e6-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:23:19.227: INFO: Pod pod-secrets-fa150f2b-45e6-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:23:19.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-ngs9l" for this suite.
Mar 13 23:23:25.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:23:25.253: INFO: namespace: e2e-tests-secrets-ngs9l, resource: bindings, ignored listing per whitelist
Mar 13 23:23:25.302: INFO: namespace e2e-tests-secrets-ngs9l deletion completed in 6.070733519s

• [SLOW TEST:10.148 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:23:25.302: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 23:23:25.348: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0021146d-45e7-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-wt9jv" to be "success or failure"
Mar 13 23:23:25.350: INFO: Pod "downwardapi-volume-0021146d-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073559ms
Mar 13 23:23:27.353: INFO: Pod "downwardapi-volume-0021146d-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005432568s
Mar 13 23:23:29.357: INFO: Pod "downwardapi-volume-0021146d-45e7-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008721682s
STEP: Saw pod success
Mar 13 23:23:29.357: INFO: Pod "downwardapi-volume-0021146d-45e7-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:23:29.359: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downwardapi-volume-0021146d-45e7-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 23:23:29.371: INFO: Waiting for pod downwardapi-volume-0021146d-45e7-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:23:29.373: INFO: Pod downwardapi-volume-0021146d-45e7-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:23:29.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wt9jv" for this suite.
Mar 13 23:23:35.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:23:35.401: INFO: namespace: e2e-tests-downward-api-wt9jv, resource: bindings, ignored listing per whitelist
Mar 13 23:23:35.451: INFO: namespace e2e-tests-downward-api-wt9jv deletion completed in 6.075057163s

• [SLOW TEST:10.149 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:23:35.451: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 13 23:23:35.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-grf6j'
Mar 13 23:23:35.633: INFO: stderr: ""
Mar 13 23:23:35.633: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Mar 13 23:23:40.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-grf6j -o json'
Mar 13 23:23:40.755: INFO: stderr: ""
Mar 13 23:23:40.755: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-03-13T23:23:35Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-grf6j\",\n        \"resourceVersion\": \"41478\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-grf6j/pods/e2e-test-nginx-pod\",\n        \"uid\": \"06027a34-45e7-11e9-b6d3-506b8df34f96\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-xxp5s\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"karbon-test-3a0ff6-k8s-worker-1\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"volumes\": [\n            {\n                \"name\": \"default-token-xxp5s\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-xxp5s\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-13T23:23:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-13T23:23:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-13T23:23:38Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-13T23:23:35Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://874b79cd53f0a369ae2a90034dae3577526d2c49aeba270a8feafea5f743bc7d\",\n                \"image\": \"docker.io/nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-03-13T23:23:38Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.40.156.31\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.20.2.101\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-03-13T23:23:35Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 13 23:23:40.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 replace -f - --namespace=e2e-tests-kubectl-grf6j'
Mar 13 23:23:40.930: INFO: stderr: ""
Mar 13 23:23:40.930: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Mar 13 23:23:40.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-grf6j'
Mar 13 23:23:42.271: INFO: stderr: ""
Mar 13 23:23:42.272: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:23:42.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-grf6j" for this suite.
Mar 13 23:23:48.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:23:48.348: INFO: namespace: e2e-tests-kubectl-grf6j, resource: bindings, ignored listing per whitelist
Mar 13 23:23:48.351: INFO: namespace e2e-tests-kubectl-grf6j deletion completed in 6.075172231s

• [SLOW TEST:12.900 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:23:48.351: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar 13 23:23:52.925: INFO: Successfully updated pod "annotationupdate0dde5bd5-45e7-11e9-8b9c-0a58ac140107"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:23:54.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xbw77" for this suite.
Mar 13 23:24:16.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:24:16.990: INFO: namespace: e2e-tests-downward-api-xbw77, resource: bindings, ignored listing per whitelist
Mar 13 23:24:17.017: INFO: namespace e2e-tests-downward-api-xbw77 deletion completed in 22.073491177s

• [SLOW TEST:28.666 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:24:17.017: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-1ef3dfb0-45e7-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 23:24:17.063: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ef42639-45e7-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-configmap-qxcdz" to be "success or failure"
Mar 13 23:24:17.065: INFO: Pod "pod-configmaps-1ef42639-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.41469ms
Mar 13 23:24:19.068: INFO: Pod "pod-configmaps-1ef42639-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005384638s
Mar 13 23:24:21.071: INFO: Pod "pod-configmaps-1ef42639-45e7-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00866642s
STEP: Saw pod success
Mar 13 23:24:21.071: INFO: Pod "pod-configmaps-1ef42639-45e7-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:24:21.073: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-configmaps-1ef42639-45e7-11e9-8b9c-0a58ac140107 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 23:24:21.087: INFO: Waiting for pod pod-configmaps-1ef42639-45e7-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:24:21.089: INFO: Pod pod-configmaps-1ef42639-45e7-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:24:21.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-qxcdz" for this suite.
Mar 13 23:24:27.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:24:27.145: INFO: namespace: e2e-tests-configmap-qxcdz, resource: bindings, ignored listing per whitelist
Mar 13 23:24:27.161: INFO: namespace e2e-tests-configmap-qxcdz deletion completed in 6.069282247s

• [SLOW TEST:10.144 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:24:27.161: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 23:24:27.203: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:24:31.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-2xmb4" for this suite.
Mar 13 23:25:09.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:25:09.361: INFO: namespace: e2e-tests-pods-2xmb4, resource: bindings, ignored listing per whitelist
Mar 13 23:25:09.370: INFO: namespace e2e-tests-pods-2xmb4 deletion completed in 38.074102561s

• [SLOW TEST:42.209 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:25:09.371: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 23:25:09.409: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 13 23:25:09.412: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 13 23:25:14.416: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 13 23:25:14.416: INFO: Creating deployment "test-rolling-update-deployment"
Mar 13 23:25:14.419: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 13 23:25:14.424: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 13 23:25:16.431: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 13 23:25:16.433: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688116314, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688116314, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688116314, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688116314, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 13 23:25:18.437: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 13 23:25:18.442: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-46zf7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-46zf7/deployments/test-rolling-update-deployment,UID:40e53318-45e7-11e9-b6d3-506b8df34f96,ResourceVersion:41770,Generation:1,CreationTimestamp:2019-03-13 23:25:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-13 23:25:14 +0000 UTC 2019-03-13 23:25:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-13 23:25:16 +0000 UTC 2019-03-13 23:25:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar 13 23:25:18.444: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-46zf7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-46zf7/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:40e65910-45e7-11e9-b6d3-506b8df34f96,ResourceVersion:41761,Generation:1,CreationTimestamp:2019-03-13 23:25:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 40e53318-45e7-11e9-b6d3-506b8df34f96 0xc00231df37 0xc00231df38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar 13 23:25:18.444: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 13 23:25:18.445: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-46zf7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-46zf7/replicasets/test-rolling-update-controller,UID:3de90ece-45e7-11e9-b6d3-506b8df34f96,ResourceVersion:41769,Generation:2,CreationTimestamp:2019-03-13 23:25:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 40e53318-45e7-11e9-b6d3-506b8df34f96 0xc00231de67 0xc00231de68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 13 23:25:18.447: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-dnd8w" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-dnd8w,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-46zf7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-46zf7/pods/test-rolling-update-deployment-68b55d7bc6-dnd8w,UID:40e6c220-45e7-11e9-b6d3-506b8df34f96,ResourceVersion:41760,Generation:0,CreationTimestamp:2019-03-13 23:25:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 40e65910-45e7-11e9-b6d3-506b8df34f96 0xc001ca3c17 0xc001ca3c18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-pnd6q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pnd6q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pnd6q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test-3a0ff6-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:25:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:25:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:25:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-13 23:25:14 +0000 UTC  }],Message:,Reason:,HostIP:10.40.155.216,PodIP:172.20.1.106,StartTime:2019-03-13 23:25:14 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-13 23:25:16 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://79ff023cfee6614337aacc3b3e760b7dede2e9fd77c1681710e561d7e853c73b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:25:18.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-46zf7" for this suite.
Mar 13 23:25:24.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:25:24.515: INFO: namespace: e2e-tests-deployment-46zf7, resource: bindings, ignored listing per whitelist
Mar 13 23:25:24.521: INFO: namespace e2e-tests-deployment-46zf7 deletion completed in 6.071144475s

• [SLOW TEST:15.150 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:25:24.521: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 13 23:25:24.574: INFO: Waiting up to 5m0s for pod "pod-4731844c-45e7-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-zmcpb" to be "success or failure"
Mar 13 23:25:24.575: INFO: Pod "pod-4731844c-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.648652ms
Mar 13 23:25:26.579: INFO: Pod "pod-4731844c-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005414883s
Mar 13 23:25:28.582: INFO: Pod "pod-4731844c-45e7-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008655546s
STEP: Saw pod success
Mar 13 23:25:28.582: INFO: Pod "pod-4731844c-45e7-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:25:28.584: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-4731844c-45e7-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:25:28.595: INFO: Waiting for pod pod-4731844c-45e7-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:25:28.597: INFO: Pod pod-4731844c-45e7-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:25:28.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-zmcpb" for this suite.
Mar 13 23:25:34.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:25:34.646: INFO: namespace: e2e-tests-emptydir-zmcpb, resource: bindings, ignored listing per whitelist
Mar 13 23:25:34.675: INFO: namespace e2e-tests-emptydir-zmcpb deletion completed in 6.075514454s

• [SLOW TEST:10.154 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:25:34.675: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:25:38.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-s76fw" for this suite.
Mar 13 23:26:22.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:26:22.807: INFO: namespace: e2e-tests-kubelet-test-s76fw, resource: bindings, ignored listing per whitelist
Mar 13 23:26:22.850: INFO: namespace e2e-tests-kubelet-test-s76fw deletion completed in 44.070410015s

• [SLOW TEST:48.175 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:26:22.850: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 13 23:26:22.952: INFO: Waiting up to 5m0s for pod "pod-69fd4693-45e7-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-7z6p6" to be "success or failure"
Mar 13 23:26:22.954: INFO: Pod "pod-69fd4693-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3476ms
Mar 13 23:26:24.958: INFO: Pod "pod-69fd4693-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005971328s
Mar 13 23:26:26.961: INFO: Pod "pod-69fd4693-45e7-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009205864s
STEP: Saw pod success
Mar 13 23:26:26.961: INFO: Pod "pod-69fd4693-45e7-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:26:26.963: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-69fd4693-45e7-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:26:26.975: INFO: Waiting for pod pod-69fd4693-45e7-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:26:26.977: INFO: Pod pod-69fd4693-45e7-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:26:26.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7z6p6" for this suite.
Mar 13 23:26:32.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:26:33.042: INFO: namespace: e2e-tests-emptydir-7z6p6, resource: bindings, ignored listing per whitelist
Mar 13 23:26:33.054: INFO: namespace e2e-tests-emptydir-7z6p6 deletion completed in 6.074510087s

• [SLOW TEST:10.204 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:26:33.054: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar 13 23:26:33.096: INFO: namespace e2e-tests-kubectl-5mhk9
Mar 13 23:26:33.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-5mhk9'
Mar 13 23:26:33.470: INFO: stderr: ""
Mar 13 23:26:33.470: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 13 23:26:34.474: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 23:26:34.474: INFO: Found 0 / 1
Mar 13 23:26:35.474: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 23:26:35.474: INFO: Found 0 / 1
Mar 13 23:26:36.474: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 23:26:36.474: INFO: Found 0 / 1
Mar 13 23:26:37.474: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 23:26:37.474: INFO: Found 1 / 1
Mar 13 23:26:37.474: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 13 23:26:37.476: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 23:26:37.476: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 13 23:26:37.476: INFO: wait on redis-master startup in e2e-tests-kubectl-5mhk9 
Mar 13 23:26:37.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 logs redis-master-6p57t redis-master --namespace=e2e-tests-kubectl-5mhk9'
Mar 13 23:26:37.571: INFO: stderr: ""
Mar 13 23:26:37.571: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Mar 23:26:36.216 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Mar 23:26:36.216 # Server started, Redis version 3.2.12\n1:M 13 Mar 23:26:36.216 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Mar 23:26:36.216 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Mar 13 23:26:37.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-5mhk9'
Mar 13 23:26:37.684: INFO: stderr: ""
Mar 13 23:26:37.684: INFO: stdout: "service/rm2 exposed\n"
Mar 13 23:26:37.687: INFO: Service rm2 in namespace e2e-tests-kubectl-5mhk9 found.
STEP: exposing service
Mar 13 23:26:39.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-5mhk9'
Mar 13 23:26:39.773: INFO: stderr: ""
Mar 13 23:26:39.773: INFO: stdout: "service/rm3 exposed\n"
Mar 13 23:26:39.776: INFO: Service rm3 in namespace e2e-tests-kubectl-5mhk9 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:26:41.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5mhk9" for this suite.
Mar 13 23:27:03.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:27:03.849: INFO: namespace: e2e-tests-kubectl-5mhk9, resource: bindings, ignored listing per whitelist
Mar 13 23:27:03.857: INFO: namespace e2e-tests-kubectl-5mhk9 deletion completed in 22.07231872s

• [SLOW TEST:30.803 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:27:03.857: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 13 23:27:03.902: INFO: Waiting up to 5m0s for pod "pod-8265d4eb-45e7-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-5x9mt" to be "success or failure"
Mar 13 23:27:03.904: INFO: Pod "pod-8265d4eb-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.836456ms
Mar 13 23:27:05.907: INFO: Pod "pod-8265d4eb-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005192974s
Mar 13 23:27:07.910: INFO: Pod "pod-8265d4eb-45e7-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008552378s
STEP: Saw pod success
Mar 13 23:27:07.910: INFO: Pod "pod-8265d4eb-45e7-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:27:07.912: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-8265d4eb-45e7-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:27:07.922: INFO: Waiting for pod pod-8265d4eb-45e7-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:27:07.923: INFO: Pod pod-8265d4eb-45e7-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:27:07.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5x9mt" for this suite.
Mar 13 23:27:13.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:27:13.941: INFO: namespace: e2e-tests-emptydir-5x9mt, resource: bindings, ignored listing per whitelist
Mar 13 23:27:14.005: INFO: namespace e2e-tests-emptydir-5x9mt deletion completed in 6.079400695s

• [SLOW TEST:10.148 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:27:14.005: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar 13 23:27:14.053: INFO: Waiting up to 5m0s for pod "downward-api-8872cc68-45e7-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-k8rwz" to be "success or failure"
Mar 13 23:27:14.055: INFO: Pod "downward-api-8872cc68-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.163997ms
Mar 13 23:27:16.058: INFO: Pod "downward-api-8872cc68-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005445735s
Mar 13 23:27:18.063: INFO: Pod "downward-api-8872cc68-45e7-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009541934s
STEP: Saw pod success
Mar 13 23:27:18.063: INFO: Pod "downward-api-8872cc68-45e7-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:27:18.064: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downward-api-8872cc68-45e7-11e9-8b9c-0a58ac140107 container dapi-container: <nil>
STEP: delete the pod
Mar 13 23:27:18.079: INFO: Waiting for pod downward-api-8872cc68-45e7-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:27:18.081: INFO: Pod downward-api-8872cc68-45e7-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:27:18.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-k8rwz" for this suite.
Mar 13 23:27:24.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:27:24.143: INFO: namespace: e2e-tests-downward-api-k8rwz, resource: bindings, ignored listing per whitelist
Mar 13 23:27:24.167: INFO: namespace e2e-tests-downward-api-k8rwz deletion completed in 6.076435172s

• [SLOW TEST:10.161 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:27:24.167: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-8e81c759-45e7-11e9-8b9c-0a58ac140107
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-8e81c759-45e7-11e9-8b9c-0a58ac140107
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:27:30.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-qcx4g" for this suite.
Mar 13 23:27:52.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:27:52.303: INFO: namespace: e2e-tests-configmap-qcx4g, resource: bindings, ignored listing per whitelist
Mar 13 23:27:52.339: INFO: namespace e2e-tests-configmap-qcx4g deletion completed in 22.083205212s

• [SLOW TEST:28.173 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:27:52.340: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0313 23:28:22.911916      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 13 23:28:22.911: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:28:22.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-9fldq" for this suite.
Mar 13 23:28:28.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:28:28.981: INFO: namespace: e2e-tests-gc-9fldq, resource: bindings, ignored listing per whitelist
Mar 13 23:28:28.984: INFO: namespace e2e-tests-gc-9fldq deletion completed in 6.069896742s

• [SLOW TEST:36.644 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:28:28.984: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-b525e71d-45e7-11e9-8b9c-0a58ac140107
STEP: Creating configMap with name cm-test-opt-upd-b525e786-45e7-11e9-8b9c-0a58ac140107
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b525e71d-45e7-11e9-8b9c-0a58ac140107
STEP: Updating configmap cm-test-opt-upd-b525e786-45e7-11e9-8b9c-0a58ac140107
STEP: Creating configMap with name cm-test-opt-create-b525e7a6-45e7-11e9-8b9c-0a58ac140107
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:28:37.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-b59sw" for this suite.
Mar 13 23:28:59.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:28:59.169: INFO: namespace: e2e-tests-configmap-b59sw, resource: bindings, ignored listing per whitelist
Mar 13 23:28:59.221: INFO: namespace e2e-tests-configmap-b59sw deletion completed in 22.081224765s

• [SLOW TEST:30.237 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:28:59.222: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0313 23:29:09.321692      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 13 23:29:09.321: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:29:09.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-fbvtc" for this suite.
Mar 13 23:29:15.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:29:15.351: INFO: namespace: e2e-tests-gc-fbvtc, resource: bindings, ignored listing per whitelist
Mar 13 23:29:15.393: INFO: namespace e2e-tests-gc-fbvtc deletion completed in 6.069097133s

• [SLOW TEST:16.171 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:29:15.393: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 23:29:15.445: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Mar 13 23:29:15.449: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-7f97s/daemonsets","resourceVersion":"42682"},"items":null}

Mar 13 23:29:15.451: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-7f97s/pods","resourceVersion":"42682"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:29:15.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-7f97s" for this suite.
Mar 13 23:29:21.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:29:21.492: INFO: namespace: e2e-tests-daemonsets-7f97s, resource: bindings, ignored listing per whitelist
Mar 13 23:29:21.533: INFO: namespace e2e-tests-daemonsets-7f97s deletion completed in 6.071212377s

S [SKIPPING] [6.140 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Mar 13 23:29:15.445: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:29:21.533: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 23:29:21.577: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d47564ba-45e7-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-zmskf" to be "success or failure"
Mar 13 23:29:21.579: INFO: Pod "downwardapi-volume-d47564ba-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.240403ms
Mar 13 23:29:23.583: INFO: Pod "downwardapi-volume-d47564ba-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006081769s
Mar 13 23:29:25.586: INFO: Pod "downwardapi-volume-d47564ba-45e7-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008894836s
STEP: Saw pod success
Mar 13 23:29:25.586: INFO: Pod "downwardapi-volume-d47564ba-45e7-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:29:25.587: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod downwardapi-volume-d47564ba-45e7-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 23:29:25.604: INFO: Waiting for pod downwardapi-volume-d47564ba-45e7-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:29:25.606: INFO: Pod downwardapi-volume-d47564ba-45e7-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:29:25.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zmskf" for this suite.
Mar 13 23:29:31.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:29:31.663: INFO: namespace: e2e-tests-downward-api-zmskf, resource: bindings, ignored listing per whitelist
Mar 13 23:29:31.691: INFO: namespace e2e-tests-downward-api-zmskf deletion completed in 6.080282702s

• [SLOW TEST:10.158 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:29:31.692: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 23:29:31.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da84bb2b-45e7-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-wgnms" to be "success or failure"
Mar 13 23:29:31.747: INFO: Pod "downwardapi-volume-da84bb2b-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.854277ms
Mar 13 23:29:33.750: INFO: Pod "downwardapi-volume-da84bb2b-45e7-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006084851s
Mar 13 23:29:35.754: INFO: Pod "downwardapi-volume-da84bb2b-45e7-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010520785s
STEP: Saw pod success
Mar 13 23:29:35.754: INFO: Pod "downwardapi-volume-da84bb2b-45e7-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:29:35.758: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downwardapi-volume-da84bb2b-45e7-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 23:29:35.772: INFO: Waiting for pod downwardapi-volume-da84bb2b-45e7-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:29:35.774: INFO: Pod downwardapi-volume-da84bb2b-45e7-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:29:35.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wgnms" for this suite.
Mar 13 23:29:41.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:29:41.842: INFO: namespace: e2e-tests-projected-wgnms, resource: bindings, ignored listing per whitelist
Mar 13 23:29:41.843: INFO: namespace e2e-tests-projected-wgnms deletion completed in 6.066091242s

• [SLOW TEST:10.152 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:29:41.843: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:29:41.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-tzbhg" for this suite.
Mar 13 23:30:03.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:30:03.953: INFO: namespace: e2e-tests-pods-tzbhg, resource: bindings, ignored listing per whitelist
Mar 13 23:30:03.969: INFO: namespace e2e-tests-pods-tzbhg deletion completed in 22.069353801s

• [SLOW TEST:22.126 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:30:03.969: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:30:13.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-swd5h" for this suite.
Mar 13 23:30:35.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:30:35.085: INFO: namespace: e2e-tests-replication-controller-swd5h, resource: bindings, ignored listing per whitelist
Mar 13 23:30:35.107: INFO: namespace e2e-tests-replication-controller-swd5h deletion completed in 22.072175399s

• [SLOW TEST:31.138 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:30:35.108: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 23:30:35.170: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0052cdac-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-hntgc" to be "success or failure"
Mar 13 23:30:35.172: INFO: Pod "downwardapi-volume-0052cdac-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.88277ms
Mar 13 23:30:37.176: INFO: Pod "downwardapi-volume-0052cdac-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005305125s
Mar 13 23:30:39.179: INFO: Pod "downwardapi-volume-0052cdac-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008698685s
STEP: Saw pod success
Mar 13 23:30:39.179: INFO: Pod "downwardapi-volume-0052cdac-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:30:39.181: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod downwardapi-volume-0052cdac-45e8-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 23:30:39.197: INFO: Waiting for pod downwardapi-volume-0052cdac-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:30:39.199: INFO: Pod downwardapi-volume-0052cdac-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:30:39.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hntgc" for this suite.
Mar 13 23:30:45.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:30:45.264: INFO: namespace: e2e-tests-projected-hntgc, resource: bindings, ignored listing per whitelist
Mar 13 23:30:45.278: INFO: namespace e2e-tests-projected-hntgc deletion completed in 6.075675856s

• [SLOW TEST:10.170 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:30:45.278: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 13 23:30:45.329: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-6cnfr,SelfLink:/api/v1/namespaces/e2e-tests-watch-6cnfr/configmaps/e2e-watch-test-watch-closed,UID:06209e5c-45e8-11e9-b6d3-506b8df34f96,ResourceVersion:42950,Generation:0,CreationTimestamp:2019-03-13 23:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 13 23:30:45.329: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-6cnfr,SelfLink:/api/v1/namespaces/e2e-tests-watch-6cnfr/configmaps/e2e-watch-test-watch-closed,UID:06209e5c-45e8-11e9-b6d3-506b8df34f96,ResourceVersion:42951,Generation:0,CreationTimestamp:2019-03-13 23:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 13 23:30:45.338: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-6cnfr,SelfLink:/api/v1/namespaces/e2e-tests-watch-6cnfr/configmaps/e2e-watch-test-watch-closed,UID:06209e5c-45e8-11e9-b6d3-506b8df34f96,ResourceVersion:42952,Generation:0,CreationTimestamp:2019-03-13 23:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 13 23:30:45.338: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-6cnfr,SelfLink:/api/v1/namespaces/e2e-tests-watch-6cnfr/configmaps/e2e-watch-test-watch-closed,UID:06209e5c-45e8-11e9-b6d3-506b8df34f96,ResourceVersion:42953,Generation:0,CreationTimestamp:2019-03-13 23:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:30:45.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-6cnfr" for this suite.
Mar 13 23:30:51.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:30:51.373: INFO: namespace: e2e-tests-watch-6cnfr, resource: bindings, ignored listing per whitelist
Mar 13 23:30:51.417: INFO: namespace e2e-tests-watch-6cnfr deletion completed in 6.07487499s

• [SLOW TEST:6.139 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:30:51.417: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-qnzt9
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-qnzt9
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-qnzt9
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-qnzt9
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-qnzt9
Mar 13 23:30:55.499: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-qnzt9, name: ss-0, uid: 0c133811-45e8-11e9-b6d3-506b8df34f96, status phase: Pending. Waiting for statefulset controller to delete.
Mar 13 23:30:55.894: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-qnzt9, name: ss-0, uid: 0c133811-45e8-11e9-b6d3-506b8df34f96, status phase: Failed. Waiting for statefulset controller to delete.
Mar 13 23:30:55.898: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-qnzt9, name: ss-0, uid: 0c133811-45e8-11e9-b6d3-506b8df34f96, status phase: Failed. Waiting for statefulset controller to delete.
Mar 13 23:30:55.903: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-qnzt9
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-qnzt9
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-qnzt9 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 13 23:30:59.919: INFO: Deleting all statefulset in ns e2e-tests-statefulset-qnzt9
Mar 13 23:30:59.921: INFO: Scaling statefulset ss to 0
Mar 13 23:31:09.936: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 23:31:09.938: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:31:09.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-qnzt9" for this suite.
Mar 13 23:31:15.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:31:16.011: INFO: namespace: e2e-tests-statefulset-qnzt9, resource: bindings, ignored listing per whitelist
Mar 13 23:31:16.045: INFO: namespace e2e-tests-statefulset-qnzt9 deletion completed in 6.094657947s

• [SLOW TEST:24.629 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:31:16.046: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-18b73f1f-45e8-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 23:31:16.095: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18b7868b-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-r6pzb" to be "success or failure"
Mar 13 23:31:16.097: INFO: Pod "pod-projected-secrets-18b7868b-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13737ms
Mar 13 23:31:18.100: INFO: Pod "pod-projected-secrets-18b7868b-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005394163s
Mar 13 23:31:20.103: INFO: Pod "pod-projected-secrets-18b7868b-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00795364s
STEP: Saw pod success
Mar 13 23:31:20.103: INFO: Pod "pod-projected-secrets-18b7868b-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:31:20.105: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-projected-secrets-18b7868b-45e8-11e9-8b9c-0a58ac140107 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 13 23:31:20.117: INFO: Waiting for pod pod-projected-secrets-18b7868b-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:31:20.120: INFO: Pod pod-projected-secrets-18b7868b-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:31:20.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-r6pzb" for this suite.
Mar 13 23:31:26.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:31:26.180: INFO: namespace: e2e-tests-projected-r6pzb, resource: bindings, ignored listing per whitelist
Mar 13 23:31:26.203: INFO: namespace e2e-tests-projected-r6pzb deletion completed in 6.080709827s

• [SLOW TEST:10.157 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:31:26.203: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Mar 13 23:31:26.248: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar 13 23:31:26.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:26.436: INFO: stderr: ""
Mar 13 23:31:26.436: INFO: stdout: "service/redis-slave created\n"
Mar 13 23:31:26.436: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar 13 23:31:26.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:26.607: INFO: stderr: ""
Mar 13 23:31:26.607: INFO: stdout: "service/redis-master created\n"
Mar 13 23:31:26.607: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 13 23:31:26.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:26.779: INFO: stderr: ""
Mar 13 23:31:26.779: INFO: stdout: "service/frontend created\n"
Mar 13 23:31:26.779: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar 13 23:31:26.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:26.934: INFO: stderr: ""
Mar 13 23:31:26.935: INFO: stdout: "deployment.extensions/frontend created\n"
Mar 13 23:31:26.935: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 13 23:31:26.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:27.104: INFO: stderr: ""
Mar 13 23:31:27.104: INFO: stdout: "deployment.extensions/redis-master created\n"
Mar 13 23:31:27.105: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar 13 23:31:27.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:27.284: INFO: stderr: ""
Mar 13 23:31:27.284: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Mar 13 23:31:27.284: INFO: Waiting for all frontend pods to be Running.
Mar 13 23:31:47.335: INFO: Waiting for frontend to serve content.
Mar 13 23:31:52.354: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Mar 13 23:31:57.369: INFO: Trying to add a new entry to the guestbook.
Mar 13 23:31:57.379: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 13 23:31:57.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:57.488: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 23:31:57.488: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar 13 23:31:57.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:57.573: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 23:31:57.573: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 13 23:31:57.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:57.661: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 23:31:57.661: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 13 23:31:57.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:57.739: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 23:31:57.739: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 13 23:31:57.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:57.825: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 23:31:57.825: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 13 23:31:57.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wr9c6'
Mar 13 23:31:57.913: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 13 23:31:57.913: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:31:57.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wr9c6" for this suite.
Mar 13 23:32:39.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:32:39.959: INFO: namespace: e2e-tests-kubectl-wr9c6, resource: bindings, ignored listing per whitelist
Mar 13 23:32:40.003: INFO: namespace e2e-tests-kubectl-wr9c6 deletion completed in 42.085762535s

• [SLOW TEST:73.800 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:32:40.003: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 23:32:40.043: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ac0fa6e-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-tkxt5" to be "success or failure"
Mar 13 23:32:40.046: INFO: Pod "downwardapi-volume-4ac0fa6e-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 3.288972ms
Mar 13 23:32:42.049: INFO: Pod "downwardapi-volume-4ac0fa6e-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006422598s
Mar 13 23:32:44.053: INFO: Pod "downwardapi-volume-4ac0fa6e-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010431945s
STEP: Saw pod success
Mar 13 23:32:44.053: INFO: Pod "downwardapi-volume-4ac0fa6e-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:32:44.055: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod downwardapi-volume-4ac0fa6e-45e8-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 23:32:44.068: INFO: Waiting for pod downwardapi-volume-4ac0fa6e-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:32:44.071: INFO: Pod downwardapi-volume-4ac0fa6e-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:32:44.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tkxt5" for this suite.
Mar 13 23:32:50.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:32:50.125: INFO: namespace: e2e-tests-downward-api-tkxt5, resource: bindings, ignored listing per whitelist
Mar 13 23:32:50.148: INFO: namespace e2e-tests-downward-api-tkxt5 deletion completed in 6.074545322s

• [SLOW TEST:10.145 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:32:50.148: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Mar 13 23:32:50.193: INFO: Waiting up to 5m0s for pod "var-expansion-50cdc2f7-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-var-expansion-9fq7m" to be "success or failure"
Mar 13 23:32:50.196: INFO: Pod "var-expansion-50cdc2f7-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.237588ms
Mar 13 23:32:52.199: INFO: Pod "var-expansion-50cdc2f7-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005484388s
Mar 13 23:32:54.202: INFO: Pod "var-expansion-50cdc2f7-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008931788s
STEP: Saw pod success
Mar 13 23:32:54.203: INFO: Pod "var-expansion-50cdc2f7-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:32:54.204: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod var-expansion-50cdc2f7-45e8-11e9-8b9c-0a58ac140107 container dapi-container: <nil>
STEP: delete the pod
Mar 13 23:32:54.216: INFO: Waiting for pod var-expansion-50cdc2f7-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:32:54.218: INFO: Pod var-expansion-50cdc2f7-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:32:54.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-9fq7m" for this suite.
Mar 13 23:33:00.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:33:00.261: INFO: namespace: e2e-tests-var-expansion-9fq7m, resource: bindings, ignored listing per whitelist
Mar 13 23:33:00.296: INFO: namespace e2e-tests-var-expansion-9fq7m deletion completed in 6.074534507s

• [SLOW TEST:10.148 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:33:00.296: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Mar 13 23:33:00.360: INFO: Waiting up to 5m0s for pod "var-expansion-56dd2ab3-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-var-expansion-gn68z" to be "success or failure"
Mar 13 23:33:00.362: INFO: Pod "var-expansion-56dd2ab3-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34032ms
Mar 13 23:33:02.364: INFO: Pod "var-expansion-56dd2ab3-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003891783s
Mar 13 23:33:04.368: INFO: Pod "var-expansion-56dd2ab3-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007914258s
STEP: Saw pod success
Mar 13 23:33:04.368: INFO: Pod "var-expansion-56dd2ab3-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:33:04.371: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod var-expansion-56dd2ab3-45e8-11e9-8b9c-0a58ac140107 container dapi-container: <nil>
STEP: delete the pod
Mar 13 23:33:04.386: INFO: Waiting for pod var-expansion-56dd2ab3-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:33:04.389: INFO: Pod var-expansion-56dd2ab3-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:33:04.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-gn68z" for this suite.
Mar 13 23:33:10.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:33:10.439: INFO: namespace: e2e-tests-var-expansion-gn68z, resource: bindings, ignored listing per whitelist
Mar 13 23:33:10.473: INFO: namespace e2e-tests-var-expansion-gn68z deletion completed in 6.079799886s

• [SLOW TEST:10.177 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:33:10.473: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:33:10.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-bnkk5" for this suite.
Mar 13 23:33:16.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:33:16.570: INFO: namespace: e2e-tests-services-bnkk5, resource: bindings, ignored listing per whitelist
Mar 13 23:33:16.597: INFO: namespace e2e-tests-services-bnkk5 deletion completed in 6.073541624s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.125 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:33:16.598: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 13 23:33:16.648: INFO: Waiting up to 5m0s for pod "pod-60924c79-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-qrzc7" to be "success or failure"
Mar 13 23:33:16.651: INFO: Pod "pod-60924c79-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.714435ms
Mar 13 23:33:18.655: INFO: Pod "pod-60924c79-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006206342s
Mar 13 23:33:20.658: INFO: Pod "pod-60924c79-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009477477s
STEP: Saw pod success
Mar 13 23:33:20.658: INFO: Pod "pod-60924c79-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:33:20.660: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod pod-60924c79-45e8-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:33:20.675: INFO: Waiting for pod pod-60924c79-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:33:20.676: INFO: Pod pod-60924c79-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:33:20.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qrzc7" for this suite.
Mar 13 23:33:26.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:33:26.733: INFO: namespace: e2e-tests-emptydir-qrzc7, resource: bindings, ignored listing per whitelist
Mar 13 23:33:26.748: INFO: namespace e2e-tests-emptydir-qrzc7 deletion completed in 6.068553016s

• [SLOW TEST:10.150 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:33:26.748: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:33:32.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-mqjq9" for this suite.
Mar 13 23:33:38.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:33:38.912: INFO: namespace: e2e-tests-namespaces-mqjq9, resource: bindings, ignored listing per whitelist
Mar 13 23:33:38.918: INFO: namespace e2e-tests-namespaces-mqjq9 deletion completed in 6.075769686s
STEP: Destroying namespace "e2e-tests-nsdeletetest-rk9s4" for this suite.
Mar 13 23:33:38.920: INFO: Namespace e2e-tests-nsdeletetest-rk9s4 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-s4sh6" for this suite.
Mar 13 23:33:44.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:33:44.945: INFO: namespace: e2e-tests-nsdeletetest-s4sh6, resource: bindings, ignored listing per whitelist
Mar 13 23:33:44.990: INFO: namespace e2e-tests-nsdeletetest-s4sh6 deletion completed in 6.07042731s

• [SLOW TEST:18.242 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:33:44.990: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar 13 23:33:49.563: INFO: Successfully updated pod "labelsupdate717ee29a-45e8-11e9-8b9c-0a58ac140107"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:33:51.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-w66d7" for this suite.
Mar 13 23:34:13.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:34:13.613: INFO: namespace: e2e-tests-downward-api-w66d7, resource: bindings, ignored listing per whitelist
Mar 13 23:34:13.659: INFO: namespace e2e-tests-downward-api-w66d7 deletion completed in 22.077171661s

• [SLOW TEST:28.668 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:34:13.659: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 13 23:34:13.710: INFO: Waiting up to 5m0s for pod "pod-8295620a-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-xlg8n" to be "success or failure"
Mar 13 23:34:13.712: INFO: Pod "pod-8295620a-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.667132ms
Mar 13 23:34:15.716: INFO: Pod "pod-8295620a-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005203782s
Mar 13 23:34:17.719: INFO: Pod "pod-8295620a-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008977681s
STEP: Saw pod success
Mar 13 23:34:17.720: INFO: Pod "pod-8295620a-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:34:17.721: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-8295620a-45e8-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:34:17.736: INFO: Waiting for pod pod-8295620a-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:34:17.738: INFO: Pod pod-8295620a-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:34:17.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xlg8n" for this suite.
Mar 13 23:34:23.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:34:23.803: INFO: namespace: e2e-tests-emptydir-xlg8n, resource: bindings, ignored listing per whitelist
Mar 13 23:34:23.814: INFO: namespace e2e-tests-emptydir-xlg8n deletion completed in 6.073050732s

• [SLOW TEST:10.155 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:34:23.814: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 13 23:34:23.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-lp54w'
Mar 13 23:34:23.998: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 13 23:34:23.998: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Mar 13 23:34:24.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-lp54w'
Mar 13 23:34:24.081: INFO: stderr: ""
Mar 13 23:34:24.081: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:34:24.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lp54w" for this suite.
Mar 13 23:34:30.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:34:30.154: INFO: namespace: e2e-tests-kubectl-lp54w, resource: bindings, ignored listing per whitelist
Mar 13 23:34:30.163: INFO: namespace e2e-tests-kubectl-lp54w deletion completed in 6.075501362s

• [SLOW TEST:6.348 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:34:30.163: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-8c6ea213-45e8-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume configMaps
Mar 13 23:34:30.239: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c6f1129-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-configmap-p8wp7" to be "success or failure"
Mar 13 23:34:30.242: INFO: Pod "pod-configmaps-8c6f1129-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 3.290023ms
Mar 13 23:34:32.245: INFO: Pod "pod-configmaps-8c6f1129-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006593586s
Mar 13 23:34:34.249: INFO: Pod "pod-configmaps-8c6f1129-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010329826s
STEP: Saw pod success
Mar 13 23:34:34.249: INFO: Pod "pod-configmaps-8c6f1129-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:34:34.251: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-configmaps-8c6f1129-45e8-11e9-8b9c-0a58ac140107 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 13 23:34:34.264: INFO: Waiting for pod pod-configmaps-8c6f1129-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:34:34.266: INFO: Pod pod-configmaps-8c6f1129-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:34:34.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-p8wp7" for this suite.
Mar 13 23:34:40.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:34:40.311: INFO: namespace: e2e-tests-configmap-p8wp7, resource: bindings, ignored listing per whitelist
Mar 13 23:34:40.340: INFO: namespace e2e-tests-configmap-p8wp7 deletion completed in 6.07085445s

• [SLOW TEST:10.178 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:34:40.340: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 13 23:34:40.392: INFO: Waiting up to 5m0s for pod "pod-927c906d-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-emptydir-6hc9s" to be "success or failure"
Mar 13 23:34:40.394: INFO: Pod "pod-927c906d-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.441874ms
Mar 13 23:34:42.398: INFO: Pod "pod-927c906d-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006094122s
Mar 13 23:34:44.401: INFO: Pod "pod-927c906d-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009604812s
STEP: Saw pod success
Mar 13 23:34:44.401: INFO: Pod "pod-927c906d-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:34:44.403: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-927c906d-45e8-11e9-8b9c-0a58ac140107 container test-container: <nil>
STEP: delete the pod
Mar 13 23:34:44.416: INFO: Waiting for pod pod-927c906d-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:34:44.418: INFO: Pod pod-927c906d-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:34:44.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6hc9s" for this suite.
Mar 13 23:34:50.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:34:50.483: INFO: namespace: e2e-tests-emptydir-6hc9s, resource: bindings, ignored listing per whitelist
Mar 13 23:34:50.488: INFO: namespace e2e-tests-emptydir-6hc9s deletion completed in 6.067616961s

• [SLOW TEST:10.148 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:34:50.489: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0313 23:34:51.130092      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 13 23:34:51.130: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:34:51.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-gxxj6" for this suite.
Mar 13 23:34:57.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:34:57.148: INFO: namespace: e2e-tests-gc-gxxj6, resource: bindings, ignored listing per whitelist
Mar 13 23:34:57.203: INFO: namespace e2e-tests-gc-gxxj6 deletion completed in 6.070970671s

• [SLOW TEST:6.715 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:34:57.203: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-9c890099-45e8-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 23:34:57.251: INFO: Waiting up to 5m0s for pod "pod-secrets-9c8955f7-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-secrets-6dxhl" to be "success or failure"
Mar 13 23:34:57.256: INFO: Pod "pod-secrets-9c8955f7-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.443976ms
Mar 13 23:34:59.259: INFO: Pod "pod-secrets-9c8955f7-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007837943s
Mar 13 23:35:01.263: INFO: Pod "pod-secrets-9c8955f7-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011833904s
STEP: Saw pod success
Mar 13 23:35:01.263: INFO: Pod "pod-secrets-9c8955f7-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:35:01.266: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-secrets-9c8955f7-45e8-11e9-8b9c-0a58ac140107 container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 23:35:01.291: INFO: Waiting for pod pod-secrets-9c8955f7-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:35:01.294: INFO: Pod pod-secrets-9c8955f7-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:35:01.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6dxhl" for this suite.
Mar 13 23:35:07.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:35:07.331: INFO: namespace: e2e-tests-secrets-6dxhl, resource: bindings, ignored listing per whitelist
Mar 13 23:35:07.380: INFO: namespace e2e-tests-secrets-6dxhl deletion completed in 6.083716413s

• [SLOW TEST:10.177 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:35:07.380: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Mar 13 23:35:07.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 api-versions'
Mar 13 23:35:07.511: INFO: stderr: ""
Mar 13 23:35:07.511: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:35:07.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bf76n" for this suite.
Mar 13 23:35:13.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:35:13.547: INFO: namespace: e2e-tests-kubectl-bf76n, resource: bindings, ignored listing per whitelist
Mar 13 23:35:13.590: INFO: namespace e2e-tests-kubectl-bf76n deletion completed in 6.075505687s

• [SLOW TEST:6.210 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:35:13.590: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 23:35:13.635: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a64d3509-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-vmdl8" to be "success or failure"
Mar 13 23:35:13.638: INFO: Pod "downwardapi-volume-a64d3509-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 3.438858ms
Mar 13 23:35:15.642: INFO: Pod "downwardapi-volume-a64d3509-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006857262s
Mar 13 23:35:17.646: INFO: Pod "downwardapi-volume-a64d3509-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010655971s
STEP: Saw pod success
Mar 13 23:35:17.646: INFO: Pod "downwardapi-volume-a64d3509-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:35:17.647: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downwardapi-volume-a64d3509-45e8-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 23:35:17.661: INFO: Waiting for pod downwardapi-volume-a64d3509-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:35:17.663: INFO: Pod downwardapi-volume-a64d3509-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:35:17.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vmdl8" for this suite.
Mar 13 23:35:23.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:35:23.683: INFO: namespace: e2e-tests-downward-api-vmdl8, resource: bindings, ignored listing per whitelist
Mar 13 23:35:23.740: INFO: namespace e2e-tests-downward-api-vmdl8 deletion completed in 6.074382091s

• [SLOW TEST:10.150 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:35:23.741: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-bbx6x
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Mar 13 23:35:23.800: INFO: Found 0 stateful pods, waiting for 3
Mar 13 23:35:33.805: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 23:35:33.805: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 23:35:33.805: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 13 23:35:43.804: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 23:35:43.804: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 23:35:43.804: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar 13 23:35:43.825: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 13 23:35:53.852: INFO: Updating stateful set ss2
Mar 13 23:35:53.857: INFO: Waiting for Pod e2e-tests-statefulset-bbx6x/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Mar 13 23:36:03.901: INFO: Found 2 stateful pods, waiting for 3
Mar 13 23:36:13.905: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 23:36:13.906: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 13 23:36:13.906: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 13 23:36:13.927: INFO: Updating stateful set ss2
Mar 13 23:36:13.933: INFO: Waiting for Pod e2e-tests-statefulset-bbx6x/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar 13 23:36:23.954: INFO: Updating stateful set ss2
Mar 13 23:36:23.958: INFO: Waiting for StatefulSet e2e-tests-statefulset-bbx6x/ss2 to complete update
Mar 13 23:36:23.958: INFO: Waiting for Pod e2e-tests-statefulset-bbx6x/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 13 23:36:33.964: INFO: Deleting all statefulset in ns e2e-tests-statefulset-bbx6x
Mar 13 23:36:33.965: INFO: Scaling statefulset ss2 to 0
Mar 13 23:36:53.977: INFO: Waiting for statefulset status.replicas updated to 0
Mar 13 23:36:53.979: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:36:53.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-bbx6x" for this suite.
Mar 13 23:37:00.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:37:00.032: INFO: namespace: e2e-tests-statefulset-bbx6x, resource: bindings, ignored listing per whitelist
Mar 13 23:37:00.069: INFO: namespace e2e-tests-statefulset-bbx6x deletion completed in 6.070578512s

• [SLOW TEST:96.328 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:37:00.069: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 13 23:37:00.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e5c3fb43-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-dsrrh" to be "success or failure"
Mar 13 23:37:00.111: INFO: Pod "downwardapi-volume-e5c3fb43-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.703289ms
Mar 13 23:37:02.115: INFO: Pod "downwardapi-volume-e5c3fb43-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005318431s
STEP: Saw pod success
Mar 13 23:37:02.115: INFO: Pod "downwardapi-volume-e5c3fb43-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:37:02.117: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-0 pod downwardapi-volume-e5c3fb43-45e8-11e9-8b9c-0a58ac140107 container client-container: <nil>
STEP: delete the pod
Mar 13 23:37:02.130: INFO: Waiting for pod downwardapi-volume-e5c3fb43-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:37:02.134: INFO: Pod downwardapi-volume-e5c3fb43-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:37:02.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dsrrh" for this suite.
Mar 13 23:37:08.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:37:08.206: INFO: namespace: e2e-tests-projected-dsrrh, resource: bindings, ignored listing per whitelist
Mar 13 23:37:08.210: INFO: namespace e2e-tests-projected-dsrrh deletion completed in 6.073268791s

• [SLOW TEST:8.141 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:37:08.211: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 23:37:08.261: INFO: (0) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 3.822629ms)
Mar 13 23:37:08.264: INFO: (1) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 3.271316ms)
Mar 13 23:37:08.268: INFO: (2) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 3.829861ms)
Mar 13 23:37:08.272: INFO: (3) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 3.6417ms)
Mar 13 23:37:08.274: INFO: (4) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.73408ms)
Mar 13 23:37:08.277: INFO: (5) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.679186ms)
Mar 13 23:37:08.280: INFO: (6) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 3.252821ms)
Mar 13 23:37:08.283: INFO: (7) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.405471ms)
Mar 13 23:37:08.285: INFO: (8) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.662497ms)
Mar 13 23:37:08.288: INFO: (9) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.278221ms)
Mar 13 23:37:08.291: INFO: (10) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 3.365745ms)
Mar 13 23:37:08.294: INFO: (11) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 3.367774ms)
Mar 13 23:37:08.297: INFO: (12) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.003714ms)
Mar 13 23:37:08.299: INFO: (13) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.974926ms)
Mar 13 23:37:08.302: INFO: (14) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.210191ms)
Mar 13 23:37:08.304: INFO: (15) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.194385ms)
Mar 13 23:37:08.306: INFO: (16) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.072473ms)
Mar 13 23:37:08.308: INFO: (17) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 1.879545ms)
Mar 13 23:37:08.310: INFO: (18) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 2.020837ms)
Mar 13 23:37:08.312: INFO: (19) /api/v1/nodes/karbon-test-3a0ff6-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="boot.log-20190313">boot.log-20190313</a>
<a href="... (200; 1.919006ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:37:08.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-k7gth" for this suite.
Mar 13 23:37:14.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:37:14.375: INFO: namespace: e2e-tests-proxy-k7gth, resource: bindings, ignored listing per whitelist
Mar 13 23:37:14.384: INFO: namespace e2e-tests-proxy-k7gth deletion completed in 6.069698656s

• [SLOW TEST:6.174 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:37:14.385: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-ee4c8d34-45e8-11e9-8b9c-0a58ac140107
STEP: Creating secret with name secret-projected-all-test-volume-ee4c8d22-45e8-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 13 23:37:14.431: INFO: Waiting up to 5m0s for pod "projected-volume-ee4c8cf0-45e8-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-sfw42" to be "success or failure"
Mar 13 23:37:14.435: INFO: Pod "projected-volume-ee4c8cf0-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 3.419058ms
Mar 13 23:37:16.438: INFO: Pod "projected-volume-ee4c8cf0-45e8-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006458808s
Mar 13 23:37:18.441: INFO: Pod "projected-volume-ee4c8cf0-45e8-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009523217s
STEP: Saw pod success
Mar 13 23:37:18.441: INFO: Pod "projected-volume-ee4c8cf0-45e8-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:37:18.443: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod projected-volume-ee4c8cf0-45e8-11e9-8b9c-0a58ac140107 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 13 23:37:18.452: INFO: Waiting for pod projected-volume-ee4c8cf0-45e8-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:37:18.454: INFO: Pod projected-volume-ee4c8cf0-45e8-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:37:18.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sfw42" for this suite.
Mar 13 23:37:24.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:37:24.508: INFO: namespace: e2e-tests-projected-sfw42, resource: bindings, ignored listing per whitelist
Mar 13 23:37:24.531: INFO: namespace e2e-tests-projected-sfw42 deletion completed in 6.074760987s

• [SLOW TEST:10.146 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:37:24.531: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 13 23:37:30.659: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-47ptt PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 23:37:30.659: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 23:37:30.740: INFO: Exec stderr: ""
Mar 13 23:37:30.740: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-47ptt PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 23:37:30.740: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 23:37:30.821: INFO: Exec stderr: ""
Mar 13 23:37:30.821: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-47ptt PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 23:37:30.821: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 23:37:30.899: INFO: Exec stderr: ""
Mar 13 23:37:30.899: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-47ptt PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 23:37:30.899: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 23:37:30.977: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 13 23:37:30.977: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-47ptt PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 23:37:30.977: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 23:37:31.054: INFO: Exec stderr: ""
Mar 13 23:37:31.054: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-47ptt PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 23:37:31.054: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 23:37:31.133: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 13 23:37:31.133: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-47ptt PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 23:37:31.133: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 23:37:31.221: INFO: Exec stderr: ""
Mar 13 23:37:31.221: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-47ptt PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 23:37:31.221: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 23:37:31.297: INFO: Exec stderr: ""
Mar 13 23:37:31.297: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-47ptt PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 23:37:31.297: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 23:37:31.373: INFO: Exec stderr: ""
Mar 13 23:37:31.373: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-47ptt PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 13 23:37:31.373: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
Mar 13 23:37:31.448: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:37:31.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-47ptt" for this suite.
Mar 13 23:38:21.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:38:21.500: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-47ptt, resource: bindings, ignored listing per whitelist
Mar 13 23:38:21.522: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-47ptt deletion completed in 50.068924118s

• [SLOW TEST:56.991 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:38:21.522: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-16508088-45e9-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 23:38:21.574: INFO: Waiting up to 5m0s for pod "pod-secrets-1650df74-45e9-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-secrets-qqhdl" to be "success or failure"
Mar 13 23:38:21.593: INFO: Pod "pod-secrets-1650df74-45e9-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 19.300209ms
Mar 13 23:38:23.597: INFO: Pod "pod-secrets-1650df74-45e9-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022692643s
Mar 13 23:38:25.600: INFO: Pod "pod-secrets-1650df74-45e9-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026048627s
STEP: Saw pod success
Mar 13 23:38:25.600: INFO: Pod "pod-secrets-1650df74-45e9-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:38:25.602: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-secrets-1650df74-45e9-11e9-8b9c-0a58ac140107 container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 23:38:25.615: INFO: Waiting for pod pod-secrets-1650df74-45e9-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:38:25.617: INFO: Pod pod-secrets-1650df74-45e9-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:38:25.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-qqhdl" for this suite.
Mar 13 23:38:31.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:38:31.698: INFO: namespace: e2e-tests-secrets-qqhdl, resource: bindings, ignored listing per whitelist
Mar 13 23:38:31.699: INFO: namespace e2e-tests-secrets-qqhdl deletion completed in 6.0790475s

• [SLOW TEST:10.177 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:38:31.699: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-1c6a8e0d-45e9-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 23:38:31.799: INFO: Waiting up to 5m0s for pod "pod-secrets-1c6ad071-45e9-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-secrets-tjmcl" to be "success or failure"
Mar 13 23:38:31.802: INFO: Pod "pod-secrets-1c6ad071-45e9-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12901ms
Mar 13 23:38:33.805: INFO: Pod "pod-secrets-1c6ad071-45e9-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005480991s
Mar 13 23:38:35.808: INFO: Pod "pod-secrets-1c6ad071-45e9-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008930348s
STEP: Saw pod success
Mar 13 23:38:35.808: INFO: Pod "pod-secrets-1c6ad071-45e9-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:38:35.810: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-secrets-1c6ad071-45e9-11e9-8b9c-0a58ac140107 container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 23:38:35.822: INFO: Waiting for pod pod-secrets-1c6ad071-45e9-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:38:35.824: INFO: Pod pod-secrets-1c6ad071-45e9-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:38:35.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-tjmcl" for this suite.
Mar 13 23:38:41.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:38:41.895: INFO: namespace: e2e-tests-secrets-tjmcl, resource: bindings, ignored listing per whitelist
Mar 13 23:38:41.900: INFO: namespace e2e-tests-secrets-tjmcl deletion completed in 6.07233126s

• [SLOW TEST:10.200 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:38:41.900: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-v568j.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-v568j.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-v568j.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-v568j.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-v568j.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-v568j.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 13 23:38:55.993: INFO: DNS probes using e2e-tests-dns-v568j/dns-test-2276bb88-45e9-11e9-8b9c-0a58ac140107 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:38:55.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-v568j" for this suite.
Mar 13 23:39:02.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:39:02.078: INFO: namespace: e2e-tests-dns-v568j, resource: bindings, ignored listing per whitelist
Mar 13 23:39:02.083: INFO: namespace e2e-tests-dns-v568j deletion completed in 6.08065432s

• [SLOW TEST:20.183 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:39:02.083: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-2e7ebdaf-45e9-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 23:39:02.132: INFO: Waiting up to 5m0s for pod "pod-secrets-2e7f09bd-45e9-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-secrets-fk8sl" to be "success or failure"
Mar 13 23:39:02.136: INFO: Pod "pod-secrets-2e7f09bd-45e9-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 3.666315ms
Mar 13 23:39:04.140: INFO: Pod "pod-secrets-2e7f09bd-45e9-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008007916s
Mar 13 23:39:06.143: INFO: Pod "pod-secrets-2e7f09bd-45e9-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011251408s
STEP: Saw pod success
Mar 13 23:39:06.143: INFO: Pod "pod-secrets-2e7f09bd-45e9-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:39:06.145: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod pod-secrets-2e7f09bd-45e9-11e9-8b9c-0a58ac140107 container secret-volume-test: <nil>
STEP: delete the pod
Mar 13 23:39:06.160: INFO: Waiting for pod pod-secrets-2e7f09bd-45e9-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:39:06.161: INFO: Pod pod-secrets-2e7f09bd-45e9-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:39:06.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-fk8sl" for this suite.
Mar 13 23:39:12.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:39:12.226: INFO: namespace: e2e-tests-secrets-fk8sl, resource: bindings, ignored listing per whitelist
Mar 13 23:39:12.245: INFO: namespace e2e-tests-secrets-fk8sl deletion completed in 6.081324379s

• [SLOW TEST:10.162 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:39:12.245: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0313 23:39:52.325735      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 13 23:39:52.325: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:39:52.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-hp5dz" for this suite.
Mar 13 23:39:58.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:39:58.381: INFO: namespace: e2e-tests-gc-hp5dz, resource: bindings, ignored listing per whitelist
Mar 13 23:39:58.397: INFO: namespace e2e-tests-gc-hp5dz deletion completed in 6.069746019s

• [SLOW TEST:46.152 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:39:58.398: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar 13 23:39:58.452: INFO: Waiting up to 5m0s for pod "downward-api-5010c5ad-45e9-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-downward-api-c94jv" to be "success or failure"
Mar 13 23:39:58.453: INFO: Pod "downward-api-5010c5ad-45e9-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580407ms
Mar 13 23:40:00.457: INFO: Pod "downward-api-5010c5ad-45e9-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005056593s
Mar 13 23:40:02.460: INFO: Pod "downward-api-5010c5ad-45e9-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00831405s
STEP: Saw pod success
Mar 13 23:40:02.460: INFO: Pod "downward-api-5010c5ad-45e9-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:40:02.462: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-1 pod downward-api-5010c5ad-45e9-11e9-8b9c-0a58ac140107 container dapi-container: <nil>
STEP: delete the pod
Mar 13 23:40:02.476: INFO: Waiting for pod downward-api-5010c5ad-45e9-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:40:02.478: INFO: Pod downward-api-5010c5ad-45e9-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:40:02.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-c94jv" for this suite.
Mar 13 23:40:08.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:40:08.552: INFO: namespace: e2e-tests-downward-api-c94jv, resource: bindings, ignored listing per whitelist
Mar 13 23:40:08.553: INFO: namespace e2e-tests-downward-api-c94jv deletion completed in 6.071832974s

• [SLOW TEST:10.156 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:40:08.553: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 13 23:40:28.608: INFO: Container started at 2019-03-13 23:40:10 +0000 UTC, pod became ready at 2019-03-13 23:40:26 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:40:28.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-j56z6" for this suite.
Mar 13 23:40:50.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:40:50.665: INFO: namespace: e2e-tests-container-probe-j56z6, resource: bindings, ignored listing per whitelist
Mar 13 23:40:50.682: INFO: namespace e2e-tests-container-probe-j56z6 deletion completed in 22.070814468s

• [SLOW TEST:42.128 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:40:50.682: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-6f3b8755-45e9-11e9-8b9c-0a58ac140107
STEP: Creating a pod to test consume secrets
Mar 13 23:40:50.744: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6f3bd0fe-45e9-11e9-8b9c-0a58ac140107" in namespace "e2e-tests-projected-7slw7" to be "success or failure"
Mar 13 23:40:50.749: INFO: Pod "pod-projected-secrets-6f3bd0fe-45e9-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.995852ms
Mar 13 23:40:52.752: INFO: Pod "pod-projected-secrets-6f3bd0fe-45e9-11e9-8b9c-0a58ac140107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008626537s
Mar 13 23:40:54.756: INFO: Pod "pod-projected-secrets-6f3bd0fe-45e9-11e9-8b9c-0a58ac140107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012295669s
STEP: Saw pod success
Mar 13 23:40:54.756: INFO: Pod "pod-projected-secrets-6f3bd0fe-45e9-11e9-8b9c-0a58ac140107" satisfied condition "success or failure"
Mar 13 23:40:54.758: INFO: Trying to get logs from node karbon-test-3a0ff6-k8s-worker-2 pod pod-projected-secrets-6f3bd0fe-45e9-11e9-8b9c-0a58ac140107 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 13 23:40:54.769: INFO: Waiting for pod pod-projected-secrets-6f3bd0fe-45e9-11e9-8b9c-0a58ac140107 to disappear
Mar 13 23:40:54.772: INFO: Pod pod-projected-secrets-6f3bd0fe-45e9-11e9-8b9c-0a58ac140107 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:40:54.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7slw7" for this suite.
Mar 13 23:41:00.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:41:00.821: INFO: namespace: e2e-tests-projected-7slw7, resource: bindings, ignored listing per whitelist
Mar 13 23:41:00.844: INFO: namespace e2e-tests-projected-7slw7 deletion completed in 6.06917534s

• [SLOW TEST:10.162 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar 13 23:41:00.844: INFO: >>> kubeConfig: /tmp/kubeconfig-703856963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar 13 23:41:00.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 create -f - --namespace=e2e-tests-kubectl-n2f7v'
Mar 13 23:41:01.265: INFO: stderr: ""
Mar 13 23:41:01.265: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 13 23:41:02.269: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 23:41:02.269: INFO: Found 0 / 1
Mar 13 23:41:03.269: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 23:41:03.269: INFO: Found 0 / 1
Mar 13 23:41:04.268: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 23:41:04.268: INFO: Found 1 / 1
Mar 13 23:41:04.268: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 13 23:41:04.270: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 23:41:04.270: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 13 23:41:04.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-703856963 patch pod redis-master-xwzzn --namespace=e2e-tests-kubectl-n2f7v -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 13 23:41:04.362: INFO: stderr: ""
Mar 13 23:41:04.362: INFO: stdout: "pod/redis-master-xwzzn patched\n"
STEP: checking annotations
Mar 13 23:41:04.366: INFO: Selector matched 1 pods for map[app:redis]
Mar 13 23:41:04.366: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar 13 23:41:04.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-n2f7v" for this suite.
Mar 13 23:41:26.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 13 23:41:26.422: INFO: namespace: e2e-tests-kubectl-n2f7v, resource: bindings, ignored listing per whitelist
Mar 13 23:41:26.446: INFO: namespace e2e-tests-kubectl-n2f7v deletion completed in 22.077425764s

• [SLOW TEST:25.602 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSMar 13 23:41:26.447: INFO: Running AfterSuite actions on all nodes
Mar 13 23:41:26.447: INFO: Running AfterSuite actions on node 1
Mar 13 23:41:26.447: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5582.347 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h33m3.098062197s
Test Suite Passed
