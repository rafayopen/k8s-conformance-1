Aug  6 14:51:01.145: INFO: The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
I0806 14:51:01.145136    4087 e2e.go:224] Starting e2e run "9b38c52e-b859-11e9-8d18-525400524259" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1565103060 - Will randomize all specs
Will run 201 of 2162 specs

Aug  6 14:51:01.181: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 14:51:01.184: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
Aug  6 14:51:01.311: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug  6 14:51:01.387: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug  6 14:51:01.387: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Aug  6 14:51:01.387: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug  6 14:51:01.414: INFO: e2e test version: v1.13.10-beta.0.1+b5c04d0f249f61
Aug  6 14:51:01.434: INFO: kube-apiserver version: v1.13.4+3a25c9b
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 14:51:01.434: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
Aug  6 14:51:02.777: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug  6 14:51:02.827: INFO: Waiting up to 5m0s for pod "pod-9c746221-b859-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-cvjns" to be "success or failure"
Aug  6 14:51:02.849: INFO: Pod "pod-9c746221-b859-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 21.93726ms
Aug  6 14:51:04.873: INFO: Pod "pod-9c746221-b859-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04528518s
Aug  6 14:51:06.895: INFO: Pod "pod-9c746221-b859-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067939154s
Aug  6 14:51:08.918: INFO: Pod "pod-9c746221-b859-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090283075s
Aug  6 14:51:10.940: INFO: Pod "pod-9c746221-b859-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.112959408s
Aug  6 14:51:12.963: INFO: Pod "pod-9c746221-b859-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 10.135590193s
Aug  6 14:51:14.986: INFO: Pod "pod-9c746221-b859-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.158301897s
STEP: Saw pod success
Aug  6 14:51:14.986: INFO: Pod "pod-9c746221-b859-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 14:51:15.008: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-9c746221-b859-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 14:51:15.074: INFO: Waiting for pod pod-9c746221-b859-11e9-8d18-525400524259 to disappear
Aug  6 14:51:15.096: INFO: Pod pod-9c746221-b859-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 14:51:15.096: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cvjns" for this suite.
Aug  6 14:51:21.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 14:51:22.050: INFO: namespace: e2e-tests-emptydir-cvjns, resource: packagemanifests, items remaining: 3
Aug  6 14:51:22.461: INFO: namespace: e2e-tests-emptydir-cvjns, resource: bindings, ignored listing per whitelist
Aug  6 14:51:23.639: INFO: namespace: e2e-tests-emptydir-cvjns no longer exists
Aug  6 14:51:23.685: INFO: namespace: e2e-tests-emptydir-cvjns, total namespaces: 50, active: 50, terminating: 0
Aug  6 14:51:23.706: INFO: namespace e2e-tests-emptydir-cvjns deletion completed in 8.54894276s

• [SLOW TEST:22.273 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 14:51:23.707: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug  6 14:51:26.319: INFO: Pod name wrapped-volume-race-aa6aee35-b859-11e9-8d18-525400524259: Found 1 pods out of 5
Aug  6 14:51:31.382: INFO: Pod name wrapped-volume-race-aa6aee35-b859-11e9-8d18-525400524259: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-aa6aee35-b859-11e9-8d18-525400524259 in namespace e2e-tests-emptydir-wrapper-dn9p7, will wait for the garbage collector to delete the pods
Aug  6 14:51:49.688: INFO: Deleting ReplicationController wrapped-volume-race-aa6aee35-b859-11e9-8d18-525400524259 took: 26.975241ms
Aug  6 14:51:49.788: INFO: Terminating ReplicationController wrapped-volume-race-aa6aee35-b859-11e9-8d18-525400524259 pods took: 100.281841ms
STEP: Creating RC which spawns configmap-volume pods
Aug  6 14:52:33.979: INFO: Pod name wrapped-volume-race-d2bfb842-b859-11e9-8d18-525400524259: Found 1 pods out of 5
Aug  6 14:52:39.023: INFO: Pod name wrapped-volume-race-d2bfb842-b859-11e9-8d18-525400524259: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d2bfb842-b859-11e9-8d18-525400524259 in namespace e2e-tests-emptydir-wrapper-dn9p7, will wait for the garbage collector to delete the pods
Aug  6 14:53:21.303: INFO: Deleting ReplicationController wrapped-volume-race-d2bfb842-b859-11e9-8d18-525400524259 took: 26.828248ms
Aug  6 14:53:21.403: INFO: Terminating ReplicationController wrapped-volume-race-d2bfb842-b859-11e9-8d18-525400524259 pods took: 100.345735ms
STEP: Creating RC which spawns configmap-volume pods
Aug  6 14:53:57.102: INFO: Pod name wrapped-volume-race-044a0cf8-b85a-11e9-8d18-525400524259: Found 0 pods out of 5
Aug  6 14:54:02.148: INFO: Pod name wrapped-volume-race-044a0cf8-b85a-11e9-8d18-525400524259: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-044a0cf8-b85a-11e9-8d18-525400524259 in namespace e2e-tests-emptydir-wrapper-dn9p7, will wait for the garbage collector to delete the pods
Aug  6 14:54:44.450: INFO: Deleting ReplicationController wrapped-volume-race-044a0cf8-b85a-11e9-8d18-525400524259 took: 26.121326ms
Aug  6 14:54:44.650: INFO: Terminating ReplicationController wrapped-volume-race-044a0cf8-b85a-11e9-8d18-525400524259 pods took: 200.34831ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 14:55:20.795: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-dn9p7" for this suite.
Aug  6 14:55:26.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 14:55:27.784: INFO: namespace: e2e-tests-emptydir-wrapper-dn9p7, resource: packagemanifests, items remaining: 3
Aug  6 14:55:27.955: INFO: namespace: e2e-tests-emptydir-wrapper-dn9p7, resource: bindings, ignored listing per whitelist
Aug  6 14:55:29.293: INFO: namespace: e2e-tests-emptydir-wrapper-dn9p7 no longer exists
Aug  6 14:55:29.316: INFO: namespace: e2e-tests-emptydir-wrapper-dn9p7, total namespaces: 50, active: 50, terminating: 0
Aug  6 14:55:29.340: INFO: namespace e2e-tests-emptydir-wrapper-dn9p7 deletion completed in 8.497705575s

• [SLOW TEST:245.633 seconds]
[sig-storage] EmptyDir wrapper volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 14:55:29.341: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 14:55:30.746: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3c29f437-b85a-11e9-8d18-525400524259
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-3c29f437-b85a-11e9-8d18-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 14:56:43.764: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jj4tp" for this suite.
Aug  6 14:57:05.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 14:57:07.317: INFO: namespace: e2e-tests-projected-jj4tp, resource: bindings, ignored listing per whitelist
Aug  6 14:57:07.432: INFO: namespace: e2e-tests-projected-jj4tp, resource: packagemanifests, items remaining: 3
Aug  6 14:57:08.280: INFO: namespace: e2e-tests-projected-jj4tp no longer exists
Aug  6 14:57:08.303: INFO: namespace: e2e-tests-projected-jj4tp, total namespaces: 50, active: 50, terminating: 0
Aug  6 14:57:08.324: INFO: namespace e2e-tests-projected-jj4tp deletion completed in 24.498891371s

• [SLOW TEST:98.983 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 14:57:08.325: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Aug  6 14:57:19.905: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 14:57:44.088: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-wjzfh" for this suite.
Aug  6 14:57:50.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 14:57:52.004: INFO: namespace: e2e-tests-namespaces-wjzfh, resource: bindings, ignored listing per whitelist
Aug  6 14:57:52.383: INFO: namespace: e2e-tests-namespaces-wjzfh, resource: packagemanifests, items remaining: 3
Aug  6 14:57:52.621: INFO: namespace: e2e-tests-namespaces-wjzfh no longer exists
Aug  6 14:57:52.645: INFO: namespace: e2e-tests-namespaces-wjzfh, total namespaces: 51, active: 51, terminating: 0
Aug  6 14:57:52.667: INFO: namespace e2e-tests-namespaces-wjzfh deletion completed in 8.517564244s
STEP: Destroying namespace "e2e-tests-nsdeletetest-rbmhm" for this suite.
Aug  6 14:57:52.689: INFO: Namespace e2e-tests-nsdeletetest-rbmhm was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-7t9t2" for this suite.
Aug  6 14:57:58.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 14:57:59.637: INFO: namespace: e2e-tests-nsdeletetest-7t9t2, resource: bindings, ignored listing per whitelist
Aug  6 14:58:01.128: INFO: namespace: e2e-tests-nsdeletetest-7t9t2, resource: packagemanifests, items remaining: 3
Aug  6 14:58:01.150: INFO: namespace: e2e-tests-nsdeletetest-7t9t2 no longer exists
Aug  6 14:58:01.172: INFO: namespace: e2e-tests-nsdeletetest-7t9t2, total namespaces: 50, active: 50, terminating: 0
Aug  6 14:58:01.194: INFO: namespace e2e-tests-nsdeletetest-7t9t2 deletion completed in 8.504901579s

• [SLOW TEST:52.870 seconds]
[sig-api-machinery] Namespaces [Serial]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 14:58:01.195: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-sjp4g
[It] Should recreate evicted statefulset [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-sjp4g
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-sjp4g
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-sjp4g
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-sjp4g
Aug  6 14:58:14.773: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-sjp4g, name: ss-0, uid: 9030d90e-b85a-11e9-9dff-0e6de702691c, status phase: Pending. Waiting for statefulset controller to delete.
Aug  6 14:58:16.086: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-sjp4g, name: ss-0, uid: 9030d90e-b85a-11e9-9dff-0e6de702691c, status phase: Failed. Waiting for statefulset controller to delete.
Aug  6 14:58:16.092: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-sjp4g, name: ss-0, uid: 9030d90e-b85a-11e9-9dff-0e6de702691c, status phase: Failed. Waiting for statefulset controller to delete.
Aug  6 14:58:16.096: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-sjp4g
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-sjp4g
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-sjp4g and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug  6 14:58:26.266: INFO: Deleting all statefulset in ns e2e-tests-statefulset-sjp4g
Aug  6 14:58:26.289: INFO: Scaling statefulset ss to 0
Aug  6 14:58:36.390: INFO: Waiting for statefulset status.replicas updated to 0
Aug  6 14:58:36.415: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 14:58:36.486: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-sjp4g" for this suite.
Aug  6 14:58:42.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 14:58:43.659: INFO: namespace: e2e-tests-statefulset-sjp4g, resource: packagemanifests, items remaining: 3
Aug  6 14:58:44.319: INFO: namespace: e2e-tests-statefulset-sjp4g, resource: bindings, ignored listing per whitelist
Aug  6 14:58:44.991: INFO: namespace: e2e-tests-statefulset-sjp4g no longer exists
Aug  6 14:58:45.014: INFO: namespace: e2e-tests-statefulset-sjp4g, total namespaces: 50, active: 50, terminating: 0
Aug  6 14:58:45.036: INFO: namespace e2e-tests-statefulset-sjp4g deletion completed in 8.506060082s

• [SLOW TEST:43.840 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 14:58:45.036: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Aug  6 14:58:46.406: INFO: Waiting up to 5m0s for pod "client-containers-b0c3a470-b85a-11e9-8d18-525400524259" in namespace "e2e-tests-containers-7pm42" to be "success or failure"
Aug  6 14:58:46.428: INFO: Pod "client-containers-b0c3a470-b85a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 21.832792ms
Aug  6 14:58:48.450: INFO: Pod "client-containers-b0c3a470-b85a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044533431s
Aug  6 14:58:50.473: INFO: Pod "client-containers-b0c3a470-b85a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067061494s
Aug  6 14:58:52.496: INFO: Pod "client-containers-b0c3a470-b85a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090606867s
Aug  6 14:58:54.519: INFO: Pod "client-containers-b0c3a470-b85a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113416679s
Aug  6 14:58:56.542: INFO: Pod "client-containers-b0c3a470-b85a-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.135690859s
STEP: Saw pod success
Aug  6 14:58:56.542: INFO: Pod "client-containers-b0c3a470-b85a-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 14:58:56.563: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod client-containers-b0c3a470-b85a-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 14:58:56.619: INFO: Waiting for pod client-containers-b0c3a470-b85a-11e9-8d18-525400524259 to disappear
Aug  6 14:58:56.641: INFO: Pod client-containers-b0c3a470-b85a-11e9-8d18-525400524259 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 14:58:56.641: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-7pm42" for this suite.
Aug  6 14:59:02.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 14:59:04.370: INFO: namespace: e2e-tests-containers-7pm42, resource: bindings, ignored listing per whitelist
Aug  6 14:59:05.079: INFO: namespace: e2e-tests-containers-7pm42, resource: packagemanifests, items remaining: 3
Aug  6 14:59:05.165: INFO: namespace: e2e-tests-containers-7pm42 no longer exists
Aug  6 14:59:05.188: INFO: namespace: e2e-tests-containers-7pm42, total namespaces: 50, active: 50, terminating: 0
Aug  6 14:59:05.209: INFO: namespace e2e-tests-containers-7pm42 deletion completed in 8.50644868s

• [SLOW TEST:20.174 seconds]
[k8s.io] Docker Containers
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 14:59:05.210: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Aug  6 14:59:17.300: INFO: Successfully updated pod "labelsupdatebcd6e277-b85a-11e9-8d18-525400524259"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 14:59:19.351: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-f5lj7" for this suite.
Aug  6 14:59:41.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 14:59:42.152: INFO: namespace: e2e-tests-projected-f5lj7, resource: packagemanifests, items remaining: 3
Aug  6 14:59:43.801: INFO: namespace: e2e-tests-projected-f5lj7, resource: bindings, ignored listing per whitelist
Aug  6 14:59:43.865: INFO: namespace: e2e-tests-projected-f5lj7 no longer exists
Aug  6 14:59:43.888: INFO: namespace: e2e-tests-projected-f5lj7, total namespaces: 50, active: 50, terminating: 0
Aug  6 14:59:43.909: INFO: namespace e2e-tests-projected-f5lj7 deletion completed in 24.495405399s

• [SLOW TEST:38.701 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 14:59:43.911: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-t5g24
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-t5g24
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-t5g24
Aug  6 14:59:45.350: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug  6 14:59:55.375: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug  6 15:00:05.374: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug  6 15:00:05.397: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-t5g24 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  6 15:00:05.833: INFO: stderr: ""
Aug  6 15:00:05.833: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  6 15:00:05.833: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  6 15:00:05.855: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug  6 15:00:15.880: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug  6 15:00:15.880: INFO: Waiting for statefulset status.replicas updated to 0
Aug  6 15:00:15.970: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999773s
Aug  6 15:00:16.993: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.978099113s
Aug  6 15:00:18.016: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.955349107s
Aug  6 15:00:19.040: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.932181862s
Aug  6 15:00:20.063: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.908456062s
Aug  6 15:00:21.087: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.885727282s
Aug  6 15:00:22.111: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.861891671s
Aug  6 15:00:23.135: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.837796981s
Aug  6 15:00:24.160: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.813249412s
Aug  6 15:00:25.183: INFO: Verifying statefulset ss doesn't scale past 1 for another 788.898227ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-t5g24
Aug  6 15:00:26.206: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-t5g24 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  6 15:00:26.554: INFO: stderr: ""
Aug  6 15:00:26.554: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  6 15:00:26.554: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  6 15:00:26.577: INFO: Found 1 stateful pods, waiting for 3
Aug  6 15:00:36.600: INFO: Found 2 stateful pods, waiting for 3
Aug  6 15:00:46.609: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 15:00:46.609: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 15:00:46.609: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug  6 15:00:56.600: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 15:00:56.600: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 15:00:56.600: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug  6 15:00:56.646: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-t5g24 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  6 15:00:57.045: INFO: stderr: ""
Aug  6 15:00:57.045: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  6 15:00:57.045: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  6 15:00:57.045: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-t5g24 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  6 15:00:57.452: INFO: stderr: ""
Aug  6 15:00:57.452: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  6 15:00:57.452: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  6 15:00:57.452: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-t5g24 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  6 15:00:57.831: INFO: stderr: ""
Aug  6 15:00:57.831: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  6 15:00:57.831: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  6 15:00:57.831: INFO: Waiting for statefulset status.replicas updated to 0
Aug  6 15:00:57.853: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug  6 15:01:07.898: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug  6 15:01:07.898: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug  6 15:01:07.898: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug  6 15:01:07.966: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999779s
Aug  6 15:01:08.988: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.97678945s
Aug  6 15:01:10.013: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.954124316s
Aug  6 15:01:11.035: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.929991251s
Aug  6 15:01:12.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.907352319s
Aug  6 15:01:13.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.884829513s
Aug  6 15:01:14.107: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.86170213s
Aug  6 15:01:15.135: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.8360952s
Aug  6 15:01:16.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.807197983s
Aug  6 15:01:17.186: INFO: Verifying statefulset ss doesn't scale past 3 for another 782.243262ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-t5g24
Aug  6 15:01:18.211: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-t5g24 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  6 15:01:18.606: INFO: stderr: ""
Aug  6 15:01:18.606: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  6 15:01:18.606: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  6 15:01:18.606: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-t5g24 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  6 15:01:18.965: INFO: stderr: ""
Aug  6 15:01:18.965: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  6 15:01:18.965: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  6 15:01:18.965: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-t5g24 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  6 15:01:19.370: INFO: stderr: ""
Aug  6 15:01:19.370: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  6 15:01:19.370: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  6 15:01:19.370: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug  6 15:01:29.471: INFO: Deleting all statefulset in ns e2e-tests-statefulset-t5g24
Aug  6 15:01:29.495: INFO: Scaling statefulset ss to 0
Aug  6 15:01:29.566: INFO: Waiting for statefulset status.replicas updated to 0
Aug  6 15:01:29.588: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:01:29.658: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-t5g24" for this suite.
Aug  6 15:01:35.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:01:36.809: INFO: namespace: e2e-tests-statefulset-t5g24, resource: packagemanifests, items remaining: 3
Aug  6 15:01:37.702: INFO: namespace: e2e-tests-statefulset-t5g24, resource: bindings, ignored listing per whitelist
Aug  6 15:01:38.180: INFO: namespace: e2e-tests-statefulset-t5g24 no longer exists
Aug  6 15:01:38.203: INFO: namespace: e2e-tests-statefulset-t5g24, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:01:38.226: INFO: namespace e2e-tests-statefulset-t5g24 deletion completed in 8.522968863s

• [SLOW TEST:114.315 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:01:38.227: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug  6 15:01:39.730: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-cdptc,SelfLink:/api/v1/namespaces/e2e-tests-watch-cdptc/configmaps/e2e-watch-test-label-changed,UID:0a941b3f-b85b-11e9-9dff-0e6de702691c,ResourceVersion:82927,Generation:0,CreationTimestamp:2019-08-06 15:01:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug  6 15:01:39.731: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-cdptc,SelfLink:/api/v1/namespaces/e2e-tests-watch-cdptc/configmaps/e2e-watch-test-label-changed,UID:0a941b3f-b85b-11e9-9dff-0e6de702691c,ResourceVersion:82929,Generation:0,CreationTimestamp:2019-08-06 15:01:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug  6 15:01:39.731: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-cdptc,SelfLink:/api/v1/namespaces/e2e-tests-watch-cdptc/configmaps/e2e-watch-test-label-changed,UID:0a941b3f-b85b-11e9-9dff-0e6de702691c,ResourceVersion:82931,Generation:0,CreationTimestamp:2019-08-06 15:01:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug  6 15:01:49.890: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-cdptc,SelfLink:/api/v1/namespaces/e2e-tests-watch-cdptc/configmaps/e2e-watch-test-label-changed,UID:0a941b3f-b85b-11e9-9dff-0e6de702691c,ResourceVersion:83042,Generation:0,CreationTimestamp:2019-08-06 15:01:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug  6 15:01:49.891: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-cdptc,SelfLink:/api/v1/namespaces/e2e-tests-watch-cdptc/configmaps/e2e-watch-test-label-changed,UID:0a941b3f-b85b-11e9-9dff-0e6de702691c,ResourceVersion:83043,Generation:0,CreationTimestamp:2019-08-06 15:01:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug  6 15:01:49.891: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-cdptc,SelfLink:/api/v1/namespaces/e2e-tests-watch-cdptc/configmaps/e2e-watch-test-label-changed,UID:0a941b3f-b85b-11e9-9dff-0e6de702691c,ResourceVersion:83044,Generation:0,CreationTimestamp:2019-08-06 15:01:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:01:49.891: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-cdptc" for this suite.
Aug  6 15:01:56.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:01:57.153: INFO: namespace: e2e-tests-watch-cdptc, resource: bindings, ignored listing per whitelist
Aug  6 15:01:58.016: INFO: namespace: e2e-tests-watch-cdptc, resource: packagemanifests, items remaining: 3
Aug  6 15:01:58.498: INFO: namespace: e2e-tests-watch-cdptc no longer exists
Aug  6 15:01:58.540: INFO: namespace: e2e-tests-watch-cdptc, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:01:58.562: INFO: namespace e2e-tests-watch-cdptc deletion completed in 8.609794522s

• [SLOW TEST:20.335 seconds]
[sig-api-machinery] Watchers
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:01:58.562: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Aug  6 15:01:59.909: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug  6 15:01:59.909: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:01.956: INFO: stderr: ""
Aug  6 15:02:01.956: INFO: stdout: "service/redis-slave created\n"
Aug  6 15:02:01.956: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug  6 15:02:01.956: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:02.472: INFO: stderr: ""
Aug  6 15:02:02.472: INFO: stdout: "service/redis-master created\n"
Aug  6 15:02:02.472: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug  6 15:02:02.472: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:03.335: INFO: stderr: ""
Aug  6 15:02:03.335: INFO: stdout: "service/frontend created\n"
Aug  6 15:02:03.336: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug  6 15:02:03.336: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:03.874: INFO: stderr: ""
Aug  6 15:02:03.874: INFO: stdout: "deployment.extensions/frontend created\n"
Aug  6 15:02:03.874: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug  6 15:02:03.874: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:04.300: INFO: stderr: ""
Aug  6 15:02:04.300: INFO: stdout: "deployment.extensions/redis-master created\n"
Aug  6 15:02:04.301: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug  6 15:02:04.301: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:05.132: INFO: stderr: ""
Aug  6 15:02:05.132: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Aug  6 15:02:05.132: INFO: Waiting for all frontend pods to be Running.
Aug  6 15:02:30.183: INFO: Waiting for frontend to serve content.
Aug  6 15:02:35.217: INFO: Trying to add a new entry to the guestbook.
Aug  6 15:02:40.256: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug  6 15:02:40.286: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:40.493: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  6 15:02:40.493: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug  6 15:02:40.493: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:40.683: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  6 15:02:40.683: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug  6 15:02:40.684: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:40.870: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  6 15:02:40.870: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug  6 15:02:40.870: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:41.044: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  6 15:02:41.044: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug  6 15:02:41.044: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:41.217: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  6 15:02:41.217: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug  6 15:02:41.217: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9cl2w'
Aug  6 15:02:41.391: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  6 15:02:41.391: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:02:41.391: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9cl2w" for this suite.
Aug  6 15:03:21.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:03:22.229: INFO: namespace: e2e-tests-kubectl-9cl2w, resource: bindings, ignored listing per whitelist
Aug  6 15:03:22.388: INFO: namespace: e2e-tests-kubectl-9cl2w, resource: packagemanifests, items remaining: 3
Aug  6 15:03:23.928: INFO: namespace: e2e-tests-kubectl-9cl2w no longer exists
Aug  6 15:03:23.951: INFO: namespace: e2e-tests-kubectl-9cl2w, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:03:23.973: INFO: namespace e2e-tests-kubectl-9cl2w deletion completed in 42.519783455s

• [SLOW TEST:85.411 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:03:23.973: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 15:03:25.346: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57074c39-b85b-11e9-8d18-525400524259" in namespace "e2e-tests-projected-hfx6s" to be "success or failure"
Aug  6 15:03:25.371: INFO: Pod "downwardapi-volume-57074c39-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 24.647417ms
Aug  6 15:03:27.396: INFO: Pod "downwardapi-volume-57074c39-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050307773s
Aug  6 15:03:29.420: INFO: Pod "downwardapi-volume-57074c39-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07396722s
Aug  6 15:03:31.443: INFO: Pod "downwardapi-volume-57074c39-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096976123s
Aug  6 15:03:33.466: INFO: Pod "downwardapi-volume-57074c39-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.119786022s
Aug  6 15:03:35.488: INFO: Pod "downwardapi-volume-57074c39-b85b-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.142155698s
STEP: Saw pod success
Aug  6 15:03:35.488: INFO: Pod "downwardapi-volume-57074c39-b85b-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:03:35.510: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-57074c39-b85b-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 15:03:35.565: INFO: Waiting for pod downwardapi-volume-57074c39-b85b-11e9-8d18-525400524259 to disappear
Aug  6 15:03:35.588: INFO: Pod downwardapi-volume-57074c39-b85b-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:03:35.588: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hfx6s" for this suite.
Aug  6 15:03:41.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:03:42.767: INFO: namespace: e2e-tests-projected-hfx6s, resource: bindings, ignored listing per whitelist
Aug  6 15:03:42.974: INFO: namespace: e2e-tests-projected-hfx6s, resource: packagemanifests, items remaining: 3
Aug  6 15:03:44.102: INFO: namespace: e2e-tests-projected-hfx6s no longer exists
Aug  6 15:03:44.124: INFO: namespace: e2e-tests-projected-hfx6s, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:03:44.146: INFO: namespace e2e-tests-projected-hfx6s deletion completed in 8.495526704s

• [SLOW TEST:20.173 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:03:44.146: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-630f0205-b85b-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 15:03:45.551: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-63128195-b85b-11e9-8d18-525400524259" in namespace "e2e-tests-projected-ssw45" to be "success or failure"
Aug  6 15:03:45.573: INFO: Pod "pod-projected-secrets-63128195-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.107388ms
Aug  6 15:03:47.595: INFO: Pod "pod-projected-secrets-63128195-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04421344s
Aug  6 15:03:49.619: INFO: Pod "pod-projected-secrets-63128195-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067777672s
Aug  6 15:03:51.642: INFO: Pod "pod-projected-secrets-63128195-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090225784s
Aug  6 15:03:53.664: INFO: Pod "pod-projected-secrets-63128195-b85b-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.112671694s
STEP: Saw pod success
Aug  6 15:03:53.664: INFO: Pod "pod-projected-secrets-63128195-b85b-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:03:53.686: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-secrets-63128195-b85b-11e9-8d18-525400524259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug  6 15:03:53.741: INFO: Waiting for pod pod-projected-secrets-63128195-b85b-11e9-8d18-525400524259 to disappear
Aug  6 15:03:53.763: INFO: Pod pod-projected-secrets-63128195-b85b-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:03:53.763: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ssw45" for this suite.
Aug  6 15:03:59.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:04:00.657: INFO: namespace: e2e-tests-projected-ssw45, resource: packagemanifests, items remaining: 3
Aug  6 15:04:01.822: INFO: namespace: e2e-tests-projected-ssw45, resource: bindings, ignored listing per whitelist
Aug  6 15:04:02.278: INFO: namespace: e2e-tests-projected-ssw45 no longer exists
Aug  6 15:04:02.301: INFO: namespace: e2e-tests-projected-ssw45, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:04:02.323: INFO: namespace e2e-tests-projected-ssw45 deletion completed in 8.496333716s

• [SLOW TEST:18.177 seconds]
[sig-storage] Projected secret
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:04:02.323: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-6de3d854-b85b-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 15:04:03.725: INFO: Waiting up to 5m0s for pod "pod-secrets-6de74c7c-b85b-11e9-8d18-525400524259" in namespace "e2e-tests-secrets-z4tpn" to be "success or failure"
Aug  6 15:04:03.748: INFO: Pod "pod-secrets-6de74c7c-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.156862ms
Aug  6 15:04:05.771: INFO: Pod "pod-secrets-6de74c7c-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045586415s
Aug  6 15:04:07.794: INFO: Pod "pod-secrets-6de74c7c-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068542075s
Aug  6 15:04:09.816: INFO: Pod "pod-secrets-6de74c7c-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091302435s
Aug  6 15:04:11.842: INFO: Pod "pod-secrets-6de74c7c-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.11700632s
Aug  6 15:04:13.865: INFO: Pod "pod-secrets-6de74c7c-b85b-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.139789122s
STEP: Saw pod success
Aug  6 15:04:13.865: INFO: Pod "pod-secrets-6de74c7c-b85b-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:04:13.887: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-secrets-6de74c7c-b85b-11e9-8d18-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Aug  6 15:04:13.940: INFO: Waiting for pod pod-secrets-6de74c7c-b85b-11e9-8d18-525400524259 to disappear
Aug  6 15:04:13.962: INFO: Pod pod-secrets-6de74c7c-b85b-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:04:13.962: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-z4tpn" for this suite.
Aug  6 15:04:20.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:04:20.721: INFO: namespace: e2e-tests-secrets-z4tpn, resource: packagemanifests, items remaining: 3
Aug  6 15:04:21.674: INFO: namespace: e2e-tests-secrets-z4tpn, resource: bindings, ignored listing per whitelist
Aug  6 15:04:22.498: INFO: namespace: e2e-tests-secrets-z4tpn no longer exists
Aug  6 15:04:22.521: INFO: namespace: e2e-tests-secrets-z4tpn, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:04:22.543: INFO: namespace e2e-tests-secrets-z4tpn deletion completed in 8.519004796s

• [SLOW TEST:20.219 seconds]
[sig-storage] Secrets
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:04:22.543: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Aug  6 15:04:23.925: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-fwhnq" to be "success or failure"
Aug  6 15:04:23.946: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 21.84992ms
Aug  6 15:04:25.969: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044055673s
Aug  6 15:04:27.991: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066499994s
Aug  6 15:04:30.078: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.15383135s
Aug  6 15:04:32.102: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.177415574s
Aug  6 15:04:34.124: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.199892404s
STEP: Saw pod success
Aug  6 15:04:34.125: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug  6 15:04:34.147: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug  6 15:04:34.200: INFO: Waiting for pod pod-host-path-test to disappear
Aug  6 15:04:34.222: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:04:34.222: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-fwhnq" for this suite.
Aug  6 15:04:40.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:04:41.535: INFO: namespace: e2e-tests-hostpath-fwhnq, resource: packagemanifests, items remaining: 3
Aug  6 15:04:41.578: INFO: namespace: e2e-tests-hostpath-fwhnq, resource: bindings, ignored listing per whitelist
Aug  6 15:04:42.815: INFO: namespace: e2e-tests-hostpath-fwhnq no longer exists
Aug  6 15:04:42.860: INFO: namespace: e2e-tests-hostpath-fwhnq, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:04:42.882: INFO: namespace e2e-tests-hostpath-fwhnq deletion completed in 8.558334799s

• [SLOW TEST:20.339 seconds]
[sig-storage] HostPath
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:04:42.882: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug  6 15:04:54.911: INFO: Successfully updated pod "pod-update-activedeadlineseconds-8611c154-b85b-11e9-8d18-525400524259"
Aug  6 15:04:54.911: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8611c154-b85b-11e9-8d18-525400524259" in namespace "e2e-tests-pods-xzhfs" to be "terminated due to deadline exceeded"
Aug  6 15:04:54.933: INFO: Pod "pod-update-activedeadlineseconds-8611c154-b85b-11e9-8d18-525400524259": Phase="Running", Reason="", readiness=true. Elapsed: 21.948029ms
Aug  6 15:04:56.957: INFO: Pod "pod-update-activedeadlineseconds-8611c154-b85b-11e9-8d18-525400524259": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.045245505s
Aug  6 15:04:56.957: INFO: Pod "pod-update-activedeadlineseconds-8611c154-b85b-11e9-8d18-525400524259" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:04:56.957: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-xzhfs" for this suite.
Aug  6 15:05:03.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:05:03.995: INFO: namespace: e2e-tests-pods-xzhfs, resource: packagemanifests, items remaining: 3
Aug  6 15:05:05.389: INFO: namespace: e2e-tests-pods-xzhfs, resource: bindings, ignored listing per whitelist
Aug  6 15:05:05.496: INFO: namespace: e2e-tests-pods-xzhfs no longer exists
Aug  6 15:05:05.537: INFO: namespace: e2e-tests-pods-xzhfs, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:05:05.560: INFO: namespace e2e-tests-pods-xzhfs deletion completed in 8.539983509s

• [SLOW TEST:22.678 seconds]
[k8s.io] Pods
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:05:05.561: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Aug  6 15:05:17.032: INFO: Pod pod-hostip-9395c6f5-b85b-11e9-8d18-525400524259 has hostIP: 10.0.130.134
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:05:17.032: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-t456w" for this suite.
Aug  6 15:05:39.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:05:41.358: INFO: namespace: e2e-tests-pods-t456w, resource: packagemanifests, items remaining: 3
Aug  6 15:05:41.379: INFO: namespace: e2e-tests-pods-t456w, resource: bindings, ignored listing per whitelist
Aug  6 15:05:41.554: INFO: namespace: e2e-tests-pods-t456w no longer exists
Aug  6 15:05:41.577: INFO: namespace: e2e-tests-pods-t456w, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:05:41.599: INFO: namespace e2e-tests-pods-t456w deletion completed in 24.506310686s

• [SLOW TEST:36.038 seconds]
[k8s.io] Pods
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:05:41.600: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug  6 15:05:42.986: INFO: Waiting up to 5m0s for pod "pod-a9120465-b85b-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-qbkq2" to be "success or failure"
Aug  6 15:05:43.008: INFO: Pod "pod-a9120465-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 21.823509ms
Aug  6 15:05:45.031: INFO: Pod "pod-a9120465-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044762446s
Aug  6 15:05:47.053: INFO: Pod "pod-a9120465-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066993113s
Aug  6 15:05:49.075: INFO: Pod "pod-a9120465-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089518089s
Aug  6 15:05:51.099: INFO: Pod "pod-a9120465-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.11270558s
Aug  6 15:05:53.121: INFO: Pod "pod-a9120465-b85b-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.13552916s
STEP: Saw pod success
Aug  6 15:05:53.121: INFO: Pod "pod-a9120465-b85b-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:05:53.143: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-a9120465-b85b-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 15:05:53.198: INFO: Waiting for pod pod-a9120465-b85b-11e9-8d18-525400524259 to disappear
Aug  6 15:05:53.220: INFO: Pod pod-a9120465-b85b-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:05:53.220: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qbkq2" for this suite.
Aug  6 15:05:59.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:06:00.575: INFO: namespace: e2e-tests-emptydir-qbkq2, resource: bindings, ignored listing per whitelist
Aug  6 15:06:01.164: INFO: namespace: e2e-tests-emptydir-qbkq2, resource: packagemanifests, items remaining: 3
Aug  6 15:06:01.730: INFO: namespace: e2e-tests-emptydir-qbkq2 no longer exists
Aug  6 15:06:01.754: INFO: namespace: e2e-tests-emptydir-qbkq2, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:06:01.776: INFO: namespace e2e-tests-emptydir-qbkq2 deletion completed in 8.495790834s

• [SLOW TEST:20.176 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:06:01.777: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0806 15:06:09.412221    4087 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug  6 15:06:09.412: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:06:09.412: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-rgjd5" for this suite.
Aug  6 15:06:15.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:06:16.958: INFO: namespace: e2e-tests-gc-rgjd5, resource: bindings, ignored listing per whitelist
Aug  6 15:06:17.832: INFO: namespace: e2e-tests-gc-rgjd5, resource: packagemanifests, items remaining: 3
Aug  6 15:06:17.917: INFO: namespace: e2e-tests-gc-rgjd5 no longer exists
Aug  6 15:06:17.942: INFO: namespace: e2e-tests-gc-rgjd5, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:06:17.963: INFO: namespace e2e-tests-gc-rgjd5 deletion completed in 8.50955145s

• [SLOW TEST:16.187 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:06:17.964: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Aug  6 15:06:19.389: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance cluster-info'
Aug  6 15:06:19.609: INFO: stderr: ""
Aug  6 15:06:19.609: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.je-cncf-conform.v3f0.p1.openshiftapps.com:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:06:19.609: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dvz8b" for this suite.
Aug  6 15:06:25.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:06:27.539: INFO: namespace: e2e-tests-kubectl-dvz8b, resource: bindings, ignored listing per whitelist
Aug  6 15:06:27.657: INFO: namespace: e2e-tests-kubectl-dvz8b, resource: packagemanifests, items remaining: 3
Aug  6 15:06:28.136: INFO: namespace: e2e-tests-kubectl-dvz8b no longer exists
Aug  6 15:06:28.159: INFO: namespace: e2e-tests-kubectl-dvz8b, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:06:28.181: INFO: namespace e2e-tests-kubectl-dvz8b deletion completed in 8.510818166s

• [SLOW TEST:10.218 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:06:28.181: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-c4d27ed6-b85b-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 15:06:29.571: INFO: Waiting up to 5m0s for pod "pod-secrets-c4d617ff-b85b-11e9-8d18-525400524259" in namespace "e2e-tests-secrets-mw55n" to be "success or failure"
Aug  6 15:06:29.592: INFO: Pod "pod-secrets-c4d617ff-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 21.686947ms
Aug  6 15:06:31.615: INFO: Pod "pod-secrets-c4d617ff-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044308062s
Aug  6 15:06:33.637: INFO: Pod "pod-secrets-c4d617ff-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066590324s
Aug  6 15:06:35.660: INFO: Pod "pod-secrets-c4d617ff-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.08909026s
Aug  6 15:06:37.682: INFO: Pod "pod-secrets-c4d617ff-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.111056875s
Aug  6 15:06:39.705: INFO: Pod "pod-secrets-c4d617ff-b85b-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.134199772s
STEP: Saw pod success
Aug  6 15:06:39.705: INFO: Pod "pod-secrets-c4d617ff-b85b-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:06:39.727: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-secrets-c4d617ff-b85b-11e9-8d18-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Aug  6 15:06:39.780: INFO: Waiting for pod pod-secrets-c4d617ff-b85b-11e9-8d18-525400524259 to disappear
Aug  6 15:06:39.803: INFO: Pod pod-secrets-c4d617ff-b85b-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:06:39.803: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mw55n" for this suite.
Aug  6 15:06:45.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:06:47.645: INFO: namespace: e2e-tests-secrets-mw55n, resource: bindings, ignored listing per whitelist
Aug  6 15:06:47.869: INFO: namespace: e2e-tests-secrets-mw55n, resource: packagemanifests, items remaining: 3
Aug  6 15:06:48.324: INFO: namespace: e2e-tests-secrets-mw55n no longer exists
Aug  6 15:06:48.347: INFO: namespace: e2e-tests-secrets-mw55n, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:06:48.368: INFO: namespace e2e-tests-secrets-mw55n deletion completed in 8.50388837s

• [SLOW TEST:20.187 seconds]
[sig-storage] Secrets
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:06:48.368: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  6 15:06:49.709: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-ft6tk'
Aug  6 15:06:49.881: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug  6 15:06:49.881: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Aug  6 15:06:49.902: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-ft6tk'
Aug  6 15:06:50.082: INFO: stderr: ""
Aug  6 15:06:50.082: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:06:50.082: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ft6tk" for this suite.
Aug  6 15:07:12.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:07:13.877: INFO: namespace: e2e-tests-kubectl-ft6tk, resource: bindings, ignored listing per whitelist
Aug  6 15:07:14.233: INFO: namespace: e2e-tests-kubectl-ft6tk, resource: packagemanifests, items remaining: 3
Aug  6 15:07:14.619: INFO: namespace: e2e-tests-kubectl-ft6tk no longer exists
Aug  6 15:07:14.643: INFO: namespace: e2e-tests-kubectl-ft6tk, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:07:14.665: INFO: namespace e2e-tests-kubectl-ft6tk deletion completed in 24.540920006s

• [SLOW TEST:26.297 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:07:14.667: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 15:07:16.038: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e08836fc-b85b-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-r7pvh" to be "success or failure"
Aug  6 15:07:16.064: INFO: Pod "downwardapi-volume-e08836fc-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 26.605696ms
Aug  6 15:07:18.087: INFO: Pod "downwardapi-volume-e08836fc-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049303983s
Aug  6 15:07:20.110: INFO: Pod "downwardapi-volume-e08836fc-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072032614s
Aug  6 15:07:22.133: INFO: Pod "downwardapi-volume-e08836fc-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094733443s
Aug  6 15:07:24.155: INFO: Pod "downwardapi-volume-e08836fc-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117662434s
Aug  6 15:07:26.179: INFO: Pod "downwardapi-volume-e08836fc-b85b-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.141249001s
STEP: Saw pod success
Aug  6 15:07:26.179: INFO: Pod "downwardapi-volume-e08836fc-b85b-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:07:26.202: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-e08836fc-b85b-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 15:07:26.257: INFO: Waiting for pod downwardapi-volume-e08836fc-b85b-11e9-8d18-525400524259 to disappear
Aug  6 15:07:26.281: INFO: Pod downwardapi-volume-e08836fc-b85b-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:07:26.281: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-r7pvh" for this suite.
Aug  6 15:07:32.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:07:33.862: INFO: namespace: e2e-tests-downward-api-r7pvh, resource: bindings, ignored listing per whitelist
Aug  6 15:07:34.128: INFO: namespace: e2e-tests-downward-api-r7pvh, resource: packagemanifests, items remaining: 3
Aug  6 15:07:34.837: INFO: namespace: e2e-tests-downward-api-r7pvh no longer exists
Aug  6 15:07:34.861: INFO: namespace: e2e-tests-downward-api-r7pvh, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:07:34.882: INFO: namespace e2e-tests-downward-api-r7pvh deletion completed in 8.538074279s

• [SLOW TEST:20.216 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:07:34.883: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-ec9350b0-b85b-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 15:07:36.267: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec96e180-b85b-11e9-8d18-525400524259" in namespace "e2e-tests-configmap-hcktj" to be "success or failure"
Aug  6 15:07:36.290: INFO: Pod "pod-configmaps-ec96e180-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.497759ms
Aug  6 15:07:38.313: INFO: Pod "pod-configmaps-ec96e180-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046273781s
Aug  6 15:07:40.337: INFO: Pod "pod-configmaps-ec96e180-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069618976s
Aug  6 15:07:42.359: INFO: Pod "pod-configmaps-ec96e180-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091890493s
Aug  6 15:07:44.383: INFO: Pod "pod-configmaps-ec96e180-b85b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115969573s
Aug  6 15:07:46.407: INFO: Pod "pod-configmaps-ec96e180-b85b-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.139423329s
STEP: Saw pod success
Aug  6 15:07:46.407: INFO: Pod "pod-configmaps-ec96e180-b85b-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:07:46.429: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-configmaps-ec96e180-b85b-11e9-8d18-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 15:07:46.483: INFO: Waiting for pod pod-configmaps-ec96e180-b85b-11e9-8d18-525400524259 to disappear
Aug  6 15:07:46.505: INFO: Pod pod-configmaps-ec96e180-b85b-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:07:46.505: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hcktj" for this suite.
Aug  6 15:07:52.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:07:53.302: INFO: namespace: e2e-tests-configmap-hcktj, resource: bindings, ignored listing per whitelist
Aug  6 15:07:53.557: INFO: namespace: e2e-tests-configmap-hcktj, resource: packagemanifests, items remaining: 3
Aug  6 15:07:55.037: INFO: namespace: e2e-tests-configmap-hcktj no longer exists
Aug  6 15:07:55.059: INFO: namespace: e2e-tests-configmap-hcktj, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:07:55.081: INFO: namespace e2e-tests-configmap-hcktj deletion completed in 8.51472081s

• [SLOW TEST:20.198 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:07:55.082: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-8gblc
Aug  6 15:08:06.523: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-8gblc
STEP: checking the pod's current state and verifying that restartCount is present
Aug  6 15:08:06.546: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:12:07.377: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-8gblc" for this suite.
Aug  6 15:12:13.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:12:14.870: INFO: namespace: e2e-tests-container-probe-8gblc, resource: bindings, ignored listing per whitelist
Aug  6 15:12:15.897: INFO: namespace: e2e-tests-container-probe-8gblc, resource: packagemanifests, items remaining: 3
Aug  6 15:12:15.918: INFO: namespace: e2e-tests-container-probe-8gblc no longer exists
Aug  6 15:12:15.941: INFO: namespace: e2e-tests-container-probe-8gblc, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:12:15.963: INFO: namespace e2e-tests-container-probe-8gblc deletion completed in 8.523020243s

• [SLOW TEST:260.881 seconds]
[k8s.io] Probing container
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:12:15.964: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Aug  6 15:12:17.323: INFO: PodSpec: initContainers in spec.initContainers
Aug  6 15:13:13.263: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9421782e-b85c-11e9-8d18-525400524259", GenerateName:"", Namespace:"e2e-tests-init-container-8j8ns", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-8j8ns/pods/pod-init-9421782e-b85c-11e9-8d18-525400524259", UID:"86af9c7e-b85c-11e9-9dff-0e6de702691c", ResourceVersion:"90734", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63700701114, loc:(*time.Location)(0x81cdd60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"323866194"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.46\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xfxpm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0012c47c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xfxpm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0022a7220), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xfxpm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0022a7270), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xfxpm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0022a71d0), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002490768), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-130-134.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002373a40), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-9rwb5"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002490820)}, v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002490840)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002490848), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00249084c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701114, loc:(*time.Location)(0x81cdd60)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701114, loc:(*time.Location)(0x81cdd60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701114, loc:(*time.Location)(0x81cdd60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701114, loc:(*time.Location)(0x81cdd60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.130.134", PodIP:"10.131.0.46", StartTime:(*v1.Time)(0xc001b72020), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00198c770)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00198c7e0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"cri-o://56505086c1b01813cfd0bf08d5eb44af5c0b856de47f689d642f414c221ef425"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b720c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b72080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Burstable"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:13:13.264: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-8j8ns" for this suite.
Aug  6 15:13:35.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:13:36.550: INFO: namespace: e2e-tests-init-container-8j8ns, resource: bindings, ignored listing per whitelist
Aug  6 15:13:36.602: INFO: namespace: e2e-tests-init-container-8j8ns, resource: packagemanifests, items remaining: 3
Aug  6 15:13:37.842: INFO: namespace: e2e-tests-init-container-8j8ns no longer exists
Aug  6 15:13:37.884: INFO: namespace: e2e-tests-init-container-8j8ns, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:13:37.906: INFO: namespace e2e-tests-init-container-8j8ns deletion completed in 24.580482471s

• [SLOW TEST:81.942 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:13:37.907: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Aug  6 15:13:39.253: INFO: Asynchronously running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:13:39.460: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vpk47" for this suite.
Aug  6 15:13:45.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:13:46.651: INFO: namespace: e2e-tests-kubectl-vpk47, resource: packagemanifests, items remaining: 3
Aug  6 15:13:47.197: INFO: namespace: e2e-tests-kubectl-vpk47, resource: bindings, ignored listing per whitelist
Aug  6 15:13:47.980: INFO: namespace: e2e-tests-kubectl-vpk47 no longer exists
Aug  6 15:13:48.022: INFO: namespace: e2e-tests-kubectl-vpk47, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:13:48.044: INFO: namespace e2e-tests-kubectl-vpk47 deletion completed in 8.521897154s

• [SLOW TEST:10.137 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:13:48.044: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-ncp8f
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-ncp8f to expose endpoints map[]
Aug  6 15:13:49.487: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-ncp8f exposes endpoints map[] (31.003409ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-ncp8f
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-ncp8f to expose endpoints map[pod1:[100]]
Aug  6 15:13:53.758: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.22354285s elapsed, will retry)
Aug  6 15:13:57.934: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-ncp8f exposes endpoints map[pod1:[100]] (8.400393636s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-ncp8f
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-ncp8f to expose endpoints map[pod1:[100] pod2:[101]]
Aug  6 15:14:02.289: INFO: Unexpected endpoints: found map[bda12c3e-b85c-11e9-9dff-0e6de702691c:[100]], expected map[pod1:[100] pod2:[101]] (4.32673049s elapsed, will retry)
Aug  6 15:14:07.620: INFO: Unexpected endpoints: found map[bda12c3e-b85c-11e9-9dff-0e6de702691c:[100]], expected map[pod1:[100] pod2:[101]] (9.658514363s elapsed, will retry)
Aug  6 15:14:08.687: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-ncp8f exposes endpoints map[pod1:[100] pod2:[101]] (10.724995551s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-ncp8f
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-ncp8f to expose endpoints map[pod2:[101]]
Aug  6 15:14:08.766: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-ncp8f exposes endpoints map[pod2:[101]] (50.111319ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-ncp8f
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-ncp8f to expose endpoints map[]
Aug  6 15:14:08.824: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-ncp8f exposes endpoints map[] (21.699263ms elapsed)
[AfterEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:14:08.860: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-ncp8f" for this suite.
Aug  6 15:14:31.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:14:32.462: INFO: namespace: e2e-tests-services-ncp8f, resource: packagemanifests, items remaining: 3
Aug  6 15:14:32.964: INFO: namespace: e2e-tests-services-ncp8f, resource: bindings, ignored listing per whitelist
Aug  6 15:14:33.437: INFO: namespace: e2e-tests-services-ncp8f no longer exists
Aug  6 15:14:33.518: INFO: namespace: e2e-tests-services-ncp8f, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:14:33.539: INFO: namespace e2e-tests-services-ncp8f deletion completed in 24.595553492s
[AfterEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:45.495 seconds]
[sig-network] Services
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:14:33.540: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-jsj9v
Aug  6 15:14:45.041: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-jsj9v
STEP: checking the pod's current state and verifying that restartCount is present
Aug  6 15:14:45.063: INFO: Initial restart count of pod liveness-http is 0
Aug  6 15:15:03.288: INFO: Restart count of pod e2e-tests-container-probe-jsj9v/liveness-http is now 1 (18.225328113s elapsed)
Aug  6 15:15:21.492: INFO: Restart count of pod e2e-tests-container-probe-jsj9v/liveness-http is now 2 (36.428749472s elapsed)
Aug  6 15:15:41.748: INFO: Restart count of pod e2e-tests-container-probe-jsj9v/liveness-http is now 3 (56.685289427s elapsed)
Aug  6 15:16:01.978: INFO: Restart count of pod e2e-tests-container-probe-jsj9v/liveness-http is now 4 (1m16.915072885s elapsed)
Aug  6 15:17:16.817: INFO: Restart count of pod e2e-tests-container-probe-jsj9v/liveness-http is now 5 (2m31.753757809s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:17:16.847: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-jsj9v" for this suite.
Aug  6 15:17:23.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:17:23.732: INFO: namespace: e2e-tests-container-probe-jsj9v, resource: packagemanifests, items remaining: 3
Aug  6 15:17:24.995: INFO: namespace: e2e-tests-container-probe-jsj9v, resource: bindings, ignored listing per whitelist
Aug  6 15:17:25.453: INFO: namespace: e2e-tests-container-probe-jsj9v no longer exists
Aug  6 15:17:25.495: INFO: namespace: e2e-tests-container-probe-jsj9v, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:17:25.518: INFO: namespace e2e-tests-container-probe-jsj9v deletion completed in 8.551417023s

• [SLOW TEST:171.978 seconds]
[k8s.io] Probing container
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:17:25.519: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 15:17:26.876: INFO: Creating deployment "test-recreate-deployment"
Aug  6 15:17:26.900: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug  6 15:17:26.947: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug  6 15:17:26.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-66c9894dbf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 15:17:28.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-66c9894dbf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 15:17:30.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-66c9894dbf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 15:17:32.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-66c9894dbf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 15:17:34.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700701424, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-66c9894dbf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 15:17:36.995: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug  6 15:17:37.039: INFO: Updating deployment test-recreate-deployment
Aug  6 15:17:37.039: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug  6 15:17:37.118: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-h8kr4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h8kr4/deployments/test-recreate-deployment,UID:3f30e892-b85d-11e9-9dff-0e6de702691c,ResourceVersion:93142,Generation:2,CreationTimestamp:2019-08-06 15:17:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-08-06 15:17:14 +0000 UTC 2019-08-06 15:17:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-06 15:17:14 +0000 UTC 2019-08-06 15:17:04 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-887bbcdbd" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Aug  6 15:17:37.140: INFO: New ReplicaSet "test-recreate-deployment-887bbcdbd" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-887bbcdbd,GenerateName:,Namespace:e2e-tests-deployment-h8kr4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h8kr4/replicasets/test-recreate-deployment-887bbcdbd,UID:4540bc46-b85d-11e9-9dff-0e6de702691c,ResourceVersion:93141,Generation:1,CreationTimestamp:2019-08-06 15:17:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 887bbcdbd,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 3f30e892-b85d-11e9-9dff-0e6de702691c 0xc000df3a50 0xc000df3a51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 887bbcdbd,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 887bbcdbd,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  6 15:17:37.140: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug  6 15:17:37.140: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-66c9894dbf,GenerateName:,Namespace:e2e-tests-deployment-h8kr4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h8kr4/replicasets/test-recreate-deployment-66c9894dbf,UID:3f319c08-b85d-11e9-9dff-0e6de702691c,ResourceVersion:93131,Generation:2,CreationTimestamp:2019-08-06 15:17:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 66c9894dbf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 3f30e892-b85d-11e9-9dff-0e6de702691c 0xc000df3987 0xc000df3988}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 66c9894dbf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 66c9894dbf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  6 15:17:37.163: INFO: Pod "test-recreate-deployment-887bbcdbd-sm29b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-887bbcdbd-sm29b,GenerateName:test-recreate-deployment-887bbcdbd-,Namespace:e2e-tests-deployment-h8kr4,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8kr4/pods/test-recreate-deployment-887bbcdbd-sm29b,UID:4541ed3b-b85d-11e9-9dff-0e6de702691c,ResourceVersion:93143,Generation:0,CreationTimestamp:2019-08-06 15:17:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 887bbcdbd,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-887bbcdbd 4540bc46-b85d-11e9-9dff-0e6de702691c 0xc001636f70 0xc001636f71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vpbg5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vpbg5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vpbg5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-22vd5}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001637100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001637120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:17:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:17:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:17:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:17:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:,StartTime:2019-08-06 15:17:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:17:37.163: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-h8kr4" for this suite.
Aug  6 15:17:43.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:17:45.319: INFO: namespace: e2e-tests-deployment-h8kr4, resource: packagemanifests, items remaining: 3
Aug  6 15:17:45.451: INFO: namespace: e2e-tests-deployment-h8kr4, resource: bindings, ignored listing per whitelist
Aug  6 15:17:45.711: INFO: namespace: e2e-tests-deployment-h8kr4 no longer exists
Aug  6 15:17:45.753: INFO: namespace: e2e-tests-deployment-h8kr4, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:17:45.775: INFO: namespace e2e-tests-deployment-h8kr4 deletion completed in 8.527725597s

• [SLOW TEST:20.256 seconds]
[sig-apps] Deployment
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:17:45.775: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Aug  6 15:17:47.123: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-g6f25'
Aug  6 15:17:49.444: INFO: stderr: ""
Aug  6 15:17:49.444: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Aug  6 15:17:50.467: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:17:50.467: INFO: Found 0 / 1
Aug  6 15:17:51.467: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:17:51.467: INFO: Found 0 / 1
Aug  6 15:17:52.468: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:17:52.468: INFO: Found 0 / 1
Aug  6 15:17:53.467: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:17:53.467: INFO: Found 0 / 1
Aug  6 15:17:54.467: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:17:54.467: INFO: Found 0 / 1
Aug  6 15:17:55.466: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:17:55.466: INFO: Found 0 / 1
Aug  6 15:17:56.467: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:17:56.467: INFO: Found 0 / 1
Aug  6 15:17:57.467: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:17:57.467: INFO: Found 0 / 1
Aug  6 15:17:58.466: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:17:58.467: INFO: Found 1 / 1
Aug  6 15:17:58.467: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug  6 15:17:58.489: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:17:58.489: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Aug  6 15:17:58.489: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance logs redis-master-gbxx8 redis-master --namespace=e2e-tests-kubectl-g6f25'
Aug  6 15:17:58.690: INFO: stderr: ""
Aug  6 15:17:58.690: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Aug 15:17:34.326 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Aug 15:17:34.326 # Server started, Redis version 3.2.12\n1:M 06 Aug 15:17:34.326 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Aug 15:17:34.326 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Aug  6 15:17:58.690: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance log redis-master-gbxx8 redis-master --namespace=e2e-tests-kubectl-g6f25 --tail=1'
Aug  6 15:17:58.871: INFO: stderr: ""
Aug  6 15:17:58.871: INFO: stdout: "1:M 06 Aug 15:17:34.326 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Aug  6 15:17:58.871: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance log redis-master-gbxx8 redis-master --namespace=e2e-tests-kubectl-g6f25 --limit-bytes=1'
Aug  6 15:17:59.053: INFO: stderr: ""
Aug  6 15:17:59.053: INFO: stdout: " "
STEP: exposing timestamps
Aug  6 15:17:59.053: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance log redis-master-gbxx8 redis-master --namespace=e2e-tests-kubectl-g6f25 --tail=1 --timestamps'
Aug  6 15:17:59.227: INFO: stderr: ""
Aug  6 15:17:59.227: INFO: stdout: "2019-08-06T15:17:34.326201599Z 1:M 06 Aug 15:17:34.326 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Aug  6 15:18:01.727: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance log redis-master-gbxx8 redis-master --namespace=e2e-tests-kubectl-g6f25 --since=1s'
Aug  6 15:18:01.905: INFO: stderr: ""
Aug  6 15:18:01.905: INFO: stdout: ""
Aug  6 15:18:01.905: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance log redis-master-gbxx8 redis-master --namespace=e2e-tests-kubectl-g6f25 --since=24h'
Aug  6 15:18:02.086: INFO: stderr: ""
Aug  6 15:18:02.086: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Aug 15:17:34.326 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Aug 15:17:34.326 # Server started, Redis version 3.2.12\n1:M 06 Aug 15:17:34.326 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Aug 15:17:34.326 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Aug  6 15:18:02.086: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-g6f25'
Aug  6 15:18:02.258: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  6 15:18:02.258: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Aug  6 15:18:02.258: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-g6f25'
Aug  6 15:18:02.438: INFO: stderr: "No resources found.\n"
Aug  6 15:18:02.438: INFO: stdout: ""
Aug  6 15:18:02.439: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -l name=nginx --namespace=e2e-tests-kubectl-g6f25 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug  6 15:18:02.590: INFO: stderr: ""
Aug  6 15:18:02.590: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:18:02.590: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-g6f25" for this suite.
Aug  6 15:18:24.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:18:25.923: INFO: namespace: e2e-tests-kubectl-g6f25, resource: bindings, ignored listing per whitelist
Aug  6 15:18:27.025: INFO: namespace: e2e-tests-kubectl-g6f25, resource: packagemanifests, items remaining: 3
Aug  6 15:18:27.133: INFO: namespace: e2e-tests-kubectl-g6f25 no longer exists
Aug  6 15:18:27.176: INFO: namespace: e2e-tests-kubectl-g6f25, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:18:27.198: INFO: namespace e2e-tests-kubectl-g6f25 deletion completed in 24.546678141s

• [SLOW TEST:41.423 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:18:27.199: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-71660ffc-b85d-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 15:18:28.705: INFO: Waiting up to 5m0s for pod "pod-secrets-7178c6b2-b85d-11e9-8d18-525400524259" in namespace "e2e-tests-secrets-ptz72" to be "success or failure"
Aug  6 15:18:28.728: INFO: Pod "pod-secrets-7178c6b2-b85d-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.288551ms
Aug  6 15:18:30.750: INFO: Pod "pod-secrets-7178c6b2-b85d-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044678335s
Aug  6 15:18:32.773: INFO: Pod "pod-secrets-7178c6b2-b85d-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067825287s
Aug  6 15:18:34.798: INFO: Pod "pod-secrets-7178c6b2-b85d-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092658747s
Aug  6 15:18:36.822: INFO: Pod "pod-secrets-7178c6b2-b85d-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116404037s
Aug  6 15:18:38.845: INFO: Pod "pod-secrets-7178c6b2-b85d-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.139788152s
STEP: Saw pod success
Aug  6 15:18:38.845: INFO: Pod "pod-secrets-7178c6b2-b85d-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:18:38.867: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-secrets-7178c6b2-b85d-11e9-8d18-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Aug  6 15:18:38.923: INFO: Waiting for pod pod-secrets-7178c6b2-b85d-11e9-8d18-525400524259 to disappear
Aug  6 15:18:38.944: INFO: Pod pod-secrets-7178c6b2-b85d-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:18:38.944: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-ptz72" for this suite.
Aug  6 15:18:45.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:18:45.825: INFO: namespace: e2e-tests-secrets-ptz72, resource: bindings, ignored listing per whitelist
Aug  6 15:18:45.922: INFO: namespace: e2e-tests-secrets-ptz72, resource: packagemanifests, items remaining: 3
Aug  6 15:18:47.460: INFO: namespace: e2e-tests-secrets-ptz72 no longer exists
Aug  6 15:18:47.483: INFO: namespace: e2e-tests-secrets-ptz72, total namespaces: 51, active: 51, terminating: 0
Aug  6 15:18:47.505: INFO: namespace e2e-tests-secrets-ptz72 deletion completed in 8.498168481s
STEP: Destroying namespace "e2e-tests-secret-namespace-zvmkp" for this suite.
Aug  6 15:18:53.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:18:54.961: INFO: namespace: e2e-tests-secret-namespace-zvmkp, resource: packagemanifests, items remaining: 3
Aug  6 15:18:55.775: INFO: namespace: e2e-tests-secret-namespace-zvmkp, resource: bindings, ignored listing per whitelist
Aug  6 15:18:55.968: INFO: namespace: e2e-tests-secret-namespace-zvmkp no longer exists
Aug  6 15:18:55.992: INFO: namespace: e2e-tests-secret-namespace-zvmkp, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:18:56.014: INFO: namespace e2e-tests-secret-namespace-zvmkp deletion completed in 8.50923389s

• [SLOW TEST:28.815 seconds]
[sig-storage] Secrets
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:18:56.014: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0806 15:19:07.770408    4087 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug  6 15:19:07.770: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:19:07.770: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-d46fp" for this suite.
Aug  6 15:19:13.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:19:14.849: INFO: namespace: e2e-tests-gc-d46fp, resource: bindings, ignored listing per whitelist
Aug  6 15:19:16.113: INFO: namespace: e2e-tests-gc-d46fp, resource: packagemanifests, items remaining: 3
Aug  6 15:19:16.264: INFO: namespace: e2e-tests-gc-d46fp no longer exists
Aug  6 15:19:16.286: INFO: namespace: e2e-tests-gc-d46fp, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:19:16.308: INFO: namespace e2e-tests-gc-d46fp deletion completed in 8.495614807s

• [SLOW TEST:20.294 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:19:16.309: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-slz6p
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug  6 15:19:17.632: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Aug  6 15:19:54.240: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.57:8080/dial?request=hostName&protocol=http&host=10.130.2.15&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-slz6p PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:19:54.240: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:19:54.480: INFO: Waiting for endpoints: map[]
Aug  6 15:19:54.502: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.57:8080/dial?request=hostName&protocol=http&host=10.131.0.56&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-slz6p PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:19:54.502: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:19:54.721: INFO: Waiting for endpoints: map[]
Aug  6 15:19:54.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.57:8080/dial?request=hostName&protocol=http&host=10.129.2.38&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-slz6p PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:19:54.743: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:19:54.947: INFO: Waiting for endpoints: map[]
Aug  6 15:19:54.969: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.57:8080/dial?request=hostName&protocol=http&host=10.128.2.18&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-slz6p PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:19:54.969: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:19:55.164: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:19:55.164: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-slz6p" for this suite.
Aug  6 15:20:17.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:20:18.266: INFO: namespace: e2e-tests-pod-network-test-slz6p, resource: bindings, ignored listing per whitelist
Aug  6 15:20:18.489: INFO: namespace: e2e-tests-pod-network-test-slz6p, resource: packagemanifests, items remaining: 3
Aug  6 15:20:19.684: INFO: namespace: e2e-tests-pod-network-test-slz6p no longer exists
Aug  6 15:20:19.707: INFO: namespace: e2e-tests-pod-network-test-slz6p, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:20:19.733: INFO: namespace e2e-tests-pod-network-test-slz6p deletion completed in 24.506949733s

• [SLOW TEST:63.425 seconds]
[sig-network] Networking
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:20:19.734: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Aug  6 15:20:21.074: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance api-versions'
Aug  6 15:20:21.257: INFO: stderr: ""
Aug  6 15:20:21.257: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nhealthchecking.openshift.io/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nlogging.openshift.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.openshift.io/v1\nnetworking.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:20:21.258: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wfgmm" for this suite.
Aug  6 15:20:27.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:20:28.726: INFO: namespace: e2e-tests-kubectl-wfgmm, resource: packagemanifests, items remaining: 3
Aug  6 15:20:29.794: INFO: namespace: e2e-tests-kubectl-wfgmm, resource: bindings, ignored listing per whitelist
Aug  6 15:20:29.861: INFO: namespace: e2e-tests-kubectl-wfgmm no longer exists
Aug  6 15:20:29.884: INFO: namespace: e2e-tests-kubectl-wfgmm, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:20:29.907: INFO: namespace e2e-tests-kubectl-wfgmm deletion completed in 8.607465843s

• [SLOW TEST:10.174 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:20:29.908: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-6w88m
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug  6 15:20:31.265: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Aug  6 15:21:07.955: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.131.0.58 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-6w88m PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:21:07.955: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:21:09.222: INFO: Found all expected endpoints: [netserver-0]
Aug  6 15:21:09.244: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.130.2.16 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-6w88m PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:21:09.244: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:21:10.449: INFO: Found all expected endpoints: [netserver-1]
Aug  6 15:21:10.472: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.129.2.39 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-6w88m PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:21:10.472: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:21:11.671: INFO: Found all expected endpoints: [netserver-2]
Aug  6 15:21:11.736: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.128.2.19 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-6w88m PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:21:11.736: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:21:12.963: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:21:12.963: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-6w88m" for this suite.
Aug  6 15:21:25.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:21:26.242: INFO: namespace: e2e-tests-pod-network-test-6w88m, resource: packagemanifests, items remaining: 3
Aug  6 15:21:27.450: INFO: namespace: e2e-tests-pod-network-test-6w88m, resource: bindings, ignored listing per whitelist
Aug  6 15:21:27.516: INFO: namespace: e2e-tests-pod-network-test-6w88m no longer exists
Aug  6 15:21:27.558: INFO: namespace: e2e-tests-pod-network-test-6w88m, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:21:27.579: INFO: namespace e2e-tests-pod-network-test-6w88m deletion completed in 14.534050011s

• [SLOW TEST:57.671 seconds]
[sig-network] Networking
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:21:27.580: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-5zjbs
I0806 15:21:28.971312    4087 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-5zjbs, replica count: 1
I0806 15:21:30.022068    4087 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 15:21:31.022318    4087 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 15:21:32.022571    4087 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 15:21:33.022865    4087 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 15:21:34.023183    4087 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 15:21:35.023553    4087 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 15:21:36.023792    4087 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 15:21:37.024259    4087 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 15:21:38.024527    4087 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug  6 15:21:38.155: INFO: Created: latency-svc-6psqz
Aug  6 15:21:38.161: INFO: Got endpoints: latency-svc-6psqz [36.389892ms]
Aug  6 15:21:38.194: INFO: Created: latency-svc-nthgk
Aug  6 15:21:38.214: INFO: Got endpoints: latency-svc-nthgk [52.990959ms]
Aug  6 15:21:38.214: INFO: Created: latency-svc-8s2n2
Aug  6 15:21:38.214: INFO: Got endpoints: latency-svc-8s2n2 [52.859186ms]
Aug  6 15:21:38.214: INFO: Created: latency-svc-pctgt
Aug  6 15:21:38.222: INFO: Got endpoints: latency-svc-pctgt [61.240923ms]
Aug  6 15:21:38.224: INFO: Created: latency-svc-vdmjl
Aug  6 15:21:38.233: INFO: Got endpoints: latency-svc-vdmjl [71.828225ms]
Aug  6 15:21:38.243: INFO: Created: latency-svc-g57vr
Aug  6 15:21:38.247: INFO: Got endpoints: latency-svc-g57vr [85.811255ms]
Aug  6 15:21:38.255: INFO: Created: latency-svc-z52f5
Aug  6 15:21:38.260: INFO: Got endpoints: latency-svc-z52f5 [99.39512ms]
Aug  6 15:21:38.262: INFO: Created: latency-svc-c7rg9
Aug  6 15:21:38.275: INFO: Created: latency-svc-ppnwv
Aug  6 15:21:38.275: INFO: Got endpoints: latency-svc-c7rg9 [113.934267ms]
Aug  6 15:21:38.281: INFO: Got endpoints: latency-svc-ppnwv [119.443932ms]
Aug  6 15:21:38.287: INFO: Created: latency-svc-zj6xt
Aug  6 15:21:38.304: INFO: Got endpoints: latency-svc-zj6xt [143.382292ms]
Aug  6 15:21:38.307: INFO: Created: latency-svc-g2lsq
Aug  6 15:21:38.316: INFO: Got endpoints: latency-svc-g2lsq [155.155165ms]
Aug  6 15:21:38.327: INFO: Created: latency-svc-lbqnx
Aug  6 15:21:38.374: INFO: Created: latency-svc-twsgj
Aug  6 15:21:38.374: INFO: Got endpoints: latency-svc-lbqnx [213.201945ms]
Aug  6 15:21:38.374: INFO: Created: latency-svc-5g668
Aug  6 15:21:38.374: INFO: Got endpoints: latency-svc-5g668 [213.269425ms]
Aug  6 15:21:38.380: INFO: Got endpoints: latency-svc-twsgj [219.107923ms]
Aug  6 15:21:38.392: INFO: Created: latency-svc-c8qz4
Aug  6 15:21:38.402: INFO: Got endpoints: latency-svc-c8qz4 [241.149637ms]
Aug  6 15:21:38.412: INFO: Created: latency-svc-v97p2
Aug  6 15:21:38.417: INFO: Got endpoints: latency-svc-v97p2 [256.290028ms]
Aug  6 15:21:38.420: INFO: Created: latency-svc-ckzgf
Aug  6 15:21:38.425: INFO: Got endpoints: latency-svc-ckzgf [211.44872ms]
Aug  6 15:21:38.436: INFO: Created: latency-svc-95vgc
Aug  6 15:21:38.448: INFO: Got endpoints: latency-svc-95vgc [234.515555ms]
Aug  6 15:21:38.452: INFO: Created: latency-svc-gg2pt
Aug  6 15:21:38.461: INFO: Got endpoints: latency-svc-gg2pt [238.573858ms]
Aug  6 15:21:38.472: INFO: Created: latency-svc-l664q
Aug  6 15:21:38.476: INFO: Got endpoints: latency-svc-l664q [243.093404ms]
Aug  6 15:21:38.478: INFO: Created: latency-svc-r2nkq
Aug  6 15:21:38.486: INFO: Got endpoints: latency-svc-r2nkq [238.84317ms]
Aug  6 15:21:38.492: INFO: Created: latency-svc-88xcx
Aug  6 15:21:38.506: INFO: Created: latency-svc-nk2qt
Aug  6 15:21:38.516: INFO: Got endpoints: latency-svc-88xcx [255.976166ms]
Aug  6 15:21:38.520: INFO: Got endpoints: latency-svc-nk2qt [245.270144ms]
Aug  6 15:21:38.537: INFO: Created: latency-svc-jd4kx
Aug  6 15:21:38.545: INFO: Got endpoints: latency-svc-jd4kx [264.282302ms]
Aug  6 15:21:38.554: INFO: Created: latency-svc-76srm
Aug  6 15:21:38.560: INFO: Got endpoints: latency-svc-76srm [256.027238ms]
Aug  6 15:21:38.563: INFO: Created: latency-svc-pgb59
Aug  6 15:21:38.582: INFO: Got endpoints: latency-svc-pgb59 [265.957092ms]
Aug  6 15:21:38.622: INFO: Created: latency-svc-lm5ht
Aug  6 15:21:38.622: INFO: Got endpoints: latency-svc-lm5ht [247.322158ms]
Aug  6 15:21:38.622: INFO: Created: latency-svc-pr6hj
Aug  6 15:21:38.622: INFO: Created: latency-svc-jwsth
Aug  6 15:21:38.625: INFO: Got endpoints: latency-svc-pr6hj [250.486756ms]
Aug  6 15:21:38.638: INFO: Got endpoints: latency-svc-jwsth [257.302946ms]
Aug  6 15:21:38.641: INFO: Created: latency-svc-c2dvm
Aug  6 15:21:38.646: INFO: Got endpoints: latency-svc-c2dvm [243.593035ms]
Aug  6 15:21:38.657: INFO: Created: latency-svc-d7d9k
Aug  6 15:21:38.663: INFO: Got endpoints: latency-svc-d7d9k [245.065809ms]
Aug  6 15:21:38.666: INFO: Created: latency-svc-bqjqg
Aug  6 15:21:38.674: INFO: Got endpoints: latency-svc-bqjqg [248.241952ms]
Aug  6 15:21:38.680: INFO: Created: latency-svc-qdrpf
Aug  6 15:21:38.688: INFO: Got endpoints: latency-svc-qdrpf [239.844508ms]
Aug  6 15:21:38.693: INFO: Created: latency-svc-6m97m
Aug  6 15:21:38.701: INFO: Got endpoints: latency-svc-6m97m [240.588006ms]
Aug  6 15:21:38.705: INFO: Created: latency-svc-fkqtk
Aug  6 15:21:38.728: INFO: Got endpoints: latency-svc-fkqtk [252.304838ms]
Aug  6 15:21:38.737: INFO: Created: latency-svc-lmvzx
Aug  6 15:21:38.753: INFO: Got endpoints: latency-svc-lmvzx [266.769357ms]
Aug  6 15:21:38.765: INFO: Created: latency-svc-pzg8h
Aug  6 15:21:38.771: INFO: Got endpoints: latency-svc-pzg8h [254.370447ms]
Aug  6 15:21:38.780: INFO: Created: latency-svc-rw2r2
Aug  6 15:21:38.791: INFO: Created: latency-svc-bjxt4
Aug  6 15:21:38.793: INFO: Got endpoints: latency-svc-rw2r2 [272.514414ms]
Aug  6 15:21:38.800: INFO: Got endpoints: latency-svc-bjxt4 [254.810785ms]
Aug  6 15:21:38.813: INFO: Created: latency-svc-tnmdc
Aug  6 15:21:38.820: INFO: Got endpoints: latency-svc-tnmdc [259.055922ms]
Aug  6 15:21:38.820: INFO: Created: latency-svc-8nnjw
Aug  6 15:21:38.835: INFO: Got endpoints: latency-svc-8nnjw [252.904319ms]
Aug  6 15:21:38.835: INFO: Created: latency-svc-9tw2g
Aug  6 15:21:38.840: INFO: Got endpoints: latency-svc-9tw2g [218.12961ms]
Aug  6 15:21:38.848: INFO: Created: latency-svc-zbln5
Aug  6 15:21:38.854: INFO: Created: latency-svc-2c6fz
Aug  6 15:21:38.857: INFO: Got endpoints: latency-svc-zbln5 [231.703354ms]
Aug  6 15:21:38.867: INFO: Created: latency-svc-fxj69
Aug  6 15:21:38.867: INFO: Got endpoints: latency-svc-2c6fz [229.599482ms]
Aug  6 15:21:38.873: INFO: Got endpoints: latency-svc-fxj69 [226.936468ms]
Aug  6 15:21:38.876: INFO: Created: latency-svc-n6vkv
Aug  6 15:21:38.884: INFO: Got endpoints: latency-svc-n6vkv [221.349423ms]
Aug  6 15:21:38.892: INFO: Created: latency-svc-rz8k4
Aug  6 15:21:38.910: INFO: Got endpoints: latency-svc-rz8k4 [235.886196ms]
Aug  6 15:21:38.911: INFO: Created: latency-svc-6snqj
Aug  6 15:21:38.919: INFO: Created: latency-svc-dj6k8
Aug  6 15:21:38.922: INFO: Got endpoints: latency-svc-6snqj [233.999179ms]
Aug  6 15:21:38.923: INFO: Got endpoints: latency-svc-dj6k8 [222.00773ms]
Aug  6 15:21:38.930: INFO: Created: latency-svc-6mgs8
Aug  6 15:21:38.938: INFO: Created: latency-svc-rr5dx
Aug  6 15:21:38.942: INFO: Got endpoints: latency-svc-rr5dx [189.027229ms]
Aug  6 15:21:38.942: INFO: Got endpoints: latency-svc-6mgs8 [213.228784ms]
Aug  6 15:21:38.954: INFO: Created: latency-svc-npklg
Aug  6 15:21:38.963: INFO: Got endpoints: latency-svc-npklg [191.937766ms]
Aug  6 15:21:38.964: INFO: Created: latency-svc-h86qd
Aug  6 15:21:38.985: INFO: Got endpoints: latency-svc-h86qd [191.593956ms]
Aug  6 15:21:38.987: INFO: Created: latency-svc-w5fjt
Aug  6 15:21:38.993: INFO: Created: latency-svc-p62j8
Aug  6 15:21:38.993: INFO: Got endpoints: latency-svc-w5fjt [193.538536ms]
Aug  6 15:21:39.001: INFO: Got endpoints: latency-svc-p62j8 [181.467798ms]
Aug  6 15:21:39.004: INFO: Created: latency-svc-zpjjp
Aug  6 15:21:39.013: INFO: Created: latency-svc-jllrr
Aug  6 15:21:39.013: INFO: Got endpoints: latency-svc-zpjjp [178.142256ms]
Aug  6 15:21:39.025: INFO: Created: latency-svc-ndm9j
Aug  6 15:21:39.027: INFO: Got endpoints: latency-svc-jllrr [187.130895ms]
Aug  6 15:21:39.032: INFO: Got endpoints: latency-svc-ndm9j [175.386593ms]
Aug  6 15:21:39.034: INFO: Created: latency-svc-qjsrs
Aug  6 15:21:39.051: INFO: Got endpoints: latency-svc-qjsrs [183.732656ms]
Aug  6 15:21:39.054: INFO: Created: latency-svc-dxfrc
Aug  6 15:21:39.060: INFO: Created: latency-svc-qhjvv
Aug  6 15:21:39.062: INFO: Got endpoints: latency-svc-dxfrc [188.68503ms]
Aug  6 15:21:39.066: INFO: Got endpoints: latency-svc-qhjvv [182.361688ms]
Aug  6 15:21:39.072: INFO: Created: latency-svc-g49ll
Aug  6 15:21:39.082: INFO: Got endpoints: latency-svc-g49ll [172.646011ms]
Aug  6 15:21:39.089: INFO: Created: latency-svc-q46bc
Aug  6 15:21:39.107: INFO: Created: latency-svc-szp4z
Aug  6 15:21:39.107: INFO: Got endpoints: latency-svc-q46bc [184.407295ms]
Aug  6 15:21:39.114: INFO: Got endpoints: latency-svc-szp4z [52.457939ms]
Aug  6 15:21:39.117: INFO: Created: latency-svc-c66d6
Aug  6 15:21:39.124: INFO: Got endpoints: latency-svc-c66d6 [200.82924ms]
Aug  6 15:21:39.128: INFO: Created: latency-svc-ll7vc
Aug  6 15:21:39.134: INFO: Got endpoints: latency-svc-ll7vc [192.127992ms]
Aug  6 15:21:39.136: INFO: Created: latency-svc-77v8j
Aug  6 15:21:39.160: INFO: Got endpoints: latency-svc-77v8j [218.534168ms]
Aug  6 15:21:39.161: INFO: Created: latency-svc-lmzfs
Aug  6 15:21:39.172: INFO: Got endpoints: latency-svc-lmzfs [208.658071ms]
Aug  6 15:21:39.174: INFO: Created: latency-svc-ff9w9
Aug  6 15:21:39.181: INFO: Got endpoints: latency-svc-ff9w9 [196.593503ms]
Aug  6 15:21:39.188: INFO: Created: latency-svc-7ckx5
Aug  6 15:21:39.195: INFO: Got endpoints: latency-svc-7ckx5 [201.71307ms]
Aug  6 15:21:39.198: INFO: Created: latency-svc-lwm66
Aug  6 15:21:39.208: INFO: Created: latency-svc-dq4mb
Aug  6 15:21:39.209: INFO: Got endpoints: latency-svc-lwm66 [207.875063ms]
Aug  6 15:21:39.212: INFO: Got endpoints: latency-svc-dq4mb [198.367ms]
Aug  6 15:21:39.219: INFO: Created: latency-svc-kgbfm
Aug  6 15:21:39.226: INFO: Got endpoints: latency-svc-kgbfm [199.339326ms]
Aug  6 15:21:39.229: INFO: Created: latency-svc-4j692
Aug  6 15:21:39.235: INFO: Created: latency-svc-gz9c7
Aug  6 15:21:39.238: INFO: Got endpoints: latency-svc-4j692 [205.639022ms]
Aug  6 15:21:39.239: INFO: Got endpoints: latency-svc-gz9c7 [188.0445ms]
Aug  6 15:21:39.244: INFO: Created: latency-svc-vwj9n
Aug  6 15:21:39.249: INFO: Got endpoints: latency-svc-vwj9n [182.762495ms]
Aug  6 15:21:39.254: INFO: Created: latency-svc-k2mhp
Aug  6 15:21:39.263: INFO: Created: latency-svc-r7n28
Aug  6 15:21:39.263: INFO: Got endpoints: latency-svc-k2mhp [180.97265ms]
Aug  6 15:21:39.268: INFO: Got endpoints: latency-svc-r7n28 [161.037529ms]
Aug  6 15:21:39.273: INFO: Created: latency-svc-42l6z
Aug  6 15:21:39.279: INFO: Created: latency-svc-hkwm4
Aug  6 15:21:39.280: INFO: Got endpoints: latency-svc-42l6z [165.8575ms]
Aug  6 15:21:39.283: INFO: Got endpoints: latency-svc-hkwm4 [158.385681ms]
Aug  6 15:21:39.290: INFO: Created: latency-svc-9f6x5
Aug  6 15:21:39.294: INFO: Got endpoints: latency-svc-9f6x5 [160.537001ms]
Aug  6 15:21:39.304: INFO: Created: latency-svc-b82f2
Aug  6 15:21:39.312: INFO: Got endpoints: latency-svc-b82f2 [151.255747ms]
Aug  6 15:21:39.312: INFO: Created: latency-svc-b6tj8
Aug  6 15:21:39.321: INFO: Created: latency-svc-tkw8x
Aug  6 15:21:39.326: INFO: Got endpoints: latency-svc-b6tj8 [153.989711ms]
Aug  6 15:21:39.331: INFO: Got endpoints: latency-svc-tkw8x [149.378977ms]
Aug  6 15:21:39.337: INFO: Created: latency-svc-zg4cr
Aug  6 15:21:39.343: INFO: Got endpoints: latency-svc-zg4cr [147.51362ms]
Aug  6 15:21:39.347: INFO: Created: latency-svc-cxv48
Aug  6 15:21:39.355: INFO: Created: latency-svc-6l2p7
Aug  6 15:21:39.358: INFO: Got endpoints: latency-svc-cxv48 [148.947692ms]
Aug  6 15:21:39.375: INFO: Got endpoints: latency-svc-6l2p7 [162.963235ms]
Aug  6 15:21:39.378: INFO: Created: latency-svc-c4k7w
Aug  6 15:21:39.385: INFO: Got endpoints: latency-svc-c4k7w [158.079792ms]
Aug  6 15:21:39.387: INFO: Created: latency-svc-dgrn7
Aug  6 15:21:39.393: INFO: Got endpoints: latency-svc-dgrn7 [155.676627ms]
Aug  6 15:21:39.395: INFO: Created: latency-svc-6qxsv
Aug  6 15:21:39.404: INFO: Got endpoints: latency-svc-6qxsv [164.691753ms]
Aug  6 15:21:39.413: INFO: Created: latency-svc-g9v6l
Aug  6 15:21:39.420: INFO: Got endpoints: latency-svc-g9v6l [171.330012ms]
Aug  6 15:21:39.423: INFO: Created: latency-svc-qxlrk
Aug  6 15:21:39.430: INFO: Got endpoints: latency-svc-qxlrk [166.407201ms]
Aug  6 15:21:39.435: INFO: Created: latency-svc-5n456
Aug  6 15:21:39.443: INFO: Got endpoints: latency-svc-5n456 [174.919398ms]
Aug  6 15:21:39.451: INFO: Created: latency-svc-nff6j
Aug  6 15:21:39.461: INFO: Created: latency-svc-zbn8q
Aug  6 15:21:39.464: INFO: Got endpoints: latency-svc-nff6j [184.255165ms]
Aug  6 15:21:39.471: INFO: Got endpoints: latency-svc-zbn8q [188.218171ms]
Aug  6 15:21:39.477: INFO: Created: latency-svc-qhrpv
Aug  6 15:21:39.485: INFO: Got endpoints: latency-svc-qhrpv [190.572647ms]
Aug  6 15:21:39.496: INFO: Created: latency-svc-6dvb6
Aug  6 15:21:39.504: INFO: Got endpoints: latency-svc-6dvb6 [192.140439ms]
Aug  6 15:21:39.513: INFO: Created: latency-svc-4hd78
Aug  6 15:21:39.521: INFO: Got endpoints: latency-svc-4hd78 [195.84554ms]
Aug  6 15:21:39.527: INFO: Created: latency-svc-c9mmr
Aug  6 15:21:39.538: INFO: Got endpoints: latency-svc-c9mmr [207.624416ms]
Aug  6 15:21:39.540: INFO: Created: latency-svc-nr89x
Aug  6 15:21:39.550: INFO: Got endpoints: latency-svc-nr89x [207.74255ms]
Aug  6 15:21:39.559: INFO: Created: latency-svc-p44wg
Aug  6 15:21:39.565: INFO: Got endpoints: latency-svc-p44wg [207.396605ms]
Aug  6 15:21:39.568: INFO: Created: latency-svc-kng76
Aug  6 15:21:39.577: INFO: Got endpoints: latency-svc-kng76 [201.676685ms]
Aug  6 15:21:39.582: INFO: Created: latency-svc-4z82l
Aug  6 15:21:39.595: INFO: Created: latency-svc-plxn4
Aug  6 15:21:39.595: INFO: Got endpoints: latency-svc-4z82l [210.24118ms]
Aug  6 15:21:39.601: INFO: Got endpoints: latency-svc-plxn4 [208.026525ms]
Aug  6 15:21:39.604: INFO: Created: latency-svc-hrktn
Aug  6 15:21:39.611: INFO: Got endpoints: latency-svc-hrktn [207.456356ms]
Aug  6 15:21:39.620: INFO: Created: latency-svc-kqhk4
Aug  6 15:21:39.629: INFO: Created: latency-svc-gspn9
Aug  6 15:21:39.631: INFO: Got endpoints: latency-svc-kqhk4 [210.339598ms]
Aug  6 15:21:39.639: INFO: Got endpoints: latency-svc-gspn9 [209.106164ms]
Aug  6 15:21:39.651: INFO: Created: latency-svc-lb6ss
Aug  6 15:21:39.663: INFO: Created: latency-svc-n7stk
Aug  6 15:21:39.665: INFO: Got endpoints: latency-svc-lb6ss [221.96049ms]
Aug  6 15:21:39.671: INFO: Got endpoints: latency-svc-n7stk [206.519482ms]
Aug  6 15:21:39.674: INFO: Created: latency-svc-swvw4
Aug  6 15:21:39.680: INFO: Got endpoints: latency-svc-swvw4 [209.551908ms]
Aug  6 15:21:39.695: INFO: Created: latency-svc-gj4gg
Aug  6 15:21:39.705: INFO: Got endpoints: latency-svc-gj4gg [219.704477ms]
Aug  6 15:21:39.715: INFO: Created: latency-svc-mnplv
Aug  6 15:21:39.721: INFO: Got endpoints: latency-svc-mnplv [217.182615ms]
Aug  6 15:21:39.738: INFO: Created: latency-svc-k5mfd
Aug  6 15:21:39.801: INFO: Got endpoints: latency-svc-k5mfd [279.182355ms]
Aug  6 15:21:39.805: INFO: Created: latency-svc-pqktm
Aug  6 15:21:39.812: INFO: Got endpoints: latency-svc-pqktm [274.174722ms]
Aug  6 15:21:39.832: INFO: Created: latency-svc-fdf8f
Aug  6 15:21:39.841: INFO: Got endpoints: latency-svc-fdf8f [291.021392ms]
Aug  6 15:21:39.844: INFO: Created: latency-svc-pqfm4
Aug  6 15:21:39.853: INFO: Got endpoints: latency-svc-pqfm4 [287.662926ms]
Aug  6 15:21:39.853: INFO: Created: latency-svc-c4k8c
Aug  6 15:21:39.864: INFO: Got endpoints: latency-svc-c4k8c [287.582669ms]
Aug  6 15:21:39.864: INFO: Created: latency-svc-4xtgw
Aug  6 15:21:39.873: INFO: Got endpoints: latency-svc-4xtgw [278.474448ms]
Aug  6 15:21:39.878: INFO: Created: latency-svc-djf6q
Aug  6 15:21:39.890: INFO: Got endpoints: latency-svc-djf6q [288.075916ms]
Aug  6 15:21:39.892: INFO: Created: latency-svc-457qq
Aug  6 15:21:39.905: INFO: Got endpoints: latency-svc-457qq [294.002688ms]
Aug  6 15:21:39.911: INFO: Created: latency-svc-hs6wn
Aug  6 15:21:39.920: INFO: Got endpoints: latency-svc-hs6wn [288.733853ms]
Aug  6 15:21:39.920: INFO: Created: latency-svc-bp6wl
Aug  6 15:21:39.927: INFO: Got endpoints: latency-svc-bp6wl [287.984245ms]
Aug  6 15:21:39.934: INFO: Created: latency-svc-x2cgq
Aug  6 15:21:39.944: INFO: Got endpoints: latency-svc-x2cgq [278.669195ms]
Aug  6 15:21:39.953: INFO: Created: latency-svc-bpxnn
Aug  6 15:21:39.957: INFO: Got endpoints: latency-svc-bpxnn [286.36228ms]
Aug  6 15:21:39.963: INFO: Created: latency-svc-68hnp
Aug  6 15:21:39.978: INFO: Created: latency-svc-cddhg
Aug  6 15:21:39.981: INFO: Got endpoints: latency-svc-68hnp [300.448339ms]
Aug  6 15:21:39.986: INFO: Got endpoints: latency-svc-cddhg [281.280881ms]
Aug  6 15:21:39.998: INFO: Created: latency-svc-7szws
Aug  6 15:21:40.011: INFO: Got endpoints: latency-svc-7szws [289.859964ms]
Aug  6 15:21:40.015: INFO: Created: latency-svc-9njkp
Aug  6 15:21:40.021: INFO: Got endpoints: latency-svc-9njkp [219.930751ms]
Aug  6 15:21:40.023: INFO: Created: latency-svc-phn7l
Aug  6 15:21:40.030: INFO: Got endpoints: latency-svc-phn7l [217.625183ms]
Aug  6 15:21:40.036: INFO: Created: latency-svc-nt9r8
Aug  6 15:21:40.042: INFO: Got endpoints: latency-svc-nt9r8 [200.499741ms]
Aug  6 15:21:40.049: INFO: Created: latency-svc-fmb2w
Aug  6 15:21:40.057: INFO: Created: latency-svc-bnwb2
Aug  6 15:21:40.059: INFO: Got endpoints: latency-svc-fmb2w [206.360309ms]
Aug  6 15:21:40.067: INFO: Created: latency-svc-9c5dp
Aug  6 15:21:40.067: INFO: Got endpoints: latency-svc-bnwb2 [202.788641ms]
Aug  6 15:21:40.072: INFO: Got endpoints: latency-svc-9c5dp [198.867386ms]
Aug  6 15:21:40.081: INFO: Created: latency-svc-j7zxw
Aug  6 15:21:40.090: INFO: Created: latency-svc-p4mp2
Aug  6 15:21:40.091: INFO: Got endpoints: latency-svc-j7zxw [201.155524ms]
Aug  6 15:21:40.094: INFO: Got endpoints: latency-svc-p4mp2 [189.17662ms]
Aug  6 15:21:40.101: INFO: Created: latency-svc-t7w4f
Aug  6 15:21:40.107: INFO: Got endpoints: latency-svc-t7w4f [187.35215ms]
Aug  6 15:21:40.110: INFO: Created: latency-svc-gmk7m
Aug  6 15:21:40.117: INFO: Created: latency-svc-rmwqv
Aug  6 15:21:40.117: INFO: Got endpoints: latency-svc-gmk7m [190.58166ms]
Aug  6 15:21:40.126: INFO: Got endpoints: latency-svc-rmwqv [182.806371ms]
Aug  6 15:21:40.128: INFO: Created: latency-svc-hczk5
Aug  6 15:21:40.134: INFO: Got endpoints: latency-svc-hczk5 [176.615377ms]
Aug  6 15:21:40.136: INFO: Created: latency-svc-2l8qt
Aug  6 15:21:40.141: INFO: Got endpoints: latency-svc-2l8qt [160.376914ms]
Aug  6 15:21:40.147: INFO: Created: latency-svc-wbc6p
Aug  6 15:21:40.154: INFO: Got endpoints: latency-svc-wbc6p [167.895063ms]
Aug  6 15:21:40.156: INFO: Created: latency-svc-hhqwd
Aug  6 15:21:40.162: INFO: Got endpoints: latency-svc-hhqwd [150.988611ms]
Aug  6 15:21:40.165: INFO: Created: latency-svc-gs7qr
Aug  6 15:21:40.171: INFO: Got endpoints: latency-svc-gs7qr [149.977989ms]
Aug  6 15:21:40.173: INFO: Created: latency-svc-8dds7
Aug  6 15:21:40.179: INFO: Got endpoints: latency-svc-8dds7 [149.020571ms]
Aug  6 15:21:40.182: INFO: Created: latency-svc-vc8df
Aug  6 15:21:40.189: INFO: Got endpoints: latency-svc-vc8df [146.775165ms]
Aug  6 15:21:40.189: INFO: Created: latency-svc-cgkx5
Aug  6 15:21:40.193: INFO: Got endpoints: latency-svc-cgkx5 [133.006918ms]
Aug  6 15:21:40.197: INFO: Created: latency-svc-svx4w
Aug  6 15:21:40.202: INFO: Got endpoints: latency-svc-svx4w [134.926462ms]
Aug  6 15:21:40.202: INFO: Created: latency-svc-dh2b6
Aug  6 15:21:40.213: INFO: Created: latency-svc-cbb8x
Aug  6 15:21:40.213: INFO: Got endpoints: latency-svc-dh2b6 [141.065694ms]
Aug  6 15:21:40.219: INFO: Got endpoints: latency-svc-cbb8x [128.394403ms]
Aug  6 15:21:40.223: INFO: Created: latency-svc-lsl7t
Aug  6 15:21:40.227: INFO: Got endpoints: latency-svc-lsl7t [132.675128ms]
Aug  6 15:21:40.228: INFO: Created: latency-svc-44qv8
Aug  6 15:21:40.234: INFO: Got endpoints: latency-svc-44qv8 [126.627545ms]
Aug  6 15:21:40.236: INFO: Created: latency-svc-mq6d2
Aug  6 15:21:40.243: INFO: Got endpoints: latency-svc-mq6d2 [125.092593ms]
Aug  6 15:21:40.245: INFO: Created: latency-svc-sfkc7
Aug  6 15:21:40.250: INFO: Got endpoints: latency-svc-sfkc7 [124.119839ms]
Aug  6 15:21:40.251: INFO: Created: latency-svc-df75f
Aug  6 15:21:40.261: INFO: Got endpoints: latency-svc-df75f [127.52961ms]
Aug  6 15:21:40.262: INFO: Created: latency-svc-vdgnx
Aug  6 15:21:40.268: INFO: Created: latency-svc-vkl7c
Aug  6 15:21:40.269: INFO: Got endpoints: latency-svc-vdgnx [127.215436ms]
Aug  6 15:21:40.273: INFO: Got endpoints: latency-svc-vkl7c [119.514942ms]
Aug  6 15:21:40.275: INFO: Created: latency-svc-hj2wn
Aug  6 15:21:40.280: INFO: Got endpoints: latency-svc-hj2wn [118.649622ms]
Aug  6 15:21:40.284: INFO: Created: latency-svc-pqg7c
Aug  6 15:21:40.289: INFO: Created: latency-svc-lkqjh
Aug  6 15:21:40.290: INFO: Got endpoints: latency-svc-pqg7c [119.686716ms]
Aug  6 15:21:40.294: INFO: Got endpoints: latency-svc-lkqjh [114.723797ms]
Aug  6 15:21:40.298: INFO: Created: latency-svc-gmgwc
Aug  6 15:21:40.304: INFO: Got endpoints: latency-svc-gmgwc [115.601321ms]
Aug  6 15:21:40.306: INFO: Created: latency-svc-7fzqm
Aug  6 15:21:40.312: INFO: Got endpoints: latency-svc-7fzqm [119.77263ms]
Aug  6 15:21:40.314: INFO: Created: latency-svc-j2gcm
Aug  6 15:21:40.321: INFO: Created: latency-svc-q25tk
Aug  6 15:21:40.322: INFO: Got endpoints: latency-svc-j2gcm [120.011167ms]
Aug  6 15:21:40.326: INFO: Got endpoints: latency-svc-q25tk [112.952536ms]
Aug  6 15:21:40.329: INFO: Created: latency-svc-fp2mp
Aug  6 15:21:40.356: INFO: Created: latency-svc-wqk4g
Aug  6 15:21:40.363: INFO: Got endpoints: latency-svc-fp2mp [143.744658ms]
Aug  6 15:21:40.364: INFO: Got endpoints: latency-svc-wqk4g [136.900378ms]
Aug  6 15:21:40.389: INFO: Created: latency-svc-c69tn
Aug  6 15:21:40.400: INFO: Got endpoints: latency-svc-c69tn [165.957212ms]
Aug  6 15:21:40.402: INFO: Created: latency-svc-77cgh
Aug  6 15:21:40.409: INFO: Got endpoints: latency-svc-77cgh [166.287937ms]
Aug  6 15:21:40.410: INFO: Created: latency-svc-zk5dw
Aug  6 15:21:40.415: INFO: Got endpoints: latency-svc-zk5dw [164.604381ms]
Aug  6 15:21:40.418: INFO: Created: latency-svc-c85s9
Aug  6 15:21:40.424: INFO: Got endpoints: latency-svc-c85s9 [162.738966ms]
Aug  6 15:21:40.428: INFO: Created: latency-svc-4hqrq
Aug  6 15:21:40.433: INFO: Got endpoints: latency-svc-4hqrq [164.221298ms]
Aug  6 15:21:40.435: INFO: Created: latency-svc-8sssg
Aug  6 15:21:40.442: INFO: Got endpoints: latency-svc-8sssg [168.707278ms]
Aug  6 15:21:40.443: INFO: Created: latency-svc-6h7cm
Aug  6 15:21:40.449: INFO: Got endpoints: latency-svc-6h7cm [168.004145ms]
Aug  6 15:21:40.450: INFO: Created: latency-svc-psq5m
Aug  6 15:21:40.455: INFO: Got endpoints: latency-svc-psq5m [164.586674ms]
Aug  6 15:21:40.459: INFO: Created: latency-svc-dgs8c
Aug  6 15:21:40.464: INFO: Got endpoints: latency-svc-dgs8c [170.112542ms]
Aug  6 15:21:40.465: INFO: Created: latency-svc-9m9gr
Aug  6 15:21:40.471: INFO: Created: latency-svc-h685z
Aug  6 15:21:40.472: INFO: Got endpoints: latency-svc-9m9gr [168.018086ms]
Aug  6 15:21:40.477: INFO: Got endpoints: latency-svc-h685z [164.462603ms]
Aug  6 15:21:40.480: INFO: Created: latency-svc-bsdkl
Aug  6 15:21:40.487: INFO: Created: latency-svc-4hnq9
Aug  6 15:21:40.487: INFO: Got endpoints: latency-svc-bsdkl [165.406298ms]
Aug  6 15:21:40.494: INFO: Got endpoints: latency-svc-4hnq9 [167.922504ms]
Aug  6 15:21:40.497: INFO: Created: latency-svc-44lrz
Aug  6 15:21:40.502: INFO: Got endpoints: latency-svc-44lrz [139.121045ms]
Aug  6 15:21:40.508: INFO: Created: latency-svc-bctv7
Aug  6 15:21:40.512: INFO: Got endpoints: latency-svc-bctv7 [147.566835ms]
Aug  6 15:21:40.514: INFO: Created: latency-svc-kzwxl
Aug  6 15:21:40.521: INFO: Got endpoints: latency-svc-kzwxl [121.088461ms]
Aug  6 15:21:40.526: INFO: Created: latency-svc-g2nvn
Aug  6 15:21:40.533: INFO: Got endpoints: latency-svc-g2nvn [123.741817ms]
Aug  6 15:21:40.535: INFO: Created: latency-svc-rj7cr
Aug  6 15:21:40.541: INFO: Got endpoints: latency-svc-rj7cr [125.460848ms]
Aug  6 15:21:40.542: INFO: Created: latency-svc-2qv6h
Aug  6 15:21:40.549: INFO: Got endpoints: latency-svc-2qv6h [124.388819ms]
Aug  6 15:21:40.551: INFO: Created: latency-svc-rbhhm
Aug  6 15:21:40.557: INFO: Got endpoints: latency-svc-rbhhm [123.710866ms]
Aug  6 15:21:40.557: INFO: Created: latency-svc-k6cqj
Aug  6 15:21:40.564: INFO: Got endpoints: latency-svc-k6cqj [121.519917ms]
Aug  6 15:21:40.566: INFO: Created: latency-svc-2bssg
Aug  6 15:21:40.573: INFO: Got endpoints: latency-svc-2bssg [124.103752ms]
Aug  6 15:21:40.576: INFO: Created: latency-svc-gk2nh
Aug  6 15:21:40.579: INFO: Got endpoints: latency-svc-gk2nh [124.334501ms]
Aug  6 15:21:40.582: INFO: Created: latency-svc-2qsrv
Aug  6 15:21:40.589: INFO: Created: latency-svc-26qxp
Aug  6 15:21:40.590: INFO: Got endpoints: latency-svc-2qsrv [125.989208ms]
Aug  6 15:21:40.599: INFO: Created: latency-svc-zwv4s
Aug  6 15:21:40.599: INFO: Got endpoints: latency-svc-26qxp [126.991774ms]
Aug  6 15:21:40.603: INFO: Created: latency-svc-vkhk7
Aug  6 15:21:40.604: INFO: Got endpoints: latency-svc-zwv4s [127.619311ms]
Aug  6 15:21:40.609: INFO: Got endpoints: latency-svc-vkhk7 [121.344416ms]
Aug  6 15:21:40.611: INFO: Created: latency-svc-hzkmr
Aug  6 15:21:40.617: INFO: Got endpoints: latency-svc-hzkmr [122.40052ms]
Aug  6 15:21:40.618: INFO: Created: latency-svc-2tj7j
Aug  6 15:21:40.623: INFO: Created: latency-svc-ctrjk
Aug  6 15:21:40.624: INFO: Got endpoints: latency-svc-2tj7j [121.969477ms]
Aug  6 15:21:40.630: INFO: Got endpoints: latency-svc-ctrjk [118.543888ms]
Aug  6 15:21:40.636: INFO: Created: latency-svc-vgbnk
Aug  6 15:21:40.639: INFO: Got endpoints: latency-svc-vgbnk [118.031508ms]
Aug  6 15:21:40.641: INFO: Created: latency-svc-grvqk
Aug  6 15:21:40.646: INFO: Created: latency-svc-7c9sv
Aug  6 15:21:40.647: INFO: Got endpoints: latency-svc-grvqk [114.259765ms]
Aug  6 15:21:40.650: INFO: Got endpoints: latency-svc-7c9sv [109.600587ms]
Aug  6 15:21:40.654: INFO: Created: latency-svc-5t8hr
Aug  6 15:21:40.659: INFO: Got endpoints: latency-svc-5t8hr [109.877044ms]
Aug  6 15:21:40.662: INFO: Created: latency-svc-tkktb
Aug  6 15:21:40.667: INFO: Created: latency-svc-j2pgp
Aug  6 15:21:40.668: INFO: Got endpoints: latency-svc-tkktb [110.941974ms]
Aug  6 15:21:40.671: INFO: Got endpoints: latency-svc-j2pgp [107.382269ms]
Aug  6 15:21:40.671: INFO: Latencies: [52.457939ms 52.859186ms 52.990959ms 61.240923ms 71.828225ms 85.811255ms 99.39512ms 107.382269ms 109.600587ms 109.877044ms 110.941974ms 112.952536ms 113.934267ms 114.259765ms 114.723797ms 115.601321ms 118.031508ms 118.543888ms 118.649622ms 119.443932ms 119.514942ms 119.686716ms 119.77263ms 120.011167ms 121.088461ms 121.344416ms 121.519917ms 121.969477ms 122.40052ms 123.710866ms 123.741817ms 124.103752ms 124.119839ms 124.334501ms 124.388819ms 125.092593ms 125.460848ms 125.989208ms 126.627545ms 126.991774ms 127.215436ms 127.52961ms 127.619311ms 128.394403ms 132.675128ms 133.006918ms 134.926462ms 136.900378ms 139.121045ms 141.065694ms 143.382292ms 143.744658ms 146.775165ms 147.51362ms 147.566835ms 148.947692ms 149.020571ms 149.378977ms 149.977989ms 150.988611ms 151.255747ms 153.989711ms 155.155165ms 155.676627ms 158.079792ms 158.385681ms 160.376914ms 160.537001ms 161.037529ms 162.738966ms 162.963235ms 164.221298ms 164.462603ms 164.586674ms 164.604381ms 164.691753ms 165.406298ms 165.8575ms 165.957212ms 166.287937ms 166.407201ms 167.895063ms 167.922504ms 168.004145ms 168.018086ms 168.707278ms 170.112542ms 171.330012ms 172.646011ms 174.919398ms 175.386593ms 176.615377ms 178.142256ms 180.97265ms 181.467798ms 182.361688ms 182.762495ms 182.806371ms 183.732656ms 184.255165ms 184.407295ms 187.130895ms 187.35215ms 188.0445ms 188.218171ms 188.68503ms 189.027229ms 189.17662ms 190.572647ms 190.58166ms 191.593956ms 191.937766ms 192.127992ms 192.140439ms 193.538536ms 195.84554ms 196.593503ms 198.367ms 198.867386ms 199.339326ms 200.499741ms 200.82924ms 201.155524ms 201.676685ms 201.71307ms 202.788641ms 205.639022ms 206.360309ms 206.519482ms 207.396605ms 207.456356ms 207.624416ms 207.74255ms 207.875063ms 208.026525ms 208.658071ms 209.106164ms 209.551908ms 210.24118ms 210.339598ms 211.44872ms 213.201945ms 213.228784ms 213.269425ms 217.182615ms 217.625183ms 218.12961ms 218.534168ms 219.107923ms 219.704477ms 219.930751ms 221.349423ms 221.96049ms 222.00773ms 226.936468ms 229.599482ms 231.703354ms 233.999179ms 234.515555ms 235.886196ms 238.573858ms 238.84317ms 239.844508ms 240.588006ms 241.149637ms 243.093404ms 243.593035ms 245.065809ms 245.270144ms 247.322158ms 248.241952ms 250.486756ms 252.304838ms 252.904319ms 254.370447ms 254.810785ms 255.976166ms 256.027238ms 256.290028ms 257.302946ms 259.055922ms 264.282302ms 265.957092ms 266.769357ms 272.514414ms 274.174722ms 278.474448ms 278.669195ms 279.182355ms 281.280881ms 286.36228ms 287.582669ms 287.662926ms 287.984245ms 288.075916ms 288.733853ms 289.859964ms 291.021392ms 294.002688ms 300.448339ms]
Aug  6 15:21:40.671: INFO: 50 %ile: 184.407295ms
Aug  6 15:21:40.671: INFO: 90 %ile: 259.055922ms
Aug  6 15:21:40.671: INFO: 99 %ile: 294.002688ms
Aug  6 15:21:40.671: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:21:40.671: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-5zjbs" for this suite.
Aug  6 15:21:50.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:21:51.468: INFO: namespace: e2e-tests-svc-latency-5zjbs, resource: bindings, ignored listing per whitelist
Aug  6 15:21:52.630: INFO: namespace: e2e-tests-svc-latency-5zjbs, resource: packagemanifests, items remaining: 3
Aug  6 15:21:53.241: INFO: namespace: e2e-tests-svc-latency-5zjbs no longer exists
Aug  6 15:21:53.284: INFO: namespace: e2e-tests-svc-latency-5zjbs, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:21:53.306: INFO: namespace e2e-tests-svc-latency-5zjbs deletion completed in 12.57268087s

• [SLOW TEST:25.726 seconds]
[sig-network] Service endpoints latency
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:21:53.306: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Aug  6 15:22:05.408: INFO: Successfully updated pod "annotationupdateec467b62-b85d-11e9-8d18-525400524259"
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:22:07.459: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zfscw" for this suite.
Aug  6 15:22:29.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:22:31.101: INFO: namespace: e2e-tests-downward-api-zfscw, resource: packagemanifests, items remaining: 3
Aug  6 15:22:31.429: INFO: namespace: e2e-tests-downward-api-zfscw, resource: bindings, ignored listing per whitelist
Aug  6 15:22:32.015: INFO: namespace: e2e-tests-downward-api-zfscw no longer exists
Aug  6 15:22:32.057: INFO: namespace: e2e-tests-downward-api-zfscw, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:22:32.079: INFO: namespace e2e-tests-downward-api-zfscw deletion completed in 24.539288634s

• [SLOW TEST:38.773 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:22:32.079: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 15:22:33.421: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance version'
Aug  6 15:22:33.590: INFO: stderr: ""
Aug  6 15:22:33.590: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13+\", GitVersion:\"v1.13.10-beta.0.1+b5c04d0f249f61\", GitCommit:\"b5c04d0f249f611b9c46bc8bdf58c96a96fc2f64\", GitTreeState:\"clean\", BuildDate:\"2019-08-06T14:50:33Z\", GoVersion:\"go1.12.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13+\", GitVersion:\"v1.13.4+3a25c9b\", GitCommit:\"3a25c9b\", GitTreeState:\"clean\", BuildDate:\"2019-07-18T00:10:31Z\", GoVersion:\"go1.11.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:22:33.590: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-n2xtk" for this suite.
Aug  6 15:22:39.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:22:40.868: INFO: namespace: e2e-tests-kubectl-n2xtk, resource: bindings, ignored listing per whitelist
Aug  6 15:22:40.963: INFO: namespace: e2e-tests-kubectl-n2xtk, resource: packagemanifests, items remaining: 3
Aug  6 15:22:42.138: INFO: namespace: e2e-tests-kubectl-n2xtk no longer exists
Aug  6 15:22:42.180: INFO: namespace: e2e-tests-kubectl-n2xtk, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:22:42.202: INFO: namespace e2e-tests-kubectl-n2xtk deletion completed in 8.528071802s

• [SLOW TEST:10.123 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:22:42.202: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug  6 15:22:53.666: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-09638784-b85e-11e9-8d18-525400524259,GenerateName:,Namespace:e2e-tests-events-2t5hb,SelfLink:/api/v1/namespaces/e2e-tests-events-2t5hb/pods/send-events-09638784-b85e-11e9-8d18-525400524259,UID:fbf1756a-b85d-11e9-9dff-0e6de702691c,ResourceVersion:98796,Generation:0,CreationTimestamp:2019-08-06 15:22:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 546515022,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.62"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: anyuid,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-dpz2h {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dpz2h,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-dpz2h true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c20,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-m4htz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc002a61810} {node.kubernetes.io/not-ready Exists  NoExecute 0xc002a61880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:22:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:22:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:22:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:22:21 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:10.131.0.62,StartTime:2019-08-06 15:22:21 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-08-06 15:22:30 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:53c28beabd3509fb5b1d1185b2962e8204384cef7562982d8b216b71292aabf9 cri-o://787143e430eeede3e475a5f8c7c1ee643876b97e2088e3b3771baed5981f9aa5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Aug  6 15:22:55.689: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug  6 15:22:57.713: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:22:57.739: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-2t5hb" for this suite.
Aug  6 15:23:35.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:23:37.382: INFO: namespace: e2e-tests-events-2t5hb, resource: bindings, ignored listing per whitelist
Aug  6 15:23:38.254: INFO: namespace: e2e-tests-events-2t5hb, resource: packagemanifests, items remaining: 3
Aug  6 15:23:38.299: INFO: namespace: e2e-tests-events-2t5hb no longer exists
Aug  6 15:23:38.346: INFO: namespace: e2e-tests-events-2t5hb, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:23:38.369: INFO: namespace e2e-tests-events-2t5hb deletion completed in 40.568423755s

• [SLOW TEST:56.167 seconds]
[k8s.io] [sig-node] Events
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:23:38.370: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:23:47.839: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-bknww" for this suite.
Aug  6 15:23:53.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:23:55.282: INFO: namespace: e2e-tests-kubelet-test-bknww, resource: packagemanifests, items remaining: 3
Aug  6 15:23:55.813: INFO: namespace: e2e-tests-kubelet-test-bknww, resource: bindings, ignored listing per whitelist
Aug  6 15:23:56.461: INFO: namespace: e2e-tests-kubelet-test-bknww no longer exists
Aug  6 15:23:56.503: INFO: namespace: e2e-tests-kubelet-test-bknww, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:23:56.526: INFO: namespace e2e-tests-kubelet-test-bknww deletion completed in 8.624466613s

• [SLOW TEST:18.156 seconds]
[k8s.io] Kubelet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:23:56.526: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Aug  6 15:23:57.929: INFO: Waiting up to 5m0s for pod "downward-api-35b4bb2b-b85e-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-svf9n" to be "success or failure"
Aug  6 15:23:57.952: INFO: Pod "downward-api-35b4bb2b-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.667543ms
Aug  6 15:23:59.974: INFO: Pod "downward-api-35b4bb2b-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045345771s
Aug  6 15:24:01.997: INFO: Pod "downward-api-35b4bb2b-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068368342s
Aug  6 15:24:04.020: INFO: Pod "downward-api-35b4bb2b-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091337428s
Aug  6 15:24:06.044: INFO: Pod "downward-api-35b4bb2b-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115171737s
Aug  6 15:24:08.068: INFO: Pod "downward-api-35b4bb2b-b85e-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.13859021s
STEP: Saw pod success
Aug  6 15:24:08.068: INFO: Pod "downward-api-35b4bb2b-b85e-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:24:08.093: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downward-api-35b4bb2b-b85e-11e9-8d18-525400524259 container dapi-container: <nil>
STEP: delete the pod
Aug  6 15:24:08.155: INFO: Waiting for pod downward-api-35b4bb2b-b85e-11e9-8d18-525400524259 to disappear
Aug  6 15:24:08.177: INFO: Pod downward-api-35b4bb2b-b85e-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:24:08.177: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-svf9n" for this suite.
Aug  6 15:24:14.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:24:15.929: INFO: namespace: e2e-tests-downward-api-svf9n, resource: packagemanifests, items remaining: 3
Aug  6 15:24:16.241: INFO: namespace: e2e-tests-downward-api-svf9n, resource: bindings, ignored listing per whitelist
Aug  6 15:24:16.757: INFO: namespace: e2e-tests-downward-api-svf9n no longer exists
Aug  6 15:24:16.799: INFO: namespace: e2e-tests-downward-api-svf9n, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:24:16.822: INFO: namespace e2e-tests-downward-api-svf9n deletion completed in 8.560742151s

• [SLOW TEST:20.296 seconds]
[sig-node] Downward API
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:24:16.822: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 15:24:18.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41d447b4-b85e-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-6k2dg" to be "success or failure"
Aug  6 15:24:18.293: INFO: Pod "downwardapi-volume-41d447b4-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.178963ms
Aug  6 15:24:20.316: INFO: Pod "downwardapi-volume-41d447b4-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045149151s
Aug  6 15:24:22.340: INFO: Pod "downwardapi-volume-41d447b4-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068832502s
Aug  6 15:24:24.363: INFO: Pod "downwardapi-volume-41d447b4-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09177844s
Aug  6 15:24:26.386: INFO: Pod "downwardapi-volume-41d447b4-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114613126s
Aug  6 15:24:28.409: INFO: Pod "downwardapi-volume-41d447b4-b85e-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137628293s
STEP: Saw pod success
Aug  6 15:24:28.409: INFO: Pod "downwardapi-volume-41d447b4-b85e-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:24:28.431: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-41d447b4-b85e-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 15:24:28.486: INFO: Waiting for pod downwardapi-volume-41d447b4-b85e-11e9-8d18-525400524259 to disappear
Aug  6 15:24:28.508: INFO: Pod downwardapi-volume-41d447b4-b85e-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:24:28.508: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6k2dg" for this suite.
Aug  6 15:24:34.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:24:35.606: INFO: namespace: e2e-tests-downward-api-6k2dg, resource: bindings, ignored listing per whitelist
Aug  6 15:24:36.124: INFO: namespace: e2e-tests-downward-api-6k2dg, resource: packagemanifests, items remaining: 3
Aug  6 15:24:37.106: INFO: namespace: e2e-tests-downward-api-6k2dg no longer exists
Aug  6 15:24:37.149: INFO: namespace: e2e-tests-downward-api-6k2dg, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:24:37.171: INFO: namespace e2e-tests-downward-api-6k2dg deletion completed in 8.58028418s

• [SLOW TEST:20.349 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:24:37.172: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-9k2j
STEP: Creating a pod to test atomic-volume-subpath
Aug  6 15:24:38.625: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-9k2j" in namespace "e2e-tests-subpath-t89xh" to be "success or failure"
Aug  6 15:24:38.647: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004456ms
Aug  6 15:24:40.670: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04560703s
Aug  6 15:24:42.694: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069189668s
Aug  6 15:24:44.717: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092079958s
Aug  6 15:24:46.740: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115577991s
Aug  6 15:24:48.771: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Running", Reason="", readiness=false. Elapsed: 10.14632989s
Aug  6 15:24:50.794: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Running", Reason="", readiness=false. Elapsed: 12.169668747s
Aug  6 15:24:52.817: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Running", Reason="", readiness=false. Elapsed: 14.192778361s
Aug  6 15:24:54.841: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Running", Reason="", readiness=false. Elapsed: 16.216291714s
Aug  6 15:24:56.865: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Running", Reason="", readiness=false. Elapsed: 18.240235286s
Aug  6 15:24:58.888: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Running", Reason="", readiness=false. Elapsed: 20.263392792s
Aug  6 15:25:00.911: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Running", Reason="", readiness=false. Elapsed: 22.286578116s
Aug  6 15:25:02.934: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Running", Reason="", readiness=false. Elapsed: 24.309372065s
Aug  6 15:25:04.957: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Running", Reason="", readiness=false. Elapsed: 26.332316122s
Aug  6 15:25:06.980: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Running", Reason="", readiness=false. Elapsed: 28.355274382s
Aug  6 15:25:09.003: INFO: Pod "pod-subpath-test-secret-9k2j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.378445684s
STEP: Saw pod success
Aug  6 15:25:09.004: INFO: Pod "pod-subpath-test-secret-9k2j" satisfied condition "success or failure"
Aug  6 15:25:09.026: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-subpath-test-secret-9k2j container test-container-subpath-secret-9k2j: <nil>
STEP: delete the pod
Aug  6 15:25:09.082: INFO: Waiting for pod pod-subpath-test-secret-9k2j to disappear
Aug  6 15:25:09.104: INFO: Pod pod-subpath-test-secret-9k2j no longer exists
STEP: Deleting pod pod-subpath-test-secret-9k2j
Aug  6 15:25:09.104: INFO: Deleting pod "pod-subpath-test-secret-9k2j" in namespace "e2e-tests-subpath-t89xh"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:25:09.127: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-t89xh" for this suite.
Aug  6 15:25:15.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:25:17.192: INFO: namespace: e2e-tests-subpath-t89xh, resource: bindings, ignored listing per whitelist
Aug  6 15:25:17.421: INFO: namespace: e2e-tests-subpath-t89xh, resource: packagemanifests, items remaining: 3
Aug  6 15:25:17.685: INFO: namespace: e2e-tests-subpath-t89xh no longer exists
Aug  6 15:25:17.727: INFO: namespace: e2e-tests-subpath-t89xh, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:25:17.750: INFO: namespace e2e-tests-subpath-t89xh deletion completed in 8.559821541s

• [SLOW TEST:40.578 seconds]
[sig-storage] Subpath
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:25:17.750: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Aug  6 15:25:29.890: INFO: Successfully updated pod "labelsupdate662b9705-b85e-11e9-8d18-525400524259"
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:25:31.946: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vx9mr" for this suite.
Aug  6 15:25:54.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:25:55.223: INFO: namespace: e2e-tests-downward-api-vx9mr, resource: bindings, ignored listing per whitelist
Aug  6 15:25:55.809: INFO: namespace: e2e-tests-downward-api-vx9mr, resource: packagemanifests, items remaining: 3
Aug  6 15:25:56.541: INFO: namespace: e2e-tests-downward-api-vx9mr no longer exists
Aug  6 15:25:56.584: INFO: namespace: e2e-tests-downward-api-vx9mr, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:25:56.607: INFO: namespace e2e-tests-downward-api-vx9mr deletion completed in 24.559263129s

• [SLOW TEST:38.857 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:25:56.608: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Aug  6 15:25:57.970: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:25:58.817: INFO: stderr: ""
Aug  6 15:25:58.817: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  6 15:25:58.817: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:25:58.978: INFO: stderr: ""
Aug  6 15:25:58.978: INFO: stdout: "update-demo-nautilus-dp5rq update-demo-nautilus-hcv62 "
Aug  6 15:25:58.978: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-dp5rq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:25:59.131: INFO: stderr: ""
Aug  6 15:25:59.131: INFO: stdout: ""
Aug  6 15:25:59.131: INFO: update-demo-nautilus-dp5rq is created but not running
Aug  6 15:26:04.132: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:04.296: INFO: stderr: ""
Aug  6 15:26:04.297: INFO: stdout: "update-demo-nautilus-dp5rq update-demo-nautilus-hcv62 "
Aug  6 15:26:04.297: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-dp5rq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:04.445: INFO: stderr: ""
Aug  6 15:26:04.445: INFO: stdout: ""
Aug  6 15:26:04.445: INFO: update-demo-nautilus-dp5rq is created but not running
Aug  6 15:26:09.445: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:09.603: INFO: stderr: ""
Aug  6 15:26:09.603: INFO: stdout: "update-demo-nautilus-dp5rq update-demo-nautilus-hcv62 "
Aug  6 15:26:09.603: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-dp5rq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:09.755: INFO: stderr: ""
Aug  6 15:26:09.755: INFO: stdout: "true"
Aug  6 15:26:09.755: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-dp5rq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:09.955: INFO: stderr: ""
Aug  6 15:26:09.955: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  6 15:26:09.955: INFO: validating pod update-demo-nautilus-dp5rq
Aug  6 15:26:09.980: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  6 15:26:09.980: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  6 15:26:09.981: INFO: update-demo-nautilus-dp5rq is verified up and running
Aug  6 15:26:09.981: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-hcv62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:10.131: INFO: stderr: ""
Aug  6 15:26:10.131: INFO: stdout: "true"
Aug  6 15:26:10.131: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-hcv62 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:10.281: INFO: stderr: ""
Aug  6 15:26:10.281: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  6 15:26:10.281: INFO: validating pod update-demo-nautilus-hcv62
Aug  6 15:26:10.306: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  6 15:26:10.306: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  6 15:26:10.306: INFO: update-demo-nautilus-hcv62 is verified up and running
STEP: scaling down the replication controller
Aug  6 15:26:10.500: INFO: scanned /home/jeder for discovery docs: <nil>
Aug  6 15:26:10.500: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:10.741: INFO: stderr: ""
Aug  6 15:26:10.741: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  6 15:26:10.741: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:10.896: INFO: stderr: ""
Aug  6 15:26:10.896: INFO: stdout: "update-demo-nautilus-dp5rq update-demo-nautilus-hcv62 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug  6 15:26:15.897: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:16.055: INFO: stderr: ""
Aug  6 15:26:16.055: INFO: stdout: "update-demo-nautilus-dp5rq update-demo-nautilus-hcv62 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug  6 15:26:21.056: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:21.218: INFO: stderr: ""
Aug  6 15:26:21.218: INFO: stdout: "update-demo-nautilus-dp5rq "
Aug  6 15:26:21.218: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-dp5rq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:21.374: INFO: stderr: ""
Aug  6 15:26:21.374: INFO: stdout: "true"
Aug  6 15:26:21.374: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-dp5rq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:21.525: INFO: stderr: ""
Aug  6 15:26:21.525: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  6 15:26:21.525: INFO: validating pod update-demo-nautilus-dp5rq
Aug  6 15:26:21.549: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  6 15:26:21.549: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  6 15:26:21.549: INFO: update-demo-nautilus-dp5rq is verified up and running
STEP: scaling up the replication controller
Aug  6 15:26:21.658: INFO: scanned /home/jeder for discovery docs: <nil>
Aug  6 15:26:21.658: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:21.894: INFO: stderr: ""
Aug  6 15:26:21.894: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  6 15:26:21.894: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:22.046: INFO: stderr: ""
Aug  6 15:26:22.046: INFO: stdout: "update-demo-nautilus-4kmzl update-demo-nautilus-dp5rq "
Aug  6 15:26:22.047: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-4kmzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:22.194: INFO: stderr: ""
Aug  6 15:26:22.194: INFO: stdout: ""
Aug  6 15:26:22.194: INFO: update-demo-nautilus-4kmzl is created but not running
Aug  6 15:26:27.195: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:27.356: INFO: stderr: ""
Aug  6 15:26:27.356: INFO: stdout: "update-demo-nautilus-4kmzl update-demo-nautilus-dp5rq "
Aug  6 15:26:27.356: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-4kmzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:27.505: INFO: stderr: ""
Aug  6 15:26:27.505: INFO: stdout: ""
Aug  6 15:26:27.505: INFO: update-demo-nautilus-4kmzl is created but not running
Aug  6 15:26:32.505: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:32.662: INFO: stderr: ""
Aug  6 15:26:32.662: INFO: stdout: "update-demo-nautilus-4kmzl update-demo-nautilus-dp5rq "
Aug  6 15:26:32.662: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-4kmzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:32.816: INFO: stderr: ""
Aug  6 15:26:32.816: INFO: stdout: "true"
Aug  6 15:26:32.816: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-4kmzl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:32.971: INFO: stderr: ""
Aug  6 15:26:32.971: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  6 15:26:32.971: INFO: validating pod update-demo-nautilus-4kmzl
Aug  6 15:26:32.996: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  6 15:26:32.996: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  6 15:26:32.996: INFO: update-demo-nautilus-4kmzl is verified up and running
Aug  6 15:26:32.996: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-dp5rq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:33.147: INFO: stderr: ""
Aug  6 15:26:33.147: INFO: stdout: "true"
Aug  6 15:26:33.147: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-dp5rq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:33.297: INFO: stderr: ""
Aug  6 15:26:33.297: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  6 15:26:33.297: INFO: validating pod update-demo-nautilus-dp5rq
Aug  6 15:26:33.321: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  6 15:26:33.321: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  6 15:26:33.321: INFO: update-demo-nautilus-dp5rq is verified up and running
STEP: using delete to clean up resources
Aug  6 15:26:33.321: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:33.518: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  6 15:26:33.518: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug  6 15:26:33.518: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-8z5zs'
Aug  6 15:26:33.697: INFO: stderr: "No resources found.\n"
Aug  6 15:26:33.697: INFO: stdout: ""
Aug  6 15:26:33.697: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -l name=update-demo --namespace=e2e-tests-kubectl-8z5zs -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug  6 15:26:33.854: INFO: stderr: ""
Aug  6 15:26:33.854: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:26:33.854: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8z5zs" for this suite.
Aug  6 15:26:55.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:26:56.717: INFO: namespace: e2e-tests-kubectl-8z5zs, resource: bindings, ignored listing per whitelist
Aug  6 15:26:57.616: INFO: namespace: e2e-tests-kubectl-8z5zs, resource: packagemanifests, items remaining: 3
Aug  6 15:26:58.466: INFO: namespace: e2e-tests-kubectl-8z5zs no longer exists
Aug  6 15:26:58.508: INFO: namespace: e2e-tests-kubectl-8z5zs, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:26:58.530: INFO: namespace e2e-tests-kubectl-8z5zs deletion completed in 24.613398233s

• [SLOW TEST:61.922 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:26:58.530: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-kqk2w
Aug  6 15:27:09.975: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-kqk2w
STEP: checking the pod's current state and verifying that restartCount is present
Aug  6 15:27:09.997: INFO: Initial restart count of pod liveness-exec is 0
Aug  6 15:28:02.623: INFO: Restart count of pod e2e-tests-container-probe-kqk2w/liveness-exec is now 1 (52.625864693s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:28:02.652: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-kqk2w" for this suite.
Aug  6 15:28:08.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:28:10.030: INFO: namespace: e2e-tests-container-probe-kqk2w, resource: packagemanifests, items remaining: 3
Aug  6 15:28:10.810: INFO: namespace: e2e-tests-container-probe-kqk2w, resource: bindings, ignored listing per whitelist
Aug  6 15:28:11.233: INFO: namespace: e2e-tests-container-probe-kqk2w no longer exists
Aug  6 15:28:11.276: INFO: namespace: e2e-tests-container-probe-kqk2w, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:28:11.298: INFO: namespace e2e-tests-container-probe-kqk2w deletion completed in 8.563796616s

• [SLOW TEST:72.768 seconds]
[k8s.io] Probing container
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:28:11.299: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-cd8e38e3-b85e-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 15:28:12.713: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd91ceca-b85e-11e9-8d18-525400524259" in namespace "e2e-tests-configmap-c5l97" to be "success or failure"
Aug  6 15:28:12.736: INFO: Pod "pod-configmaps-cd91ceca-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.394554ms
Aug  6 15:28:14.759: INFO: Pod "pod-configmaps-cd91ceca-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045604663s
Aug  6 15:28:16.783: INFO: Pod "pod-configmaps-cd91ceca-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069266906s
Aug  6 15:28:18.806: INFO: Pod "pod-configmaps-cd91ceca-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092347097s
Aug  6 15:28:20.830: INFO: Pod "pod-configmaps-cd91ceca-b85e-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116555361s
Aug  6 15:28:22.854: INFO: Pod "pod-configmaps-cd91ceca-b85e-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.140159499s
STEP: Saw pod success
Aug  6 15:28:22.854: INFO: Pod "pod-configmaps-cd91ceca-b85e-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:28:22.878: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-configmaps-cd91ceca-b85e-11e9-8d18-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 15:28:22.933: INFO: Waiting for pod pod-configmaps-cd91ceca-b85e-11e9-8d18-525400524259 to disappear
Aug  6 15:28:22.955: INFO: Pod pod-configmaps-cd91ceca-b85e-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:28:22.955: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-c5l97" for this suite.
Aug  6 15:28:29.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:28:30.521: INFO: namespace: e2e-tests-configmap-c5l97, resource: packagemanifests, items remaining: 3
Aug  6 15:28:31.162: INFO: namespace: e2e-tests-configmap-c5l97, resource: bindings, ignored listing per whitelist
Aug  6 15:28:31.519: INFO: namespace: e2e-tests-configmap-c5l97 no longer exists
Aug  6 15:28:31.561: INFO: namespace: e2e-tests-configmap-c5l97, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:28:31.583: INFO: namespace e2e-tests-configmap-c5l97 deletion completed in 8.564077762s

• [SLOW TEST:20.284 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:28:31.584: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 15:28:32.996: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-d9ad707f-b85e-11e9-8d18-525400524259
STEP: Creating secret with name s-test-opt-upd-d9ad70c3-b85e-11e9-8d18-525400524259
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d9ad707f-b85e-11e9-8d18-525400524259
STEP: Updating secret s-test-opt-upd-d9ad70c3-b85e-11e9-8d18-525400524259
STEP: Creating secret with name s-test-opt-create-d9ad70d6-b85e-11e9-8d18-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:29:56.395: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-fhfg4" for this suite.
Aug  6 15:30:18.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:30:19.825: INFO: namespace: e2e-tests-secrets-fhfg4, resource: bindings, ignored listing per whitelist
Aug  6 15:30:20.777: INFO: namespace: e2e-tests-secrets-fhfg4, resource: packagemanifests, items remaining: 3
Aug  6 15:30:20.954: INFO: namespace: e2e-tests-secrets-fhfg4 no longer exists
Aug  6 15:30:20.977: INFO: namespace: e2e-tests-secrets-fhfg4, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:30:21.000: INFO: namespace e2e-tests-secrets-fhfg4 deletion completed in 24.55818261s

• [SLOW TEST:109.416 seconds]
[sig-storage] Secrets
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:30:21.001: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Aug  6 15:30:33.076: INFO: Successfully updated pod "annotationupdate1adfa3e2-b85f-11e9-8d18-525400524259"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:30:35.164: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tbc8x" for this suite.
Aug  6 15:30:57.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:30:58.706: INFO: namespace: e2e-tests-projected-tbc8x, resource: bindings, ignored listing per whitelist
Aug  6 15:30:58.997: INFO: namespace: e2e-tests-projected-tbc8x, resource: packagemanifests, items remaining: 3
Aug  6 15:30:59.736: INFO: namespace: e2e-tests-projected-tbc8x no longer exists
Aug  6 15:30:59.760: INFO: namespace: e2e-tests-projected-tbc8x, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:30:59.783: INFO: namespace e2e-tests-projected-tbc8x deletion completed in 24.55518253s

• [SLOW TEST:38.783 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:30:59.784: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:31:11.394: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-wn8jv" for this suite.
Aug  6 15:32:01.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:32:03.043: INFO: namespace: e2e-tests-kubelet-test-wn8jv, resource: packagemanifests, items remaining: 3
Aug  6 15:32:03.043: INFO: namespace: e2e-tests-kubelet-test-wn8jv, resource: bindings, ignored listing per whitelist
Aug  6 15:32:04.014: INFO: namespace: e2e-tests-kubelet-test-wn8jv no longer exists
Aug  6 15:32:04.058: INFO: namespace: e2e-tests-kubelet-test-wn8jv, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:32:04.080: INFO: namespace e2e-tests-kubelet-test-wn8jv deletion completed in 52.62236863s

• [SLOW TEST:64.296 seconds]
[k8s.io] Kubelet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:32:04.081: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 15:32:05.491: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259" in namespace "e2e-tests-projected-wdm7v" to be "success or failure"
Aug  6 15:32:05.513: INFO: Pod "downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.393065ms
Aug  6 15:32:07.536: INFO: Pod "downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045720458s
Aug  6 15:32:09.561: INFO: Pod "downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069762256s
Aug  6 15:32:11.583: INFO: Pod "downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092715684s
Aug  6 15:32:13.608: INFO: Pod "downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116891546s
Aug  6 15:32:15.631: INFO: Pod "downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 10.140393893s
Aug  6 15:32:17.654: INFO: Pod "downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.163265514s
STEP: Saw pod success
Aug  6 15:32:17.654: INFO: Pod "downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:32:17.677: INFO: Trying to get logs from node ip-10-0-128-44.ec2.internal pod downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 15:32:17.753: INFO: Waiting for pod downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259 to disappear
Aug  6 15:32:17.775: INFO: Pod downwardapi-volume-5850a527-b85f-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:32:17.775: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wdm7v" for this suite.
Aug  6 15:32:23.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:32:25.223: INFO: namespace: e2e-tests-projected-wdm7v, resource: packagemanifests, items remaining: 3
Aug  6 15:32:25.290: INFO: namespace: e2e-tests-projected-wdm7v, resource: bindings, ignored listing per whitelist
Aug  6 15:32:26.357: INFO: namespace: e2e-tests-projected-wdm7v no longer exists
Aug  6 15:32:26.400: INFO: namespace: e2e-tests-projected-wdm7v, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:32:26.423: INFO: namespace e2e-tests-projected-wdm7v deletion completed in 8.56482851s

• [SLOW TEST:22.342 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:32:26.423: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-65a15368-b85f-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 15:32:27.854: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-65a5199b-b85f-11e9-8d18-525400524259" in namespace "e2e-tests-projected-l2jq6" to be "success or failure"
Aug  6 15:32:27.877: INFO: Pod "pod-projected-configmaps-65a5199b-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.983122ms
Aug  6 15:32:29.900: INFO: Pod "pod-projected-configmaps-65a5199b-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045887278s
Aug  6 15:32:31.923: INFO: Pod "pod-projected-configmaps-65a5199b-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068554813s
Aug  6 15:32:33.947: INFO: Pod "pod-projected-configmaps-65a5199b-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092819847s
Aug  6 15:32:35.970: INFO: Pod "pod-projected-configmaps-65a5199b-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115540875s
Aug  6 15:32:37.993: INFO: Pod "pod-projected-configmaps-65a5199b-b85f-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.138451837s
STEP: Saw pod success
Aug  6 15:32:37.993: INFO: Pod "pod-projected-configmaps-65a5199b-b85f-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:32:38.015: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-configmaps-65a5199b-b85f-11e9-8d18-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 15:32:38.070: INFO: Waiting for pod pod-projected-configmaps-65a5199b-b85f-11e9-8d18-525400524259 to disappear
Aug  6 15:32:38.092: INFO: Pod pod-projected-configmaps-65a5199b-b85f-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:32:38.092: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l2jq6" for this suite.
Aug  6 15:32:44.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:32:45.258: INFO: namespace: e2e-tests-projected-l2jq6, resource: packagemanifests, items remaining: 3
Aug  6 15:32:45.956: INFO: namespace: e2e-tests-projected-l2jq6, resource: bindings, ignored listing per whitelist
Aug  6 15:32:46.740: INFO: namespace: e2e-tests-projected-l2jq6 no longer exists
Aug  6 15:32:46.783: INFO: namespace: e2e-tests-projected-l2jq6, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:32:46.805: INFO: namespace e2e-tests-projected-l2jq6 deletion completed in 8.650536255s

• [SLOW TEST:20.382 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:32:46.805: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-71c87df9-b85f-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 15:32:48.242: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-71cc0edf-b85f-11e9-8d18-525400524259" in namespace "e2e-tests-projected-w6tmg" to be "success or failure"
Aug  6 15:32:48.264: INFO: Pod "pod-projected-configmaps-71cc0edf-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.232331ms
Aug  6 15:32:50.288: INFO: Pod "pod-projected-configmaps-71cc0edf-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045606628s
Aug  6 15:32:52.311: INFO: Pod "pod-projected-configmaps-71cc0edf-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068779469s
Aug  6 15:32:54.333: INFO: Pod "pod-projected-configmaps-71cc0edf-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091488464s
Aug  6 15:32:56.357: INFO: Pod "pod-projected-configmaps-71cc0edf-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114859353s
Aug  6 15:32:58.380: INFO: Pod "pod-projected-configmaps-71cc0edf-b85f-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.138325055s
STEP: Saw pod success
Aug  6 15:32:58.380: INFO: Pod "pod-projected-configmaps-71cc0edf-b85f-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:32:58.403: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-configmaps-71cc0edf-b85f-11e9-8d18-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 15:32:58.460: INFO: Waiting for pod pod-projected-configmaps-71cc0edf-b85f-11e9-8d18-525400524259 to disappear
Aug  6 15:32:58.482: INFO: Pod pod-projected-configmaps-71cc0edf-b85f-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:32:58.482: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-w6tmg" for this suite.
Aug  6 15:33:04.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:33:05.443: INFO: namespace: e2e-tests-projected-w6tmg, resource: packagemanifests, items remaining: 3
Aug  6 15:33:06.636: INFO: namespace: e2e-tests-projected-w6tmg, resource: bindings, ignored listing per whitelist
Aug  6 15:33:07.084: INFO: namespace: e2e-tests-projected-w6tmg no longer exists
Aug  6 15:33:07.127: INFO: namespace: e2e-tests-projected-w6tmg, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:33:07.149: INFO: namespace e2e-tests-projected-w6tmg deletion completed in 8.583033205s

• [SLOW TEST:20.343 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:33:07.149: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-7de78266-b85f-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 15:33:08.586: INFO: Waiting up to 5m0s for pod "pod-configmaps-7deb934c-b85f-11e9-8d18-525400524259" in namespace "e2e-tests-configmap-n2m47" to be "success or failure"
Aug  6 15:33:08.612: INFO: Pod "pod-configmaps-7deb934c-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 25.919793ms
Aug  6 15:33:10.635: INFO: Pod "pod-configmaps-7deb934c-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049028612s
Aug  6 15:33:12.660: INFO: Pod "pod-configmaps-7deb934c-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073931204s
Aug  6 15:33:14.683: INFO: Pod "pod-configmaps-7deb934c-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09711524s
Aug  6 15:33:16.706: INFO: Pod "pod-configmaps-7deb934c-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.120347081s
Aug  6 15:33:18.736: INFO: Pod "pod-configmaps-7deb934c-b85f-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.14962751s
STEP: Saw pod success
Aug  6 15:33:18.736: INFO: Pod "pod-configmaps-7deb934c-b85f-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:33:18.758: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-configmaps-7deb934c-b85f-11e9-8d18-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 15:33:18.814: INFO: Waiting for pod pod-configmaps-7deb934c-b85f-11e9-8d18-525400524259 to disappear
Aug  6 15:33:18.836: INFO: Pod pod-configmaps-7deb934c-b85f-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:33:18.836: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-n2m47" for this suite.
Aug  6 15:33:24.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:33:26.330: INFO: namespace: e2e-tests-configmap-n2m47, resource: bindings, ignored listing per whitelist
Aug  6 15:33:26.402: INFO: namespace: e2e-tests-configmap-n2m47, resource: packagemanifests, items remaining: 3
Aug  6 15:33:27.399: INFO: namespace: e2e-tests-configmap-n2m47 no longer exists
Aug  6 15:33:27.442: INFO: namespace: e2e-tests-configmap-n2m47, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:33:27.465: INFO: namespace e2e-tests-configmap-n2m47 deletion completed in 8.565661281s

• [SLOW TEST:20.316 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:33:27.465: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-8a014c86-b85f-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 15:33:28.879: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8a04ea33-b85f-11e9-8d18-525400524259" in namespace "e2e-tests-projected-xt9jh" to be "success or failure"
Aug  6 15:33:28.901: INFO: Pod "pod-projected-configmaps-8a04ea33-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.146707ms
Aug  6 15:33:30.926: INFO: Pod "pod-projected-configmaps-8a04ea33-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046337452s
Aug  6 15:33:32.949: INFO: Pod "pod-projected-configmaps-8a04ea33-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06958484s
Aug  6 15:33:34.972: INFO: Pod "pod-projected-configmaps-8a04ea33-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092355766s
Aug  6 15:33:36.995: INFO: Pod "pod-projected-configmaps-8a04ea33-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115355755s
Aug  6 15:33:39.018: INFO: Pod "pod-projected-configmaps-8a04ea33-b85f-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.138859033s
STEP: Saw pod success
Aug  6 15:33:39.018: INFO: Pod "pod-projected-configmaps-8a04ea33-b85f-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:33:39.041: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-configmaps-8a04ea33-b85f-11e9-8d18-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 15:33:39.097: INFO: Waiting for pod pod-projected-configmaps-8a04ea33-b85f-11e9-8d18-525400524259 to disappear
Aug  6 15:33:39.119: INFO: Pod pod-projected-configmaps-8a04ea33-b85f-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:33:39.119: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xt9jh" for this suite.
Aug  6 15:33:45.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:33:46.556: INFO: namespace: e2e-tests-projected-xt9jh, resource: packagemanifests, items remaining: 3
Aug  6 15:33:47.589: INFO: namespace: e2e-tests-projected-xt9jh, resource: bindings, ignored listing per whitelist
Aug  6 15:33:47.680: INFO: namespace: e2e-tests-projected-xt9jh no longer exists
Aug  6 15:33:47.702: INFO: namespace: e2e-tests-projected-xt9jh, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:33:47.728: INFO: namespace e2e-tests-projected-xt9jh deletion completed in 8.545222793s

• [SLOW TEST:20.263 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:33:47.728: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-96152142-b85f-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 15:33:49.143: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9618de72-b85f-11e9-8d18-525400524259" in namespace "e2e-tests-projected-swpkm" to be "success or failure"
Aug  6 15:33:49.165: INFO: Pod "pod-projected-secrets-9618de72-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00985ms
Aug  6 15:33:51.188: INFO: Pod "pod-projected-secrets-9618de72-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0451299s
Aug  6 15:33:53.211: INFO: Pod "pod-projected-secrets-9618de72-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068332724s
Aug  6 15:33:55.234: INFO: Pod "pod-projected-secrets-9618de72-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091684866s
Aug  6 15:33:57.258: INFO: Pod "pod-projected-secrets-9618de72-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114817055s
Aug  6 15:33:59.281: INFO: Pod "pod-projected-secrets-9618de72-b85f-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137761853s
STEP: Saw pod success
Aug  6 15:33:59.281: INFO: Pod "pod-projected-secrets-9618de72-b85f-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:33:59.304: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-secrets-9618de72-b85f-11e9-8d18-525400524259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug  6 15:33:59.359: INFO: Waiting for pod pod-projected-secrets-9618de72-b85f-11e9-8d18-525400524259 to disappear
Aug  6 15:33:59.382: INFO: Pod pod-projected-secrets-9618de72-b85f-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:33:59.382: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-swpkm" for this suite.
Aug  6 15:34:05.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:34:06.650: INFO: namespace: e2e-tests-projected-swpkm, resource: bindings, ignored listing per whitelist
Aug  6 15:34:07.403: INFO: namespace: e2e-tests-projected-swpkm, resource: packagemanifests, items remaining: 3
Aug  6 15:34:08.014: INFO: namespace: e2e-tests-projected-swpkm no longer exists
Aug  6 15:34:08.057: INFO: namespace: e2e-tests-projected-swpkm, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:34:08.080: INFO: namespace e2e-tests-projected-swpkm deletion completed in 8.634480602s

• [SLOW TEST:20.351 seconds]
[sig-storage] Projected secret
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:34:08.080: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  6 15:34:09.470: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-dhvrw'
Aug  6 15:34:11.006: INFO: stderr: ""
Aug  6 15:34:11.006: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Aug  6 15:34:21.057: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-dhvrw -o json'
Aug  6 15:34:21.225: INFO: stderr: ""
Aug  6 15:34:21.225: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"openshift-sdn\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.0.80\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2019-08-06T15:33:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-dhvrw\",\n        \"resourceVersion\": \"106688\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-dhvrw/pods/e2e-test-nginx-pod\",\n        \"uid\": \"95ad30b2-b85f-11e9-9dff-0e6de702691c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    },\n                    \"procMount\": \"Default\"\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-hc6dg\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-ntz85\"\n            }\n        ],\n        \"nodeName\": \"ip-10-0-130-134.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c33,c17\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-hc6dg\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-hc6dg\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-06T15:33:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-06T15:33:56Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-06T15:33:56Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-06T15:33:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://7b5dbf5f9e3a4c522141631baa72f24d8a83a726901d411ef9ab04bfd7c7d7c5\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-08-06T15:33:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.130.134\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.131.0.80\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-08-06T15:33:48Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug  6 15:34:21.225: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance replace -f - --namespace=e2e-tests-kubectl-dhvrw'
Aug  6 15:34:22.131: INFO: stderr: ""
Aug  6 15:34:22.131: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Aug  6 15:34:22.154: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-dhvrw'
Aug  6 15:34:29.306: INFO: stderr: ""
Aug  6 15:34:29.306: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:34:29.306: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dhvrw" for this suite.
Aug  6 15:34:35.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:34:37.318: INFO: namespace: e2e-tests-kubectl-dhvrw, resource: packagemanifests, items remaining: 3
Aug  6 15:34:37.625: INFO: namespace: e2e-tests-kubectl-dhvrw, resource: bindings, ignored listing per whitelist
Aug  6 15:34:37.888: INFO: namespace: e2e-tests-kubectl-dhvrw no longer exists
Aug  6 15:34:37.931: INFO: namespace: e2e-tests-kubectl-dhvrw, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:34:37.953: INFO: namespace e2e-tests-kubectl-dhvrw deletion completed in 8.558586552s

• [SLOW TEST:29.872 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:34:37.953: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 15:34:39.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b405251d-b85f-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-5mlms" to be "success or failure"
Aug  6 15:34:39.394: INFO: Pod "downwardapi-volume-b405251d-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.576161ms
Aug  6 15:34:41.419: INFO: Pod "downwardapi-volume-b405251d-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04737657s
Aug  6 15:34:43.443: INFO: Pod "downwardapi-volume-b405251d-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070746195s
Aug  6 15:34:45.466: INFO: Pod "downwardapi-volume-b405251d-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093924457s
Aug  6 15:34:47.489: INFO: Pod "downwardapi-volume-b405251d-b85f-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117248036s
Aug  6 15:34:49.512: INFO: Pod "downwardapi-volume-b405251d-b85f-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.140556007s
STEP: Saw pod success
Aug  6 15:34:49.512: INFO: Pod "downwardapi-volume-b405251d-b85f-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:34:49.535: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-b405251d-b85f-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 15:34:49.591: INFO: Waiting for pod downwardapi-volume-b405251d-b85f-11e9-8d18-525400524259 to disappear
Aug  6 15:34:49.613: INFO: Pod downwardapi-volume-b405251d-b85f-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:34:49.613: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5mlms" for this suite.
Aug  6 15:34:55.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:34:58.105: INFO: namespace: e2e-tests-downward-api-5mlms, resource: packagemanifests, items remaining: 3
Aug  6 15:34:58.150: INFO: namespace: e2e-tests-downward-api-5mlms, resource: bindings, ignored listing per whitelist
Aug  6 15:34:58.172: INFO: namespace: e2e-tests-downward-api-5mlms no longer exists
Aug  6 15:34:58.217: INFO: namespace: e2e-tests-downward-api-5mlms, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:34:58.239: INFO: namespace e2e-tests-downward-api-5mlms deletion completed in 8.562542232s

• [SLOW TEST:20.286 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:34:58.239: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Aug  6 15:35:00.223: INFO: created pod pod-service-account-defaultsa
Aug  6 15:35:00.223: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug  6 15:35:00.251: INFO: created pod pod-service-account-mountsa
Aug  6 15:35:00.251: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug  6 15:35:00.281: INFO: created pod pod-service-account-nomountsa
Aug  6 15:35:00.282: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug  6 15:35:00.309: INFO: created pod pod-service-account-defaultsa-mountspec
Aug  6 15:35:00.309: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug  6 15:35:00.338: INFO: created pod pod-service-account-mountsa-mountspec
Aug  6 15:35:00.338: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug  6 15:35:00.366: INFO: created pod pod-service-account-nomountsa-mountspec
Aug  6 15:35:00.366: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug  6 15:35:00.394: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug  6 15:35:00.394: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug  6 15:35:00.434: INFO: created pod pod-service-account-mountsa-nomountspec
Aug  6 15:35:00.434: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug  6 15:35:00.462: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug  6 15:35:00.462: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:35:00.462: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-rxckk" for this suite.
Aug  6 15:35:22.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:35:23.328: INFO: namespace: e2e-tests-svcaccounts-rxckk, resource: bindings, ignored listing per whitelist
Aug  6 15:35:24.076: INFO: namespace: e2e-tests-svcaccounts-rxckk, resource: packagemanifests, items remaining: 3
Aug  6 15:35:25.047: INFO: namespace: e2e-tests-svcaccounts-rxckk no longer exists
Aug  6 15:35:25.070: INFO: namespace: e2e-tests-svcaccounts-rxckk, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:35:25.092: INFO: namespace e2e-tests-svcaccounts-rxckk deletion completed in 24.566529493s

• [SLOW TEST:26.853 seconds]
[sig-auth] ServiceAccounts
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:35:25.093: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug  6 15:35:42.715: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  6 15:35:42.737: INFO: Pod pod-with-poststart-http-hook still exists
Aug  6 15:35:44.737: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  6 15:35:44.760: INFO: Pod pod-with-poststart-http-hook still exists
Aug  6 15:35:46.737: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  6 15:35:46.760: INFO: Pod pod-with-poststart-http-hook still exists
Aug  6 15:35:48.737: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  6 15:35:48.760: INFO: Pod pod-with-poststart-http-hook still exists
Aug  6 15:35:50.737: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  6 15:35:50.760: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:35:50.761: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-7prr4" for this suite.
Aug  6 15:36:12.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:36:13.971: INFO: namespace: e2e-tests-container-lifecycle-hook-7prr4, resource: bindings, ignored listing per whitelist
Aug  6 15:36:14.787: INFO: namespace: e2e-tests-container-lifecycle-hook-7prr4, resource: packagemanifests, items remaining: 3
Aug  6 15:36:15.385: INFO: namespace: e2e-tests-container-lifecycle-hook-7prr4 no longer exists
Aug  6 15:36:15.429: INFO: namespace: e2e-tests-container-lifecycle-hook-7prr4, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:36:15.452: INFO: namespace e2e-tests-container-lifecycle-hook-7prr4 deletion completed in 24.627540872s

• [SLOW TEST:50.358 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:36:15.452: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug  6 15:36:16.985: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-5578n,SelfLink:/api/v1/namespaces/e2e-tests-watch-5578n/configmaps/e2e-watch-test-resource-version,UID:e0b09827-b85f-11e9-9dff-0e6de702691c,ResourceVersion:108181,Generation:0,CreationTimestamp:2019-08-06 15:35:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug  6 15:36:16.986: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-5578n,SelfLink:/api/v1/namespaces/e2e-tests-watch-5578n/configmaps/e2e-watch-test-resource-version,UID:e0b09827-b85f-11e9-9dff-0e6de702691c,ResourceVersion:108183,Generation:0,CreationTimestamp:2019-08-06 15:35:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:36:16.986: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-5578n" for this suite.
Aug  6 15:36:23.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:36:24.494: INFO: namespace: e2e-tests-watch-5578n, resource: packagemanifests, items remaining: 3
Aug  6 15:36:25.121: INFO: namespace: e2e-tests-watch-5578n, resource: bindings, ignored listing per whitelist
Aug  6 15:36:25.566: INFO: namespace: e2e-tests-watch-5578n no longer exists
Aug  6 15:36:25.609: INFO: namespace: e2e-tests-watch-5578n, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:36:25.632: INFO: namespace e2e-tests-watch-5578n deletion completed in 8.572602081s

• [SLOW TEST:10.180 seconds]
[sig-api-machinery] Watchers
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:36:25.632: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug  6 15:36:27.255: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:27.255: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:27.255: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:27.278: INFO: Number of nodes with available pods: 0
Aug  6 15:36:27.278: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:36:28.341: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:28.341: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:28.341: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:28.367: INFO: Number of nodes with available pods: 0
Aug  6 15:36:28.367: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:36:29.342: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:29.342: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:29.342: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:29.364: INFO: Number of nodes with available pods: 0
Aug  6 15:36:29.364: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:36:30.342: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:30.342: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:30.342: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:30.365: INFO: Number of nodes with available pods: 0
Aug  6 15:36:30.365: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:36:31.344: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:31.344: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:31.344: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:31.370: INFO: Number of nodes with available pods: 0
Aug  6 15:36:31.370: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:36:32.349: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:32.349: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:32.349: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:32.374: INFO: Number of nodes with available pods: 0
Aug  6 15:36:32.374: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:36:33.342: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:33.342: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:33.342: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:33.364: INFO: Number of nodes with available pods: 0
Aug  6 15:36:33.365: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:36:34.346: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:34.346: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:34.346: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:34.370: INFO: Number of nodes with available pods: 0
Aug  6 15:36:34.370: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:36:35.342: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:35.342: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:35.342: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:35.365: INFO: Number of nodes with available pods: 1
Aug  6 15:36:35.365: INFO: Node ip-10-0-130-134.ec2.internal is running more than one daemon pod
Aug  6 15:36:36.342: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:36.342: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:36.343: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:36.366: INFO: Number of nodes with available pods: 4
Aug  6 15:36:36.366: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug  6 15:36:36.501: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:36.501: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:36.501: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:36.553: INFO: Number of nodes with available pods: 3
Aug  6 15:36:36.553: INFO: Node ip-10-0-137-62.ec2.internal is running more than one daemon pod
Aug  6 15:36:37.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:37.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:37.636: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:37.661: INFO: Number of nodes with available pods: 3
Aug  6 15:36:37.661: INFO: Node ip-10-0-137-62.ec2.internal is running more than one daemon pod
Aug  6 15:36:38.635: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:38.635: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:38.635: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:38.658: INFO: Number of nodes with available pods: 3
Aug  6 15:36:38.658: INFO: Node ip-10-0-137-62.ec2.internal is running more than one daemon pod
Aug  6 15:36:39.616: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:39.616: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:39.616: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:39.639: INFO: Number of nodes with available pods: 3
Aug  6 15:36:39.639: INFO: Node ip-10-0-137-62.ec2.internal is running more than one daemon pod
Aug  6 15:36:40.616: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:40.616: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:40.616: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:40.639: INFO: Number of nodes with available pods: 3
Aug  6 15:36:40.639: INFO: Node ip-10-0-137-62.ec2.internal is running more than one daemon pod
Aug  6 15:36:41.618: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:41.618: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:41.618: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:41.641: INFO: Number of nodes with available pods: 3
Aug  6 15:36:41.641: INFO: Node ip-10-0-137-62.ec2.internal is running more than one daemon pod
Aug  6 15:36:42.617: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:42.617: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:42.617: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:42.640: INFO: Number of nodes with available pods: 3
Aug  6 15:36:42.640: INFO: Node ip-10-0-137-62.ec2.internal is running more than one daemon pod
Aug  6 15:36:43.616: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:43.616: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:43.616: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:43.639: INFO: Number of nodes with available pods: 3
Aug  6 15:36:43.639: INFO: Node ip-10-0-137-62.ec2.internal is running more than one daemon pod
Aug  6 15:36:44.616: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:44.616: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:44.616: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:44.640: INFO: Number of nodes with available pods: 3
Aug  6 15:36:44.640: INFO: Node ip-10-0-137-62.ec2.internal is running more than one daemon pod
Aug  6 15:36:45.616: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:45.617: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:45.617: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:45.640: INFO: Number of nodes with available pods: 3
Aug  6 15:36:45.640: INFO: Node ip-10-0-137-62.ec2.internal is running more than one daemon pod
Aug  6 15:36:46.616: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:46.616: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:46.616: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:36:46.639: INFO: Number of nodes with available pods: 4
Aug  6 15:36:46.639: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-hs7jb, will wait for the garbage collector to delete the pods
Aug  6 15:36:46.783: INFO: Deleting DaemonSet.extensions daemon-set took: 25.976388ms
Aug  6 15:36:46.884: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.495135ms
Aug  6 15:36:59.407: INFO: Number of nodes with available pods: 0
Aug  6 15:36:59.407: INFO: Number of running nodes: 0, number of available pods: 0
Aug  6 15:36:59.431: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-hs7jb/daemonsets","resourceVersion":"108729"},"items":null}

Aug  6 15:36:59.453: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-hs7jb/pods","resourceVersion":"108729"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:36:59.584: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-hs7jb" for this suite.
Aug  6 15:37:05.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:37:06.356: INFO: namespace: e2e-tests-daemonsets-hs7jb, resource: packagemanifests, items remaining: 3
Aug  6 15:37:06.533: INFO: namespace: e2e-tests-daemonsets-hs7jb, resource: bindings, ignored listing per whitelist
Aug  6 15:37:08.146: INFO: namespace: e2e-tests-daemonsets-hs7jb no longer exists
Aug  6 15:37:08.171: INFO: namespace: e2e-tests-daemonsets-hs7jb, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:37:08.193: INFO: namespace e2e-tests-daemonsets-hs7jb deletion completed in 8.566497289s

• [SLOW TEST:42.561 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:37:08.195: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-0d9331e5-b860-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 15:37:09.616: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0d96bd74-b860-11e9-8d18-525400524259" in namespace "e2e-tests-projected-fkx6p" to be "success or failure"
Aug  6 15:37:09.638: INFO: Pod "pod-projected-secrets-0d96bd74-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.218972ms
Aug  6 15:37:11.661: INFO: Pod "pod-projected-secrets-0d96bd74-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045290507s
Aug  6 15:37:13.685: INFO: Pod "pod-projected-secrets-0d96bd74-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068399946s
Aug  6 15:37:15.708: INFO: Pod "pod-projected-secrets-0d96bd74-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091651717s
Aug  6 15:37:17.735: INFO: Pod "pod-projected-secrets-0d96bd74-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.119239772s
Aug  6 15:37:19.759: INFO: Pod "pod-projected-secrets-0d96bd74-b860-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.143030317s
STEP: Saw pod success
Aug  6 15:37:19.759: INFO: Pod "pod-projected-secrets-0d96bd74-b860-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:37:19.782: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-secrets-0d96bd74-b860-11e9-8d18-525400524259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug  6 15:37:19.852: INFO: Waiting for pod pod-projected-secrets-0d96bd74-b860-11e9-8d18-525400524259 to disappear
Aug  6 15:37:19.874: INFO: Pod pod-projected-secrets-0d96bd74-b860-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:37:19.874: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fkx6p" for this suite.
Aug  6 15:37:26.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:37:27.271: INFO: namespace: e2e-tests-projected-fkx6p, resource: bindings, ignored listing per whitelist
Aug  6 15:37:27.408: INFO: namespace: e2e-tests-projected-fkx6p, resource: packagemanifests, items remaining: 3
Aug  6 15:37:28.425: INFO: namespace: e2e-tests-projected-fkx6p no longer exists
Aug  6 15:37:28.448: INFO: namespace: e2e-tests-projected-fkx6p, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:37:28.471: INFO: namespace e2e-tests-projected-fkx6p deletion completed in 8.532662577s

• [SLOW TEST:20.276 seconds]
[sig-storage] Projected secret
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:37:28.471: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 15:37:29.859: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19a75751-b860-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-59k5q" to be "success or failure"
Aug  6 15:37:29.882: INFO: Pod "downwardapi-volume-19a75751-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.49095ms
Aug  6 15:37:31.906: INFO: Pod "downwardapi-volume-19a75751-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047443654s
Aug  6 15:37:33.929: INFO: Pod "downwardapi-volume-19a75751-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070255809s
Aug  6 15:37:35.952: INFO: Pod "downwardapi-volume-19a75751-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093337392s
Aug  6 15:37:37.979: INFO: Pod "downwardapi-volume-19a75751-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.119640391s
Aug  6 15:37:40.001: INFO: Pod "downwardapi-volume-19a75751-b860-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.142406355s
STEP: Saw pod success
Aug  6 15:37:40.001: INFO: Pod "downwardapi-volume-19a75751-b860-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:37:40.024: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-19a75751-b860-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 15:37:40.080: INFO: Waiting for pod downwardapi-volume-19a75751-b860-11e9-8d18-525400524259 to disappear
Aug  6 15:37:40.102: INFO: Pod downwardapi-volume-19a75751-b860-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:37:40.102: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-59k5q" for this suite.
Aug  6 15:37:46.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:37:48.583: INFO: namespace: e2e-tests-downward-api-59k5q, resource: packagemanifests, items remaining: 3
Aug  6 15:37:48.583: INFO: namespace: e2e-tests-downward-api-59k5q, resource: bindings, ignored listing per whitelist
Aug  6 15:37:48.651: INFO: namespace: e2e-tests-downward-api-59k5q no longer exists
Aug  6 15:37:48.673: INFO: namespace: e2e-tests-downward-api-59k5q, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:37:48.696: INFO: namespace e2e-tests-downward-api-59k5q deletion completed in 8.53114196s

• [SLOW TEST:20.225 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:37:48.696: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:37:50.124: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-l6qgr" for this suite.
Aug  6 15:38:12.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:38:12.925: INFO: namespace: e2e-tests-pods-l6qgr, resource: bindings, ignored listing per whitelist
Aug  6 15:38:13.598: INFO: namespace: e2e-tests-pods-l6qgr, resource: packagemanifests, items remaining: 3
Aug  6 15:38:14.659: INFO: namespace: e2e-tests-pods-l6qgr no longer exists
Aug  6 15:38:14.682: INFO: namespace: e2e-tests-pods-l6qgr, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:38:14.705: INFO: namespace e2e-tests-pods-l6qgr deletion completed in 24.537312292s

• [SLOW TEST:26.009 seconds]
[k8s.io] [sig-node] Pods Extended
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:38:14.706: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug  6 15:38:16.223: INFO: Waiting up to 5m0s for pod "pod-354a2e2e-b860-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-jl5dl" to be "success or failure"
Aug  6 15:38:16.245: INFO: Pod "pod-354a2e2e-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 21.925005ms
Aug  6 15:38:18.268: INFO: Pod "pod-354a2e2e-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045069916s
Aug  6 15:38:20.291: INFO: Pod "pod-354a2e2e-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068370194s
Aug  6 15:38:22.314: INFO: Pod "pod-354a2e2e-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091129628s
Aug  6 15:38:24.337: INFO: Pod "pod-354a2e2e-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114118584s
Aug  6 15:38:26.360: INFO: Pod "pod-354a2e2e-b860-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137310741s
STEP: Saw pod success
Aug  6 15:38:26.360: INFO: Pod "pod-354a2e2e-b860-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:38:26.385: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-354a2e2e-b860-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 15:38:26.503: INFO: Waiting for pod pod-354a2e2e-b860-11e9-8d18-525400524259 to disappear
Aug  6 15:38:26.526: INFO: Pod pod-354a2e2e-b860-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:38:26.526: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jl5dl" for this suite.
Aug  6 15:38:32.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:38:33.419: INFO: namespace: e2e-tests-emptydir-jl5dl, resource: bindings, ignored listing per whitelist
Aug  6 15:38:34.111: INFO: namespace: e2e-tests-emptydir-jl5dl, resource: packagemanifests, items remaining: 3
Aug  6 15:38:35.095: INFO: namespace: e2e-tests-emptydir-jl5dl no longer exists
Aug  6 15:38:35.119: INFO: namespace: e2e-tests-emptydir-jl5dl, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:38:35.141: INFO: namespace e2e-tests-emptydir-jl5dl deletion completed in 8.550741362s

• [SLOW TEST:20.436 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:38:35.142: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-n7kr
STEP: Creating a pod to test atomic-volume-subpath
Aug  6 15:38:36.583: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-n7kr" in namespace "e2e-tests-subpath-br2s2" to be "success or failure"
Aug  6 15:38:36.605: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Pending", Reason="", readiness=false. Elapsed: 21.948082ms
Aug  6 15:38:38.629: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045821133s
Aug  6 15:38:40.652: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068849305s
Aug  6 15:38:42.676: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092775636s
Aug  6 15:38:44.699: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116476361s
Aug  6 15:38:46.722: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Running", Reason="", readiness=false. Elapsed: 10.139389048s
Aug  6 15:38:48.747: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Running", Reason="", readiness=false. Elapsed: 12.163746583s
Aug  6 15:38:50.770: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Running", Reason="", readiness=false. Elapsed: 14.187182951s
Aug  6 15:38:52.793: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Running", Reason="", readiness=false. Elapsed: 16.210418954s
Aug  6 15:38:54.816: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Running", Reason="", readiness=false. Elapsed: 18.233461206s
Aug  6 15:38:56.840: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Running", Reason="", readiness=false. Elapsed: 20.256573539s
Aug  6 15:38:58.862: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Running", Reason="", readiness=false. Elapsed: 22.279258741s
Aug  6 15:39:00.885: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Running", Reason="", readiness=false. Elapsed: 24.302392548s
Aug  6 15:39:02.909: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Running", Reason="", readiness=false. Elapsed: 26.325655905s
Aug  6 15:39:04.932: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Running", Reason="", readiness=false. Elapsed: 28.348842628s
Aug  6 15:39:06.955: INFO: Pod "pod-subpath-test-downwardapi-n7kr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.37159792s
STEP: Saw pod success
Aug  6 15:39:06.955: INFO: Pod "pod-subpath-test-downwardapi-n7kr" satisfied condition "success or failure"
Aug  6 15:39:06.977: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-subpath-test-downwardapi-n7kr container test-container-subpath-downwardapi-n7kr: <nil>
STEP: delete the pod
Aug  6 15:39:07.031: INFO: Waiting for pod pod-subpath-test-downwardapi-n7kr to disappear
Aug  6 15:39:07.053: INFO: Pod pod-subpath-test-downwardapi-n7kr no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-n7kr
Aug  6 15:39:07.053: INFO: Deleting pod "pod-subpath-test-downwardapi-n7kr" in namespace "e2e-tests-subpath-br2s2"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:39:07.075: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-br2s2" for this suite.
Aug  6 15:39:13.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:39:14.102: INFO: namespace: e2e-tests-subpath-br2s2, resource: packagemanifests, items remaining: 3
Aug  6 15:39:15.522: INFO: namespace: e2e-tests-subpath-br2s2, resource: bindings, ignored listing per whitelist
Aug  6 15:39:15.633: INFO: namespace: e2e-tests-subpath-br2s2 no longer exists
Aug  6 15:39:15.656: INFO: namespace: e2e-tests-subpath-br2s2, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:39:15.678: INFO: namespace e2e-tests-subpath-br2s2 deletion completed in 8.539728114s

• [SLOW TEST:40.536 seconds]
[sig-storage] Subpath
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:39:15.679: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-59961639-b860-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 15:39:17.144: INFO: Waiting up to 5m0s for pod "pod-configmaps-5999a734-b860-11e9-8d18-525400524259" in namespace "e2e-tests-configmap-bc85f" to be "success or failure"
Aug  6 15:39:17.167: INFO: Pod "pod-configmaps-5999a734-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.79165ms
Aug  6 15:39:19.190: INFO: Pod "pod-configmaps-5999a734-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045783167s
Aug  6 15:39:21.216: INFO: Pod "pod-configmaps-5999a734-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07180556s
Aug  6 15:39:23.240: INFO: Pod "pod-configmaps-5999a734-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09564273s
Aug  6 15:39:25.264: INFO: Pod "pod-configmaps-5999a734-b860-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.120360995s
STEP: Saw pod success
Aug  6 15:39:25.264: INFO: Pod "pod-configmaps-5999a734-b860-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:39:25.288: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-configmaps-5999a734-b860-11e9-8d18-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 15:39:25.347: INFO: Waiting for pod pod-configmaps-5999a734-b860-11e9-8d18-525400524259 to disappear
Aug  6 15:39:25.370: INFO: Pod pod-configmaps-5999a734-b860-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:39:25.370: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bc85f" for this suite.
Aug  6 15:39:31.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:39:32.503: INFO: namespace: e2e-tests-configmap-bc85f, resource: bindings, ignored listing per whitelist
Aug  6 15:39:33.264: INFO: namespace: e2e-tests-configmap-bc85f, resource: packagemanifests, items remaining: 3
Aug  6 15:39:33.925: INFO: namespace: e2e-tests-configmap-bc85f no longer exists
Aug  6 15:39:33.968: INFO: namespace: e2e-tests-configmap-bc85f, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:39:33.991: INFO: namespace e2e-tests-configmap-bc85f deletion completed in 8.557271404s

• [SLOW TEST:18.311 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:39:33.991: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 15:39:35.388: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:39:36.100: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-l24kh" for this suite.
Aug  6 15:39:42.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:39:43.066: INFO: namespace: e2e-tests-custom-resource-definition-l24kh, resource: packagemanifests, items remaining: 3
Aug  6 15:39:44.529: INFO: namespace: e2e-tests-custom-resource-definition-l24kh, resource: bindings, ignored listing per whitelist
Aug  6 15:39:44.707: INFO: namespace: e2e-tests-custom-resource-definition-l24kh no longer exists
Aug  6 15:39:44.750: INFO: namespace: e2e-tests-custom-resource-definition-l24kh, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:39:44.773: INFO: namespace e2e-tests-custom-resource-definition-l24kh deletion completed in 8.607598982s

• [SLOW TEST:10.782 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:39:44.774: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug  6 15:39:46.167: INFO: Waiting up to 5m0s for pod "pod-6ae668a4-b860-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-jr4gr" to be "success or failure"
Aug  6 15:39:46.190: INFO: Pod "pod-6ae668a4-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.043875ms
Aug  6 15:39:48.218: INFO: Pod "pod-6ae668a4-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050546878s
Aug  6 15:39:50.241: INFO: Pod "pod-6ae668a4-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073792345s
Aug  6 15:39:52.266: INFO: Pod "pod-6ae668a4-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098874542s
Aug  6 15:39:54.290: INFO: Pod "pod-6ae668a4-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.122251284s
Aug  6 15:39:56.344: INFO: Pod "pod-6ae668a4-b860-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.176693786s
STEP: Saw pod success
Aug  6 15:39:56.344: INFO: Pod "pod-6ae668a4-b860-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:39:56.367: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-6ae668a4-b860-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 15:39:56.425: INFO: Waiting for pod pod-6ae668a4-b860-11e9-8d18-525400524259 to disappear
Aug  6 15:39:56.447: INFO: Pod pod-6ae668a4-b860-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:39:56.447: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jr4gr" for this suite.
Aug  6 15:40:02.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:40:03.942: INFO: namespace: e2e-tests-emptydir-jr4gr, resource: packagemanifests, items remaining: 3
Aug  6 15:40:04.445: INFO: namespace: e2e-tests-emptydir-jr4gr, resource: bindings, ignored listing per whitelist
Aug  6 15:40:05.192: INFO: namespace: e2e-tests-emptydir-jr4gr no longer exists
Aug  6 15:40:05.294: INFO: namespace: e2e-tests-emptydir-jr4gr, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:40:05.316: INFO: namespace e2e-tests-emptydir-jr4gr deletion completed in 8.764865738s

• [SLOW TEST:20.543 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:40:05.317: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  6 15:40:06.762: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-kdvrt'
Aug  6 15:40:10.135: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug  6 15:40:10.135: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Aug  6 15:40:12.182: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-kdvrt'
Aug  6 15:40:12.373: INFO: stderr: ""
Aug  6 15:40:12.373: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:40:12.373: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-kdvrt" for this suite.
Aug  6 15:40:34.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:40:35.422: INFO: namespace: e2e-tests-kubectl-kdvrt, resource: packagemanifests, items remaining: 3
Aug  6 15:40:36.176: INFO: namespace: e2e-tests-kubectl-kdvrt, resource: bindings, ignored listing per whitelist
Aug  6 15:40:36.981: INFO: namespace: e2e-tests-kubectl-kdvrt no longer exists
Aug  6 15:40:37.025: INFO: namespace: e2e-tests-kubectl-kdvrt, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:40:37.047: INFO: namespace e2e-tests-kubectl-kdvrt deletion completed in 24.570147258s

• [SLOW TEST:31.730 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:40:37.047: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Aug  6 15:40:38.580: INFO: Waiting up to 5m0s for pod "var-expansion-8a23f4c6-b860-11e9-8d18-525400524259" in namespace "e2e-tests-var-expansion-pw7xx" to be "success or failure"
Aug  6 15:40:38.604: INFO: Pod "var-expansion-8a23f4c6-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006692ms
Aug  6 15:40:40.629: INFO: Pod "var-expansion-8a23f4c6-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048966088s
Aug  6 15:40:42.652: INFO: Pod "var-expansion-8a23f4c6-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072067522s
Aug  6 15:40:44.676: INFO: Pod "var-expansion-8a23f4c6-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096386546s
Aug  6 15:40:46.699: INFO: Pod "var-expansion-8a23f4c6-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.119435411s
Aug  6 15:40:48.724: INFO: Pod "var-expansion-8a23f4c6-b860-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.144142887s
STEP: Saw pod success
Aug  6 15:40:48.724: INFO: Pod "var-expansion-8a23f4c6-b860-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:40:48.746: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod var-expansion-8a23f4c6-b860-11e9-8d18-525400524259 container dapi-container: <nil>
STEP: delete the pod
Aug  6 15:40:48.812: INFO: Waiting for pod var-expansion-8a23f4c6-b860-11e9-8d18-525400524259 to disappear
Aug  6 15:40:48.834: INFO: Pod var-expansion-8a23f4c6-b860-11e9-8d18-525400524259 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:40:48.834: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-pw7xx" for this suite.
Aug  6 15:40:54.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:40:56.417: INFO: namespace: e2e-tests-var-expansion-pw7xx, resource: packagemanifests, items remaining: 3
Aug  6 15:40:57.104: INFO: namespace: e2e-tests-var-expansion-pw7xx, resource: bindings, ignored listing per whitelist
Aug  6 15:40:57.416: INFO: namespace: e2e-tests-var-expansion-pw7xx no longer exists
Aug  6 15:40:57.461: INFO: namespace: e2e-tests-var-expansion-pw7xx, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:40:57.483: INFO: namespace e2e-tests-var-expansion-pw7xx deletion completed in 8.564919907s

• [SLOW TEST:20.436 seconds]
[k8s.io] Variable Expansion
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:40:57.484: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-mgdmz
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-mgdmz
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-mgdmz
Aug  6 15:40:58.931: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug  6 15:41:08.954: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug  6 15:41:08.977: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-mgdmz ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  6 15:41:09.352: INFO: stderr: ""
Aug  6 15:41:09.352: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  6 15:41:09.352: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  6 15:41:09.375: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug  6 15:41:19.399: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug  6 15:41:19.399: INFO: Waiting for statefulset status.replicas updated to 0
Aug  6 15:41:19.498: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999588s
Aug  6 15:41:20.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.972046406s
Aug  6 15:41:21.546: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.948154829s
Aug  6 15:41:22.569: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.924134465s
Aug  6 15:41:23.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.901244684s
Aug  6 15:41:24.616: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.877819749s
Aug  6 15:41:25.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.854431907s
Aug  6 15:41:26.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.831328098s
Aug  6 15:41:27.685: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.807920535s
Aug  6 15:41:28.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 785.00239ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-mgdmz
Aug  6 15:41:29.736: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-mgdmz ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  6 15:41:30.130: INFO: stderr: ""
Aug  6 15:41:30.130: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  6 15:41:30.130: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  6 15:41:30.130: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-mgdmz ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  6 15:41:30.495: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Aug  6 15:41:30.495: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  6 15:41:30.495: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  6 15:41:30.495: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-mgdmz ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  6 15:41:30.868: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Aug  6 15:41:30.868: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  6 15:41:30.868: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  6 15:41:30.891: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug  6 15:41:40.916: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 15:41:40.916: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 15:41:40.916: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug  6 15:41:40.939: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-mgdmz ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  6 15:41:41.415: INFO: stderr: ""
Aug  6 15:41:41.415: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  6 15:41:41.415: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  6 15:41:41.415: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-mgdmz ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  6 15:41:41.767: INFO: stderr: ""
Aug  6 15:41:41.767: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  6 15:41:41.767: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  6 15:41:41.767: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-mgdmz ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  6 15:41:42.124: INFO: stderr: ""
Aug  6 15:41:42.124: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  6 15:41:42.124: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  6 15:41:42.124: INFO: Waiting for statefulset status.replicas updated to 0
Aug  6 15:41:42.147: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug  6 15:41:52.193: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug  6 15:41:52.193: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug  6 15:41:52.193: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug  6 15:41:52.263: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug  6 15:41:52.263: INFO: ss-0  ip-10-0-130-134.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:36 +0000 UTC  }]
Aug  6 15:41:52.263: INFO: ss-1  ip-10-0-137-62.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:56 +0000 UTC  }]
Aug  6 15:41:52.263: INFO: ss-2  ip-10-0-128-44.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:56 +0000 UTC  }]
Aug  6 15:41:52.263: INFO: 
Aug  6 15:41:52.263: INFO: StatefulSet ss has not reached scale 0, at 3
Aug  6 15:41:53.287: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug  6 15:41:53.287: INFO: ss-0  ip-10-0-130-134.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:36 +0000 UTC  }]
Aug  6 15:41:53.287: INFO: ss-1  ip-10-0-137-62.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:56 +0000 UTC  }]
Aug  6 15:41:53.287: INFO: ss-2  ip-10-0-128-44.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:41:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:40:56 +0000 UTC  }]
Aug  6 15:41:53.287: INFO: 
Aug  6 15:41:53.287: INFO: StatefulSet ss has not reached scale 0, at 3
Aug  6 15:41:54.310: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.950798431s
Aug  6 15:41:55.334: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.927769275s
Aug  6 15:41:56.356: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.904682113s
Aug  6 15:41:57.379: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.881905269s
Aug  6 15:41:58.402: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.858961256s
Aug  6 15:41:59.425: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.836046638s
Aug  6 15:42:00.448: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.81294749s
Aug  6 15:42:01.471: INFO: Verifying statefulset ss doesn't scale past 0 for another 790.159433ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-mgdmz
Aug  6 15:42:02.494: INFO: Scaling statefulset ss to 0
Aug  6 15:42:02.561: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug  6 15:42:02.583: INFO: Deleting all statefulset in ns e2e-tests-statefulset-mgdmz
Aug  6 15:42:02.605: INFO: Scaling statefulset ss to 0
Aug  6 15:42:02.672: INFO: Waiting for statefulset status.replicas updated to 0
Aug  6 15:42:02.694: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:42:02.763: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-mgdmz" for this suite.
Aug  6 15:42:08.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:42:10.129: INFO: namespace: e2e-tests-statefulset-mgdmz, resource: bindings, ignored listing per whitelist
Aug  6 15:42:10.490: INFO: namespace: e2e-tests-statefulset-mgdmz, resource: packagemanifests, items remaining: 3
Aug  6 15:42:11.309: INFO: namespace: e2e-tests-statefulset-mgdmz no longer exists
Aug  6 15:42:11.332: INFO: namespace: e2e-tests-statefulset-mgdmz, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:42:11.354: INFO: namespace e2e-tests-statefulset-mgdmz deletion completed in 8.528821924s

• [SLOW TEST:73.871 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:42:11.356: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-rwxbf
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug  6 15:42:12.725: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Aug  6 15:42:45.377: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.23:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-rwxbf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:42:45.378: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:42:45.634: INFO: Found all expected endpoints: [netserver-0]
Aug  6 15:42:45.657: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.130.2.22:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-rwxbf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:42:45.657: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:42:45.872: INFO: Found all expected endpoints: [netserver-1]
Aug  6 15:42:45.894: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.44:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-rwxbf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:42:45.894: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:42:46.092: INFO: Found all expected endpoints: [netserver-2]
Aug  6 15:42:46.115: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.0.99:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-rwxbf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 15:42:46.115: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 15:42:46.326: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:42:46.326: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-rwxbf" for this suite.
Aug  6 15:43:08.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:43:09.871: INFO: namespace: e2e-tests-pod-network-test-rwxbf, resource: packagemanifests, items remaining: 3
Aug  6 15:43:10.546: INFO: namespace: e2e-tests-pod-network-test-rwxbf, resource: bindings, ignored listing per whitelist
Aug  6 15:43:10.903: INFO: namespace: e2e-tests-pod-network-test-rwxbf no longer exists
Aug  6 15:43:10.926: INFO: namespace: e2e-tests-pod-network-test-rwxbf, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:43:10.948: INFO: namespace e2e-tests-pod-network-test-rwxbf deletion completed in 24.559156838s

• [SLOW TEST:59.593 seconds]
[sig-network] Networking
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:43:10.948: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 15:43:12.321: INFO: Creating deployment "nginx-deployment"
Aug  6 15:43:12.586: INFO: Waiting for observed generation 1
Aug  6 15:43:12.609: INFO: Waiting for all required pods to come up
Aug  6 15:43:12.632: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug  6 15:43:22.702: INFO: Waiting for deployment "nginx-deployment" to complete
Aug  6 15:43:22.748: INFO: Updating deployment "nginx-deployment" with a non-existent image
Aug  6 15:43:22.805: INFO: Updating deployment nginx-deployment
Aug  6 15:43:22.805: INFO: Waiting for observed generation 2
Aug  6 15:43:22.831: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug  6 15:43:22.853: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug  6 15:43:22.875: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug  6 15:43:22.942: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug  6 15:43:22.942: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug  6 15:43:22.963: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug  6 15:43:23.008: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Aug  6 15:43:23.008: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Aug  6 15:43:23.053: INFO: Updating deployment nginx-deployment
Aug  6 15:43:23.053: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Aug  6 15:43:23.099: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug  6 15:43:25.150: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug  6 15:43:25.194: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-5c7mt/deployments/nginx-deployment,UID:d858e356-b860-11e9-9dff-0e6de702691c,ResourceVersion:114030,Generation:3,CreationTimestamp:2019-08-06 15:42:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-08-06 15:43:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-06 15:43:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-77544b8d75" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Aug  6 15:43:25.216: INFO: New ReplicaSet "nginx-deployment-77544b8d75" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75,GenerateName:,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-5c7mt/replicasets/nginx-deployment-77544b8d75,UID:de958dc4-b860-11e9-9dff-0e6de702691c,ResourceVersion:114029,Generation:3,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment d858e356-b860-11e9-9dff-0e6de702691c 0xc00164bed7 0xc00164bed8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  6 15:43:25.216: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Aug  6 15:43:25.216: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b,GenerateName:,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-5c7mt/replicasets/nginx-deployment-5879655c7b,UID:d8595bc6-b860-11e9-9dff-0e6de702691c,ResourceVersion:113989,Generation:3,CreationTimestamp:2019-08-06 15:42:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment d858e356-b860-11e9-9dff-0e6de702691c 0xc00164bd87 0xc00164bd88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Aug  6 15:43:25.281: INFO: Pod "nginx-deployment-5879655c7b-22kz4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-22kz4,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-22kz4,UID:d85fc1bd-b860-11e9-9dff-0e6de702691c,ResourceVersion:113788,Generation:0,CreationTimestamp:2019-08-06 15:42:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.45"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc002535837 0xc002535838}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025358a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002535e10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  }],Message:,Reason:,HostIP:10.0.128.44,PodIP:10.129.2.45,StartTime:2019-08-06 15:42:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-06 15:42:57 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://03cf93a4512ea45e3b84ccfda631bc96220782fac16d9817ec0e4aaf3dbe2328}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.281: INFO: Pod "nginx-deployment-5879655c7b-2bhdw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-2bhdw,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-2bhdw,UID:debc87e5-b860-11e9-9dff-0e6de702691c,ResourceVersion:113942,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc002566620 0xc002566621}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-96.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002566700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002566720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.96,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.281: INFO: Pod "nginx-deployment-5879655c7b-6dcv2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-6dcv2,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-6dcv2,UID:d8630814-b860-11e9-9dff-0e6de702691c,ResourceVersion:113844,Generation:0,CreationTimestamp:2019-08-06 15:42:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.47"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc0025668c0 0xc0025668c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002566930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002566950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  }],Message:,Reason:,HostIP:10.0.128.44,PodIP:10.129.2.47,StartTime:2019-08-06 15:42:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-06 15:42:57 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://d1dcaf84b1721c0cd49316c79565a3c6c8b26f190c6deb5ef96e0f84f544ee88}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.281: INFO: Pod "nginx-deployment-5879655c7b-94cth" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-94cth,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-94cth,UID:dec52155-b860-11e9-9dff-0e6de702691c,ResourceVersion:113993,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc002566be0 0xc002566be1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc002566c50} {node.kubernetes.io/not-ready Exists  NoExecute 0xc002566c70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.282: INFO: Pod "nginx-deployment-5879655c7b-9b8r5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-9b8r5,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-9b8r5,UID:debeaae6-b860-11e9-9dff-0e6de702691c,ResourceVersion:113950,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc002566e67 0xc002566e68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-96.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc002566ed0} {node.kubernetes.io/not-ready Exists  NoExecute 0xc002566ef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.96,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.282: INFO: Pod "nginx-deployment-5879655c7b-cz99s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-cz99s,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-cz99s,UID:dec14bc6-b860-11e9-9dff-0e6de702691c,ResourceVersion:113968,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc002567010 0xc002567011}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-62.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002567080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025670b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.137.62,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.282: INFO: Pod "nginx-deployment-5879655c7b-dkrkh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-dkrkh,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-dkrkh,UID:dec576b1-b860-11e9-9dff-0e6de702691c,ResourceVersion:114003,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc00351e080 0xc00351e081}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00351e0f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00351e110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.282: INFO: Pod "nginx-deployment-5879655c7b-ht882" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-ht882,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-ht882,UID:d85ce8d4-b860-11e9-9dff-0e6de702691c,ResourceVersion:113837,Generation:0,CreationTimestamp:2019-08-06 15:42:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.25"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc00351e2d7 0xc00351e2d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-96.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00351e340} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00351e4f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.96,PodIP:10.128.2.25,StartTime:2019-08-06 15:42:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-06 15:42:57 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://01ebd331a624222c1daa28c5cb45e9a0e6a84cae86f779ff93ba97556a2f6548}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.282: INFO: Pod "nginx-deployment-5879655c7b-lbbvs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-lbbvs,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-lbbvs,UID:d85fb803-b860-11e9-9dff-0e6de702691c,ResourceVersion:113827,Generation:0,CreationTimestamp:2019-08-06 15:42:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.103"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc00351e5d0 0xc00351e5d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00351e6e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00351e700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:10.131.0.103,StartTime:2019-08-06 15:42:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-06 15:42:57 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://380964b9fe6a18ebde9d6b568ec3a7f18ed35f446b47fddffa6150b4c7c13d82}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.282: INFO: Pod "nginx-deployment-5879655c7b-mn2wn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-mn2wn,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-mn2wn,UID:dec18c25-b860-11e9-9dff-0e6de702691c,ResourceVersion:113974,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc00351e7e7 0xc00351e7e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00351e9a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00351e9c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.282: INFO: Pod "nginx-deployment-5879655c7b-n4ltk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-n4ltk,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-n4ltk,UID:debf0a9f-b860-11e9-9dff-0e6de702691c,ResourceVersion:113958,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc00351ea97 0xc00351ea98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00351eb00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00351eb20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.128.44,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.282: INFO: Pod "nginx-deployment-5879655c7b-njj8x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-njj8x,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-njj8x,UID:d86297bb-b860-11e9-9dff-0e6de702691c,ResourceVersion:113851,Generation:0,CreationTimestamp:2019-08-06 15:42:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.130.2.24"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc00351ebf0 0xc00351ebf1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-62.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00351ec60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00351ec80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  }],Message:,Reason:,HostIP:10.0.137.62,PodIP:10.130.2.24,StartTime:2019-08-06 15:42:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-06 15:42:58 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://4cb742dcaae2cd600d2c397eb5e850de0e55a65f3ddc9eb0165c0f096470612f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.283: INFO: Pod "nginx-deployment-5879655c7b-p79l7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-p79l7,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-p79l7,UID:d85cd197-b860-11e9-9dff-0e6de702691c,ResourceVersion:113854,Generation:0,CreationTimestamp:2019-08-06 15:42:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.130.2.23"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc00351ed60 0xc00351ed61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-62.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00351edd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00351edf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  }],Message:,Reason:,HostIP:10.0.137.62,PodIP:10.130.2.23,StartTime:2019-08-06 15:42:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-06 15:42:57 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://47b5af7c412ee44e077948f7a5498b35e6fcb0e8ed6e1f282ffc91e791143361}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.283: INFO: Pod "nginx-deployment-5879655c7b-qlvlj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-qlvlj,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-qlvlj,UID:dec57bce-b860-11e9-9dff-0e6de702691c,ResourceVersion:114042,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc00351eed0 0xc00351eed1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00351ef40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00351ef60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.283: INFO: Pod "nginx-deployment-5879655c7b-s7g87" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-s7g87,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-s7g87,UID:dec550d9-b860-11e9-9dff-0e6de702691c,ResourceVersion:114053,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc00351f037 0xc00351f038}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-96.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00351f280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00351f2a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.96,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.283: INFO: Pod "nginx-deployment-5879655c7b-sl29w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-sl29w,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-sl29w,UID:dec55792-b860-11e9-9dff-0e6de702691c,ResourceVersion:114031,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc00351f4c0 0xc00351f4c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-96.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00351f6d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00351f6f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.96,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.284: INFO: Pod "nginx-deployment-5879655c7b-tchsx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-tchsx,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-tchsx,UID:d85acd86-b860-11e9-9dff-0e6de702691c,ResourceVersion:113832,Generation:0,CreationTimestamp:2019-08-06 15:42:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.101"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc00351f7c0 0xc00351f7c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed4020} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed4040}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:10.131.0.101,StartTime:2019-08-06 15:42:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-06 15:42:57 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://b05633656fcd7a8bd696058b12ca7978acf0521121d81b27a4f0ce1291a8eb99}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.284: INFO: Pod "nginx-deployment-5879655c7b-tq6fv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-tq6fv,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-tq6fv,UID:d85fba15-b860-11e9-9dff-0e6de702691c,ResourceVersion:113830,Generation:0,CreationTimestamp:2019-08-06 15:42:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.102"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc002ed4717 0xc002ed4718}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed4780} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed47a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:42:49 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:10.131.0.102,StartTime:2019-08-06 15:42:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-06 15:42:58 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://52166967f52b0daec2010b73e3c66f7245c62ae937d30f2e5ec12b1ef9fa57ec}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.284: INFO: Pod "nginx-deployment-5879655c7b-tqvlg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-tqvlg,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-tqvlg,UID:dec1aff5-b860-11e9-9dff-0e6de702691c,ResourceVersion:113967,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc002ed48f7 0xc002ed48f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-96.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed4960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed4980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.96,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.284: INFO: Pod "nginx-deployment-5879655c7b-xdl99" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5879655c7b-xdl99,GenerateName:nginx-deployment-5879655c7b-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-5879655c7b-xdl99,UID:dec18564-b860-11e9-9dff-0e6de702691c,ResourceVersion:114054,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5879655c7b,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5879655c7b d8595bc6-b860-11e9-9dff-0e6de702691c 0xc002ed4af0 0xc002ed4af1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed4b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed4b90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.128.44,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.284: INFO: Pod "nginx-deployment-77544b8d75-22mvn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-22mvn,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-22mvn,UID:ded01f11-b860-11e9-9dff-0e6de702691c,ResourceVersion:114023,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002ed4e20 0xc002ed4e21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-62.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed4eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed50f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.137.62,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.284: INFO: Pod "nginx-deployment-77544b8d75-28fjp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-28fjp,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-28fjp,UID:dea2bb13-b860-11e9-9dff-0e6de702691c,ResourceVersion:113918,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002ed51d0 0xc002ed51d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed52b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed52d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.128.44,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.284: INFO: Pod "nginx-deployment-77544b8d75-2d45d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-2d45d,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-2d45d,UID:dea5e00c-b860-11e9-9dff-0e6de702691c,ResourceVersion:113926,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002ed53b0 0xc002ed53b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed5490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed54b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.284: INFO: Pod "nginx-deployment-77544b8d75-2wf9m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-2wf9m,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-2wf9m,UID:ded04b34-b860-11e9-9dff-0e6de702691c,ResourceVersion:114032,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002ed5590 0xc002ed5591}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-62.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed5620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed5640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.137.62,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.284: INFO: Pod "nginx-deployment-77544b8d75-82fzs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-82fzs,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-82fzs,UID:dec79bbc-b860-11e9-9dff-0e6de702691c,ResourceVersion:114059,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002ed5730 0xc002ed5731}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed57b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed57d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.285: INFO: Pod "nginx-deployment-77544b8d75-8chq7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-8chq7,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-8chq7,UID:de9cbd99-b860-11e9-9dff-0e6de702691c,ResourceVersion:113911,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002ed58b0 0xc002ed58b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed5950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed5970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.285: INFO: Pod "nginx-deployment-77544b8d75-8s7jr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-8s7jr,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-8s7jr,UID:ded0306a-b860-11e9-9dff-0e6de702691c,ResourceVersion:114013,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002ed5a70 0xc002ed5a71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed5b00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed5b20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.285: INFO: Pod "nginx-deployment-77544b8d75-bdkkk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-bdkkk,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-bdkkk,UID:de9d6b28-b860-11e9-9dff-0e6de702691c,ResourceVersion:113912,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002ed5bb0 0xc002ed5bb1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-62.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed5c30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed5c50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.137.62,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.285: INFO: Pod "nginx-deployment-77544b8d75-gjlhq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-gjlhq,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-gjlhq,UID:debf0d4d-b860-11e9-9dff-0e6de702691c,ResourceVersion:114037,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002ed5d50 0xc002ed5d51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed5e00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed5e20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.128.44,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.285: INFO: Pod "nginx-deployment-77544b8d75-gt59f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-gt59f,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-gt59f,UID:ded05d38-b860-11e9-9dff-0e6de702691c,ResourceVersion:114014,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002ed5f00 0xc002ed5f01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ed5f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ed5fa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.285: INFO: Pod "nginx-deployment-77544b8d75-mmmgr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-mmmgr,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-mmmgr,UID:dec776ef-b860-11e9-9dff-0e6de702691c,ResourceVersion:113994,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002d46310 0xc002d46311}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-96.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d46390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d463b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.285: INFO: Pod "nginx-deployment-77544b8d75-nrdw4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-nrdw4,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-nrdw4,UID:ded4b3a7-b860-11e9-9dff-0e6de702691c,ResourceVersion:114025,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002d46550 0xc002d46551}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-96.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d46610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d46630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  6 15:43:25.285: INFO: Pod "nginx-deployment-77544b8d75-pvqhw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-77544b8d75-pvqhw,GenerateName:nginx-deployment-77544b8d75-,Namespace:e2e-tests-deployment-5c7mt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5c7mt/pods/nginx-deployment-77544b8d75-pvqhw,UID:de97ccd8-b860-11e9-9dff-0e6de702691c,ResourceVersion:113900,Generation:0,CreationTimestamp:2019-08-06 15:43:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 77544b8d75,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-77544b8d75 de958dc4-b860-11e9-9dff-0e6de702691c 0xc002d467a0 0xc002d467a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kjpv9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kjpv9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kjpv9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-96.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-z6b5v}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d46820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d46840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:43:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.96,PodIP:,StartTime:2019-08-06 15:43:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:43:25.285: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-5c7mt" for this suite.
Aug  6 15:43:33.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:43:34.260: INFO: namespace: e2e-tests-deployment-5c7mt, resource: bindings, ignored listing per whitelist
Aug  6 15:43:35.248: INFO: namespace: e2e-tests-deployment-5c7mt, resource: packagemanifests, items remaining: 3
Aug  6 15:43:35.958: INFO: namespace: e2e-tests-deployment-5c7mt no longer exists
Aug  6 15:43:35.981: INFO: namespace: e2e-tests-deployment-5c7mt, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:43:36.003: INFO: namespace e2e-tests-deployment-5c7mt deletion completed in 10.654119187s

• [SLOW TEST:25.054 seconds]
[sig-apps] Deployment
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:43:36.003: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-f4b8cf17-b860-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 15:43:37.451: INFO: Waiting up to 5m0s for pod "pod-configmaps-f4c1865f-b860-11e9-8d18-525400524259" in namespace "e2e-tests-configmap-jtmqx" to be "success or failure"
Aug  6 15:43:37.474: INFO: Pod "pod-configmaps-f4c1865f-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.874ms
Aug  6 15:43:39.497: INFO: Pod "pod-configmaps-f4c1865f-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045763975s
Aug  6 15:43:41.520: INFO: Pod "pod-configmaps-f4c1865f-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068790699s
Aug  6 15:43:43.543: INFO: Pod "pod-configmaps-f4c1865f-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091387803s
Aug  6 15:43:45.566: INFO: Pod "pod-configmaps-f4c1865f-b860-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114218945s
Aug  6 15:43:47.588: INFO: Pod "pod-configmaps-f4c1865f-b860-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137055623s
STEP: Saw pod success
Aug  6 15:43:47.588: INFO: Pod "pod-configmaps-f4c1865f-b860-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:43:47.611: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-configmaps-f4c1865f-b860-11e9-8d18-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 15:43:47.665: INFO: Waiting for pod pod-configmaps-f4c1865f-b860-11e9-8d18-525400524259 to disappear
Aug  6 15:43:47.688: INFO: Pod pod-configmaps-f4c1865f-b860-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:43:47.688: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jtmqx" for this suite.
Aug  6 15:43:53.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:43:54.983: INFO: namespace: e2e-tests-configmap-jtmqx, resource: packagemanifests, items remaining: 3
Aug  6 15:43:56.175: INFO: namespace: e2e-tests-configmap-jtmqx, resource: bindings, ignored listing per whitelist
Aug  6 15:43:56.241: INFO: namespace: e2e-tests-configmap-jtmqx no longer exists
Aug  6 15:43:56.263: INFO: namespace: e2e-tests-configmap-jtmqx, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:43:56.286: INFO: namespace e2e-tests-configmap-jtmqx deletion completed in 8.534993025s

• [SLOW TEST:20.283 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:43:56.287: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 15:43:57.661: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00cd3f68-b861-11e9-8d18-525400524259" in namespace "e2e-tests-projected-sdj5s" to be "success or failure"
Aug  6 15:43:57.683: INFO: Pod "downwardapi-volume-00cd3f68-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.048193ms
Aug  6 15:43:59.706: INFO: Pod "downwardapi-volume-00cd3f68-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044768448s
Aug  6 15:44:01.729: INFO: Pod "downwardapi-volume-00cd3f68-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067456018s
Aug  6 15:44:03.751: INFO: Pod "downwardapi-volume-00cd3f68-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090166131s
Aug  6 15:44:05.776: INFO: Pod "downwardapi-volume-00cd3f68-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114711037s
Aug  6 15:44:07.799: INFO: Pod "downwardapi-volume-00cd3f68-b861-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137525037s
STEP: Saw pod success
Aug  6 15:44:07.799: INFO: Pod "downwardapi-volume-00cd3f68-b861-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:44:07.821: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-00cd3f68-b861-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 15:44:07.876: INFO: Waiting for pod downwardapi-volume-00cd3f68-b861-11e9-8d18-525400524259 to disappear
Aug  6 15:44:07.898: INFO: Pod downwardapi-volume-00cd3f68-b861-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:44:07.898: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sdj5s" for this suite.
Aug  6 15:44:14.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:44:14.715: INFO: namespace: e2e-tests-projected-sdj5s, resource: bindings, ignored listing per whitelist
Aug  6 15:44:16.230: INFO: namespace: e2e-tests-projected-sdj5s, resource: packagemanifests, items remaining: 3
Aug  6 15:44:16.452: INFO: namespace: e2e-tests-projected-sdj5s no longer exists
Aug  6 15:44:16.475: INFO: namespace: e2e-tests-projected-sdj5s, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:44:16.497: INFO: namespace e2e-tests-projected-sdj5s deletion completed in 8.536365739s

• [SLOW TEST:20.211 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:44:16.498: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-6xm6
STEP: Creating a pod to test atomic-volume-subpath
Aug  6 15:44:17.926: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6xm6" in namespace "e2e-tests-subpath-4ngw5" to be "success or failure"
Aug  6 15:44:17.949: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Pending", Reason="", readiness=false. Elapsed: 23.205209ms
Aug  6 15:44:19.972: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045757101s
Aug  6 15:44:21.996: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069332226s
Aug  6 15:44:24.019: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092470138s
Aug  6 15:44:26.042: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115849644s
Aug  6 15:44:28.065: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Running", Reason="", readiness=false. Elapsed: 10.13862498s
Aug  6 15:44:30.088: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Running", Reason="", readiness=false. Elapsed: 12.161568365s
Aug  6 15:44:32.111: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Running", Reason="", readiness=false. Elapsed: 14.184453918s
Aug  6 15:44:34.134: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Running", Reason="", readiness=false. Elapsed: 16.207881522s
Aug  6 15:44:36.157: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Running", Reason="", readiness=false. Elapsed: 18.231018619s
Aug  6 15:44:38.181: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Running", Reason="", readiness=false. Elapsed: 20.254963742s
Aug  6 15:44:40.205: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Running", Reason="", readiness=false. Elapsed: 22.278271208s
Aug  6 15:44:42.229: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Running", Reason="", readiness=false. Elapsed: 24.303063682s
Aug  6 15:44:44.261: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Running", Reason="", readiness=false. Elapsed: 26.334350223s
Aug  6 15:44:46.285: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Running", Reason="", readiness=false. Elapsed: 28.358671262s
Aug  6 15:44:48.308: INFO: Pod "pod-subpath-test-projected-6xm6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.381635199s
STEP: Saw pod success
Aug  6 15:44:48.308: INFO: Pod "pod-subpath-test-projected-6xm6" satisfied condition "success or failure"
Aug  6 15:44:48.330: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-subpath-test-projected-6xm6 container test-container-subpath-projected-6xm6: <nil>
STEP: delete the pod
Aug  6 15:44:48.385: INFO: Waiting for pod pod-subpath-test-projected-6xm6 to disappear
Aug  6 15:44:48.407: INFO: Pod pod-subpath-test-projected-6xm6 no longer exists
STEP: Deleting pod pod-subpath-test-projected-6xm6
Aug  6 15:44:48.407: INFO: Deleting pod "pod-subpath-test-projected-6xm6" in namespace "e2e-tests-subpath-4ngw5"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:44:48.429: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-4ngw5" for this suite.
Aug  6 15:44:54.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:44:56.170: INFO: namespace: e2e-tests-subpath-4ngw5, resource: bindings, ignored listing per whitelist
Aug  6 15:44:56.663: INFO: namespace: e2e-tests-subpath-4ngw5, resource: packagemanifests, items remaining: 3
Aug  6 15:44:56.991: INFO: namespace: e2e-tests-subpath-4ngw5 no longer exists
Aug  6 15:44:57.014: INFO: namespace: e2e-tests-subpath-4ngw5, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:44:57.036: INFO: namespace e2e-tests-subpath-4ngw5 deletion completed in 8.543998042s

• [SLOW TEST:40.538 seconds]
[sig-storage] Subpath
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:44:57.036: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:44:58.426: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-85bgh" for this suite.
Aug  6 15:45:04.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:45:05.600: INFO: namespace: e2e-tests-services-85bgh, resource: bindings, ignored listing per whitelist
Aug  6 15:45:06.046: INFO: namespace: e2e-tests-services-85bgh, resource: packagemanifests, items remaining: 3
Aug  6 15:45:06.947: INFO: namespace: e2e-tests-services-85bgh no longer exists
Aug  6 15:45:06.970: INFO: namespace: e2e-tests-services-85bgh, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:45:06.993: INFO: namespace e2e-tests-services-85bgh deletion completed in 8.523421982s
[AfterEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:9.956 seconds]
[sig-network] Services
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:45:06.993: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Aug  6 15:45:08.358: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:45:19.234: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-28ksk" for this suite.
Aug  6 15:45:25.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:45:26.205: INFO: namespace: e2e-tests-init-container-28ksk, resource: bindings, ignored listing per whitelist
Aug  6 15:45:26.523: INFO: namespace: e2e-tests-init-container-28ksk, resource: packagemanifests, items remaining: 3
Aug  6 15:45:27.785: INFO: namespace: e2e-tests-init-container-28ksk no longer exists
Aug  6 15:45:27.808: INFO: namespace: e2e-tests-init-container-28ksk, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:45:27.830: INFO: namespace e2e-tests-init-container-28ksk deletion completed in 8.533989705s

• [SLOW TEST:20.837 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:45:27.831: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 15:45:29.212: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug  6 15:45:29.258: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug  6 15:45:37.304: INFO: Creating deployment "test-rolling-update-deployment"
Aug  6 15:45:37.327: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug  6 15:45:37.371: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug  6 15:45:37.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-755b64976f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 15:45:39.415: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-755b64976f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 15:45:41.416: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-755b64976f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 15:45:43.415: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703114, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-755b64976f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 15:45:45.415: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug  6 15:45:45.481: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-tcv6c,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tcv6c/deployments/test-rolling-update-deployment,UID:2ec3657f-b861-11e9-9dff-0e6de702691c,ResourceVersion:116141,Generation:1,CreationTimestamp:2019-08-06 15:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-06 15:45:14 +0000 UTC 2019-08-06 15:45:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-06 15:45:22 +0000 UTC 2019-08-06 15:45:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-755b64976f" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug  6 15:45:45.504: INFO: New ReplicaSet "test-rolling-update-deployment-755b64976f" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-755b64976f,GenerateName:,Namespace:e2e-tests-deployment-tcv6c,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tcv6c/replicasets/test-rolling-update-deployment-755b64976f,UID:2ec526e0-b861-11e9-9dff-0e6de702691c,ResourceVersion:116129,Generation:1,CreationTimestamp:2019-08-06 15:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 755b64976f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2ec3657f-b861-11e9-9dff-0e6de702691c 0xc002e80ee7 0xc002e80ee8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 755b64976f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 755b64976f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug  6 15:45:45.504: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug  6 15:45:45.505: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-tcv6c,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tcv6c/replicasets/test-rolling-update-controller,UID:29f0bd0e-b861-11e9-9dff-0e6de702691c,ResourceVersion:116139,Generation:2,CreationTimestamp:2019-08-06 15:45:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2ec3657f-b861-11e9-9dff-0e6de702691c 0xc002e80e27 0xc002e80e28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  6 15:45:45.527: INFO: Pod "test-rolling-update-deployment-755b64976f-f47hk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-755b64976f-f47hk,GenerateName:test-rolling-update-deployment-755b64976f-,Namespace:e2e-tests-deployment-tcv6c,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tcv6c/pods/test-rolling-update-deployment-755b64976f-f47hk,UID:2ec63ce5-b861-11e9-9dff-0e6de702691c,ResourceVersion:116128,Generation:0,CreationTimestamp:2019-08-06 15:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 755b64976f,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.111"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-755b64976f 2ec526e0-b861-11e9-9dff-0e6de702691c 0xc002e817c7 0xc002e817c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zhkcc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zhkcc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-zhkcc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-mmmsj}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e81830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e81850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:45:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:45:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:45:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 15:45:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:10.131.0.111,StartTime:2019-08-06 15:45:14 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-06 15:45:22 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://bcaf41205ef9c685ae46c3904ed1da5c311c662baf06240c17c1d7a478f0d7ab}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:45:45.527: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-tcv6c" for this suite.
Aug  6 15:45:51.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:45:53.592: INFO: namespace: e2e-tests-deployment-tcv6c, resource: bindings, ignored listing per whitelist
Aug  6 15:45:53.976: INFO: namespace: e2e-tests-deployment-tcv6c, resource: packagemanifests, items remaining: 3
Aug  6 15:45:54.089: INFO: namespace: e2e-tests-deployment-tcv6c no longer exists
Aug  6 15:45:54.112: INFO: namespace: e2e-tests-deployment-tcv6c, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:45:54.134: INFO: namespace e2e-tests-deployment-tcv6c deletion completed in 8.543679763s

• [SLOW TEST:26.303 seconds]
[sig-apps] Deployment
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:45:54.134: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-vzw5g/secret-test-470f9182-b861-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 15:45:55.589: INFO: Waiting up to 5m0s for pod "pod-configmaps-47142d78-b861-11e9-8d18-525400524259" in namespace "e2e-tests-secrets-vzw5g" to be "success or failure"
Aug  6 15:45:55.625: INFO: Pod "pod-configmaps-47142d78-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 36.38754ms
Aug  6 15:45:57.648: INFO: Pod "pod-configmaps-47142d78-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059475272s
Aug  6 15:45:59.671: INFO: Pod "pod-configmaps-47142d78-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.082120914s
Aug  6 15:46:01.694: INFO: Pod "pod-configmaps-47142d78-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.10473519s
Aug  6 15:46:03.716: INFO: Pod "pod-configmaps-47142d78-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.127195371s
Aug  6 15:46:05.739: INFO: Pod "pod-configmaps-47142d78-b861-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.15045937s
STEP: Saw pod success
Aug  6 15:46:05.739: INFO: Pod "pod-configmaps-47142d78-b861-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:46:05.762: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-configmaps-47142d78-b861-11e9-8d18-525400524259 container env-test: <nil>
STEP: delete the pod
Aug  6 15:46:05.817: INFO: Waiting for pod pod-configmaps-47142d78-b861-11e9-8d18-525400524259 to disappear
Aug  6 15:46:05.840: INFO: Pod pod-configmaps-47142d78-b861-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:46:05.840: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-vzw5g" for this suite.
Aug  6 15:46:11.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:46:12.799: INFO: namespace: e2e-tests-secrets-vzw5g, resource: packagemanifests, items remaining: 3
Aug  6 15:46:12.959: INFO: namespace: e2e-tests-secrets-vzw5g, resource: bindings, ignored listing per whitelist
Aug  6 15:46:14.399: INFO: namespace: e2e-tests-secrets-vzw5g no longer exists
Aug  6 15:46:14.422: INFO: namespace: e2e-tests-secrets-vzw5g, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:46:14.444: INFO: namespace e2e-tests-secrets-vzw5g deletion completed in 8.541780979s

• [SLOW TEST:20.310 seconds]
[sig-api-machinery] Secrets
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:46:14.445: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 15:46:15.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-532a9042-b861-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-l9tqm" to be "success or failure"
Aug  6 15:46:15.869: INFO: Pod "downwardapi-volume-532a9042-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.577706ms
Aug  6 15:46:17.892: INFO: Pod "downwardapi-volume-532a9042-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046145958s
Aug  6 15:46:19.915: INFO: Pod "downwardapi-volume-532a9042-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069644915s
Aug  6 15:46:21.939: INFO: Pod "downwardapi-volume-532a9042-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093473602s
Aug  6 15:46:23.962: INFO: Pod "downwardapi-volume-532a9042-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116310937s
Aug  6 15:46:25.986: INFO: Pod "downwardapi-volume-532a9042-b861-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.140457495s
STEP: Saw pod success
Aug  6 15:46:25.986: INFO: Pod "downwardapi-volume-532a9042-b861-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:46:26.009: INFO: Trying to get logs from node ip-10-0-128-44.ec2.internal pod downwardapi-volume-532a9042-b861-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 15:46:26.101: INFO: Waiting for pod downwardapi-volume-532a9042-b861-11e9-8d18-525400524259 to disappear
Aug  6 15:46:26.124: INFO: Pod downwardapi-volume-532a9042-b861-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:46:26.124: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-l9tqm" for this suite.
Aug  6 15:46:32.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:46:33.100: INFO: namespace: e2e-tests-downward-api-l9tqm, resource: bindings, ignored listing per whitelist
Aug  6 15:46:33.216: INFO: namespace: e2e-tests-downward-api-l9tqm, resource: packagemanifests, items remaining: 3
Aug  6 15:46:34.675: INFO: namespace: e2e-tests-downward-api-l9tqm no longer exists
Aug  6 15:46:34.708: INFO: namespace: e2e-tests-downward-api-l9tqm, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:46:34.731: INFO: namespace e2e-tests-downward-api-l9tqm deletion completed in 8.543541566s

• [SLOW TEST:20.286 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:46:34.731: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug  6 15:46:36.106: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Aug  6 15:46:36.193: INFO: Waiting for terminating namespaces to be deleted...
Aug  6 15:46:36.216: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-128-44.ec2.internal before test
Aug  6 15:46:36.283: INFO: machine-config-daemon-rbg8b from openshift-machine-config-operator started at 2019-08-06 12:32:31 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug  6 15:46:36.283: INFO: elasticsearch-operator-7c5cc4bff9-spd5k from openshift-operators started at 2019-08-06 12:40:44 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container elasticsearch-operator ready: true, restart count 0
Aug  6 15:46:36.283: INFO: ovs-tmlr8 from openshift-sdn started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 15:46:36.283: INFO: sdn-6rmrm from openshift-sdn started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container sdn ready: true, restart count 0
Aug  6 15:46:36.283: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-08-06 12:40:46 +0000 UTC (6 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 15:46:36.283: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug  6 15:46:36.283: INFO: 	Container prometheus ready: true, restart count 1
Aug  6 15:46:36.283: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug  6 15:46:36.283: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug  6 15:46:36.283: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug  6 15:46:36.283: INFO: sre-dns-latency-exporter-n784c from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container main ready: true, restart count 0
Aug  6 15:46:36.283: INFO: sre-machine-api-status-exporter-1-deploy from openshift-monitoring started at 2019-08-06 12:40:13 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container deployment ready: false, restart count 0
Aug  6 15:46:36.283: INFO: sre-stuck-ebs-vols-1-xv82v from openshift-monitoring started at 2019-08-06 12:40:20 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container main ready: true, restart count 0
Aug  6 15:46:36.283: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-08-06 12:40:50 +0000 UTC (3 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container alertmanager ready: true, restart count 0
Aug  6 15:46:36.283: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug  6 15:46:36.283: INFO: 	Container config-reloader ready: true, restart count 0
Aug  6 15:46:36.283: INFO: multus-jgdnj from openshift-multus started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 15:46:36.283: INFO: node-ca-bh6hh from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 15:46:36.283: INFO: router-default-7bc87fbb58-bm9g9 from openshift-ingress started at 2019-08-06 12:58:06 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container router ready: true, restart count 0
Aug  6 15:46:36.283: INFO: tuned-qbg52 from openshift-cluster-node-tuning-operator started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container tuned ready: true, restart count 0
Aug  6 15:46:36.283: INFO: certified-operators-7589fc6b59-44rkm from openshift-marketplace started at 2019-08-06 12:32:38 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container certified-operators ready: true, restart count 0
Aug  6 15:46:36.283: INFO: node-exporter-scqng from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 15:46:36.283: INFO: 	Container node-exporter ready: true, restart count 0
Aug  6 15:46:36.283: INFO: dns-default-7wl42 from openshift-dns started at 2019-08-06 12:32:11 +0000 UTC (2 container statuses recorded)
Aug  6 15:46:36.283: INFO: 	Container dns ready: true, restart count 0
Aug  6 15:46:36.283: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 15:46:36.283: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-134.ec2.internal before test
Aug  6 15:46:36.356: INFO: image-pruner-1565100000-4h6gc from openshift-sre-pruning started at 2019-08-06 14:00:03 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container image-pruner ready: false, restart count 0
Aug  6 15:46:36.357: INFO: builds-pruner-1565103600-pq7mw from openshift-sre-pruning started at 2019-08-06 15:00:08 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container builds-pruner ready: false, restart count 0
Aug  6 15:46:36.357: INFO: dns-default-j5pgg from openshift-dns started at 2019-08-06 12:31:19 +0000 UTC (2 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container dns ready: true, restart count 0
Aug  6 15:46:36.357: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 15:46:36.357: INFO: kube-state-metrics-645d9dc5b9-hklgd from openshift-monitoring started at 2019-08-06 12:32:43 +0000 UTC (3 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug  6 15:46:36.357: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug  6 15:46:36.357: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug  6 15:46:36.357: INFO: router-default-7bc87fbb58-8jfjn from openshift-ingress started at 2019-08-06 12:58:29 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container router ready: true, restart count 0
Aug  6 15:46:36.357: INFO: builds-pruner-1565100000-bt5b6 from openshift-sre-pruning started at 2019-08-06 14:00:03 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container builds-pruner ready: false, restart count 0
Aug  6 15:46:36.357: INFO: node-ca-d98qg from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 15:46:36.357: INFO: image-pruner-1565096400-s2gnf from openshift-sre-pruning started at 2019-08-06 13:00:07 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container image-pruner ready: false, restart count 0
Aug  6 15:46:36.357: INFO: deployments-pruner-1565100000-ht542 from openshift-sre-pruning started at 2019-08-06 14:00:03 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container deployments-pruner ready: false, restart count 0
Aug  6 15:46:36.357: INFO: machine-config-daemon-zf9nq from openshift-machine-config-operator started at 2019-08-06 12:31:49 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug  6 15:46:36.357: INFO: redhat-operators-6fc9f9c9ff-nld4d from openshift-marketplace started at 2019-08-06 12:32:38 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container redhat-operators ready: true, restart count 0
Aug  6 15:46:36.357: INFO: prometheus-adapter-74d56f8767-krgfn from openshift-monitoring started at 2019-08-06 12:34:16 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug  6 15:46:36.357: INFO: sre-dns-latency-exporter-pl7hx from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container main ready: true, restart count 0
Aug  6 15:46:36.357: INFO: ovs-j2g2j from openshift-sdn started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 15:46:36.357: INFO: tuned-x8rhd from openshift-cluster-node-tuning-operator started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container tuned ready: true, restart count 0
Aug  6 15:46:36.357: INFO: deployments-pruner-1565103600-294mv from openshift-sre-pruning started at 2019-08-06 15:00:08 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container deployments-pruner ready: false, restart count 0
Aug  6 15:46:36.357: INFO: sdn-gh6xp from openshift-sdn started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container sdn ready: true, restart count 0
Aug  6 15:46:36.357: INFO: sre-ebs-iops-reporter-1-deploy from openshift-monitoring started at 2019-08-06 12:40:12 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container deployment ready: false, restart count 0
Aug  6 15:46:36.357: INFO: installed-redhat-openshift-operators-547f964d5f-4t496 from openshift-marketplace started at 2019-08-06 12:40:15 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container installed-redhat-openshift-operators ready: true, restart count 0
Aug  6 15:46:36.357: INFO: deployments-pruner-1565096400-zm7g9 from openshift-sre-pruning started at 2019-08-06 13:00:07 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container deployments-pruner ready: false, restart count 0
Aug  6 15:46:36.357: INFO: builds-pruner-1565096400-d5sbn from openshift-sre-pruning started at 2019-08-06 13:00:07 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container builds-pruner ready: false, restart count 0
Aug  6 15:46:36.357: INFO: sre-stuck-ebs-vols-1-deploy from openshift-monitoring started at 2019-08-06 12:40:11 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container deployment ready: false, restart count 0
Aug  6 15:46:36.357: INFO: dedicated-admin-operator-77956f4b8-nd6fz from openshift-dedicated-admin started at 2019-08-06 12:47:48 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container dedicated-admin-operator ready: true, restart count 0
Aug  6 15:46:36.357: INFO: configure-alertmanager-operator-74579866b-lzq26 from openshift-monitoring started at 2019-08-06 12:47:48 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Aug  6 15:46:36.357: INFO: multus-6mjbh from openshift-multus started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 15:46:36.357: INFO: node-exporter-hrdtt from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 15:46:36.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 15:46:36.357: INFO: 	Container node-exporter ready: true, restart count 0
Aug  6 15:46:36.357: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-135-96.ec2.internal before test
Aug  6 15:46:36.452: INFO: node-ca-5w4gz from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.452: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 15:46:36.452: INFO: community-operators-7b9c4c96b8-s6c84 from openshift-marketplace started at 2019-08-06 12:32:38 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.452: INFO: 	Container community-operators ready: true, restart count 0
Aug  6 15:46:36.452: INFO: ovs-s6jvw from openshift-sdn started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.452: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 15:46:36.452: INFO: tuned-dkb6k from openshift-cluster-node-tuning-operator started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.452: INFO: 	Container tuned ready: true, restart count 0
Aug  6 15:46:36.452: INFO: image-registry-57774c6454-5lpmh from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.452: INFO: 	Container registry ready: true, restart count 0
Aug  6 15:46:36.452: INFO: node-exporter-ccph9 from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 15:46:36.452: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 15:46:36.452: INFO: 	Container node-exporter ready: true, restart count 0
Aug  6 15:46:36.452: INFO: configure-alertmanager-operator-registry-bvv62 from openshift-operator-lifecycle-manager started at 2019-08-06 12:40:07 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container registry-server ready: true, restart count 0
Aug  6 15:46:36.453: INFO: image-pruner-1565103600-44bxp from openshift-sre-pruning started at 2019-08-06 15:00:08 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container image-pruner ready: false, restart count 0
Aug  6 15:46:36.453: INFO: sdn-nrctn from openshift-sdn started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container sdn ready: true, restart count 0
Aug  6 15:46:36.453: INFO: dns-default-qhqr4 from openshift-dns started at 2019-08-06 12:31:52 +0000 UTC (2 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container dns ready: true, restart count 0
Aug  6 15:46:36.453: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 15:46:36.453: INFO: telemeter-client-6878d94476-kncjs from openshift-monitoring started at 2019-08-06 12:32:46 +0000 UTC (3 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 15:46:36.453: INFO: 	Container reload ready: true, restart count 0
Aug  6 15:46:36.453: INFO: 	Container telemeter-client ready: true, restart count 0
Aug  6 15:46:36.453: INFO: sre-dns-latency-exporter-xdr2p from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container main ready: true, restart count 0
Aug  6 15:46:36.453: INFO: prometheus-operator-7cbf84478f-9cqcb from openshift-monitoring started at 2019-08-06 12:40:13 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug  6 15:46:36.453: INFO: multus-zwdrq from openshift-multus started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 15:46:36.453: INFO: machine-config-daemon-rsm96 from openshift-machine-config-operator started at 2019-08-06 12:32:21 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug  6 15:46:36.453: INFO: installed-redhat-openshift-logging-7b6d9d86f5-2whm2 from openshift-marketplace started at 2019-08-06 12:40:15 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container installed-redhat-openshift-logging ready: true, restart count 0
Aug  6 15:46:36.453: INFO: cluster-logging-operator-7c58ddb664-kvlv7 from openshift-logging started at 2019-08-06 12:40:56 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container cluster-logging-operator ready: true, restart count 0
Aug  6 15:46:36.453: INFO: dedicated-admin-operator-registry-dvtrw from openshift-operator-lifecycle-manager started at 2019-08-06 12:40:14 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container registry-server ready: true, restart count 0
Aug  6 15:46:36.453: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-08-06 12:40:19 +0000 UTC (3 container statuses recorded)
Aug  6 15:46:36.453: INFO: 	Container alertmanager ready: true, restart count 0
Aug  6 15:46:36.453: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug  6 15:46:36.453: INFO: 	Container config-reloader ready: true, restart count 0
Aug  6 15:46:36.453: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-62.ec2.internal before test
Aug  6 15:46:36.522: INFO: sdn-55nwk from openshift-sdn started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container sdn ready: true, restart count 0
Aug  6 15:46:36.522: INFO: dns-default-kc256 from openshift-dns started at 2019-08-06 12:32:36 +0000 UTC (2 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container dns ready: true, restart count 0
Aug  6 15:46:36.522: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 15:46:36.522: INFO: machine-config-daemon-m7b9d from openshift-machine-config-operator started at 2019-08-06 12:33:06 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug  6 15:46:36.522: INFO: prometheus-adapter-74d56f8767-wc4bj from openshift-monitoring started at 2019-08-06 12:34:16 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug  6 15:46:36.522: INFO: multus-4qwmn from openshift-multus started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 15:46:36.522: INFO: sre-ebs-iops-reporter-1-nvmlq from openshift-monitoring started at 2019-08-06 12:40:20 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container main ready: true, restart count 0
Aug  6 15:46:36.522: INFO: ovs-h9vns from openshift-sdn started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 15:46:36.522: INFO: node-exporter-56p4z from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 15:46:36.522: INFO: 	Container node-exporter ready: true, restart count 0
Aug  6 15:46:36.522: INFO: node-ca-9tl67 from openshift-image-registry started at 2019-08-06 12:33:06 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 15:46:36.522: INFO: sre-machine-api-status-exporter-1-clk9g from openshift-monitoring started at 2019-08-06 12:40:33 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container main ready: true, restart count 0
Aug  6 15:46:36.522: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-08-06 12:41:25 +0000 UTC (3 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container alertmanager ready: true, restart count 0
Aug  6 15:46:36.522: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug  6 15:46:36.522: INFO: 	Container config-reloader ready: true, restart count 0
Aug  6 15:46:36.522: INFO: tuned-mjhtk from openshift-cluster-node-tuning-operator started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.522: INFO: 	Container tuned ready: true, restart count 0
Aug  6 15:46:36.522: INFO: grafana-86f8cd476c-gm5tf from openshift-monitoring started at 2019-08-06 12:33:08 +0000 UTC (2 container statuses recorded)
Aug  6 15:46:36.523: INFO: 	Container grafana ready: true, restart count 0
Aug  6 15:46:36.523: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug  6 15:46:36.523: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-08-06 12:40:46 +0000 UTC (6 container statuses recorded)
Aug  6 15:46:36.523: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 15:46:36.523: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug  6 15:46:36.523: INFO: 	Container prometheus ready: true, restart count 1
Aug  6 15:46:36.523: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug  6 15:46:36.523: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug  6 15:46:36.523: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug  6 15:46:36.523: INFO: sre-dns-latency-exporter-crjdn from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 15:46:36.523: INFO: 	Container main ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node ip-10-0-128-44.ec2.internal
STEP: verifying the node has the label node ip-10-0-130-134.ec2.internal
STEP: verifying the node has the label node ip-10-0-135-96.ec2.internal
STEP: verifying the node has the label node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.277: INFO: Pod tuned-dkb6k requesting resource cpu=10m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.277: INFO: Pod tuned-mjhtk requesting resource cpu=10m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.277: INFO: Pod tuned-qbg52 requesting resource cpu=10m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.277: INFO: Pod tuned-x8rhd requesting resource cpu=10m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.277: INFO: Pod dedicated-admin-operator-77956f4b8-nd6fz requesting resource cpu=0m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.277: INFO: Pod dns-default-7wl42 requesting resource cpu=110m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.277: INFO: Pod dns-default-j5pgg requesting resource cpu=110m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.277: INFO: Pod dns-default-kc256 requesting resource cpu=110m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.277: INFO: Pod dns-default-qhqr4 requesting resource cpu=110m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.277: INFO: Pod image-registry-57774c6454-5lpmh requesting resource cpu=100m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.277: INFO: Pod node-ca-5w4gz requesting resource cpu=10m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.277: INFO: Pod node-ca-9tl67 requesting resource cpu=10m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.277: INFO: Pod node-ca-bh6hh requesting resource cpu=10m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.277: INFO: Pod node-ca-d98qg requesting resource cpu=10m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.277: INFO: Pod router-default-7bc87fbb58-8jfjn requesting resource cpu=100m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.277: INFO: Pod router-default-7bc87fbb58-bm9g9 requesting resource cpu=100m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.277: INFO: Pod cluster-logging-operator-7c58ddb664-kvlv7 requesting resource cpu=0m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.277: INFO: Pod machine-config-daemon-m7b9d requesting resource cpu=20m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.277: INFO: Pod machine-config-daemon-rbg8b requesting resource cpu=20m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.277: INFO: Pod machine-config-daemon-rsm96 requesting resource cpu=20m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.277: INFO: Pod machine-config-daemon-zf9nq requesting resource cpu=20m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.277: INFO: Pod certified-operators-7589fc6b59-44rkm requesting resource cpu=0m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.277: INFO: Pod community-operators-7b9c4c96b8-s6c84 requesting resource cpu=0m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.277: INFO: Pod installed-redhat-openshift-logging-7b6d9d86f5-2whm2 requesting resource cpu=0m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.277: INFO: Pod installed-redhat-openshift-operators-547f964d5f-4t496 requesting resource cpu=0m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.277: INFO: Pod redhat-operators-6fc9f9c9ff-nld4d requesting resource cpu=0m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.277: INFO: Pod alertmanager-main-0 requesting resource cpu=0m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.277: INFO: Pod alertmanager-main-1 requesting resource cpu=0m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.277: INFO: Pod alertmanager-main-2 requesting resource cpu=0m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.278: INFO: Pod configure-alertmanager-operator-74579866b-lzq26 requesting resource cpu=0m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.278: INFO: Pod grafana-86f8cd476c-gm5tf requesting resource cpu=100m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.278: INFO: Pod kube-state-metrics-645d9dc5b9-hklgd requesting resource cpu=0m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.278: INFO: Pod node-exporter-56p4z requesting resource cpu=10m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.278: INFO: Pod node-exporter-ccph9 requesting resource cpu=10m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.278: INFO: Pod node-exporter-hrdtt requesting resource cpu=10m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.278: INFO: Pod node-exporter-scqng requesting resource cpu=10m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.278: INFO: Pod prometheus-adapter-74d56f8767-krgfn requesting resource cpu=0m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.278: INFO: Pod prometheus-adapter-74d56f8767-wc4bj requesting resource cpu=0m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.278: INFO: Pod prometheus-k8s-0 requesting resource cpu=0m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.278: INFO: Pod prometheus-k8s-1 requesting resource cpu=0m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.278: INFO: Pod prometheus-operator-7cbf84478f-9cqcb requesting resource cpu=0m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.278: INFO: Pod sre-dns-latency-exporter-crjdn requesting resource cpu=0m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.278: INFO: Pod sre-dns-latency-exporter-n784c requesting resource cpu=0m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.278: INFO: Pod sre-dns-latency-exporter-pl7hx requesting resource cpu=0m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.278: INFO: Pod sre-dns-latency-exporter-xdr2p requesting resource cpu=0m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.278: INFO: Pod sre-ebs-iops-reporter-1-nvmlq requesting resource cpu=0m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.278: INFO: Pod sre-machine-api-status-exporter-1-clk9g requesting resource cpu=0m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.278: INFO: Pod sre-stuck-ebs-vols-1-xv82v requesting resource cpu=0m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.278: INFO: Pod telemeter-client-6878d94476-kncjs requesting resource cpu=10m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.278: INFO: Pod multus-4qwmn requesting resource cpu=0m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.278: INFO: Pod multus-6mjbh requesting resource cpu=0m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.278: INFO: Pod multus-jgdnj requesting resource cpu=0m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.278: INFO: Pod multus-zwdrq requesting resource cpu=0m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.278: INFO: Pod configure-alertmanager-operator-registry-bvv62 requesting resource cpu=0m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.278: INFO: Pod dedicated-admin-operator-registry-dvtrw requesting resource cpu=0m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.278: INFO: Pod elasticsearch-operator-7c5cc4bff9-spd5k requesting resource cpu=0m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.278: INFO: Pod ovs-h9vns requesting resource cpu=200m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.278: INFO: Pod ovs-j2g2j requesting resource cpu=200m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.278: INFO: Pod ovs-s6jvw requesting resource cpu=200m on Node ip-10-0-135-96.ec2.internal
Aug  6 15:46:37.278: INFO: Pod ovs-tmlr8 requesting resource cpu=200m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.278: INFO: Pod sdn-55nwk requesting resource cpu=100m on Node ip-10-0-137-62.ec2.internal
Aug  6 15:46:37.278: INFO: Pod sdn-6rmrm requesting resource cpu=100m on Node ip-10-0-128-44.ec2.internal
Aug  6 15:46:37.278: INFO: Pod sdn-gh6xp requesting resource cpu=100m on Node ip-10-0-130-134.ec2.internal
Aug  6 15:46:37.278: INFO: Pod sdn-nrctn requesting resource cpu=100m on Node ip-10-0-135-96.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ff5a20d-b861-11e9-8d18-525400524259.15b85ff49d8bdd90], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-wn75b/filler-pod-5ff5a20d-b861-11e9-8d18-525400524259 to ip-10-0-128-44.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ff5a20d-b861-11e9-8d18-525400524259.15b85ff65d29d096], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ff5a20d-b861-11e9-8d18-525400524259.15b85ff66674c78e], Reason = [Created], Message = [Created container filler-pod-5ff5a20d-b861-11e9-8d18-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ff5a20d-b861-11e9-8d18-525400524259.15b85ff6678dfd6b], Reason = [Started], Message = [Started container filler-pod-5ff5a20d-b861-11e9-8d18-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ffa33a1-b861-11e9-8d18-525400524259.15b85ff49f2fc2ff], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-wn75b/filler-pod-5ffa33a1-b861-11e9-8d18-525400524259 to ip-10-0-130-134.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ffa33a1-b861-11e9-8d18-525400524259.15b85ff67a0335ed], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ffa33a1-b861-11e9-8d18-525400524259.15b85ff686b9ea09], Reason = [Created], Message = [Created container filler-pod-5ffa33a1-b861-11e9-8d18-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ffa33a1-b861-11e9-8d18-525400524259.15b85ff6879e1de1], Reason = [Started], Message = [Started container filler-pod-5ffa33a1-b861-11e9-8d18-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ffe5f81-b861-11e9-8d18-525400524259.15b85ff4a0c6ecb1], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-wn75b/filler-pod-5ffe5f81-b861-11e9-8d18-525400524259 to ip-10-0-135-96.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ffe5f81-b861-11e9-8d18-525400524259.15b85ff6547523de], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ffe5f81-b861-11e9-8d18-525400524259.15b85ff69c37daac], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ffe5f81-b861-11e9-8d18-525400524259.15b85ff6a44f46b3], Reason = [Created], Message = [Created container filler-pod-5ffe5f81-b861-11e9-8d18-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ffe5f81-b861-11e9-8d18-525400524259.15b85ff6a52fc9c4], Reason = [Started], Message = [Started container filler-pod-5ffe5f81-b861-11e9-8d18-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60027c34-b861-11e9-8d18-525400524259.15b85ff4a26cd52a], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-wn75b/filler-pod-60027c34-b861-11e9-8d18-525400524259 to ip-10-0-137-62.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60027c34-b861-11e9-8d18-525400524259.15b85ff67f5f6a8c], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60027c34-b861-11e9-8d18-525400524259.15b85ff6c7d46b9d], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60027c34-b861-11e9-8d18-525400524259.15b85ff6cfa68a1f], Reason = [Created], Message = [Created container filler-pod-60027c34-b861-11e9-8d18-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60027c34-b861-11e9-8d18-525400524259.15b85ff6d09df6a3], Reason = [Started], Message = [Started container filler-pod-60027c34-b861-11e9-8d18-525400524259]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15b85ff77b08ab11], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) were unschedulable, 4 Insufficient cpu.]
STEP: removing the label node off the node ip-10-0-135-96.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-137-62.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-128-44.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-130-134.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:46:50.950: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-wn75b" for this suite.
Aug  6 15:46:57.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:46:58.112: INFO: namespace: e2e-tests-sched-pred-wn75b, resource: packagemanifests, items remaining: 3
Aug  6 15:46:58.527: INFO: namespace: e2e-tests-sched-pred-wn75b, resource: bindings, ignored listing per whitelist
Aug  6 15:46:59.501: INFO: namespace: e2e-tests-sched-pred-wn75b no longer exists
Aug  6 15:46:59.543: INFO: namespace: e2e-tests-sched-pred-wn75b, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:46:59.565: INFO: namespace e2e-tests-sched-pred-wn75b deletion completed in 8.552295673s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:24.834 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:46:59.565: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 15:47:00.944: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e0c1011-b861-11e9-8d18-525400524259" in namespace "e2e-tests-projected-4wr4d" to be "success or failure"
Aug  6 15:47:00.971: INFO: Pod "downwardapi-volume-6e0c1011-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 27.112754ms
Aug  6 15:47:02.994: INFO: Pod "downwardapi-volume-6e0c1011-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050177939s
Aug  6 15:47:05.018: INFO: Pod "downwardapi-volume-6e0c1011-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07356638s
Aug  6 15:47:07.041: INFO: Pod "downwardapi-volume-6e0c1011-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.097020251s
Aug  6 15:47:09.065: INFO: Pod "downwardapi-volume-6e0c1011-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.120549483s
Aug  6 15:47:11.087: INFO: Pod "downwardapi-volume-6e0c1011-b861-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.143418522s
STEP: Saw pod success
Aug  6 15:47:11.088: INFO: Pod "downwardapi-volume-6e0c1011-b861-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:47:11.110: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-6e0c1011-b861-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 15:47:11.165: INFO: Waiting for pod downwardapi-volume-6e0c1011-b861-11e9-8d18-525400524259 to disappear
Aug  6 15:47:11.188: INFO: Pod downwardapi-volume-6e0c1011-b861-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:47:11.188: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4wr4d" for this suite.
Aug  6 15:47:17.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:47:18.008: INFO: namespace: e2e-tests-projected-4wr4d, resource: bindings, ignored listing per whitelist
Aug  6 15:47:18.373: INFO: namespace: e2e-tests-projected-4wr4d, resource: packagemanifests, items remaining: 3
Aug  6 15:47:19.761: INFO: namespace: e2e-tests-projected-4wr4d no longer exists
Aug  6 15:47:19.804: INFO: namespace: e2e-tests-projected-4wr4d, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:47:19.827: INFO: namespace e2e-tests-projected-4wr4d deletion completed in 8.575769519s

• [SLOW TEST:20.261 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:47:19.827: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  6 15:47:21.192: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-8tlqd'
Aug  6 15:47:22.689: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug  6 15:47:22.689: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Aug  6 15:47:22.734: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-vht7f]
Aug  6 15:47:22.735: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-vht7f" in namespace "e2e-tests-kubectl-8tlqd" to be "running and ready"
Aug  6 15:47:22.757: INFO: Pod "e2e-test-nginx-rc-vht7f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.282412ms
Aug  6 15:47:24.780: INFO: Pod "e2e-test-nginx-rc-vht7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045493864s
Aug  6 15:47:26.803: INFO: Pod "e2e-test-nginx-rc-vht7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068602392s
Aug  6 15:47:28.826: INFO: Pod "e2e-test-nginx-rc-vht7f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091335896s
Aug  6 15:47:30.850: INFO: Pod "e2e-test-nginx-rc-vht7f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.11498308s
Aug  6 15:47:32.873: INFO: Pod "e2e-test-nginx-rc-vht7f": Phase="Running", Reason="", readiness=true. Elapsed: 10.138212271s
Aug  6 15:47:32.873: INFO: Pod "e2e-test-nginx-rc-vht7f" satisfied condition "running and ready"
Aug  6 15:47:32.873: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-vht7f]
Aug  6 15:47:32.873: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-8tlqd'
Aug  6 15:47:33.089: INFO: stderr: ""
Aug  6 15:47:33.089: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Aug  6 15:47:33.089: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-8tlqd'
Aug  6 15:47:33.281: INFO: stderr: ""
Aug  6 15:47:33.282: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:47:33.282: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8tlqd" for this suite.
Aug  6 15:47:39.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:47:40.035: INFO: namespace: e2e-tests-kubectl-8tlqd, resource: bindings, ignored listing per whitelist
Aug  6 15:47:40.733: INFO: namespace: e2e-tests-kubectl-8tlqd, resource: packagemanifests, items remaining: 3
Aug  6 15:47:41.836: INFO: namespace: e2e-tests-kubectl-8tlqd no longer exists
Aug  6 15:47:41.859: INFO: namespace: e2e-tests-kubectl-8tlqd, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:47:41.881: INFO: namespace e2e-tests-kubectl-8tlqd deletion completed in 8.536992765s

• [SLOW TEST:22.054 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:47:41.882: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 15:47:43.284: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-874d7651-b861-11e9-8d18-525400524259
STEP: Creating configMap with name cm-test-opt-upd-874d7683-b861-11e9-8d18-525400524259
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-874d7651-b861-11e9-8d18-525400524259
STEP: Updating configmap cm-test-opt-upd-874d7683-b861-11e9-8d18-525400524259
STEP: Creating configMap with name cm-test-opt-create-874d7694-b861-11e9-8d18-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:49:14.771: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-q6zzc" for this suite.
Aug  6 15:49:36.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:49:38.317: INFO: namespace: e2e-tests-configmap-q6zzc, resource: bindings, ignored listing per whitelist
Aug  6 15:49:39.182: INFO: namespace: e2e-tests-configmap-q6zzc, resource: packagemanifests, items remaining: 3
Aug  6 15:49:39.314: INFO: namespace: e2e-tests-configmap-q6zzc no longer exists
Aug  6 15:49:39.339: INFO: namespace: e2e-tests-configmap-q6zzc, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:49:39.361: INFO: namespace e2e-tests-configmap-q6zzc deletion completed in 24.545047738s

• [SLOW TEST:117.479 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:49:39.362: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 15:49:40.771: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd4f49c3-b861-11e9-8d18-525400524259" in namespace "e2e-tests-projected-dxjv7" to be "success or failure"
Aug  6 15:49:40.794: INFO: Pod "downwardapi-volume-cd4f49c3-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.62178ms
Aug  6 15:49:42.817: INFO: Pod "downwardapi-volume-cd4f49c3-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045653199s
Aug  6 15:49:44.841: INFO: Pod "downwardapi-volume-cd4f49c3-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070014204s
Aug  6 15:49:46.865: INFO: Pod "downwardapi-volume-cd4f49c3-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093288871s
Aug  6 15:49:48.889: INFO: Pod "downwardapi-volume-cd4f49c3-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117071966s
Aug  6 15:49:50.912: INFO: Pod "downwardapi-volume-cd4f49c3-b861-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.140266387s
STEP: Saw pod success
Aug  6 15:49:50.912: INFO: Pod "downwardapi-volume-cd4f49c3-b861-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:49:50.934: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-cd4f49c3-b861-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 15:49:50.990: INFO: Waiting for pod downwardapi-volume-cd4f49c3-b861-11e9-8d18-525400524259 to disappear
Aug  6 15:49:51.012: INFO: Pod downwardapi-volume-cd4f49c3-b861-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:49:51.012: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dxjv7" for this suite.
Aug  6 15:49:57.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:49:57.810: INFO: namespace: e2e-tests-projected-dxjv7, resource: bindings, ignored listing per whitelist
Aug  6 15:49:59.174: INFO: namespace: e2e-tests-projected-dxjv7, resource: packagemanifests, items remaining: 3
Aug  6 15:49:59.570: INFO: namespace: e2e-tests-projected-dxjv7 no longer exists
Aug  6 15:49:59.612: INFO: namespace: e2e-tests-projected-dxjv7, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:49:59.635: INFO: namespace e2e-tests-projected-dxjv7 deletion completed in 8.559393831s

• [SLOW TEST:20.272 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:49:59.635: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-d9621fc8-b861-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 15:50:01.049: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d965a006-b861-11e9-8d18-525400524259" in namespace "e2e-tests-projected-s7qxm" to be "success or failure"
Aug  6 15:50:01.072: INFO: Pod "pod-projected-configmaps-d965a006-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.394732ms
Aug  6 15:50:03.095: INFO: Pod "pod-projected-configmaps-d965a006-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046771932s
Aug  6 15:50:05.120: INFO: Pod "pod-projected-configmaps-d965a006-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071494645s
Aug  6 15:50:07.143: INFO: Pod "pod-projected-configmaps-d965a006-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094302222s
Aug  6 15:50:09.166: INFO: Pod "pod-projected-configmaps-d965a006-b861-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.117329031s
STEP: Saw pod success
Aug  6 15:50:09.166: INFO: Pod "pod-projected-configmaps-d965a006-b861-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:50:09.188: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-configmaps-d965a006-b861-11e9-8d18-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 15:50:09.243: INFO: Waiting for pod pod-projected-configmaps-d965a006-b861-11e9-8d18-525400524259 to disappear
Aug  6 15:50:09.265: INFO: Pod pod-projected-configmaps-d965a006-b861-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:50:09.265: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s7qxm" for this suite.
Aug  6 15:50:15.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:50:16.967: INFO: namespace: e2e-tests-projected-s7qxm, resource: bindings, ignored listing per whitelist
Aug  6 15:50:17.039: INFO: namespace: e2e-tests-projected-s7qxm, resource: packagemanifests, items remaining: 3
Aug  6 15:50:17.835: INFO: namespace: e2e-tests-projected-s7qxm no longer exists
Aug  6 15:50:17.877: INFO: namespace: e2e-tests-projected-s7qxm, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:50:17.899: INFO: namespace e2e-tests-projected-s7qxm deletion completed in 8.571103047s

• [SLOW TEST:18.264 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:50:17.899: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 15:50:29.418: INFO: Waiting up to 5m0s for pod "client-envvars-ea4f4015-b861-11e9-8d18-525400524259" in namespace "e2e-tests-pods-4vcvz" to be "success or failure"
Aug  6 15:50:29.441: INFO: Pod "client-envvars-ea4f4015-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.07784ms
Aug  6 15:50:31.463: INFO: Pod "client-envvars-ea4f4015-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044727183s
Aug  6 15:50:33.486: INFO: Pod "client-envvars-ea4f4015-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067555751s
Aug  6 15:50:35.509: INFO: Pod "client-envvars-ea4f4015-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090321577s
Aug  6 15:50:37.532: INFO: Pod "client-envvars-ea4f4015-b861-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113092093s
Aug  6 15:50:39.554: INFO: Pod "client-envvars-ea4f4015-b861-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.135754832s
STEP: Saw pod success
Aug  6 15:50:39.554: INFO: Pod "client-envvars-ea4f4015-b861-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:50:39.576: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod client-envvars-ea4f4015-b861-11e9-8d18-525400524259 container env3cont: <nil>
STEP: delete the pod
Aug  6 15:50:39.632: INFO: Waiting for pod client-envvars-ea4f4015-b861-11e9-8d18-525400524259 to disappear
Aug  6 15:50:39.655: INFO: Pod client-envvars-ea4f4015-b861-11e9-8d18-525400524259 no longer exists
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:50:39.655: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4vcvz" for this suite.
Aug  6 15:51:19.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:51:21.044: INFO: namespace: e2e-tests-pods-4vcvz, resource: packagemanifests, items remaining: 3
Aug  6 15:51:21.639: INFO: namespace: e2e-tests-pods-4vcvz, resource: bindings, ignored listing per whitelist
Aug  6 15:51:22.236: INFO: namespace: e2e-tests-pods-4vcvz no longer exists
Aug  6 15:51:22.259: INFO: namespace: e2e-tests-pods-4vcvz, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:51:22.282: INFO: namespace e2e-tests-pods-4vcvz deletion completed in 42.563116727s

• [SLOW TEST:64.382 seconds]
[k8s.io] Pods
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:51:22.282: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Aug  6 15:51:23.680: INFO: Waiting up to 5m0s for pod "pod-0aa6a3dd-b862-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-mnqdk" to be "success or failure"
Aug  6 15:51:23.703: INFO: Pod "pod-0aa6a3dd-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.80174ms
Aug  6 15:51:25.726: INFO: Pod "pod-0aa6a3dd-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045834371s
Aug  6 15:51:27.748: INFO: Pod "pod-0aa6a3dd-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068294698s
Aug  6 15:51:29.771: INFO: Pod "pod-0aa6a3dd-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09079491s
Aug  6 15:51:31.793: INFO: Pod "pod-0aa6a3dd-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113351534s
Aug  6 15:51:33.816: INFO: Pod "pod-0aa6a3dd-b862-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.136080124s
STEP: Saw pod success
Aug  6 15:51:33.816: INFO: Pod "pod-0aa6a3dd-b862-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:51:33.838: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-0aa6a3dd-b862-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 15:51:33.894: INFO: Waiting for pod pod-0aa6a3dd-b862-11e9-8d18-525400524259 to disappear
Aug  6 15:51:33.916: INFO: Pod pod-0aa6a3dd-b862-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:51:33.916: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mnqdk" for this suite.
Aug  6 15:51:40.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:51:40.750: INFO: namespace: e2e-tests-emptydir-mnqdk, resource: packagemanifests, items remaining: 3
Aug  6 15:51:41.499: INFO: namespace: e2e-tests-emptydir-mnqdk, resource: bindings, ignored listing per whitelist
Aug  6 15:51:42.470: INFO: namespace: e2e-tests-emptydir-mnqdk no longer exists
Aug  6 15:51:42.493: INFO: namespace: e2e-tests-emptydir-mnqdk, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:51:42.515: INFO: namespace e2e-tests-emptydir-mnqdk deletion completed in 8.536964739s

• [SLOW TEST:20.233 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:51:42.516: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Aug  6 15:51:43.894: INFO: Waiting up to 5m0s for pod "client-containers-16b2b412-b862-11e9-8d18-525400524259" in namespace "e2e-tests-containers-8c727" to be "success or failure"
Aug  6 15:51:43.917: INFO: Pod "client-containers-16b2b412-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.791544ms
Aug  6 15:51:45.940: INFO: Pod "client-containers-16b2b412-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04575517s
Aug  6 15:51:47.964: INFO: Pod "client-containers-16b2b412-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069784285s
Aug  6 15:51:49.986: INFO: Pod "client-containers-16b2b412-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092350223s
Aug  6 15:51:52.009: INFO: Pod "client-containers-16b2b412-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115351561s
Aug  6 15:51:54.032: INFO: Pod "client-containers-16b2b412-b862-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.138385977s
STEP: Saw pod success
Aug  6 15:51:54.033: INFO: Pod "client-containers-16b2b412-b862-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:51:54.055: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod client-containers-16b2b412-b862-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 15:51:54.111: INFO: Waiting for pod client-containers-16b2b412-b862-11e9-8d18-525400524259 to disappear
Aug  6 15:51:54.136: INFO: Pod client-containers-16b2b412-b862-11e9-8d18-525400524259 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:51:54.136: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-8c727" for this suite.
Aug  6 15:52:00.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:52:01.797: INFO: namespace: e2e-tests-containers-8c727, resource: packagemanifests, items remaining: 3
Aug  6 15:52:01.841: INFO: namespace: e2e-tests-containers-8c727, resource: bindings, ignored listing per whitelist
Aug  6 15:52:02.674: INFO: namespace: e2e-tests-containers-8c727 no longer exists
Aug  6 15:52:02.697: INFO: namespace: e2e-tests-containers-8c727, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:52:02.719: INFO: namespace e2e-tests-containers-8c727 deletion completed in 8.520508893s

• [SLOW TEST:20.204 seconds]
[k8s.io] Docker Containers
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:52:02.720: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Aug  6 15:52:04.082: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-r5hmn'
Aug  6 15:52:05.059: INFO: stderr: ""
Aug  6 15:52:05.059: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug  6 15:52:06.082: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:52:06.082: INFO: Found 0 / 1
Aug  6 15:52:07.082: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:52:07.082: INFO: Found 0 / 1
Aug  6 15:52:08.082: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:52:08.082: INFO: Found 0 / 1
Aug  6 15:52:09.083: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:52:09.083: INFO: Found 0 / 1
Aug  6 15:52:10.082: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:52:10.082: INFO: Found 0 / 1
Aug  6 15:52:11.083: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:52:11.083: INFO: Found 0 / 1
Aug  6 15:52:12.082: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:52:12.082: INFO: Found 0 / 1
Aug  6 15:52:13.081: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:52:13.081: INFO: Found 0 / 1
Aug  6 15:52:14.083: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:52:14.083: INFO: Found 1 / 1
Aug  6 15:52:14.083: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug  6 15:52:14.107: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:52:14.107: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug  6 15:52:14.107: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance patch pod redis-master-2gtrh --namespace=e2e-tests-kubectl-r5hmn -p {"metadata":{"annotations":{"x":"y"}}}'
Aug  6 15:52:14.298: INFO: stderr: ""
Aug  6 15:52:14.298: INFO: stdout: "pod/redis-master-2gtrh patched\n"
STEP: checking annotations
Aug  6 15:52:14.321: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 15:52:14.321: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:52:14.321: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-r5hmn" for this suite.
Aug  6 15:52:36.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:52:37.656: INFO: namespace: e2e-tests-kubectl-r5hmn, resource: packagemanifests, items remaining: 3
Aug  6 15:52:37.836: INFO: namespace: e2e-tests-kubectl-r5hmn, resource: bindings, ignored listing per whitelist
Aug  6 15:52:38.885: INFO: namespace: e2e-tests-kubectl-r5hmn no longer exists
Aug  6 15:52:38.908: INFO: namespace: e2e-tests-kubectl-r5hmn, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:52:38.930: INFO: namespace e2e-tests-kubectl-r5hmn deletion completed in 24.545997773s

• [SLOW TEST:36.210 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:52:38.930: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug  6 15:52:40.525: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:40.525: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:40.525: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:40.547: INFO: Number of nodes with available pods: 0
Aug  6 15:52:40.547: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:41.610: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:41.610: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:41.610: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:41.633: INFO: Number of nodes with available pods: 0
Aug  6 15:52:41.633: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:42.611: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:42.611: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:42.611: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:42.634: INFO: Number of nodes with available pods: 0
Aug  6 15:52:42.634: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:43.610: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:43.610: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:43.610: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:43.633: INFO: Number of nodes with available pods: 0
Aug  6 15:52:43.633: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:44.610: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:44.610: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:44.610: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:44.633: INFO: Number of nodes with available pods: 0
Aug  6 15:52:44.633: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:45.611: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:45.611: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:45.611: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:45.634: INFO: Number of nodes with available pods: 0
Aug  6 15:52:45.634: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:46.611: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:46.611: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:46.611: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:46.633: INFO: Number of nodes with available pods: 0
Aug  6 15:52:46.633: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:47.611: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:47.611: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:47.611: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:47.634: INFO: Number of nodes with available pods: 0
Aug  6 15:52:47.634: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:48.611: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:48.611: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:48.611: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:48.633: INFO: Number of nodes with available pods: 0
Aug  6 15:52:48.633: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:49.610: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:49.610: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:49.610: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:49.633: INFO: Number of nodes with available pods: 4
Aug  6 15:52:49.633: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug  6 15:52:49.745: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:49.745: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:49.745: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:49.768: INFO: Number of nodes with available pods: 3
Aug  6 15:52:49.768: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:50.832: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:50.832: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:50.832: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:50.855: INFO: Number of nodes with available pods: 3
Aug  6 15:52:50.855: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:51.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:51.832: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:51.832: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:51.857: INFO: Number of nodes with available pods: 3
Aug  6 15:52:51.857: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:52.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:52.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:52.831: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:52.854: INFO: Number of nodes with available pods: 3
Aug  6 15:52:52.854: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:53.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:53.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:53.831: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:53.854: INFO: Number of nodes with available pods: 3
Aug  6 15:52:53.854: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:54.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:54.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:54.831: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:54.854: INFO: Number of nodes with available pods: 3
Aug  6 15:52:54.854: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:55.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:55.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:55.831: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:55.855: INFO: Number of nodes with available pods: 3
Aug  6 15:52:55.855: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:56.832: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:56.832: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:56.832: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:56.854: INFO: Number of nodes with available pods: 3
Aug  6 15:52:56.854: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:57.832: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:57.832: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:57.832: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:57.862: INFO: Number of nodes with available pods: 3
Aug  6 15:52:57.862: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:58.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:58.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:58.832: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:58.855: INFO: Number of nodes with available pods: 3
Aug  6 15:52:58.855: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:52:59.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:59.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:59.831: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:52:59.853: INFO: Number of nodes with available pods: 3
Aug  6 15:52:59.853: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:00.830: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:00.830: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:00.830: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:00.853: INFO: Number of nodes with available pods: 3
Aug  6 15:53:00.853: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:01.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:01.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:01.831: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:01.853: INFO: Number of nodes with available pods: 3
Aug  6 15:53:01.853: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:02.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:02.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:02.831: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:02.854: INFO: Number of nodes with available pods: 3
Aug  6 15:53:02.854: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:03.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:03.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:03.831: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:03.853: INFO: Number of nodes with available pods: 3
Aug  6 15:53:03.853: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:04.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:04.831: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:04.831: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-06 14:47:50 +0000 UTC}], skip checking this node
Aug  6 15:53:04.854: INFO: Number of nodes with available pods: 4
Aug  6 15:53:04.854: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-d5kjh, will wait for the garbage collector to delete the pods
Aug  6 15:53:04.975: INFO: Deleting DaemonSet.extensions daemon-set took: 26.553074ms
Aug  6 15:53:05.075: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.339805ms
Aug  6 15:53:13.709: INFO: Number of nodes with available pods: 0
Aug  6 15:53:13.709: INFO: Number of running nodes: 0, number of available pods: 0
Aug  6 15:53:13.731: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-d5kjh/daemonsets","resourceVersion":"121797"},"items":null}

Aug  6 15:53:13.753: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-d5kjh/pods","resourceVersion":"121797"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:53:13.884: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-d5kjh" for this suite.
Aug  6 15:53:19.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:53:22.531: INFO: namespace: e2e-tests-daemonsets-d5kjh, resource: packagemanifests, items remaining: 3
Aug  6 15:53:22.665: INFO: namespace: e2e-tests-daemonsets-d5kjh, resource: bindings, ignored listing per whitelist
Aug  6 15:53:22.865: INFO: namespace: e2e-tests-daemonsets-d5kjh no longer exists
Aug  6 15:53:23.032: INFO: namespace: e2e-tests-daemonsets-d5kjh, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:53:23.057: INFO: namespace e2e-tests-daemonsets-d5kjh deletion completed in 9.130633603s

• [SLOW TEST:44.127 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:53:23.058: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 15:53:24.915: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug  6 15:53:24.961: INFO: Number of nodes with available pods: 0
Aug  6 15:53:24.961: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug  6 15:53:25.108: INFO: Number of nodes with available pods: 0
Aug  6 15:53:25.108: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:26.131: INFO: Number of nodes with available pods: 0
Aug  6 15:53:26.131: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:27.131: INFO: Number of nodes with available pods: 0
Aug  6 15:53:27.131: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:28.131: INFO: Number of nodes with available pods: 0
Aug  6 15:53:28.131: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:29.131: INFO: Number of nodes with available pods: 0
Aug  6 15:53:29.131: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:30.131: INFO: Number of nodes with available pods: 0
Aug  6 15:53:30.131: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:31.131: INFO: Number of nodes with available pods: 0
Aug  6 15:53:31.131: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:32.131: INFO: Number of nodes with available pods: 0
Aug  6 15:53:32.131: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:33.132: INFO: Number of nodes with available pods: 0
Aug  6 15:53:33.132: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:34.131: INFO: Number of nodes with available pods: 1
Aug  6 15:53:34.131: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug  6 15:53:34.238: INFO: Number of nodes with available pods: 0
Aug  6 15:53:34.238: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug  6 15:53:34.287: INFO: Number of nodes with available pods: 0
Aug  6 15:53:34.287: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:35.310: INFO: Number of nodes with available pods: 0
Aug  6 15:53:35.310: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:36.310: INFO: Number of nodes with available pods: 0
Aug  6 15:53:36.310: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:37.310: INFO: Number of nodes with available pods: 0
Aug  6 15:53:37.310: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:38.310: INFO: Number of nodes with available pods: 0
Aug  6 15:53:38.310: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:39.310: INFO: Number of nodes with available pods: 0
Aug  6 15:53:39.310: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:40.310: INFO: Number of nodes with available pods: 0
Aug  6 15:53:40.310: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:41.310: INFO: Number of nodes with available pods: 0
Aug  6 15:53:41.310: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:42.310: INFO: Number of nodes with available pods: 0
Aug  6 15:53:42.310: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:43.310: INFO: Number of nodes with available pods: 0
Aug  6 15:53:43.310: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:44.310: INFO: Number of nodes with available pods: 0
Aug  6 15:53:44.310: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 15:53:45.310: INFO: Number of nodes with available pods: 1
Aug  6 15:53:45.310: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-b9xjp, will wait for the garbage collector to delete the pods
Aug  6 15:53:45.454: INFO: Deleting DaemonSet.extensions daemon-set took: 26.269645ms
Aug  6 15:53:45.554: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.420547ms
Aug  6 15:53:49.377: INFO: Number of nodes with available pods: 0
Aug  6 15:53:49.377: INFO: Number of running nodes: 0, number of available pods: 0
Aug  6 15:53:49.399: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-b9xjp/daemonsets","resourceVersion":"122324"},"items":null}

Aug  6 15:53:49.421: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-b9xjp/pods","resourceVersion":"122324"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:53:49.596: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-b9xjp" for this suite.
Aug  6 15:53:55.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:53:56.950: INFO: namespace: e2e-tests-daemonsets-b9xjp, resource: packagemanifests, items remaining: 3
Aug  6 15:53:57.365: INFO: namespace: e2e-tests-daemonsets-b9xjp, resource: bindings, ignored listing per whitelist
Aug  6 15:53:58.143: INFO: namespace: e2e-tests-daemonsets-b9xjp no longer exists
Aug  6 15:53:58.186: INFO: namespace: e2e-tests-daemonsets-b9xjp, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:53:58.208: INFO: namespace e2e-tests-daemonsets-b9xjp deletion completed in 8.549762688s

• [SLOW TEST:35.151 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:53:58.210: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 15:53:59.567: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance version --client'
Aug  6 15:53:59.620: INFO: stderr: ""
Aug  6 15:53:59.620: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13+\", GitVersion:\"v1.13.10-beta.0.1+b5c04d0f249f61\", GitCommit:\"b5c04d0f249f611b9c46bc8bdf58c96a96fc2f64\", GitTreeState:\"clean\", BuildDate:\"2019-08-06T14:50:33Z\", GoVersion:\"go1.12.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Aug  6 15:53:59.641: INFO: Not supported for server versions before "1.13.10-beta.0.1+b5c04d0f249f61"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:53:59.641: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qlds4" for this suite.
Aug  6 15:54:05.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:54:06.762: INFO: namespace: e2e-tests-kubectl-qlds4, resource: packagemanifests, items remaining: 3
Aug  6 15:54:07.268: INFO: namespace: e2e-tests-kubectl-qlds4, resource: bindings, ignored listing per whitelist
Aug  6 15:54:08.194: INFO: namespace: e2e-tests-kubectl-qlds4 no longer exists
Aug  6 15:54:08.217: INFO: namespace: e2e-tests-kubectl-qlds4, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:54:08.239: INFO: namespace e2e-tests-kubectl-qlds4 deletion completed in 8.53380429s

S [SKIPPING] [10.029 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance] [It]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

    Aug  6 15:53:59.641: Not supported for server versions before "1.13.10-beta.0.1+b5c04d0f249f61"

    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:54:08.240: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:54:51.108: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-j7kwm" for this suite.
Aug  6 15:54:57.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:54:58.963: INFO: namespace: e2e-tests-container-runtime-j7kwm, resource: bindings, ignored listing per whitelist
Aug  6 15:54:59.101: INFO: namespace: e2e-tests-container-runtime-j7kwm, resource: packagemanifests, items remaining: 3
Aug  6 15:54:59.648: INFO: namespace: e2e-tests-container-runtime-j7kwm no longer exists
Aug  6 15:54:59.671: INFO: namespace: e2e-tests-container-runtime-j7kwm, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:54:59.693: INFO: namespace e2e-tests-container-runtime-j7kwm deletion completed in 8.521995151s

• [SLOW TEST:51.453 seconds]
[k8s.io] Container Runtime
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:54:59.694: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:55:13.224: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-hbq6q" for this suite.
Aug  6 15:55:35.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:55:36.090: INFO: namespace: e2e-tests-replication-controller-hbq6q, resource: bindings, ignored listing per whitelist
Aug  6 15:55:36.750: INFO: namespace: e2e-tests-replication-controller-hbq6q, resource: packagemanifests, items remaining: 3
Aug  6 15:55:37.839: INFO: namespace: e2e-tests-replication-controller-hbq6q no longer exists
Aug  6 15:55:37.883: INFO: namespace: e2e-tests-replication-controller-hbq6q, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:55:37.906: INFO: namespace e2e-tests-replication-controller-hbq6q deletion completed in 24.618946543s

• [SLOW TEST:38.212 seconds]
[sig-apps] ReplicationController
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:55:37.907: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-zhfx
STEP: Creating a pod to test atomic-volume-subpath
Aug  6 15:55:39.447: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zhfx" in namespace "e2e-tests-subpath-hv9ms" to be "success or failure"
Aug  6 15:55:39.469: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Pending", Reason="", readiness=false. Elapsed: 22.041289ms
Aug  6 15:55:41.492: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045092757s
Aug  6 15:55:43.526: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079265977s
Aug  6 15:55:45.549: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101972403s
Aug  6 15:55:47.572: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.125223672s
Aug  6 15:55:49.595: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Running", Reason="", readiness=false. Elapsed: 10.148200552s
Aug  6 15:55:51.618: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Running", Reason="", readiness=false. Elapsed: 12.171406143s
Aug  6 15:55:53.641: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Running", Reason="", readiness=false. Elapsed: 14.194239091s
Aug  6 15:55:55.664: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Running", Reason="", readiness=false. Elapsed: 16.217354971s
Aug  6 15:55:57.688: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Running", Reason="", readiness=false. Elapsed: 18.240831704s
Aug  6 15:55:59.713: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Running", Reason="", readiness=false. Elapsed: 20.265841316s
Aug  6 15:56:01.736: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Running", Reason="", readiness=false. Elapsed: 22.289040949s
Aug  6 15:56:03.760: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Running", Reason="", readiness=false. Elapsed: 24.313157146s
Aug  6 15:56:05.783: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Running", Reason="", readiness=false. Elapsed: 26.336326907s
Aug  6 15:56:07.806: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Running", Reason="", readiness=false. Elapsed: 28.359434306s
Aug  6 15:56:09.832: INFO: Pod "pod-subpath-test-configmap-zhfx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.385246547s
STEP: Saw pod success
Aug  6 15:56:09.832: INFO: Pod "pod-subpath-test-configmap-zhfx" satisfied condition "success or failure"
Aug  6 15:56:09.855: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-subpath-test-configmap-zhfx container test-container-subpath-configmap-zhfx: <nil>
STEP: delete the pod
Aug  6 15:56:09.913: INFO: Waiting for pod pod-subpath-test-configmap-zhfx to disappear
Aug  6 15:56:09.935: INFO: Pod pod-subpath-test-configmap-zhfx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zhfx
Aug  6 15:56:09.935: INFO: Deleting pod "pod-subpath-test-configmap-zhfx" in namespace "e2e-tests-subpath-hv9ms"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:56:09.957: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-hv9ms" for this suite.
Aug  6 15:56:16.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:56:17.296: INFO: namespace: e2e-tests-subpath-hv9ms, resource: packagemanifests, items remaining: 3
Aug  6 15:56:17.544: INFO: namespace: e2e-tests-subpath-hv9ms, resource: bindings, ignored listing per whitelist
Aug  6 15:56:18.547: INFO: namespace: e2e-tests-subpath-hv9ms no longer exists
Aug  6 15:56:18.590: INFO: namespace: e2e-tests-subpath-hv9ms, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:56:18.612: INFO: namespace e2e-tests-subpath-hv9ms deletion completed in 8.572375031s

• [SLOW TEST:40.706 seconds]
[sig-storage] Subpath
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:56:18.613: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:56:28.115: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-njmtz" for this suite.
Aug  6 15:57:10.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:57:12.361: INFO: namespace: e2e-tests-kubelet-test-njmtz, resource: packagemanifests, items remaining: 3
Aug  6 15:57:12.471: INFO: namespace: e2e-tests-kubelet-test-njmtz, resource: bindings, ignored listing per whitelist
Aug  6 15:57:12.719: INFO: namespace: e2e-tests-kubelet-test-njmtz no longer exists
Aug  6 15:57:12.762: INFO: namespace: e2e-tests-kubelet-test-njmtz, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:57:12.784: INFO: namespace e2e-tests-kubelet-test-njmtz deletion completed in 44.606379351s

• [SLOW TEST:54.172 seconds]
[k8s.io] Kubelet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:57:12.786: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug  6 15:57:32.376: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug  6 15:57:32.399: INFO: Pod pod-with-prestop-http-hook still exists
Aug  6 15:57:34.399: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug  6 15:57:34.423: INFO: Pod pod-with-prestop-http-hook still exists
Aug  6 15:57:36.400: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug  6 15:57:36.424: INFO: Pod pod-with-prestop-http-hook still exists
Aug  6 15:57:38.399: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug  6 15:57:38.423: INFO: Pod pod-with-prestop-http-hook still exists
Aug  6 15:57:40.400: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug  6 15:57:40.425: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:57:40.454: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-jcdjz" for this suite.
Aug  6 15:58:02.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:58:03.451: INFO: namespace: e2e-tests-container-lifecycle-hook-jcdjz, resource: bindings, ignored listing per whitelist
Aug  6 15:58:03.502: INFO: namespace: e2e-tests-container-lifecycle-hook-jcdjz, resource: packagemanifests, items remaining: 3
Aug  6 15:58:05.033: INFO: namespace: e2e-tests-container-lifecycle-hook-jcdjz no longer exists
Aug  6 15:58:05.076: INFO: namespace: e2e-tests-container-lifecycle-hook-jcdjz, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:58:05.099: INFO: namespace e2e-tests-container-lifecycle-hook-jcdjz deletion completed in 24.561108373s

• [SLOW TEST:52.314 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:58:05.100: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-c8kt6/configmap-test-fac22be3-b862-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 15:58:06.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-fac5b4a2-b862-11e9-8d18-525400524259" in namespace "e2e-tests-configmap-c8kt6" to be "success or failure"
Aug  6 15:58:06.563: INFO: Pod "pod-configmaps-fac5b4a2-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.591895ms
Aug  6 15:58:08.586: INFO: Pod "pod-configmaps-fac5b4a2-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045588641s
Aug  6 15:58:10.609: INFO: Pod "pod-configmaps-fac5b4a2-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068585977s
Aug  6 15:58:12.632: INFO: Pod "pod-configmaps-fac5b4a2-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091614991s
Aug  6 15:58:14.655: INFO: Pod "pod-configmaps-fac5b4a2-b862-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114707821s
Aug  6 15:58:16.678: INFO: Pod "pod-configmaps-fac5b4a2-b862-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.13805275s
STEP: Saw pod success
Aug  6 15:58:16.678: INFO: Pod "pod-configmaps-fac5b4a2-b862-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:58:16.701: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-configmaps-fac5b4a2-b862-11e9-8d18-525400524259 container env-test: <nil>
STEP: delete the pod
Aug  6 15:58:16.756: INFO: Waiting for pod pod-configmaps-fac5b4a2-b862-11e9-8d18-525400524259 to disappear
Aug  6 15:58:16.778: INFO: Pod pod-configmaps-fac5b4a2-b862-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:58:16.778: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-c8kt6" for this suite.
Aug  6 15:58:22.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:58:24.343: INFO: namespace: e2e-tests-configmap-c8kt6, resource: packagemanifests, items remaining: 3
Aug  6 15:58:24.839: INFO: namespace: e2e-tests-configmap-c8kt6, resource: bindings, ignored listing per whitelist
Aug  6 15:58:25.349: INFO: namespace: e2e-tests-configmap-c8kt6 no longer exists
Aug  6 15:58:25.392: INFO: namespace: e2e-tests-configmap-c8kt6, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:58:25.415: INFO: namespace e2e-tests-configmap-c8kt6 deletion completed in 8.572000475s

• [SLOW TEST:20.315 seconds]
[sig-node] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:58:25.415: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Aug  6 15:58:26.793: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-cswp4'
Aug  6 15:58:28.774: INFO: stderr: ""
Aug  6 15:58:28.774: INFO: stdout: "pod/pause created\n"
Aug  6 15:58:28.774: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug  6 15:58:28.775: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-cswp4" to be "running and ready"
Aug  6 15:58:28.801: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 26.867765ms
Aug  6 15:58:30.825: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050171834s
Aug  6 15:58:32.848: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073324222s
Aug  6 15:58:34.872: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.097872541s
Aug  6 15:58:36.895: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.120798278s
Aug  6 15:58:38.919: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 10.144749642s
Aug  6 15:58:38.919: INFO: Pod "pause" satisfied condition "running and ready"
Aug  6 15:58:38.919: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Aug  6 15:58:38.920: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-cswp4'
Aug  6 15:58:39.115: INFO: stderr: ""
Aug  6 15:58:39.115: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug  6 15:58:39.115: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pod pause -L testing-label --namespace=e2e-tests-kubectl-cswp4'
Aug  6 15:58:39.269: INFO: stderr: ""
Aug  6 15:58:39.269: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          10s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug  6 15:58:39.269: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance label pods pause testing-label- --namespace=e2e-tests-kubectl-cswp4'
Aug  6 15:58:39.455: INFO: stderr: ""
Aug  6 15:58:39.455: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug  6 15:58:39.455: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pod pause -L testing-label --namespace=e2e-tests-kubectl-cswp4'
Aug  6 15:58:39.606: INFO: stderr: ""
Aug  6 15:58:39.606: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   \n"
[AfterEach] [k8s.io] Kubectl label
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Aug  6 15:58:39.606: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cswp4'
Aug  6 15:58:39.785: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  6 15:58:39.785: INFO: stdout: "pod \"pause\" force deleted\n"
Aug  6 15:58:39.785: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-cswp4'
Aug  6 15:58:39.970: INFO: stderr: "No resources found.\n"
Aug  6 15:58:39.970: INFO: stdout: ""
Aug  6 15:58:39.970: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -l name=pause --namespace=e2e-tests-kubectl-cswp4 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug  6 15:58:40.124: INFO: stderr: ""
Aug  6 15:58:40.124: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:58:40.124: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cswp4" for this suite.
Aug  6 15:58:46.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:58:47.740: INFO: namespace: e2e-tests-kubectl-cswp4, resource: bindings, ignored listing per whitelist
Aug  6 15:58:48.452: INFO: namespace: e2e-tests-kubectl-cswp4, resource: packagemanifests, items remaining: 3
Aug  6 15:58:48.672: INFO: namespace: e2e-tests-kubectl-cswp4 no longer exists
Aug  6 15:58:48.695: INFO: namespace: e2e-tests-kubectl-cswp4, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:58:48.717: INFO: namespace e2e-tests-kubectl-cswp4 deletion completed in 8.529457076s

• [SLOW TEST:23.303 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:58:48.718: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 15:58:50.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14bcf7d8-b863-11e9-8d18-525400524259" in namespace "e2e-tests-projected-ktv9n" to be "success or failure"
Aug  6 15:58:50.128: INFO: Pod "downwardapi-volume-14bcf7d8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.109206ms
Aug  6 15:58:52.151: INFO: Pod "downwardapi-volume-14bcf7d8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046266746s
Aug  6 15:58:54.175: INFO: Pod "downwardapi-volume-14bcf7d8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07007412s
Aug  6 15:58:56.198: INFO: Pod "downwardapi-volume-14bcf7d8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09318847s
Aug  6 15:58:58.221: INFO: Pod "downwardapi-volume-14bcf7d8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116593266s
Aug  6 15:59:00.244: INFO: Pod "downwardapi-volume-14bcf7d8-b863-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.139480492s
STEP: Saw pod success
Aug  6 15:59:00.244: INFO: Pod "downwardapi-volume-14bcf7d8-b863-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:59:00.267: INFO: Trying to get logs from node ip-10-0-128-44.ec2.internal pod downwardapi-volume-14bcf7d8-b863-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 15:59:00.321: INFO: Waiting for pod downwardapi-volume-14bcf7d8-b863-11e9-8d18-525400524259 to disappear
Aug  6 15:59:00.344: INFO: Pod downwardapi-volume-14bcf7d8-b863-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:59:00.344: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ktv9n" for this suite.
Aug  6 15:59:06.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:59:07.618: INFO: namespace: e2e-tests-projected-ktv9n, resource: packagemanifests, items remaining: 3
Aug  6 15:59:08.147: INFO: namespace: e2e-tests-projected-ktv9n, resource: bindings, ignored listing per whitelist
Aug  6 15:59:08.900: INFO: namespace: e2e-tests-projected-ktv9n no longer exists
Aug  6 15:59:08.924: INFO: namespace: e2e-tests-projected-ktv9n, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:59:08.949: INFO: namespace e2e-tests-projected-ktv9n deletion completed in 8.542135839s

• [SLOW TEST:20.231 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:59:08.949: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Aug  6 15:59:20.465: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-20cbd0bb-b863-11e9-8d18-525400524259", GenerateName:"", Namespace:"e2e-tests-pods-tt9v8", SelfLink:"/api/v1/namespaces/e2e-tests-pods-tt9v8/pods/pod-submit-remove-20cbd0bb-b863-11e9-8d18-525400524259", UID:"1360209b-b863-11e9-9dff-0e6de702691c", ResourceVersion:"126183", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63700703927, loc:(*time.Location)(0x81cdd60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"301210965"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.135\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-62scb", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0024e9180), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-62scb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002fdab40), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002bb4e48), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-130-134.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002f71b00), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-zktrw"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002bb4e90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002bb4eb0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002bb4eb8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002bb4ebc)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703927, loc:(*time.Location)(0x81cdd60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703936, loc:(*time.Location)(0x81cdd60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703936, loc:(*time.Location)(0x81cdd60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700703927, loc:(*time.Location)(0x81cdd60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.130.134", PodIP:"10.131.0.135", StartTime:(*v1.Time)(0xc00201a2e0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc00201a300), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"docker.io/library/nginx:1.14-alpine", ImageID:"docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760", ContainerID:"cri-o://e3d369101a1d783aa5eb7ba6eb8d72d77d5bc46f8ad0cf78ac683b951d4bc13d"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug  6 15:59:25.600: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:59:25.623: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-tt9v8" for this suite.
Aug  6 15:59:31.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:59:32.663: INFO: namespace: e2e-tests-pods-tt9v8, resource: bindings, ignored listing per whitelist
Aug  6 15:59:33.185: INFO: namespace: e2e-tests-pods-tt9v8, resource: packagemanifests, items remaining: 3
Aug  6 15:59:34.150: INFO: namespace: e2e-tests-pods-tt9v8 no longer exists
Aug  6 15:59:34.173: INFO: namespace: e2e-tests-pods-tt9v8, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:59:34.195: INFO: namespace e2e-tests-pods-tt9v8 deletion completed in 8.528033s

• [SLOW TEST:25.245 seconds]
[k8s.io] Pods
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:59:34.195: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug  6 15:59:35.587: INFO: Waiting up to 5m0s for pod "pod-2fd942a8-b863-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-fxh85" to be "success or failure"
Aug  6 15:59:35.609: INFO: Pod "pod-2fd942a8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.099451ms
Aug  6 15:59:37.631: INFO: Pod "pod-2fd942a8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044635188s
Aug  6 15:59:39.654: INFO: Pod "pod-2fd942a8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067587857s
Aug  6 15:59:41.677: INFO: Pod "pod-2fd942a8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0903946s
Aug  6 15:59:43.700: INFO: Pod "pod-2fd942a8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113160728s
Aug  6 15:59:45.724: INFO: Pod "pod-2fd942a8-b863-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.136901311s
STEP: Saw pod success
Aug  6 15:59:45.724: INFO: Pod "pod-2fd942a8-b863-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 15:59:45.749: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-2fd942a8-b863-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 15:59:45.814: INFO: Waiting for pod pod-2fd942a8-b863-11e9-8d18-525400524259 to disappear
Aug  6 15:59:45.836: INFO: Pod pod-2fd942a8-b863-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 15:59:45.836: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-fxh85" for this suite.
Aug  6 15:59:51.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 15:59:54.132: INFO: namespace: e2e-tests-emptydir-fxh85, resource: bindings, ignored listing per whitelist
Aug  6 15:59:54.339: INFO: namespace: e2e-tests-emptydir-fxh85, resource: packagemanifests, items remaining: 3
Aug  6 15:59:54.383: INFO: namespace: e2e-tests-emptydir-fxh85 no longer exists
Aug  6 15:59:54.405: INFO: namespace: e2e-tests-emptydir-fxh85, total namespaces: 50, active: 50, terminating: 0
Aug  6 15:59:54.427: INFO: namespace e2e-tests-emptydir-fxh85 deletion completed in 8.528033584s

• [SLOW TEST:20.232 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 15:59:54.427: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Aug  6 15:59:55.831: INFO: Waiting up to 5m0s for pod "client-containers-3bea7333-b863-11e9-8d18-525400524259" in namespace "e2e-tests-containers-5k2wr" to be "success or failure"
Aug  6 15:59:55.853: INFO: Pod "client-containers-3bea7333-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.222502ms
Aug  6 15:59:57.876: INFO: Pod "client-containers-3bea7333-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045134061s
Aug  6 15:59:59.899: INFO: Pod "client-containers-3bea7333-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067895581s
Aug  6 16:00:01.922: INFO: Pod "client-containers-3bea7333-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091252078s
Aug  6 16:00:03.945: INFO: Pod "client-containers-3bea7333-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114332653s
Aug  6 16:00:05.968: INFO: Pod "client-containers-3bea7333-b863-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137089483s
STEP: Saw pod success
Aug  6 16:00:05.968: INFO: Pod "client-containers-3bea7333-b863-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:00:05.990: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod client-containers-3bea7333-b863-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 16:00:06.046: INFO: Waiting for pod client-containers-3bea7333-b863-11e9-8d18-525400524259 to disappear
Aug  6 16:00:06.068: INFO: Pod client-containers-3bea7333-b863-11e9-8d18-525400524259 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:00:06.068: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-5k2wr" for this suite.
Aug  6 16:00:12.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:00:12.979: INFO: namespace: e2e-tests-containers-5k2wr, resource: bindings, ignored listing per whitelist
Aug  6 16:00:14.345: INFO: namespace: e2e-tests-containers-5k2wr, resource: packagemanifests, items remaining: 3
Aug  6 16:00:14.610: INFO: namespace: e2e-tests-containers-5k2wr no longer exists
Aug  6 16:00:14.633: INFO: namespace: e2e-tests-containers-5k2wr, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:00:14.655: INFO: namespace e2e-tests-containers-5k2wr deletion completed in 8.52364789s

• [SLOW TEST:20.227 seconds]
[k8s.io] Docker Containers
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:00:14.655: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-47f8f7b3-b863-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 16:00:16.099: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-47fc86b0-b863-11e9-8d18-525400524259" in namespace "e2e-tests-projected-ff6ch" to be "success or failure"
Aug  6 16:00:16.121: INFO: Pod "pod-projected-secrets-47fc86b0-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.420855ms
Aug  6 16:00:18.144: INFO: Pod "pod-projected-secrets-47fc86b0-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045176412s
Aug  6 16:00:20.171: INFO: Pod "pod-projected-secrets-47fc86b0-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07181608s
Aug  6 16:00:22.193: INFO: Pod "pod-projected-secrets-47fc86b0-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09472497s
Aug  6 16:00:24.217: INFO: Pod "pod-projected-secrets-47fc86b0-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.118251513s
Aug  6 16:00:26.240: INFO: Pod "pod-projected-secrets-47fc86b0-b863-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.141001887s
STEP: Saw pod success
Aug  6 16:00:26.240: INFO: Pod "pod-projected-secrets-47fc86b0-b863-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:00:26.262: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-secrets-47fc86b0-b863-11e9-8d18-525400524259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug  6 16:00:26.319: INFO: Waiting for pod pod-projected-secrets-47fc86b0-b863-11e9-8d18-525400524259 to disappear
Aug  6 16:00:26.341: INFO: Pod pod-projected-secrets-47fc86b0-b863-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:00:26.341: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ff6ch" for this suite.
Aug  6 16:00:32.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:00:33.554: INFO: namespace: e2e-tests-projected-ff6ch, resource: bindings, ignored listing per whitelist
Aug  6 16:00:34.264: INFO: namespace: e2e-tests-projected-ff6ch, resource: packagemanifests, items remaining: 3
Aug  6 16:00:34.885: INFO: namespace: e2e-tests-projected-ff6ch no longer exists
Aug  6 16:00:34.908: INFO: namespace: e2e-tests-projected-ff6ch, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:00:34.930: INFO: namespace e2e-tests-projected-ff6ch deletion completed in 8.525224034s

• [SLOW TEST:20.275 seconds]
[sig-storage] Projected secret
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:00:34.931: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  6 16:00:36.303: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-5zqvf'
Aug  6 16:00:36.483: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug  6 16:00:36.483: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Aug  6 16:00:36.665: INFO: scanned /home/jeder for discovery docs: <nil>
Aug  6 16:00:36.665: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-5zqvf'
Aug  6 16:00:55.262: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug  6 16:00:55.262: INFO: stdout: "Created e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4\nScaling up e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Aug  6 16:00:55.262: INFO: stdout: "Created e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4\nScaling up e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Aug  6 16:00:55.262: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-5zqvf'
Aug  6 16:00:55.418: INFO: stderr: ""
Aug  6 16:00:55.418: INFO: stdout: "e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4-d2s4g e2e-test-nginx-rc-p85n6 "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Aug  6 16:01:00.418: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-5zqvf'
Aug  6 16:01:00.578: INFO: stderr: ""
Aug  6 16:01:00.578: INFO: stdout: "e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4-d2s4g "
Aug  6 16:01:00.578: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4-d2s4g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5zqvf'
Aug  6 16:01:00.733: INFO: stderr: ""
Aug  6 16:01:00.733: INFO: stdout: "true"
Aug  6 16:01:00.733: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4-d2s4g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5zqvf'
Aug  6 16:01:00.891: INFO: stderr: ""
Aug  6 16:01:00.891: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Aug  6 16:01:00.891: INFO: e2e-test-nginx-rc-622ab98b36d426669afb3bd9718197e4-d2s4g is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Aug  6 16:01:00.891: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-5zqvf'
Aug  6 16:01:01.078: INFO: stderr: ""
Aug  6 16:01:01.078: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:01:01.078: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5zqvf" for this suite.
Aug  6 16:01:23.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:01:23.955: INFO: namespace: e2e-tests-kubectl-5zqvf, resource: packagemanifests, items remaining: 3
Aug  6 16:01:24.776: INFO: namespace: e2e-tests-kubectl-5zqvf, resource: bindings, ignored listing per whitelist
Aug  6 16:01:25.644: INFO: namespace: e2e-tests-kubectl-5zqvf no longer exists
Aug  6 16:01:25.666: INFO: namespace: e2e-tests-kubectl-5zqvf, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:01:25.689: INFO: namespace e2e-tests-kubectl-5zqvf deletion completed in 24.547738695s

• [SLOW TEST:50.758 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:01:25.689: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 16:01:27.085: INFO: Waiting up to 5m0s for pod "downwardapi-volume-724ed04b-b863-11e9-8d18-525400524259" in namespace "e2e-tests-projected-l579d" to be "success or failure"
Aug  6 16:01:27.108: INFO: Pod "downwardapi-volume-724ed04b-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.703885ms
Aug  6 16:01:29.132: INFO: Pod "downwardapi-volume-724ed04b-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047297048s
Aug  6 16:01:31.156: INFO: Pod "downwardapi-volume-724ed04b-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07052315s
Aug  6 16:01:33.178: INFO: Pod "downwardapi-volume-724ed04b-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093226984s
Aug  6 16:01:35.201: INFO: Pod "downwardapi-volume-724ed04b-b863-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.116239526s
STEP: Saw pod success
Aug  6 16:01:35.201: INFO: Pod "downwardapi-volume-724ed04b-b863-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:01:35.223: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-724ed04b-b863-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 16:01:35.277: INFO: Waiting for pod downwardapi-volume-724ed04b-b863-11e9-8d18-525400524259 to disappear
Aug  6 16:01:35.299: INFO: Pod downwardapi-volume-724ed04b-b863-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:01:35.299: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l579d" for this suite.
Aug  6 16:01:41.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:01:42.250: INFO: namespace: e2e-tests-projected-l579d, resource: bindings, ignored listing per whitelist
Aug  6 16:01:43.770: INFO: namespace: e2e-tests-projected-l579d, resource: packagemanifests, items remaining: 3
Aug  6 16:01:43.858: INFO: namespace: e2e-tests-projected-l579d no longer exists
Aug  6 16:01:43.881: INFO: namespace: e2e-tests-projected-l579d, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:01:43.903: INFO: namespace e2e-tests-projected-l579d deletion completed in 8.541687628s

• [SLOW TEST:18.215 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:01:43.904: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug  6 16:02:05.481: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:05.503: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:07.504: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:07.527: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:09.504: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:09.527: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:11.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:11.526: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:13.504: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:13.527: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:15.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:15.527: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:17.504: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:17.527: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:19.504: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:19.527: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:21.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:21.527: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:23.504: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:23.527: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:25.504: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:25.527: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:27.504: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:27.527: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  6 16:02:29.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  6 16:02:29.527: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:02:29.552: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-smq2g" for this suite.
Aug  6 16:02:51.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:02:53.461: INFO: namespace: e2e-tests-container-lifecycle-hook-smq2g, resource: bindings, ignored listing per whitelist
Aug  6 16:02:54.014: INFO: namespace: e2e-tests-container-lifecycle-hook-smq2g, resource: packagemanifests, items remaining: 3
Aug  6 16:02:54.102: INFO: namespace: e2e-tests-container-lifecycle-hook-smq2g no longer exists
Aug  6 16:02:54.125: INFO: namespace: e2e-tests-container-lifecycle-hook-smq2g, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:02:54.148: INFO: namespace e2e-tests-container-lifecycle-hook-smq2g deletion completed in 24.533441242s

• [SLOW TEST:70.245 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:02:54.148: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:03:21.605: INFO: Container started at 2019-08-06 16:02:41 +0000 UTC, pod became ready at 2019-08-06 16:02:58 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:03:21.605: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-2hs94" for this suite.
Aug  6 16:03:43.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:03:44.449: INFO: namespace: e2e-tests-container-probe-2hs94, resource: bindings, ignored listing per whitelist
Aug  6 16:03:45.760: INFO: namespace: e2e-tests-container-probe-2hs94, resource: packagemanifests, items remaining: 3
Aug  6 16:03:46.155: INFO: namespace: e2e-tests-container-probe-2hs94 no longer exists
Aug  6 16:03:46.178: INFO: namespace: e2e-tests-container-probe-2hs94, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:03:46.200: INFO: namespace e2e-tests-container-probe-2hs94 deletion completed in 24.530989159s

• [SLOW TEST:52.052 seconds]
[k8s.io] Probing container
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:03:46.200: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug  6 16:03:47.685: INFO: Waiting up to 5m0s for pod "pod-c618ad11-b863-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-d8pxp" to be "success or failure"
Aug  6 16:03:47.714: INFO: Pod "pod-c618ad11-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 29.645153ms
Aug  6 16:03:49.737: INFO: Pod "pod-c618ad11-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052267872s
Aug  6 16:03:51.760: INFO: Pod "pod-c618ad11-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075218489s
Aug  6 16:03:53.783: INFO: Pod "pod-c618ad11-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098069349s
Aug  6 16:03:55.806: INFO: Pod "pod-c618ad11-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.121044122s
Aug  6 16:03:57.830: INFO: Pod "pod-c618ad11-b863-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.145249621s
STEP: Saw pod success
Aug  6 16:03:57.830: INFO: Pod "pod-c618ad11-b863-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:03:57.852: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-c618ad11-b863-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 16:03:57.911: INFO: Waiting for pod pod-c618ad11-b863-11e9-8d18-525400524259 to disappear
Aug  6 16:03:57.933: INFO: Pod pod-c618ad11-b863-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:03:57.933: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-d8pxp" for this suite.
Aug  6 16:04:04.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:04:05.791: INFO: namespace: e2e-tests-emptydir-d8pxp, resource: bindings, ignored listing per whitelist
Aug  6 16:04:06.418: INFO: namespace: e2e-tests-emptydir-d8pxp, resource: packagemanifests, items remaining: 3
Aug  6 16:04:06.484: INFO: namespace: e2e-tests-emptydir-d8pxp no longer exists
Aug  6 16:04:06.507: INFO: namespace: e2e-tests-emptydir-d8pxp, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:04:06.530: INFO: namespace e2e-tests-emptydir-d8pxp deletion completed in 8.534454818s

• [SLOW TEST:20.330 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:04:06.530: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug  6 16:04:07.920: INFO: Waiting up to 5m0s for pod "pod-d22b1fd5-b863-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-gdsxx" to be "success or failure"
Aug  6 16:04:07.942: INFO: Pod "pod-d22b1fd5-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.487541ms
Aug  6 16:04:09.965: INFO: Pod "pod-d22b1fd5-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045261291s
Aug  6 16:04:11.988: INFO: Pod "pod-d22b1fd5-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06815243s
Aug  6 16:04:14.010: INFO: Pod "pod-d22b1fd5-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090733235s
Aug  6 16:04:16.033: INFO: Pod "pod-d22b1fd5-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.11379669s
Aug  6 16:04:18.056: INFO: Pod "pod-d22b1fd5-b863-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.136710863s
STEP: Saw pod success
Aug  6 16:04:18.056: INFO: Pod "pod-d22b1fd5-b863-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:04:18.080: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-d22b1fd5-b863-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 16:04:18.136: INFO: Waiting for pod pod-d22b1fd5-b863-11e9-8d18-525400524259 to disappear
Aug  6 16:04:18.158: INFO: Pod pod-d22b1fd5-b863-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:04:18.158: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gdsxx" for this suite.
Aug  6 16:04:24.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:04:25.087: INFO: namespace: e2e-tests-emptydir-gdsxx, resource: bindings, ignored listing per whitelist
Aug  6 16:04:25.516: INFO: namespace: e2e-tests-emptydir-gdsxx, resource: packagemanifests, items remaining: 3
Aug  6 16:04:26.718: INFO: namespace: e2e-tests-emptydir-gdsxx no longer exists
Aug  6 16:04:26.743: INFO: namespace: e2e-tests-emptydir-gdsxx, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:04:26.765: INFO: namespace e2e-tests-emptydir-gdsxx deletion completed in 8.54385071s

• [SLOW TEST:20.234 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:04:26.765: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Aug  6 16:04:28.162: INFO: Waiting up to 5m0s for pod "var-expansion-de399978-b863-11e9-8d18-525400524259" in namespace "e2e-tests-var-expansion-9vpz5" to be "success or failure"
Aug  6 16:04:28.186: INFO: Pod "var-expansion-de399978-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 24.418952ms
Aug  6 16:04:30.209: INFO: Pod "var-expansion-de399978-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047116779s
Aug  6 16:04:32.233: INFO: Pod "var-expansion-de399978-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071039596s
Aug  6 16:04:34.256: INFO: Pod "var-expansion-de399978-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093881981s
Aug  6 16:04:36.279: INFO: Pod "var-expansion-de399978-b863-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.117128926s
STEP: Saw pod success
Aug  6 16:04:36.279: INFO: Pod "var-expansion-de399978-b863-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:04:36.302: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod var-expansion-de399978-b863-11e9-8d18-525400524259 container dapi-container: <nil>
STEP: delete the pod
Aug  6 16:04:36.357: INFO: Waiting for pod var-expansion-de399978-b863-11e9-8d18-525400524259 to disappear
Aug  6 16:04:36.379: INFO: Pod var-expansion-de399978-b863-11e9-8d18-525400524259 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:04:36.380: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-9vpz5" for this suite.
Aug  6 16:04:42.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:04:44.145: INFO: namespace: e2e-tests-var-expansion-9vpz5, resource: bindings, ignored listing per whitelist
Aug  6 16:04:44.613: INFO: namespace: e2e-tests-var-expansion-9vpz5, resource: packagemanifests, items remaining: 3
Aug  6 16:04:44.926: INFO: namespace: e2e-tests-var-expansion-9vpz5 no longer exists
Aug  6 16:04:44.949: INFO: namespace: e2e-tests-var-expansion-9vpz5, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:04:44.972: INFO: namespace e2e-tests-var-expansion-9vpz5 deletion completed in 8.52963724s

• [SLOW TEST:18.206 seconds]
[k8s.io] Variable Expansion
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:04:44.972: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:04:46.394: INFO: (0) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 24.717048ms)
Aug  6 16:04:46.417: INFO: (1) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.52385ms)
Aug  6 16:04:46.440: INFO: (2) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.853648ms)
Aug  6 16:04:46.463: INFO: (3) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 23.312034ms)
Aug  6 16:04:46.486: INFO: (4) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.657485ms)
Aug  6 16:04:46.508: INFO: (5) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.355542ms)
Aug  6 16:04:46.531: INFO: (6) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.447238ms)
Aug  6 16:04:46.554: INFO: (7) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.891151ms)
Aug  6 16:04:46.576: INFO: (8) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.486771ms)
Aug  6 16:04:46.599: INFO: (9) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.557394ms)
Aug  6 16:04:46.621: INFO: (10) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.685823ms)
Aug  6 16:04:46.644: INFO: (11) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.566146ms)
Aug  6 16:04:46.667: INFO: (12) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.789106ms)
Aug  6 16:04:46.689: INFO: (13) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.599587ms)
Aug  6 16:04:46.712: INFO: (14) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.671167ms)
Aug  6 16:04:46.735: INFO: (15) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.629524ms)
Aug  6 16:04:46.757: INFO: (16) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.337291ms)
Aug  6 16:04:46.780: INFO: (17) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.405625ms)
Aug  6 16:04:46.802: INFO: (18) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.227683ms)
Aug  6 16:04:46.824: INFO: (19) /api/v1/nodes/ip-10-0-128-44.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.544097ms)
[AfterEach] version v1
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:04:46.824: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-drwxx" for this suite.
Aug  6 16:04:52.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:04:53.112: INFO: namespace: e2e-tests-proxy-drwxx, resource: bindings, ignored listing per whitelist
Aug  6 16:04:53.873: INFO: namespace: e2e-tests-proxy-drwxx, resource: packagemanifests, items remaining: 3
Aug  6 16:04:54.845: INFO: namespace: e2e-tests-proxy-drwxx no longer exists
Aug  6 16:04:54.867: INFO: namespace: e2e-tests-proxy-drwxx, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:04:54.890: INFO: namespace e2e-tests-proxy-drwxx deletion completed in 8.042013926s

• [SLOW TEST:9.918 seconds]
[sig-network] Proxy
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:04:54.890: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-eeff142a-b863-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 16:04:56.322: INFO: Waiting up to 5m0s for pod "pod-secrets-ef02a3e8-b863-11e9-8d18-525400524259" in namespace "e2e-tests-secrets-p2sxf" to be "success or failure"
Aug  6 16:04:56.345: INFO: Pod "pod-secrets-ef02a3e8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.240359ms
Aug  6 16:04:58.368: INFO: Pod "pod-secrets-ef02a3e8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045486181s
Aug  6 16:05:00.390: INFO: Pod "pod-secrets-ef02a3e8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068041879s
Aug  6 16:05:02.414: INFO: Pod "pod-secrets-ef02a3e8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091492084s
Aug  6 16:05:04.437: INFO: Pod "pod-secrets-ef02a3e8-b863-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114338634s
Aug  6 16:05:06.460: INFO: Pod "pod-secrets-ef02a3e8-b863-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137289929s
STEP: Saw pod success
Aug  6 16:05:06.460: INFO: Pod "pod-secrets-ef02a3e8-b863-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:05:06.482: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-secrets-ef02a3e8-b863-11e9-8d18-525400524259 container secret-env-test: <nil>
STEP: delete the pod
Aug  6 16:05:06.538: INFO: Waiting for pod pod-secrets-ef02a3e8-b863-11e9-8d18-525400524259 to disappear
Aug  6 16:05:06.560: INFO: Pod pod-secrets-ef02a3e8-b863-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:05:06.560: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-p2sxf" for this suite.
Aug  6 16:05:12.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:05:13.685: INFO: namespace: e2e-tests-secrets-p2sxf, resource: bindings, ignored listing per whitelist
Aug  6 16:05:14.335: INFO: namespace: e2e-tests-secrets-p2sxf, resource: packagemanifests, items remaining: 3
Aug  6 16:05:15.130: INFO: namespace: e2e-tests-secrets-p2sxf no longer exists
Aug  6 16:05:15.152: INFO: namespace: e2e-tests-secrets-p2sxf, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:05:15.175: INFO: namespace e2e-tests-secrets-p2sxf deletion completed in 8.552305919s

• [SLOW TEST:20.285 seconds]
[sig-api-machinery] Secrets
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:05:15.175: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-l7xwn
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug  6 16:05:16.536: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Aug  6 16:05:53.095: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.153:8080/dial?request=hostName&protocol=udp&host=10.131.0.152&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-l7xwn PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:05:53.095: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:05:53.365: INFO: Waiting for endpoints: map[]
Aug  6 16:05:53.387: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.153:8080/dial?request=hostName&protocol=udp&host=10.130.2.31&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-l7xwn PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:05:53.387: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:05:53.613: INFO: Waiting for endpoints: map[]
Aug  6 16:05:53.635: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.153:8080/dial?request=hostName&protocol=udp&host=10.129.2.57&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-l7xwn PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:05:53.635: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:05:53.835: INFO: Waiting for endpoints: map[]
Aug  6 16:05:53.858: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.153:8080/dial?request=hostName&protocol=udp&host=10.128.2.30&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-l7xwn PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:05:53.858: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:05:54.067: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:05:54.067: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-l7xwn" for this suite.
Aug  6 16:06:16.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:06:17.643: INFO: namespace: e2e-tests-pod-network-test-l7xwn, resource: bindings, ignored listing per whitelist
Aug  6 16:06:18.257: INFO: namespace: e2e-tests-pod-network-test-l7xwn, resource: packagemanifests, items remaining: 3
Aug  6 16:06:18.721: INFO: namespace: e2e-tests-pod-network-test-l7xwn no longer exists
Aug  6 16:06:18.784: INFO: namespace: e2e-tests-pod-network-test-l7xwn, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:06:18.807: INFO: namespace e2e-tests-pod-network-test-l7xwn deletion completed in 24.636330036s

• [SLOW TEST:63.632 seconds]
[sig-network] Networking
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:06:18.808: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug  6 16:06:32.381: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-j8g86 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:06:32.382: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:06:32.586: INFO: Exec stderr: ""
Aug  6 16:06:32.586: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-j8g86 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:06:32.586: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:06:32.787: INFO: Exec stderr: ""
Aug  6 16:06:32.787: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-j8g86 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:06:32.787: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:06:32.986: INFO: Exec stderr: ""
Aug  6 16:06:32.986: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-j8g86 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:06:32.986: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:06:33.183: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug  6 16:06:33.183: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-j8g86 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:06:33.183: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:06:33.384: INFO: Exec stderr: ""
Aug  6 16:06:33.384: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-j8g86 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:06:33.384: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:06:33.586: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug  6 16:06:33.586: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-j8g86 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:06:33.586: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:06:33.782: INFO: Exec stderr: ""
Aug  6 16:06:33.782: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-j8g86 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:06:33.782: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:06:33.977: INFO: Exec stderr: ""
Aug  6 16:06:33.977: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-j8g86 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:06:33.977: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:06:34.175: INFO: Exec stderr: ""
Aug  6 16:06:34.175: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-j8g86 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  6 16:06:34.175: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
Aug  6 16:06:34.370: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:06:34.370: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-j8g86" for this suite.
Aug  6 16:07:14.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:07:15.727: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-j8g86, resource: packagemanifests, items remaining: 3
Aug  6 16:07:16.344: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-j8g86, resource: bindings, ignored listing per whitelist
Aug  6 16:07:16.936: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-j8g86 no longer exists
Aug  6 16:07:16.980: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-j8g86, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:07:17.002: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-j8g86 deletion completed in 42.548871054s

• [SLOW TEST:58.194 seconds]
[k8s.io] KubeletManagedEtcHosts
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:07:17.003: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0806 16:07:28.556685    4087 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug  6 16:07:28.556: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:07:28.556: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4zmbq" for this suite.
Aug  6 16:07:34.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:07:36.662: INFO: namespace: e2e-tests-gc-4zmbq, resource: bindings, ignored listing per whitelist
Aug  6 16:07:37.065: INFO: namespace: e2e-tests-gc-4zmbq, resource: packagemanifests, items remaining: 3
Aug  6 16:07:37.108: INFO: namespace: e2e-tests-gc-4zmbq no longer exists
Aug  6 16:07:37.151: INFO: namespace: e2e-tests-gc-4zmbq, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:07:37.173: INFO: namespace e2e-tests-gc-4zmbq deletion completed in 8.554080686s

• [SLOW TEST:20.171 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:07:37.175: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-5lkn2.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5lkn2.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-5lkn2.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-5lkn2.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5lkn2.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-5lkn2.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug  6 16:07:55.116: INFO: DNS probes using e2e-tests-dns-5lkn2/dns-test-4fbbda0c-b864-11e9-8d18-525400524259 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:07:55.152: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-5lkn2" for this suite.
Aug  6 16:08:01.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:08:02.101: INFO: namespace: e2e-tests-dns-5lkn2, resource: bindings, ignored listing per whitelist
Aug  6 16:08:02.353: INFO: namespace: e2e-tests-dns-5lkn2, resource: packagemanifests, items remaining: 3
Aug  6 16:08:03.697: INFO: namespace: e2e-tests-dns-5lkn2 no longer exists
Aug  6 16:08:03.720: INFO: namespace: e2e-tests-dns-5lkn2, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:08:03.743: INFO: namespace e2e-tests-dns-5lkn2 deletion completed in 8.527457379s

• [SLOW TEST:26.568 seconds]
[sig-network] DNS
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:08:03.744: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-5f8f1072-b864-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 16:08:05.154: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f92a9c7-b864-11e9-8d18-525400524259" in namespace "e2e-tests-projected-t79km" to be "success or failure"
Aug  6 16:08:05.177: INFO: Pod "pod-projected-configmaps-5f92a9c7-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.781014ms
Aug  6 16:08:07.200: INFO: Pod "pod-projected-configmaps-5f92a9c7-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046084115s
Aug  6 16:08:09.224: INFO: Pod "pod-projected-configmaps-5f92a9c7-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07036017s
Aug  6 16:08:11.247: INFO: Pod "pod-projected-configmaps-5f92a9c7-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093430333s
Aug  6 16:08:13.271: INFO: Pod "pod-projected-configmaps-5f92a9c7-b864-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.117521636s
STEP: Saw pod success
Aug  6 16:08:13.271: INFO: Pod "pod-projected-configmaps-5f92a9c7-b864-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:08:13.294: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-configmaps-5f92a9c7-b864-11e9-8d18-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 16:08:13.350: INFO: Waiting for pod pod-projected-configmaps-5f92a9c7-b864-11e9-8d18-525400524259 to disappear
Aug  6 16:08:13.372: INFO: Pod pod-projected-configmaps-5f92a9c7-b864-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:08:13.372: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t79km" for this suite.
Aug  6 16:08:19.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:08:20.416: INFO: namespace: e2e-tests-projected-t79km, resource: bindings, ignored listing per whitelist
Aug  6 16:08:21.722: INFO: namespace: e2e-tests-projected-t79km, resource: packagemanifests, items remaining: 3
Aug  6 16:08:22.096: INFO: namespace: e2e-tests-projected-t79km no longer exists
Aug  6 16:08:22.120: INFO: namespace: e2e-tests-projected-t79km, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:08:22.142: INFO: namespace e2e-tests-projected-t79km deletion completed in 8.707053002s

• [SLOW TEST:18.398 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:08:22.143: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Aug  6 16:08:23.718: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance --namespace=e2e-tests-kubectl-k7pg9 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug  6 16:08:32.188: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug  6 16:08:32.188: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:08:34.232: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-k7pg9" for this suite.
Aug  6 16:08:40.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:08:41.133: INFO: namespace: e2e-tests-kubectl-k7pg9, resource: packagemanifests, items remaining: 3
Aug  6 16:08:42.289: INFO: namespace: e2e-tests-kubectl-k7pg9, resource: bindings, ignored listing per whitelist
Aug  6 16:08:42.789: INFO: namespace: e2e-tests-kubectl-k7pg9 no longer exists
Aug  6 16:08:42.812: INFO: namespace: e2e-tests-kubectl-k7pg9, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:08:42.834: INFO: namespace e2e-tests-kubectl-k7pg9 deletion completed in 8.539003947s

• [SLOW TEST:20.691 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:08:42.834: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Aug  6 16:08:44.231: INFO: Waiting up to 5m0s for pod "var-expansion-76de1e6c-b864-11e9-8d18-525400524259" in namespace "e2e-tests-var-expansion-9659q" to be "success or failure"
Aug  6 16:08:44.254: INFO: Pod "var-expansion-76de1e6c-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.268041ms
Aug  6 16:08:46.276: INFO: Pod "var-expansion-76de1e6c-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045056409s
Aug  6 16:08:48.299: INFO: Pod "var-expansion-76de1e6c-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067748412s
Aug  6 16:08:50.329: INFO: Pod "var-expansion-76de1e6c-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098053821s
Aug  6 16:08:52.352: INFO: Pod "var-expansion-76de1e6c-b864-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.12115016s
STEP: Saw pod success
Aug  6 16:08:52.352: INFO: Pod "var-expansion-76de1e6c-b864-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:08:52.375: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod var-expansion-76de1e6c-b864-11e9-8d18-525400524259 container dapi-container: <nil>
STEP: delete the pod
Aug  6 16:08:52.429: INFO: Waiting for pod var-expansion-76de1e6c-b864-11e9-8d18-525400524259 to disappear
Aug  6 16:08:52.451: INFO: Pod var-expansion-76de1e6c-b864-11e9-8d18-525400524259 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:08:52.451: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-9659q" for this suite.
Aug  6 16:08:58.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:08:59.255: INFO: namespace: e2e-tests-var-expansion-9659q, resource: packagemanifests, items remaining: 3
Aug  6 16:08:59.364: INFO: namespace: e2e-tests-var-expansion-9659q, resource: bindings, ignored listing per whitelist
Aug  6 16:09:00.985: INFO: namespace: e2e-tests-var-expansion-9659q no longer exists
Aug  6 16:09:01.008: INFO: namespace: e2e-tests-var-expansion-9659q, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:09:01.030: INFO: namespace e2e-tests-var-expansion-9659q deletion completed in 8.515785144s

• [SLOW TEST:18.196 seconds]
[k8s.io] Variable Expansion
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:09:01.030: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Aug  6 16:09:02.415: INFO: Waiting up to 5m0s for pod "downward-api-81b4607b-b864-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-ls6zd" to be "success or failure"
Aug  6 16:09:02.438: INFO: Pod "downward-api-81b4607b-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.076356ms
Aug  6 16:09:04.461: INFO: Pod "downward-api-81b4607b-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045859543s
Aug  6 16:09:06.484: INFO: Pod "downward-api-81b4607b-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068736266s
Aug  6 16:09:08.506: INFO: Pod "downward-api-81b4607b-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091596484s
Aug  6 16:09:10.530: INFO: Pod "downward-api-81b4607b-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114902743s
Aug  6 16:09:12.553: INFO: Pod "downward-api-81b4607b-b864-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137795003s
STEP: Saw pod success
Aug  6 16:09:12.553: INFO: Pod "downward-api-81b4607b-b864-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:09:12.575: INFO: Trying to get logs from node ip-10-0-135-96.ec2.internal pod downward-api-81b4607b-b864-11e9-8d18-525400524259 container dapi-container: <nil>
STEP: delete the pod
Aug  6 16:09:12.635: INFO: Waiting for pod downward-api-81b4607b-b864-11e9-8d18-525400524259 to disappear
Aug  6 16:09:12.657: INFO: Pod downward-api-81b4607b-b864-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:09:12.657: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ls6zd" for this suite.
Aug  6 16:09:18.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:09:20.043: INFO: namespace: e2e-tests-downward-api-ls6zd, resource: packagemanifests, items remaining: 3
Aug  6 16:09:20.884: INFO: namespace: e2e-tests-downward-api-ls6zd, resource: bindings, ignored listing per whitelist
Aug  6 16:09:21.235: INFO: namespace: e2e-tests-downward-api-ls6zd no longer exists
Aug  6 16:09:21.258: INFO: namespace: e2e-tests-downward-api-ls6zd, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:09:21.280: INFO: namespace e2e-tests-downward-api-ls6zd deletion completed in 8.552865422s

• [SLOW TEST:20.250 seconds]
[sig-node] Downward API
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:09:21.281: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug  6 16:09:22.671: INFO: Waiting up to 5m0s for pod "pod-8dc6f238-b864-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-nd9mv" to be "success or failure"
Aug  6 16:09:22.693: INFO: Pod "pod-8dc6f238-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 21.899652ms
Aug  6 16:09:24.718: INFO: Pod "pod-8dc6f238-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046535678s
Aug  6 16:09:26.741: INFO: Pod "pod-8dc6f238-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069778894s
Aug  6 16:09:28.764: INFO: Pod "pod-8dc6f238-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09291591s
Aug  6 16:09:30.789: INFO: Pod "pod-8dc6f238-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117497058s
Aug  6 16:09:32.812: INFO: Pod "pod-8dc6f238-b864-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.140320227s
STEP: Saw pod success
Aug  6 16:09:32.812: INFO: Pod "pod-8dc6f238-b864-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:09:32.834: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-8dc6f238-b864-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 16:09:32.889: INFO: Waiting for pod pod-8dc6f238-b864-11e9-8d18-525400524259 to disappear
Aug  6 16:09:32.911: INFO: Pod pod-8dc6f238-b864-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:09:32.911: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nd9mv" for this suite.
Aug  6 16:09:39.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:09:39.889: INFO: namespace: e2e-tests-emptydir-nd9mv, resource: bindings, ignored listing per whitelist
Aug  6 16:09:41.361: INFO: namespace: e2e-tests-emptydir-nd9mv, resource: packagemanifests, items remaining: 3
Aug  6 16:09:41.471: INFO: namespace: e2e-tests-emptydir-nd9mv no longer exists
Aug  6 16:09:41.493: INFO: namespace: e2e-tests-emptydir-nd9mv, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:09:41.516: INFO: namespace e2e-tests-emptydir-nd9mv deletion completed in 8.541841489s

• [SLOW TEST:20.235 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:09:41.516: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Aug  6 16:09:42.911: INFO: Waiting up to 5m0s for pod "downward-api-99d7ae91-b864-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-chhmn" to be "success or failure"
Aug  6 16:09:42.938: INFO: Pod "downward-api-99d7ae91-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 26.255206ms
Aug  6 16:09:44.960: INFO: Pod "downward-api-99d7ae91-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048986619s
Aug  6 16:09:46.984: INFO: Pod "downward-api-99d7ae91-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072284648s
Aug  6 16:09:49.007: INFO: Pod "downward-api-99d7ae91-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.095198435s
Aug  6 16:09:51.033: INFO: Pod "downward-api-99d7ae91-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.121117365s
Aug  6 16:09:53.056: INFO: Pod "downward-api-99d7ae91-b864-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.144350912s
STEP: Saw pod success
Aug  6 16:09:53.056: INFO: Pod "downward-api-99d7ae91-b864-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:09:53.078: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downward-api-99d7ae91-b864-11e9-8d18-525400524259 container dapi-container: <nil>
STEP: delete the pod
Aug  6 16:09:53.133: INFO: Waiting for pod downward-api-99d7ae91-b864-11e9-8d18-525400524259 to disappear
Aug  6 16:09:53.156: INFO: Pod downward-api-99d7ae91-b864-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:09:53.156: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-chhmn" for this suite.
Aug  6 16:09:59.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:09:59.952: INFO: namespace: e2e-tests-downward-api-chhmn, resource: bindings, ignored listing per whitelist
Aug  6 16:10:00.079: INFO: namespace: e2e-tests-downward-api-chhmn, resource: packagemanifests, items remaining: 3
Aug  6 16:10:01.715: INFO: namespace: e2e-tests-downward-api-chhmn no longer exists
Aug  6 16:10:01.738: INFO: namespace: e2e-tests-downward-api-chhmn, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:10:01.760: INFO: namespace e2e-tests-downward-api-chhmn deletion completed in 8.540886242s

• [SLOW TEST:20.243 seconds]
[sig-node] Downward API
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:10:01.761: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:10:03.249: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug  6 16:10:13.295: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug  6 16:10:15.317: INFO: Creating deployment "test-rollover-deployment"
Aug  6 16:10:15.364: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug  6 16:10:15.385: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug  6 16:10:15.430: INFO: Ensure that both replica sets have 1 created replica
Aug  6 16:10:15.474: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug  6 16:10:15.520: INFO: Updating deployment test-rollover-deployment
Aug  6 16:10:15.520: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug  6 16:10:15.542: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug  6 16:10:15.586: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug  6 16:10:15.630: INFO: all replica sets need to contain the pod-template-hash label
Aug  6 16:10:15.630: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b48b457d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 16:10:17.675: INFO: all replica sets need to contain the pod-template-hash label
Aug  6 16:10:17.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b48b457d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 16:10:19.675: INFO: all replica sets need to contain the pod-template-hash label
Aug  6 16:10:19.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b48b457d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 16:10:21.675: INFO: all replica sets need to contain the pod-template-hash label
Aug  6 16:10:21.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b48b457d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 16:10:23.676: INFO: all replica sets need to contain the pod-template-hash label
Aug  6 16:10:23.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b48b457d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 16:10:25.674: INFO: all replica sets need to contain the pod-template-hash label
Aug  6 16:10:25.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704601, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b48b457d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 16:10:27.675: INFO: all replica sets need to contain the pod-template-hash label
Aug  6 16:10:27.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704601, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b48b457d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 16:10:29.675: INFO: all replica sets need to contain the pod-template-hash label
Aug  6 16:10:29.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704601, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b48b457d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 16:10:31.675: INFO: all replica sets need to contain the pod-template-hash label
Aug  6 16:10:31.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704601, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b48b457d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 16:10:33.675: INFO: all replica sets need to contain the pod-template-hash label
Aug  6 16:10:33.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704601, loc:(*time.Location)(0x81cdd60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700704592, loc:(*time.Location)(0x81cdd60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b48b457d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  6 16:10:35.675: INFO: 
Aug  6 16:10:35.675: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug  6 16:10:35.741: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-5bfdt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-5bfdt/deployments/test-rollover-deployment,UID:9fb9f5ce-b864-11e9-9dff-0e6de702691c,ResourceVersion:134040,Generation:2,CreationTimestamp:2019-08-06 16:09:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-06 16:09:52 +0000 UTC 2019-08-06 16:09:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-06 16:10:11 +0000 UTC 2019-08-06 16:09:52 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b48b457d8" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug  6 16:10:35.763: INFO: New ReplicaSet "test-rollover-deployment-6b48b457d8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b48b457d8,GenerateName:,Namespace:e2e-tests-deployment-5bfdt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-5bfdt/replicasets/test-rollover-deployment-6b48b457d8,UID:9fd5e284-b864-11e9-9dff-0e6de702691c,ResourceVersion:134029,Generation:2,CreationTimestamp:2019-08-06 16:09:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b48b457d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9fb9f5ce-b864-11e9-9dff-0e6de702691c 0xc0029ed037 0xc0029ed038}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b48b457d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b48b457d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug  6 16:10:35.763: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug  6 16:10:35.763: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-5bfdt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-5bfdt/replicasets/test-rollover-controller,UID:98810b94-b864-11e9-9dff-0e6de702691c,ResourceVersion:134039,Generation:2,CreationTimestamp:2019-08-06 16:09:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9fb9f5ce-b864-11e9-9dff-0e6de702691c 0xc0029ece97 0xc0029ece98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  6 16:10:35.763: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-865d86f7d5,GenerateName:,Namespace:e2e-tests-deployment-5bfdt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-5bfdt/replicasets/test-rollover-deployment-865d86f7d5,UID:9fbb84b3-b864-11e9-9dff-0e6de702691c,ResourceVersion:133934,Generation:2,CreationTimestamp:2019-08-06 16:09:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 865d86f7d5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9fb9f5ce-b864-11e9-9dff-0e6de702691c 0xc0029ed187 0xc0029ed188}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 865d86f7d5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 865d86f7d5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  6 16:10:35.786: INFO: Pod "test-rollover-deployment-6b48b457d8-57d6r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b48b457d8-57d6r,GenerateName:test-rollover-deployment-6b48b457d8-,Namespace:e2e-tests-deployment-5bfdt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5bfdt/pods/test-rollover-deployment-6b48b457d8-57d6r,UID:9fd93d2e-b864-11e9-9dff-0e6de702691c,ResourceVersion:133985,Generation:0,CreationTimestamp:2019-08-06 16:09:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b48b457d8,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.162"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b48b457d8 9fd5e284-b864-11e9-9dff-0e6de702691c 0xc000bcfc07 0xc000bcfc08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zkkn8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zkkn8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-zkkn8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-r6jhb}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000bcfcd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000bcfcf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 16:09:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 16:10:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 16:10:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 16:09:52 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:10.131.0.162,StartTime:2019-08-06 16:09:52 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-06 16:10:01 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://f51a08fd508bf3657389594830d11c14827c96839f1e655ba0049910cb32cef3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:10:35.786: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-5bfdt" for this suite.
Aug  6 16:10:41.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:10:42.964: INFO: namespace: e2e-tests-deployment-5bfdt, resource: bindings, ignored listing per whitelist
Aug  6 16:10:43.257: INFO: namespace: e2e-tests-deployment-5bfdt, resource: packagemanifests, items remaining: 3
Aug  6 16:10:44.337: INFO: namespace: e2e-tests-deployment-5bfdt no longer exists
Aug  6 16:10:44.359: INFO: namespace: e2e-tests-deployment-5bfdt, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:10:44.381: INFO: namespace e2e-tests-deployment-5bfdt deletion completed in 8.531658183s

• [SLOW TEST:42.621 seconds]
[sig-apps] Deployment
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:10:44.382: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Aug  6 16:10:45.742: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:10:57.239: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-8d9kh" for this suite.
Aug  6 16:11:03.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:11:04.576: INFO: namespace: e2e-tests-init-container-8d9kh, resource: packagemanifests, items remaining: 3
Aug  6 16:11:05.369: INFO: namespace: e2e-tests-init-container-8d9kh, resource: bindings, ignored listing per whitelist
Aug  6 16:11:05.795: INFO: namespace: e2e-tests-init-container-8d9kh no longer exists
Aug  6 16:11:05.818: INFO: namespace: e2e-tests-init-container-8d9kh, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:11:05.840: INFO: namespace e2e-tests-init-container-8d9kh deletion completed in 8.538207146s

• [SLOW TEST:21.458 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:11:05.841: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Aug  6 16:11:07.789: INFO: Waiting up to 5m0s for pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-kgd2g" in namespace "e2e-tests-svcaccounts-gdp6w" to be "success or failure"
Aug  6 16:11:07.812: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-kgd2g": Phase="Pending", Reason="", readiness=false. Elapsed: 22.337779ms
Aug  6 16:11:09.835: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-kgd2g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045722275s
Aug  6 16:11:11.859: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-kgd2g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069415752s
Aug  6 16:11:13.882: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-kgd2g": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092511582s
Aug  6 16:11:15.905: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-kgd2g": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115637696s
Aug  6 16:11:17.928: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-kgd2g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.138820273s
STEP: Saw pod success
Aug  6 16:11:17.928: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-kgd2g" satisfied condition "success or failure"
Aug  6 16:11:17.951: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-kgd2g container token-test: <nil>
STEP: delete the pod
Aug  6 16:11:18.007: INFO: Waiting for pod pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-kgd2g to disappear
Aug  6 16:11:18.029: INFO: Pod pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-kgd2g no longer exists
STEP: Creating a pod to test consume service account root CA
Aug  6 16:11:18.058: INFO: Waiting up to 5m0s for pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr" in namespace "e2e-tests-svcaccounts-gdp6w" to be "success or failure"
Aug  6 16:11:18.080: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr": Phase="Pending", Reason="", readiness=false. Elapsed: 22.105803ms
Aug  6 16:11:20.103: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045213724s
Aug  6 16:11:22.126: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068621189s
Aug  6 16:11:24.150: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091866475s
Aug  6 16:11:26.173: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115099776s
Aug  6 16:11:28.196: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.138234867s
Aug  6 16:11:30.219: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.161389162s
STEP: Saw pod success
Aug  6 16:11:30.219: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr" satisfied condition "success or failure"
Aug  6 16:11:30.241: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr container root-ca-test: <nil>
STEP: delete the pod
Aug  6 16:11:30.297: INFO: Waiting for pod pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr to disappear
Aug  6 16:11:30.319: INFO: Pod pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-ckpqr no longer exists
STEP: Creating a pod to test consume service account namespace
Aug  6 16:11:30.347: INFO: Waiting up to 5m0s for pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv" in namespace "e2e-tests-svcaccounts-gdp6w" to be "success or failure"
Aug  6 16:11:30.369: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv": Phase="Pending", Reason="", readiness=false. Elapsed: 22.193209ms
Aug  6 16:11:32.392: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045151222s
Aug  6 16:11:34.416: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068885162s
Aug  6 16:11:36.438: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09167003s
Aug  6 16:11:38.461: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114779072s
Aug  6 16:11:40.484: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.137569848s
Aug  6 16:11:42.507: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.160282918s
STEP: Saw pod success
Aug  6 16:11:42.507: INFO: Pod "pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv" satisfied condition "success or failure"
Aug  6 16:11:42.530: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv container namespace-test: <nil>
STEP: delete the pod
Aug  6 16:11:42.585: INFO: Waiting for pod pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv to disappear
Aug  6 16:11:42.607: INFO: Pod pod-service-account-cc6f41d8-b864-11e9-8d18-525400524259-cmkqv no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:11:42.607: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-gdp6w" for this suite.
Aug  6 16:11:48.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:11:49.338: INFO: namespace: e2e-tests-svcaccounts-gdp6w, resource: bindings, ignored listing per whitelist
Aug  6 16:11:49.815: INFO: namespace: e2e-tests-svcaccounts-gdp6w, resource: packagemanifests, items remaining: 3
Aug  6 16:11:51.161: INFO: namespace: e2e-tests-svcaccounts-gdp6w no longer exists
Aug  6 16:11:51.184: INFO: namespace: e2e-tests-svcaccounts-gdp6w, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:11:51.206: INFO: namespace e2e-tests-svcaccounts-gdp6w deletion completed in 8.536280565s

• [SLOW TEST:45.366 seconds]
[sig-auth] ServiceAccounts
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:11:51.207: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-e7256544-b864-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 16:11:52.635: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7290ac9-b864-11e9-8d18-525400524259" in namespace "e2e-tests-projected-vp2tr" to be "success or failure"
Aug  6 16:11:52.659: INFO: Pod "pod-projected-secrets-e7290ac9-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.689669ms
Aug  6 16:11:54.682: INFO: Pod "pod-projected-secrets-e7290ac9-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046860227s
Aug  6 16:11:56.705: INFO: Pod "pod-projected-secrets-e7290ac9-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069931574s
Aug  6 16:11:58.728: INFO: Pod "pod-projected-secrets-e7290ac9-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093031886s
Aug  6 16:12:00.752: INFO: Pod "pod-projected-secrets-e7290ac9-b864-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116657448s
Aug  6 16:12:02.775: INFO: Pod "pod-projected-secrets-e7290ac9-b864-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.140015216s
STEP: Saw pod success
Aug  6 16:12:02.775: INFO: Pod "pod-projected-secrets-e7290ac9-b864-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:12:02.797: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-secrets-e7290ac9-b864-11e9-8d18-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Aug  6 16:12:02.855: INFO: Waiting for pod pod-projected-secrets-e7290ac9-b864-11e9-8d18-525400524259 to disappear
Aug  6 16:12:02.877: INFO: Pod pod-projected-secrets-e7290ac9-b864-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:12:02.877: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vp2tr" for this suite.
Aug  6 16:12:09.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:12:10.362: INFO: namespace: e2e-tests-projected-vp2tr, resource: bindings, ignored listing per whitelist
Aug  6 16:12:10.616: INFO: namespace: e2e-tests-projected-vp2tr, resource: packagemanifests, items remaining: 3
Aug  6 16:12:11.432: INFO: namespace: e2e-tests-projected-vp2tr no longer exists
Aug  6 16:12:11.455: INFO: namespace: e2e-tests-projected-vp2tr, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:12:11.478: INFO: namespace e2e-tests-projected-vp2tr deletion completed in 8.538177768s

• [SLOW TEST:20.271 seconds]
[sig-storage] Projected secret
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:12:11.478: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0806 16:12:53.062971    4087 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug  6 16:12:53.063: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:12:53.063: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-bkljg" for this suite.
Aug  6 16:12:59.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:12:59.843: INFO: namespace: e2e-tests-gc-bkljg, resource: bindings, ignored listing per whitelist
Aug  6 16:13:00.341: INFO: namespace: e2e-tests-gc-bkljg, resource: packagemanifests, items remaining: 3
Aug  6 16:13:01.591: INFO: namespace: e2e-tests-gc-bkljg no longer exists
Aug  6 16:13:01.614: INFO: namespace: e2e-tests-gc-bkljg, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:13:01.635: INFO: namespace e2e-tests-gc-bkljg deletion completed in 8.52931387s

• [SLOW TEST:50.157 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:13:01.636: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:13:03.139: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Aug  6 16:13:03.184: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-q6fb9/daemonsets","resourceVersion":"135727"},"items":null}

Aug  6 16:13:03.206: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-q6fb9/pods","resourceVersion":"135727"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:13:03.317: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-q6fb9" for this suite.
Aug  6 16:13:09.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:13:11.345: INFO: namespace: e2e-tests-daemonsets-q6fb9, resource: packagemanifests, items remaining: 3
Aug  6 16:13:11.547: INFO: namespace: e2e-tests-daemonsets-q6fb9, resource: bindings, ignored listing per whitelist
Aug  6 16:13:11.924: INFO: namespace: e2e-tests-daemonsets-q6fb9 no longer exists
Aug  6 16:13:11.947: INFO: namespace: e2e-tests-daemonsets-q6fb9, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:13:11.969: INFO: namespace e2e-tests-daemonsets-q6fb9 deletion completed in 8.609949175s

S [SKIPPING] [10.333 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Aug  6 16:13:03.139: Requires at least 2 nodes (not -1)

  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:13:11.970: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Aug  6 16:13:13.334: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:13.979: INFO: stderr: ""
Aug  6 16:13:13.979: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  6 16:13:13.979: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:14.139: INFO: stderr: ""
Aug  6 16:13:14.139: INFO: stdout: "update-demo-nautilus-hlkrx update-demo-nautilus-s2gvq "
Aug  6 16:13:14.139: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-hlkrx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:14.297: INFO: stderr: ""
Aug  6 16:13:14.297: INFO: stdout: ""
Aug  6 16:13:14.297: INFO: update-demo-nautilus-hlkrx is created but not running
Aug  6 16:13:19.297: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:19.458: INFO: stderr: ""
Aug  6 16:13:19.458: INFO: stdout: "update-demo-nautilus-hlkrx update-demo-nautilus-s2gvq "
Aug  6 16:13:19.458: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-hlkrx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:19.613: INFO: stderr: ""
Aug  6 16:13:19.613: INFO: stdout: ""
Aug  6 16:13:19.613: INFO: update-demo-nautilus-hlkrx is created but not running
Aug  6 16:13:24.613: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:24.772: INFO: stderr: ""
Aug  6 16:13:24.772: INFO: stdout: "update-demo-nautilus-hlkrx update-demo-nautilus-s2gvq "
Aug  6 16:13:24.772: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-hlkrx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:24.928: INFO: stderr: ""
Aug  6 16:13:24.928: INFO: stdout: "true"
Aug  6 16:13:24.928: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-hlkrx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:25.081: INFO: stderr: ""
Aug  6 16:13:25.081: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  6 16:13:25.081: INFO: validating pod update-demo-nautilus-hlkrx
Aug  6 16:13:25.107: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  6 16:13:25.107: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  6 16:13:25.107: INFO: update-demo-nautilus-hlkrx is verified up and running
Aug  6 16:13:25.107: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-s2gvq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:25.256: INFO: stderr: ""
Aug  6 16:13:25.256: INFO: stdout: "true"
Aug  6 16:13:25.256: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-s2gvq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:25.406: INFO: stderr: ""
Aug  6 16:13:25.406: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  6 16:13:25.406: INFO: validating pod update-demo-nautilus-s2gvq
Aug  6 16:13:25.432: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  6 16:13:25.432: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  6 16:13:25.432: INFO: update-demo-nautilus-s2gvq is verified up and running
STEP: rolling-update to new replication controller
Aug  6 16:13:25.578: INFO: scanned /home/jeder for discovery docs: <nil>
Aug  6 16:13:25.578: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:56.171: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug  6 16:13:56.171: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  6 16:13:56.171: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:56.328: INFO: stderr: ""
Aug  6 16:13:56.329: INFO: stdout: "update-demo-kitten-7ztvh update-demo-kitten-jsfvb "
Aug  6 16:13:56.329: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-kitten-7ztvh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:56.478: INFO: stderr: ""
Aug  6 16:13:56.478: INFO: stdout: "true"
Aug  6 16:13:56.478: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-kitten-7ztvh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:56.627: INFO: stderr: ""
Aug  6 16:13:56.627: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug  6 16:13:56.627: INFO: validating pod update-demo-kitten-7ztvh
Aug  6 16:13:56.651: INFO: got data: {
  "image": "kitten.jpg"
}

Aug  6 16:13:56.652: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug  6 16:13:56.652: INFO: update-demo-kitten-7ztvh is verified up and running
Aug  6 16:13:56.652: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-kitten-jsfvb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:56.801: INFO: stderr: ""
Aug  6 16:13:56.801: INFO: stdout: "true"
Aug  6 16:13:56.801: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-kitten-jsfvb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rft7j'
Aug  6 16:13:56.953: INFO: stderr: ""
Aug  6 16:13:56.953: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug  6 16:13:56.953: INFO: validating pod update-demo-kitten-jsfvb
Aug  6 16:13:56.977: INFO: got data: {
  "image": "kitten.jpg"
}

Aug  6 16:13:56.977: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug  6 16:13:56.977: INFO: update-demo-kitten-jsfvb is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:13:56.977: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rft7j" for this suite.
Aug  6 16:14:19.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:14:19.892: INFO: namespace: e2e-tests-kubectl-rft7j, resource: packagemanifests, items remaining: 3
Aug  6 16:14:20.008: INFO: namespace: e2e-tests-kubectl-rft7j, resource: bindings, ignored listing per whitelist
Aug  6 16:14:21.583: INFO: namespace: e2e-tests-kubectl-rft7j no longer exists
Aug  6 16:14:21.606: INFO: namespace: e2e-tests-kubectl-rft7j, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:14:21.628: INFO: namespace e2e-tests-kubectl-rft7j deletion completed in 24.586646628s

• [SLOW TEST:69.658 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:14:21.628: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-40cc5118-b865-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 16:14:23.042: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-40d04f96-b865-11e9-8d18-525400524259" in namespace "e2e-tests-projected-rwxsw" to be "success or failure"
Aug  6 16:14:23.064: INFO: Pod "pod-projected-configmaps-40d04f96-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.09891ms
Aug  6 16:14:25.087: INFO: Pod "pod-projected-configmaps-40d04f96-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045082437s
Aug  6 16:14:27.110: INFO: Pod "pod-projected-configmaps-40d04f96-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068147147s
Aug  6 16:14:29.133: INFO: Pod "pod-projected-configmaps-40d04f96-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090915642s
Aug  6 16:14:31.156: INFO: Pod "pod-projected-configmaps-40d04f96-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113906178s
Aug  6 16:14:33.179: INFO: Pod "pod-projected-configmaps-40d04f96-b865-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.136677672s
STEP: Saw pod success
Aug  6 16:14:33.179: INFO: Pod "pod-projected-configmaps-40d04f96-b865-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:14:33.201: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-configmaps-40d04f96-b865-11e9-8d18-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 16:14:33.257: INFO: Waiting for pod pod-projected-configmaps-40d04f96-b865-11e9-8d18-525400524259 to disappear
Aug  6 16:14:33.279: INFO: Pod pod-projected-configmaps-40d04f96-b865-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:14:33.279: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rwxsw" for this suite.
Aug  6 16:14:39.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:14:40.077: INFO: namespace: e2e-tests-projected-rwxsw, resource: bindings, ignored listing per whitelist
Aug  6 16:14:41.016: INFO: namespace: e2e-tests-projected-rwxsw, resource: packagemanifests, items remaining: 3
Aug  6 16:14:41.835: INFO: namespace: e2e-tests-projected-rwxsw no longer exists
Aug  6 16:14:41.877: INFO: namespace: e2e-tests-projected-rwxsw, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:14:41.899: INFO: namespace e2e-tests-projected-rwxsw deletion completed in 8.557564417s

• [SLOW TEST:20.271 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:14:41.899: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug  6 16:14:43.304: INFO: Waiting up to 5m0s for pod "pod-4ce3bbf4-b865-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-h8ctk" to be "success or failure"
Aug  6 16:14:43.326: INFO: Pod "pod-4ce3bbf4-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.247261ms
Aug  6 16:14:45.349: INFO: Pod "pod-4ce3bbf4-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044857597s
Aug  6 16:14:47.371: INFO: Pod "pod-4ce3bbf4-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067398211s
Aug  6 16:14:49.394: INFO: Pod "pod-4ce3bbf4-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090429444s
Aug  6 16:14:51.417: INFO: Pod "pod-4ce3bbf4-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113483899s
Aug  6 16:14:53.440: INFO: Pod "pod-4ce3bbf4-b865-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.136561752s
STEP: Saw pod success
Aug  6 16:14:53.440: INFO: Pod "pod-4ce3bbf4-b865-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:14:53.463: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-4ce3bbf4-b865-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 16:14:53.519: INFO: Waiting for pod pod-4ce3bbf4-b865-11e9-8d18-525400524259 to disappear
Aug  6 16:14:53.541: INFO: Pod pod-4ce3bbf4-b865-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:14:53.541: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-h8ctk" for this suite.
Aug  6 16:14:59.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:15:00.365: INFO: namespace: e2e-tests-emptydir-h8ctk, resource: packagemanifests, items remaining: 3
Aug  6 16:15:01.312: INFO: namespace: e2e-tests-emptydir-h8ctk, resource: bindings, ignored listing per whitelist
Aug  6 16:15:02.078: INFO: namespace: e2e-tests-emptydir-h8ctk no longer exists
Aug  6 16:15:02.101: INFO: namespace: e2e-tests-emptydir-h8ctk, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:15:02.123: INFO: namespace e2e-tests-emptydir-h8ctk deletion completed in 8.519538622s

• [SLOW TEST:20.224 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:15:02.123: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Aug  6 16:15:03.471: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:15:14.399: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-rzr5x" for this suite.
Aug  6 16:15:36.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:15:37.505: INFO: namespace: e2e-tests-init-container-rzr5x, resource: bindings, ignored listing per whitelist
Aug  6 16:15:38.155: INFO: namespace: e2e-tests-init-container-rzr5x, resource: packagemanifests, items remaining: 3
Aug  6 16:15:38.959: INFO: namespace: e2e-tests-init-container-rzr5x no longer exists
Aug  6 16:15:38.982: INFO: namespace: e2e-tests-init-container-rzr5x, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:15:39.004: INFO: namespace e2e-tests-init-container-rzr5x deletion completed in 24.541360409s

• [SLOW TEST:36.880 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:15:39.004: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug  6 16:15:51.032: INFO: Successfully updated pod "pod-update-6eead20a-b865-11e9-8d18-525400524259"
STEP: verifying the updated pod is in kubernetes
Aug  6 16:15:51.077: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:15:51.077: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zrvqk" for this suite.
Aug  6 16:16:13.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:16:13.986: INFO: namespace: e2e-tests-pods-zrvqk, resource: bindings, ignored listing per whitelist
Aug  6 16:16:14.338: INFO: namespace: e2e-tests-pods-zrvqk, resource: packagemanifests, items remaining: 3
Aug  6 16:16:15.667: INFO: namespace: e2e-tests-pods-zrvqk no longer exists
Aug  6 16:16:15.690: INFO: namespace: e2e-tests-pods-zrvqk, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:16:15.712: INFO: namespace e2e-tests-pods-zrvqk deletion completed in 24.572242509s

• [SLOW TEST:36.709 seconds]
[k8s.io] Pods
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:16:15.713: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:16:27.202: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-svznt" for this suite.
Aug  6 16:17:11.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:17:12.282: INFO: namespace: e2e-tests-kubelet-test-svznt, resource: packagemanifests, items remaining: 3
Aug  6 16:17:13.391: INFO: namespace: e2e-tests-kubelet-test-svznt, resource: bindings, ignored listing per whitelist
Aug  6 16:17:13.771: INFO: namespace: e2e-tests-kubelet-test-svznt no longer exists
Aug  6 16:17:13.795: INFO: namespace: e2e-tests-kubelet-test-svznt, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:17:13.818: INFO: namespace e2e-tests-kubelet-test-svznt deletion completed in 46.551415321s

• [SLOW TEST:58.105 seconds]
[k8s.io] Kubelet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:17:13.818: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Aug  6 16:17:15.202: INFO: Waiting up to 5m0s for pod "client-containers-a76de01d-b865-11e9-8d18-525400524259" in namespace "e2e-tests-containers-4bmbh" to be "success or failure"
Aug  6 16:17:15.224: INFO: Pod "client-containers-a76de01d-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.410104ms
Aug  6 16:17:17.247: INFO: Pod "client-containers-a76de01d-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045069871s
Aug  6 16:17:19.270: INFO: Pod "client-containers-a76de01d-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068335227s
Aug  6 16:17:21.295: INFO: Pod "client-containers-a76de01d-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093157608s
Aug  6 16:17:23.318: INFO: Pod "client-containers-a76de01d-b865-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116864144s
Aug  6 16:17:25.343: INFO: Pod "client-containers-a76de01d-b865-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.141344078s
STEP: Saw pod success
Aug  6 16:17:25.343: INFO: Pod "client-containers-a76de01d-b865-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:17:25.366: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod client-containers-a76de01d-b865-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 16:17:25.421: INFO: Waiting for pod client-containers-a76de01d-b865-11e9-8d18-525400524259 to disappear
Aug  6 16:17:25.443: INFO: Pod client-containers-a76de01d-b865-11e9-8d18-525400524259 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:17:25.443: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-4bmbh" for this suite.
Aug  6 16:17:31.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:17:32.198: INFO: namespace: e2e-tests-containers-4bmbh, resource: bindings, ignored listing per whitelist
Aug  6 16:17:32.380: INFO: namespace: e2e-tests-containers-4bmbh, resource: packagemanifests, items remaining: 3
Aug  6 16:17:34.009: INFO: namespace: e2e-tests-containers-4bmbh no longer exists
Aug  6 16:17:34.034: INFO: namespace: e2e-tests-containers-4bmbh, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:17:34.116: INFO: namespace e2e-tests-containers-4bmbh deletion completed in 8.608843179s

• [SLOW TEST:20.298 seconds]
[k8s.io] Docker Containers
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:17:34.119: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-9nzcx
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-9nzcx
STEP: Deleting pre-stop pod
Aug  6 16:18:02.726: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:18:02.753: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-9nzcx" for this suite.
Aug  6 16:18:40.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:18:42.658: INFO: namespace: e2e-tests-prestop-9nzcx, resource: packagemanifests, items remaining: 3
Aug  6 16:18:43.121: INFO: namespace: e2e-tests-prestop-9nzcx, resource: bindings, ignored listing per whitelist
Aug  6 16:18:43.300: INFO: namespace: e2e-tests-prestop-9nzcx no longer exists
Aug  6 16:18:43.324: INFO: namespace: e2e-tests-prestop-9nzcx, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:18:43.346: INFO: namespace e2e-tests-prestop-9nzcx deletion completed in 40.530192224s

• [SLOW TEST:69.228 seconds]
[k8s.io] [sig-node] PreStop
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:18:43.347: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:18:44.761: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-dcd430b4-b865-11e9-8d18-525400524259
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:18:54.935: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-lzd4l" for this suite.
Aug  6 16:19:17.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:19:17.855: INFO: namespace: e2e-tests-configmap-lzd4l, resource: packagemanifests, items remaining: 3
Aug  6 16:19:18.941: INFO: namespace: e2e-tests-configmap-lzd4l, resource: bindings, ignored listing per whitelist
Aug  6 16:19:19.493: INFO: namespace: e2e-tests-configmap-lzd4l no longer exists
Aug  6 16:19:19.516: INFO: namespace: e2e-tests-configmap-lzd4l, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:19:19.538: INFO: namespace e2e-tests-configmap-lzd4l deletion completed in 24.539931723s

• [SLOW TEST:36.191 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:19:19.540: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Aug  6 16:19:20.995: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:22.877: INFO: stderr: ""
Aug  6 16:19:22.877: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  6 16:19:22.877: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:23.034: INFO: stderr: ""
Aug  6 16:19:23.034: INFO: stdout: "update-demo-nautilus-kcq5w update-demo-nautilus-wbg76 "
Aug  6 16:19:23.034: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-kcq5w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:23.182: INFO: stderr: ""
Aug  6 16:19:23.182: INFO: stdout: ""
Aug  6 16:19:23.182: INFO: update-demo-nautilus-kcq5w is created but not running
Aug  6 16:19:28.182: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:28.343: INFO: stderr: ""
Aug  6 16:19:28.343: INFO: stdout: "update-demo-nautilus-kcq5w update-demo-nautilus-wbg76 "
Aug  6 16:19:28.343: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-kcq5w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:28.493: INFO: stderr: ""
Aug  6 16:19:28.493: INFO: stdout: ""
Aug  6 16:19:28.493: INFO: update-demo-nautilus-kcq5w is created but not running
Aug  6 16:19:33.494: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:33.656: INFO: stderr: ""
Aug  6 16:19:33.656: INFO: stdout: "update-demo-nautilus-kcq5w update-demo-nautilus-wbg76 "
Aug  6 16:19:33.656: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-kcq5w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:33.809: INFO: stderr: ""
Aug  6 16:19:33.809: INFO: stdout: "true"
Aug  6 16:19:33.809: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-kcq5w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:33.960: INFO: stderr: ""
Aug  6 16:19:33.960: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  6 16:19:33.960: INFO: validating pod update-demo-nautilus-kcq5w
Aug  6 16:19:33.984: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  6 16:19:33.984: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  6 16:19:33.984: INFO: update-demo-nautilus-kcq5w is verified up and running
Aug  6 16:19:33.984: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-wbg76 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:34.138: INFO: stderr: ""
Aug  6 16:19:34.138: INFO: stdout: "true"
Aug  6 16:19:34.138: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods update-demo-nautilus-wbg76 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:34.289: INFO: stderr: ""
Aug  6 16:19:34.289: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  6 16:19:34.289: INFO: validating pod update-demo-nautilus-wbg76
Aug  6 16:19:34.314: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  6 16:19:34.314: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  6 16:19:34.314: INFO: update-demo-nautilus-wbg76 is verified up and running
STEP: using delete to clean up resources
Aug  6 16:19:34.314: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:34.485: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  6 16:19:34.486: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug  6 16:19:34.486: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-mhjmg'
Aug  6 16:19:34.661: INFO: stderr: "No resources found.\n"
Aug  6 16:19:34.661: INFO: stdout: ""
Aug  6 16:19:34.661: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance get pods -l name=update-demo --namespace=e2e-tests-kubectl-mhjmg -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug  6 16:19:34.819: INFO: stderr: ""
Aug  6 16:19:34.819: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:19:34.819: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mhjmg" for this suite.
Aug  6 16:19:56.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:19:58.417: INFO: namespace: e2e-tests-kubectl-mhjmg, resource: bindings, ignored listing per whitelist
Aug  6 16:19:58.468: INFO: namespace: e2e-tests-kubectl-mhjmg, resource: packagemanifests, items remaining: 3
Aug  6 16:19:59.368: INFO: namespace: e2e-tests-kubectl-mhjmg no longer exists
Aug  6 16:19:59.391: INFO: namespace: e2e-tests-kubectl-mhjmg, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:19:59.414: INFO: namespace e2e-tests-kubectl-mhjmg deletion completed in 24.532039171s

• [SLOW TEST:39.874 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:19:59.415: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug  6 16:20:00.775: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Aug  6 16:20:00.861: INFO: Waiting for terminating namespaces to be deleted...
Aug  6 16:20:00.884: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-128-44.ec2.internal before test
Aug  6 16:20:00.952: INFO: ovs-tmlr8 from openshift-sdn started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 16:20:00.952: INFO: machine-config-daemon-rbg8b from openshift-machine-config-operator started at 2019-08-06 12:32:31 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug  6 16:20:00.952: INFO: elasticsearch-operator-7c5cc4bff9-spd5k from openshift-operators started at 2019-08-06 12:40:44 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container elasticsearch-operator ready: true, restart count 0
Aug  6 16:20:00.952: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-08-06 12:40:50 +0000 UTC (3 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container alertmanager ready: true, restart count 0
Aug  6 16:20:00.952: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug  6 16:20:00.952: INFO: 	Container config-reloader ready: true, restart count 0
Aug  6 16:20:00.952: INFO: multus-jgdnj from openshift-multus started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 16:20:00.952: INFO: sdn-6rmrm from openshift-sdn started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container sdn ready: true, restart count 0
Aug  6 16:20:00.952: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-08-06 12:40:46 +0000 UTC (6 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:20:00.952: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug  6 16:20:00.952: INFO: 	Container prometheus ready: true, restart count 1
Aug  6 16:20:00.952: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug  6 16:20:00.952: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug  6 16:20:00.952: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug  6 16:20:00.952: INFO: sre-dns-latency-exporter-n784c from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container main ready: true, restart count 0
Aug  6 16:20:00.952: INFO: sre-machine-api-status-exporter-1-deploy from openshift-monitoring started at 2019-08-06 12:40:13 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container deployment ready: false, restart count 0
Aug  6 16:20:00.952: INFO: sre-stuck-ebs-vols-1-xv82v from openshift-monitoring started at 2019-08-06 12:40:20 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container main ready: true, restart count 0
Aug  6 16:20:00.952: INFO: tuned-qbg52 from openshift-cluster-node-tuning-operator started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container tuned ready: true, restart count 0
Aug  6 16:20:00.952: INFO: node-ca-bh6hh from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 16:20:00.952: INFO: router-default-7bc87fbb58-bm9g9 from openshift-ingress started at 2019-08-06 12:58:06 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container router ready: true, restart count 0
Aug  6 16:20:00.952: INFO: dns-default-7wl42 from openshift-dns started at 2019-08-06 12:32:11 +0000 UTC (2 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container dns ready: true, restart count 0
Aug  6 16:20:00.952: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 16:20:00.952: INFO: certified-operators-7589fc6b59-44rkm from openshift-marketplace started at 2019-08-06 12:32:38 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container certified-operators ready: true, restart count 0
Aug  6 16:20:00.952: INFO: node-exporter-scqng from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 16:20:00.952: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:20:00.952: INFO: 	Container node-exporter ready: true, restart count 0
Aug  6 16:20:00.952: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-134.ec2.internal before test
Aug  6 16:20:01.021: INFO: installed-redhat-openshift-operators-547f964d5f-4t496 from openshift-marketplace started at 2019-08-06 12:40:15 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container installed-redhat-openshift-operators ready: true, restart count 0
Aug  6 16:20:01.021: INFO: image-pruner-1565107200-64tsz from openshift-sre-pruning started at 2019-08-06 16:00:04 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container image-pruner ready: false, restart count 0
Aug  6 16:20:01.021: INFO: deployments-pruner-1565107200-9jwrf from openshift-sre-pruning started at 2019-08-06 16:00:04 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container deployments-pruner ready: false, restart count 0
Aug  6 16:20:01.021: INFO: multus-6mjbh from openshift-multus started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 16:20:01.021: INFO: node-exporter-hrdtt from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:20:01.021: INFO: 	Container node-exporter ready: true, restart count 0
Aug  6 16:20:01.021: INFO: sre-stuck-ebs-vols-1-deploy from openshift-monitoring started at 2019-08-06 12:40:11 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container deployment ready: false, restart count 0
Aug  6 16:20:01.021: INFO: dedicated-admin-operator-77956f4b8-nd6fz from openshift-dedicated-admin started at 2019-08-06 12:47:48 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container dedicated-admin-operator ready: true, restart count 0
Aug  6 16:20:01.021: INFO: configure-alertmanager-operator-74579866b-lzq26 from openshift-monitoring started at 2019-08-06 12:47:48 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Aug  6 16:20:01.021: INFO: dns-default-j5pgg from openshift-dns started at 2019-08-06 12:31:19 +0000 UTC (2 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container dns ready: true, restart count 0
Aug  6 16:20:01.021: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 16:20:01.021: INFO: kube-state-metrics-645d9dc5b9-hklgd from openshift-monitoring started at 2019-08-06 12:32:43 +0000 UTC (3 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug  6 16:20:01.021: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug  6 16:20:01.021: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug  6 16:20:01.021: INFO: image-pruner-1565100000-4h6gc from openshift-sre-pruning started at 2019-08-06 14:00:03 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container image-pruner ready: false, restart count 0
Aug  6 16:20:01.021: INFO: builds-pruner-1565103600-pq7mw from openshift-sre-pruning started at 2019-08-06 15:00:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container builds-pruner ready: false, restart count 0
Aug  6 16:20:01.021: INFO: router-default-7bc87fbb58-8jfjn from openshift-ingress started at 2019-08-06 12:58:29 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container router ready: true, restart count 0
Aug  6 16:20:01.021: INFO: builds-pruner-1565100000-bt5b6 from openshift-sre-pruning started at 2019-08-06 14:00:03 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container builds-pruner ready: false, restart count 0
Aug  6 16:20:01.021: INFO: machine-config-daemon-zf9nq from openshift-machine-config-operator started at 2019-08-06 12:31:49 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug  6 16:20:01.021: INFO: redhat-operators-6fc9f9c9ff-nld4d from openshift-marketplace started at 2019-08-06 12:32:38 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container redhat-operators ready: true, restart count 0
Aug  6 16:20:01.021: INFO: node-ca-d98qg from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 16:20:01.021: INFO: deployments-pruner-1565100000-ht542 from openshift-sre-pruning started at 2019-08-06 14:00:03 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container deployments-pruner ready: false, restart count 0
Aug  6 16:20:01.021: INFO: ovs-j2g2j from openshift-sdn started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 16:20:01.021: INFO: tuned-x8rhd from openshift-cluster-node-tuning-operator started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container tuned ready: true, restart count 0
Aug  6 16:20:01.021: INFO: prometheus-adapter-74d56f8767-krgfn from openshift-monitoring started at 2019-08-06 12:34:16 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug  6 16:20:01.021: INFO: sre-dns-latency-exporter-pl7hx from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container main ready: true, restart count 0
Aug  6 16:20:01.021: INFO: sdn-gh6xp from openshift-sdn started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container sdn ready: true, restart count 0
Aug  6 16:20:01.021: INFO: sre-ebs-iops-reporter-1-deploy from openshift-monitoring started at 2019-08-06 12:40:12 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container deployment ready: false, restart count 0
Aug  6 16:20:01.021: INFO: deployments-pruner-1565103600-294mv from openshift-sre-pruning started at 2019-08-06 15:00:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container deployments-pruner ready: false, restart count 0
Aug  6 16:20:01.021: INFO: builds-pruner-1565107200-2hmhf from openshift-sre-pruning started at 2019-08-06 16:00:04 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.021: INFO: 	Container builds-pruner ready: false, restart count 0
Aug  6 16:20:01.021: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-135-96.ec2.internal before test
Aug  6 16:20:01.089: INFO: community-operators-7b9c4c96b8-s6c84 from openshift-marketplace started at 2019-08-06 12:32:38 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container community-operators ready: true, restart count 0
Aug  6 16:20:01.089: INFO: ovs-s6jvw from openshift-sdn started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 16:20:01.089: INFO: tuned-dkb6k from openshift-cluster-node-tuning-operator started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container tuned ready: true, restart count 0
Aug  6 16:20:01.089: INFO: image-pruner-1565103600-44bxp from openshift-sre-pruning started at 2019-08-06 15:00:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container image-pruner ready: false, restart count 0
Aug  6 16:20:01.089: INFO: image-registry-57774c6454-5lpmh from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container registry ready: true, restart count 0
Aug  6 16:20:01.089: INFO: node-exporter-ccph9 from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:20:01.089: INFO: 	Container node-exporter ready: true, restart count 0
Aug  6 16:20:01.089: INFO: configure-alertmanager-operator-registry-bvv62 from openshift-operator-lifecycle-manager started at 2019-08-06 12:40:07 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container registry-server ready: true, restart count 0
Aug  6 16:20:01.089: INFO: sre-dns-latency-exporter-xdr2p from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container main ready: true, restart count 0
Aug  6 16:20:01.089: INFO: prometheus-operator-7cbf84478f-9cqcb from openshift-monitoring started at 2019-08-06 12:40:13 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug  6 16:20:01.089: INFO: sdn-nrctn from openshift-sdn started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container sdn ready: true, restart count 0
Aug  6 16:20:01.089: INFO: dns-default-qhqr4 from openshift-dns started at 2019-08-06 12:31:52 +0000 UTC (2 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container dns ready: true, restart count 0
Aug  6 16:20:01.089: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 16:20:01.089: INFO: telemeter-client-6878d94476-kncjs from openshift-monitoring started at 2019-08-06 12:32:46 +0000 UTC (3 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:20:01.089: INFO: 	Container reload ready: true, restart count 0
Aug  6 16:20:01.089: INFO: 	Container telemeter-client ready: true, restart count 0
Aug  6 16:20:01.089: INFO: cluster-logging-operator-7c58ddb664-kvlv7 from openshift-logging started at 2019-08-06 12:40:56 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container cluster-logging-operator ready: true, restart count 0
Aug  6 16:20:01.089: INFO: multus-zwdrq from openshift-multus started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 16:20:01.089: INFO: machine-config-daemon-rsm96 from openshift-machine-config-operator started at 2019-08-06 12:32:21 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug  6 16:20:01.089: INFO: installed-redhat-openshift-logging-7b6d9d86f5-2whm2 from openshift-marketplace started at 2019-08-06 12:40:15 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container installed-redhat-openshift-logging ready: true, restart count 0
Aug  6 16:20:01.089: INFO: dedicated-admin-operator-registry-dvtrw from openshift-operator-lifecycle-manager started at 2019-08-06 12:40:14 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container registry-server ready: true, restart count 0
Aug  6 16:20:01.089: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-08-06 12:40:19 +0000 UTC (3 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container alertmanager ready: true, restart count 0
Aug  6 16:20:01.089: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug  6 16:20:01.089: INFO: 	Container config-reloader ready: true, restart count 0
Aug  6 16:20:01.089: INFO: node-ca-5w4gz from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.089: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 16:20:01.089: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-62.ec2.internal before test
Aug  6 16:20:01.157: INFO: multus-4qwmn from openshift-multus started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 16:20:01.157: INFO: sre-ebs-iops-reporter-1-nvmlq from openshift-monitoring started at 2019-08-06 12:40:20 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container main ready: true, restart count 0
Aug  6 16:20:01.157: INFO: sre-machine-api-status-exporter-1-clk9g from openshift-monitoring started at 2019-08-06 12:40:33 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container main ready: true, restart count 0
Aug  6 16:20:01.157: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-08-06 12:41:25 +0000 UTC (3 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container alertmanager ready: true, restart count 0
Aug  6 16:20:01.157: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug  6 16:20:01.157: INFO: 	Container config-reloader ready: true, restart count 0
Aug  6 16:20:01.157: INFO: ovs-h9vns from openshift-sdn started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 16:20:01.157: INFO: node-exporter-56p4z from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:20:01.157: INFO: 	Container node-exporter ready: true, restart count 0
Aug  6 16:20:01.157: INFO: node-ca-9tl67 from openshift-image-registry started at 2019-08-06 12:33:06 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 16:20:01.157: INFO: sre-dns-latency-exporter-crjdn from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container main ready: true, restart count 0
Aug  6 16:20:01.157: INFO: tuned-mjhtk from openshift-cluster-node-tuning-operator started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container tuned ready: true, restart count 0
Aug  6 16:20:01.157: INFO: grafana-86f8cd476c-gm5tf from openshift-monitoring started at 2019-08-06 12:33:08 +0000 UTC (2 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container grafana ready: true, restart count 0
Aug  6 16:20:01.157: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug  6 16:20:01.157: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-08-06 12:40:46 +0000 UTC (6 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:20:01.157: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug  6 16:20:01.157: INFO: 	Container prometheus ready: true, restart count 1
Aug  6 16:20:01.157: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug  6 16:20:01.157: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug  6 16:20:01.157: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug  6 16:20:01.157: INFO: prometheus-adapter-74d56f8767-wc4bj from openshift-monitoring started at 2019-08-06 12:34:16 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug  6 16:20:01.157: INFO: sdn-55nwk from openshift-sdn started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container sdn ready: true, restart count 0
Aug  6 16:20:01.157: INFO: dns-default-kc256 from openshift-dns started at 2019-08-06 12:32:36 +0000 UTC (2 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container dns ready: true, restart count 0
Aug  6 16:20:01.157: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 16:20:01.157: INFO: machine-config-daemon-m7b9d from openshift-machine-config-operator started at 2019-08-06 12:33:06 +0000 UTC (1 container statuses recorded)
Aug  6 16:20:01.157: INFO: 	Container machine-config-daemon ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-10677442-b866-11e9-8d18-525400524259 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-10677442-b866-11e9-8d18-525400524259 off the node ip-10-0-130-134.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-10677442-b866-11e9-8d18-525400524259
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:20:21.500: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-t7kmd" for this suite.
Aug  6 16:20:41.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:20:42.386: INFO: namespace: e2e-tests-sched-pred-t7kmd, resource: bindings, ignored listing per whitelist
Aug  6 16:20:42.880: INFO: namespace: e2e-tests-sched-pred-t7kmd, resource: packagemanifests, items remaining: 3
Aug  6 16:20:44.031: INFO: namespace: e2e-tests-sched-pred-t7kmd no longer exists
Aug  6 16:20:44.055: INFO: namespace: e2e-tests-sched-pred-t7kmd, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:20:44.077: INFO: namespace e2e-tests-sched-pred-t7kmd deletion completed in 22.533721162s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:44.662 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:20:44.077: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug  6 16:20:45.463: INFO: Waiting up to 5m0s for pod "pod-24c078f5-b866-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-79ds7" to be "success or failure"
Aug  6 16:20:45.485: INFO: Pod "pod-24c078f5-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.255873ms
Aug  6 16:20:47.508: INFO: Pod "pod-24c078f5-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045215378s
Aug  6 16:20:49.532: INFO: Pod "pod-24c078f5-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068302345s
Aug  6 16:20:51.554: INFO: Pod "pod-24c078f5-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09116343s
Aug  6 16:20:53.577: INFO: Pod "pod-24c078f5-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114197334s
Aug  6 16:20:55.601: INFO: Pod "pod-24c078f5-b866-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137274052s
STEP: Saw pod success
Aug  6 16:20:55.601: INFO: Pod "pod-24c078f5-b866-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:20:55.623: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-24c078f5-b866-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 16:20:55.679: INFO: Waiting for pod pod-24c078f5-b866-11e9-8d18-525400524259 to disappear
Aug  6 16:20:55.701: INFO: Pod pod-24c078f5-b866-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:20:55.701: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-79ds7" for this suite.
Aug  6 16:21:01.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:21:02.806: INFO: namespace: e2e-tests-emptydir-79ds7, resource: bindings, ignored listing per whitelist
Aug  6 16:21:03.960: INFO: namespace: e2e-tests-emptydir-79ds7, resource: packagemanifests, items remaining: 3
Aug  6 16:21:04.257: INFO: namespace: e2e-tests-emptydir-79ds7 no longer exists
Aug  6 16:21:04.280: INFO: namespace: e2e-tests-emptydir-79ds7, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:21:04.303: INFO: namespace e2e-tests-emptydir-79ds7 deletion completed in 8.538216593s

• [SLOW TEST:20.225 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:21:04.303: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug  6 16:21:05.655: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Aug  6 16:21:05.725: INFO: Waiting for terminating namespaces to be deleted...
Aug  6 16:21:05.748: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-128-44.ec2.internal before test
Aug  6 16:21:05.795: INFO: dns-default-7wl42 from openshift-dns started at 2019-08-06 12:32:11 +0000 UTC (2 container statuses recorded)
Aug  6 16:21:05.795: INFO: 	Container dns ready: true, restart count 0
Aug  6 16:21:05.795: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 16:21:05.795: INFO: certified-operators-7589fc6b59-44rkm from openshift-marketplace started at 2019-08-06 12:32:38 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.795: INFO: 	Container certified-operators ready: true, restart count 0
Aug  6 16:21:05.795: INFO: node-exporter-scqng from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 16:21:05.795: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:21:05.795: INFO: 	Container node-exporter ready: true, restart count 0
Aug  6 16:21:05.795: INFO: ovs-tmlr8 from openshift-sdn started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.795: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 16:21:05.795: INFO: machine-config-daemon-rbg8b from openshift-machine-config-operator started at 2019-08-06 12:32:31 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.795: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug  6 16:21:05.795: INFO: elasticsearch-operator-7c5cc4bff9-spd5k from openshift-operators started at 2019-08-06 12:40:44 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.795: INFO: 	Container elasticsearch-operator ready: true, restart count 0
Aug  6 16:21:05.796: INFO: multus-jgdnj from openshift-multus started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.796: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 16:21:05.796: INFO: sdn-6rmrm from openshift-sdn started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.796: INFO: 	Container sdn ready: true, restart count 0
Aug  6 16:21:05.796: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-08-06 12:40:46 +0000 UTC (6 container statuses recorded)
Aug  6 16:21:05.796: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:21:05.796: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug  6 16:21:05.796: INFO: 	Container prometheus ready: true, restart count 1
Aug  6 16:21:05.796: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug  6 16:21:05.796: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug  6 16:21:05.796: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug  6 16:21:05.796: INFO: sre-dns-latency-exporter-n784c from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.796: INFO: 	Container main ready: true, restart count 0
Aug  6 16:21:05.796: INFO: sre-machine-api-status-exporter-1-deploy from openshift-monitoring started at 2019-08-06 12:40:13 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.796: INFO: 	Container deployment ready: false, restart count 0
Aug  6 16:21:05.796: INFO: sre-stuck-ebs-vols-1-xv82v from openshift-monitoring started at 2019-08-06 12:40:20 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.796: INFO: 	Container main ready: true, restart count 0
Aug  6 16:21:05.796: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-08-06 12:40:50 +0000 UTC (3 container statuses recorded)
Aug  6 16:21:05.796: INFO: 	Container alertmanager ready: true, restart count 0
Aug  6 16:21:05.796: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug  6 16:21:05.796: INFO: 	Container config-reloader ready: true, restart count 0
Aug  6 16:21:05.796: INFO: tuned-qbg52 from openshift-cluster-node-tuning-operator started at 2019-08-06 12:32:11 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.796: INFO: 	Container tuned ready: true, restart count 0
Aug  6 16:21:05.796: INFO: node-ca-bh6hh from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.796: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 16:21:05.796: INFO: router-default-7bc87fbb58-bm9g9 from openshift-ingress started at 2019-08-06 12:58:06 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.796: INFO: 	Container router ready: true, restart count 0
Aug  6 16:21:05.796: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-134.ec2.internal before test
Aug  6 16:21:05.863: INFO: ovs-j2g2j from openshift-sdn started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 16:21:05.863: INFO: tuned-x8rhd from openshift-cluster-node-tuning-operator started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container tuned ready: true, restart count 0
Aug  6 16:21:05.863: INFO: prometheus-adapter-74d56f8767-krgfn from openshift-monitoring started at 2019-08-06 12:34:16 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug  6 16:21:05.863: INFO: sre-dns-latency-exporter-pl7hx from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container main ready: true, restart count 0
Aug  6 16:21:05.863: INFO: sdn-gh6xp from openshift-sdn started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container sdn ready: true, restart count 0
Aug  6 16:21:05.863: INFO: sre-ebs-iops-reporter-1-deploy from openshift-monitoring started at 2019-08-06 12:40:12 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container deployment ready: false, restart count 0
Aug  6 16:21:05.863: INFO: deployments-pruner-1565103600-294mv from openshift-sre-pruning started at 2019-08-06 15:00:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container deployments-pruner ready: false, restart count 0
Aug  6 16:21:05.863: INFO: builds-pruner-1565107200-2hmhf from openshift-sre-pruning started at 2019-08-06 16:00:04 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container builds-pruner ready: false, restart count 0
Aug  6 16:21:05.863: INFO: installed-redhat-openshift-operators-547f964d5f-4t496 from openshift-marketplace started at 2019-08-06 12:40:15 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container installed-redhat-openshift-operators ready: true, restart count 0
Aug  6 16:21:05.863: INFO: image-pruner-1565107200-64tsz from openshift-sre-pruning started at 2019-08-06 16:00:04 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container image-pruner ready: false, restart count 0
Aug  6 16:21:05.863: INFO: deployments-pruner-1565107200-9jwrf from openshift-sre-pruning started at 2019-08-06 16:00:04 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container deployments-pruner ready: false, restart count 0
Aug  6 16:21:05.863: INFO: configure-alertmanager-operator-74579866b-lzq26 from openshift-monitoring started at 2019-08-06 12:47:48 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Aug  6 16:21:05.863: INFO: multus-6mjbh from openshift-multus started at 2019-08-06 12:31:19 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 16:21:05.863: INFO: node-exporter-hrdtt from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:21:05.863: INFO: 	Container node-exporter ready: true, restart count 0
Aug  6 16:21:05.863: INFO: sre-stuck-ebs-vols-1-deploy from openshift-monitoring started at 2019-08-06 12:40:11 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container deployment ready: false, restart count 0
Aug  6 16:21:05.863: INFO: dedicated-admin-operator-77956f4b8-nd6fz from openshift-dedicated-admin started at 2019-08-06 12:47:48 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container dedicated-admin-operator ready: true, restart count 0
Aug  6 16:21:05.863: INFO: dns-default-j5pgg from openshift-dns started at 2019-08-06 12:31:19 +0000 UTC (2 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container dns ready: true, restart count 0
Aug  6 16:21:05.863: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 16:21:05.863: INFO: kube-state-metrics-645d9dc5b9-hklgd from openshift-monitoring started at 2019-08-06 12:32:43 +0000 UTC (3 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug  6 16:21:05.863: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug  6 16:21:05.863: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug  6 16:21:05.863: INFO: image-pruner-1565100000-4h6gc from openshift-sre-pruning started at 2019-08-06 14:00:03 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container image-pruner ready: false, restart count 0
Aug  6 16:21:05.863: INFO: builds-pruner-1565103600-pq7mw from openshift-sre-pruning started at 2019-08-06 15:00:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container builds-pruner ready: false, restart count 0
Aug  6 16:21:05.863: INFO: router-default-7bc87fbb58-8jfjn from openshift-ingress started at 2019-08-06 12:58:29 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container router ready: true, restart count 0
Aug  6 16:21:05.863: INFO: builds-pruner-1565100000-bt5b6 from openshift-sre-pruning started at 2019-08-06 14:00:03 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.863: INFO: 	Container builds-pruner ready: false, restart count 0
Aug  6 16:21:05.864: INFO: deployments-pruner-1565100000-ht542 from openshift-sre-pruning started at 2019-08-06 14:00:03 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.864: INFO: 	Container deployments-pruner ready: false, restart count 0
Aug  6 16:21:05.864: INFO: machine-config-daemon-zf9nq from openshift-machine-config-operator started at 2019-08-06 12:31:49 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.864: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug  6 16:21:05.864: INFO: redhat-operators-6fc9f9c9ff-nld4d from openshift-marketplace started at 2019-08-06 12:32:38 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.864: INFO: 	Container redhat-operators ready: true, restart count 0
Aug  6 16:21:05.864: INFO: node-ca-d98qg from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.864: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 16:21:05.864: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-135-96.ec2.internal before test
Aug  6 16:21:05.910: INFO: prometheus-operator-7cbf84478f-9cqcb from openshift-monitoring started at 2019-08-06 12:40:13 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.910: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug  6 16:21:05.910: INFO: sdn-nrctn from openshift-sdn started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.910: INFO: 	Container sdn ready: true, restart count 0
Aug  6 16:21:05.910: INFO: dns-default-qhqr4 from openshift-dns started at 2019-08-06 12:31:52 +0000 UTC (2 container statuses recorded)
Aug  6 16:21:05.910: INFO: 	Container dns ready: true, restart count 0
Aug  6 16:21:05.910: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 16:21:05.910: INFO: telemeter-client-6878d94476-kncjs from openshift-monitoring started at 2019-08-06 12:32:46 +0000 UTC (3 container statuses recorded)
Aug  6 16:21:05.910: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:21:05.910: INFO: 	Container reload ready: true, restart count 0
Aug  6 16:21:05.910: INFO: 	Container telemeter-client ready: true, restart count 0
Aug  6 16:21:05.910: INFO: sre-dns-latency-exporter-xdr2p from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.910: INFO: 	Container main ready: true, restart count 0
Aug  6 16:21:05.910: INFO: multus-zwdrq from openshift-multus started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.910: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 16:21:05.910: INFO: machine-config-daemon-rsm96 from openshift-machine-config-operator started at 2019-08-06 12:32:21 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.910: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug  6 16:21:05.911: INFO: installed-redhat-openshift-logging-7b6d9d86f5-2whm2 from openshift-marketplace started at 2019-08-06 12:40:15 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container installed-redhat-openshift-logging ready: true, restart count 0
Aug  6 16:21:05.911: INFO: cluster-logging-operator-7c58ddb664-kvlv7 from openshift-logging started at 2019-08-06 12:40:56 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container cluster-logging-operator ready: true, restart count 0
Aug  6 16:21:05.911: INFO: dedicated-admin-operator-registry-dvtrw from openshift-operator-lifecycle-manager started at 2019-08-06 12:40:14 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container registry-server ready: true, restart count 0
Aug  6 16:21:05.911: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-08-06 12:40:19 +0000 UTC (3 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container alertmanager ready: true, restart count 0
Aug  6 16:21:05.911: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug  6 16:21:05.911: INFO: 	Container config-reloader ready: true, restart count 0
Aug  6 16:21:05.911: INFO: node-ca-5w4gz from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 16:21:05.911: INFO: community-operators-7b9c4c96b8-s6c84 from openshift-marketplace started at 2019-08-06 12:32:38 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container community-operators ready: true, restart count 0
Aug  6 16:21:05.911: INFO: ovs-s6jvw from openshift-sdn started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 16:21:05.911: INFO: tuned-dkb6k from openshift-cluster-node-tuning-operator started at 2019-08-06 12:31:52 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container tuned ready: true, restart count 0
Aug  6 16:21:05.911: INFO: image-registry-57774c6454-5lpmh from openshift-image-registry started at 2019-08-06 12:32:42 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container registry ready: true, restart count 0
Aug  6 16:21:05.911: INFO: node-exporter-ccph9 from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:21:05.911: INFO: 	Container node-exporter ready: true, restart count 0
Aug  6 16:21:05.911: INFO: configure-alertmanager-operator-registry-bvv62 from openshift-operator-lifecycle-manager started at 2019-08-06 12:40:07 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container registry-server ready: true, restart count 0
Aug  6 16:21:05.911: INFO: image-pruner-1565103600-44bxp from openshift-sre-pruning started at 2019-08-06 15:00:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.911: INFO: 	Container image-pruner ready: false, restart count 0
Aug  6 16:21:05.911: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-62.ec2.internal before test
Aug  6 16:21:05.957: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-08-06 12:40:46 +0000 UTC (6 container statuses recorded)
Aug  6 16:21:05.957: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:21:05.957: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug  6 16:21:05.957: INFO: 	Container prometheus ready: true, restart count 1
Aug  6 16:21:05.958: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug  6 16:21:05.958: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug  6 16:21:05.958: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug  6 16:21:05.958: INFO: sre-dns-latency-exporter-crjdn from openshift-monitoring started at 2019-08-06 12:40:08 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container main ready: true, restart count 0
Aug  6 16:21:05.958: INFO: tuned-mjhtk from openshift-cluster-node-tuning-operator started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container tuned ready: true, restart count 0
Aug  6 16:21:05.958: INFO: grafana-86f8cd476c-gm5tf from openshift-monitoring started at 2019-08-06 12:33:08 +0000 UTC (2 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container grafana ready: true, restart count 0
Aug  6 16:21:05.958: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug  6 16:21:05.958: INFO: machine-config-daemon-m7b9d from openshift-machine-config-operator started at 2019-08-06 12:33:06 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug  6 16:21:05.958: INFO: prometheus-adapter-74d56f8767-wc4bj from openshift-monitoring started at 2019-08-06 12:34:16 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug  6 16:21:05.958: INFO: sdn-55nwk from openshift-sdn started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container sdn ready: true, restart count 0
Aug  6 16:21:05.958: INFO: dns-default-kc256 from openshift-dns started at 2019-08-06 12:32:36 +0000 UTC (2 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container dns ready: true, restart count 0
Aug  6 16:21:05.958: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug  6 16:21:05.958: INFO: multus-4qwmn from openshift-multus started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container kube-multus ready: true, restart count 0
Aug  6 16:21:05.958: INFO: sre-ebs-iops-reporter-1-nvmlq from openshift-monitoring started at 2019-08-06 12:40:20 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container main ready: true, restart count 0
Aug  6 16:21:05.958: INFO: node-ca-9tl67 from openshift-image-registry started at 2019-08-06 12:33:06 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container node-ca ready: true, restart count 0
Aug  6 16:21:05.958: INFO: sre-machine-api-status-exporter-1-clk9g from openshift-monitoring started at 2019-08-06 12:40:33 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container main ready: true, restart count 0
Aug  6 16:21:05.958: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-08-06 12:41:25 +0000 UTC (3 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container alertmanager ready: true, restart count 0
Aug  6 16:21:05.958: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug  6 16:21:05.958: INFO: 	Container config-reloader ready: true, restart count 0
Aug  6 16:21:05.958: INFO: ovs-h9vns from openshift-sdn started at 2019-08-06 12:32:36 +0000 UTC (1 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container openvswitch ready: true, restart count 0
Aug  6 16:21:05.958: INFO: node-exporter-56p4z from openshift-monitoring started at 2019-08-06 12:32:44 +0000 UTC (2 container statuses recorded)
Aug  6 16:21:05.958: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug  6 16:21:05.958: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15b861d65660e6d7], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) were unschedulable, 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:21:07.320: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-qlg5d" for this suite.
Aug  6 16:21:13.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:21:14.235: INFO: namespace: e2e-tests-sched-pred-qlg5d, resource: packagemanifests, items remaining: 3
Aug  6 16:21:14.587: INFO: namespace: e2e-tests-sched-pred-qlg5d, resource: bindings, ignored listing per whitelist
Aug  6 16:21:15.875: INFO: namespace: e2e-tests-sched-pred-qlg5d no longer exists
Aug  6 16:21:15.897: INFO: namespace: e2e-tests-sched-pred-qlg5d, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:21:15.919: INFO: namespace e2e-tests-sched-pred-qlg5d deletion completed in 8.536156791s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.616 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:21:15.920: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:21:23.571: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-cpvr7" for this suite.
Aug  6 16:21:29.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:21:31.323: INFO: namespace: e2e-tests-namespaces-cpvr7, resource: bindings, ignored listing per whitelist
Aug  6 16:21:32.057: INFO: namespace: e2e-tests-namespaces-cpvr7, resource: packagemanifests, items remaining: 3
Aug  6 16:21:32.122: INFO: namespace: e2e-tests-namespaces-cpvr7 no longer exists
Aug  6 16:21:32.145: INFO: namespace: e2e-tests-namespaces-cpvr7, total namespaces: 51, active: 51, terminating: 0
Aug  6 16:21:32.167: INFO: namespace e2e-tests-namespaces-cpvr7 deletion completed in 8.530920466s
STEP: Destroying namespace "e2e-tests-nsdeletetest-nhtpp" for this suite.
Aug  6 16:21:32.190: INFO: Namespace e2e-tests-nsdeletetest-nhtpp was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-wcgwr" for this suite.
Aug  6 16:21:38.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:21:39.768: INFO: namespace: e2e-tests-nsdeletetest-wcgwr, resource: packagemanifests, items remaining: 3
Aug  6 16:21:39.878: INFO: namespace: e2e-tests-nsdeletetest-wcgwr, resource: bindings, ignored listing per whitelist
Aug  6 16:21:40.674: INFO: namespace: e2e-tests-nsdeletetest-wcgwr no longer exists
Aug  6 16:21:40.698: INFO: namespace: e2e-tests-nsdeletetest-wcgwr, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:21:40.721: INFO: namespace e2e-tests-nsdeletetest-wcgwr deletion completed in 8.531050959s

• [SLOW TEST:24.801 seconds]
[sig-api-machinery] Namespaces [Serial]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:21:40.721: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Aug  6 16:21:42.383: INFO: Waiting up to 5m0s for pod "downward-api-46ae74f7-b866-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-mw26m" to be "success or failure"
Aug  6 16:21:42.406: INFO: Pod "downward-api-46ae74f7-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.325229ms
Aug  6 16:21:44.430: INFO: Pod "downward-api-46ae74f7-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047073722s
Aug  6 16:21:46.453: INFO: Pod "downward-api-46ae74f7-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070516635s
Aug  6 16:21:48.477: INFO: Pod "downward-api-46ae74f7-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094593981s
Aug  6 16:21:50.500: INFO: Pod "downward-api-46ae74f7-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117760807s
Aug  6 16:21:52.524: INFO: Pod "downward-api-46ae74f7-b866-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.141291487s
STEP: Saw pod success
Aug  6 16:21:52.524: INFO: Pod "downward-api-46ae74f7-b866-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:21:52.547: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downward-api-46ae74f7-b866-11e9-8d18-525400524259 container dapi-container: <nil>
STEP: delete the pod
Aug  6 16:21:52.603: INFO: Waiting for pod downward-api-46ae74f7-b866-11e9-8d18-525400524259 to disappear
Aug  6 16:21:52.625: INFO: Pod downward-api-46ae74f7-b866-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:21:52.625: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mw26m" for this suite.
Aug  6 16:21:58.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:21:59.654: INFO: namespace: e2e-tests-downward-api-mw26m, resource: packagemanifests, items remaining: 3
Aug  6 16:21:59.743: INFO: namespace: e2e-tests-downward-api-mw26m, resource: bindings, ignored listing per whitelist
Aug  6 16:22:01.175: INFO: namespace: e2e-tests-downward-api-mw26m no longer exists
Aug  6 16:22:01.197: INFO: namespace: e2e-tests-downward-api-mw26m, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:22:01.219: INFO: namespace e2e-tests-downward-api-mw26m deletion completed in 8.531390099s

• [SLOW TEST:20.498 seconds]
[sig-node] Downward API
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:22:01.220: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug  6 16:22:02.638: INFO: Pod name pod-release: Found 0 pods out of 1
Aug  6 16:22:07.662: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:22:07.735: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-zdmxh" for this suite.
Aug  6 16:22:13.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:22:14.579: INFO: namespace: e2e-tests-replication-controller-zdmxh, resource: bindings, ignored listing per whitelist
Aug  6 16:22:14.783: INFO: namespace: e2e-tests-replication-controller-zdmxh, resource: packagemanifests, items remaining: 3
Aug  6 16:22:16.285: INFO: namespace: e2e-tests-replication-controller-zdmxh no longer exists
Aug  6 16:22:16.308: INFO: namespace: e2e-tests-replication-controller-zdmxh, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:22:16.330: INFO: namespace e2e-tests-replication-controller-zdmxh deletion completed in 8.530795247s

• [SLOW TEST:15.110 seconds]
[sig-apps] ReplicationController
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:22:16.331: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-8rc9k
Aug  6 16:22:27.775: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-8rc9k
STEP: checking the pod's current state and verifying that restartCount is present
Aug  6 16:22:27.797: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:26:28.609: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-8rc9k" for this suite.
Aug  6 16:26:34.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:26:36.957: INFO: namespace: e2e-tests-container-probe-8rc9k, resource: packagemanifests, items remaining: 3
Aug  6 16:26:37.089: INFO: namespace: e2e-tests-container-probe-8rc9k, resource: bindings, ignored listing per whitelist
Aug  6 16:26:37.176: INFO: namespace: e2e-tests-container-probe-8rc9k no longer exists
Aug  6 16:26:37.200: INFO: namespace: e2e-tests-container-probe-8rc9k, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:26:37.222: INFO: namespace e2e-tests-container-probe-8rc9k deletion completed in 8.548507591s

• [SLOW TEST:260.891 seconds]
[k8s.io] Probing container
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:26:37.223: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Aug  6 16:26:38.636: INFO: Waiting up to 5m0s for pod "downward-api-f7424678-b866-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-8s6kr" to be "success or failure"
Aug  6 16:26:38.658: INFO: Pod "downward-api-f7424678-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014804ms
Aug  6 16:26:40.683: INFO: Pod "downward-api-f7424678-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046759763s
Aug  6 16:26:42.706: INFO: Pod "downward-api-f7424678-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069765125s
Aug  6 16:26:44.730: INFO: Pod "downward-api-f7424678-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093753447s
Aug  6 16:26:46.754: INFO: Pod "downward-api-f7424678-b866-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117805844s
Aug  6 16:26:48.779: INFO: Pod "downward-api-f7424678-b866-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.142821102s
STEP: Saw pod success
Aug  6 16:26:48.779: INFO: Pod "downward-api-f7424678-b866-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:26:48.802: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downward-api-f7424678-b866-11e9-8d18-525400524259 container dapi-container: <nil>
STEP: delete the pod
Aug  6 16:26:48.857: INFO: Waiting for pod downward-api-f7424678-b866-11e9-8d18-525400524259 to disappear
Aug  6 16:26:48.879: INFO: Pod downward-api-f7424678-b866-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:26:48.880: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8s6kr" for this suite.
Aug  6 16:26:55.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:26:55.687: INFO: namespace: e2e-tests-downward-api-8s6kr, resource: packagemanifests, items remaining: 3
Aug  6 16:26:55.776: INFO: namespace: e2e-tests-downward-api-8s6kr, resource: bindings, ignored listing per whitelist
Aug  6 16:26:57.431: INFO: namespace: e2e-tests-downward-api-8s6kr no longer exists
Aug  6 16:26:57.455: INFO: namespace: e2e-tests-downward-api-8s6kr, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:26:57.477: INFO: namespace e2e-tests-downward-api-8s6kr deletion completed in 8.534550373s

• [SLOW TEST:20.255 seconds]
[sig-node] Downward API
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:26:57.479: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug  6 16:26:58.888: INFO: Waiting up to 5m0s for pod "pod-035509f8-b867-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-jlqtg" to be "success or failure"
Aug  6 16:26:58.911: INFO: Pod "pod-035509f8-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.287734ms
Aug  6 16:27:00.933: INFO: Pod "pod-035509f8-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044922758s
Aug  6 16:27:02.956: INFO: Pod "pod-035509f8-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067820969s
Aug  6 16:27:04.979: INFO: Pod "pod-035509f8-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090626154s
Aug  6 16:27:07.002: INFO: Pod "pod-035509f8-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113878422s
Aug  6 16:27:09.027: INFO: Pod "pod-035509f8-b867-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.138926259s
STEP: Saw pod success
Aug  6 16:27:09.027: INFO: Pod "pod-035509f8-b867-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:27:09.051: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-035509f8-b867-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 16:27:09.136: INFO: Waiting for pod pod-035509f8-b867-11e9-8d18-525400524259 to disappear
Aug  6 16:27:09.158: INFO: Pod pod-035509f8-b867-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:27:09.158: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jlqtg" for this suite.
Aug  6 16:27:15.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:27:16.246: INFO: namespace: e2e-tests-emptydir-jlqtg, resource: bindings, ignored listing per whitelist
Aug  6 16:27:17.223: INFO: namespace: e2e-tests-emptydir-jlqtg, resource: packagemanifests, items remaining: 3
Aug  6 16:27:17.712: INFO: namespace: e2e-tests-emptydir-jlqtg no longer exists
Aug  6 16:27:17.736: INFO: namespace: e2e-tests-emptydir-jlqtg, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:27:17.758: INFO: namespace e2e-tests-emptydir-jlqtg deletion completed in 8.53776545s

• [SLOW TEST:20.280 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:27:17.760: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:27:19.169: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-0f70aecd-b867-11e9-8d18-525400524259
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-0f70aecd-b867-11e9-8d18-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:28:40.296: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gwx86" for this suite.
Aug  6 16:29:02.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:29:03.175: INFO: namespace: e2e-tests-configmap-gwx86, resource: packagemanifests, items remaining: 3
Aug  6 16:29:03.735: INFO: namespace: e2e-tests-configmap-gwx86, resource: bindings, ignored listing per whitelist
Aug  6 16:29:04.867: INFO: namespace: e2e-tests-configmap-gwx86 no longer exists
Aug  6 16:29:04.890: INFO: namespace: e2e-tests-configmap-gwx86, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:29:04.913: INFO: namespace e2e-tests-configmap-gwx86 deletion completed in 24.552341249s

• [SLOW TEST:107.153 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:29:04.913: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 16:29:06.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f498990-b867-11e9-8d18-525400524259" in namespace "e2e-tests-projected-rfc2w" to be "success or failure"
Aug  6 16:29:06.388: INFO: Pod "downwardapi-volume-4f498990-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 52.226548ms
Aug  6 16:29:08.412: INFO: Pod "downwardapi-volume-4f498990-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075749639s
Aug  6 16:29:10.435: INFO: Pod "downwardapi-volume-4f498990-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098704594s
Aug  6 16:29:12.457: INFO: Pod "downwardapi-volume-4f498990-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.121514226s
Aug  6 16:29:14.481: INFO: Pod "downwardapi-volume-4f498990-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.144563544s
Aug  6 16:29:16.505: INFO: Pod "downwardapi-volume-4f498990-b867-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.168625146s
STEP: Saw pod success
Aug  6 16:29:16.505: INFO: Pod "downwardapi-volume-4f498990-b867-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:29:16.527: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-4f498990-b867-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 16:29:16.582: INFO: Waiting for pod downwardapi-volume-4f498990-b867-11e9-8d18-525400524259 to disappear
Aug  6 16:29:16.606: INFO: Pod downwardapi-volume-4f498990-b867-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:29:16.607: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rfc2w" for this suite.
Aug  6 16:29:22.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:29:24.149: INFO: namespace: e2e-tests-projected-rfc2w, resource: packagemanifests, items remaining: 3
Aug  6 16:29:24.460: INFO: namespace: e2e-tests-projected-rfc2w, resource: bindings, ignored listing per whitelist
Aug  6 16:29:25.179: INFO: namespace: e2e-tests-projected-rfc2w no longer exists
Aug  6 16:29:25.203: INFO: namespace: e2e-tests-projected-rfc2w, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:29:25.225: INFO: namespace e2e-tests-projected-rfc2w deletion completed in 8.555669604s

• [SLOW TEST:20.312 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:29:25.226: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-5b63fd76-b867-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 16:29:26.679: INFO: Waiting up to 5m0s for pod "pod-secrets-5b6a83bc-b867-11e9-8d18-525400524259" in namespace "e2e-tests-secrets-v4n8d" to be "success or failure"
Aug  6 16:29:26.707: INFO: Pod "pod-secrets-5b6a83bc-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 28.410363ms
Aug  6 16:29:28.731: INFO: Pod "pod-secrets-5b6a83bc-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051864916s
Aug  6 16:29:30.754: INFO: Pod "pod-secrets-5b6a83bc-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074802398s
Aug  6 16:29:32.777: INFO: Pod "pod-secrets-5b6a83bc-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098153867s
Aug  6 16:29:34.800: INFO: Pod "pod-secrets-5b6a83bc-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.121349924s
Aug  6 16:29:36.823: INFO: Pod "pod-secrets-5b6a83bc-b867-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.14443225s
STEP: Saw pod success
Aug  6 16:29:36.823: INFO: Pod "pod-secrets-5b6a83bc-b867-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:29:36.846: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-secrets-5b6a83bc-b867-11e9-8d18-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Aug  6 16:29:36.900: INFO: Waiting for pod pod-secrets-5b6a83bc-b867-11e9-8d18-525400524259 to disappear
Aug  6 16:29:36.922: INFO: Pod pod-secrets-5b6a83bc-b867-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:29:36.922: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-v4n8d" for this suite.
Aug  6 16:29:43.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:29:43.730: INFO: namespace: e2e-tests-secrets-v4n8d, resource: packagemanifests, items remaining: 3
Aug  6 16:29:43.774: INFO: namespace: e2e-tests-secrets-v4n8d, resource: bindings, ignored listing per whitelist
Aug  6 16:29:45.487: INFO: namespace: e2e-tests-secrets-v4n8d no longer exists
Aug  6 16:29:45.510: INFO: namespace: e2e-tests-secrets-v4n8d, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:29:45.532: INFO: namespace e2e-tests-secrets-v4n8d deletion completed in 8.546176955s

• [SLOW TEST:20.306 seconds]
[sig-storage] Secrets
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:29:45.532: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:29:46.962: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-nxmrt" for this suite.
Aug  6 16:29:53.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:29:54.456: INFO: namespace: e2e-tests-kubelet-test-nxmrt, resource: packagemanifests, items remaining: 3
Aug  6 16:29:55.456: INFO: namespace: e2e-tests-kubelet-test-nxmrt, resource: bindings, ignored listing per whitelist
Aug  6 16:29:55.503: INFO: namespace: e2e-tests-kubelet-test-nxmrt no longer exists
Aug  6 16:29:55.526: INFO: namespace: e2e-tests-kubelet-test-nxmrt, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:29:55.548: INFO: namespace e2e-tests-kubelet-test-nxmrt deletion completed in 8.542562785s

• [SLOW TEST:10.016 seconds]
[k8s.io] Kubelet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:29:55.548: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:29:56.959: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-6d7d8427-b867-11e9-8d18-525400524259
STEP: Creating secret with name s-test-opt-upd-6d7d8466-b867-11e9-8d18-525400524259
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-6d7d8427-b867-11e9-8d18-525400524259
STEP: Updating secret s-test-opt-upd-6d7d8466-b867-11e9-8d18-525400524259
STEP: Creating secret with name s-test-opt-create-6d7d8478-b867-11e9-8d18-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:31:26.358: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4gqpc" for this suite.
Aug  6 16:31:48.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:31:49.818: INFO: namespace: e2e-tests-projected-4gqpc, resource: packagemanifests, items remaining: 3
Aug  6 16:31:50.460: INFO: namespace: e2e-tests-projected-4gqpc, resource: bindings, ignored listing per whitelist
Aug  6 16:31:50.911: INFO: namespace: e2e-tests-projected-4gqpc no longer exists
Aug  6 16:31:50.935: INFO: namespace: e2e-tests-projected-4gqpc, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:31:50.957: INFO: namespace e2e-tests-projected-4gqpc deletion completed in 24.555799096s

• [SLOW TEST:115.409 seconds]
[sig-storage] Projected secret
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:31:50.962: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-sglsd/configmap-test-b241f37a-b867-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 16:31:52.386: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2456f9d-b867-11e9-8d18-525400524259" in namespace "e2e-tests-configmap-sglsd" to be "success or failure"
Aug  6 16:31:52.408: INFO: Pod "pod-configmaps-b2456f9d-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 21.958436ms
Aug  6 16:31:54.431: INFO: Pod "pod-configmaps-b2456f9d-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044939169s
Aug  6 16:31:56.454: INFO: Pod "pod-configmaps-b2456f9d-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068158296s
Aug  6 16:31:58.477: INFO: Pod "pod-configmaps-b2456f9d-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091271987s
Aug  6 16:32:00.500: INFO: Pod "pod-configmaps-b2456f9d-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114082177s
Aug  6 16:32:02.523: INFO: Pod "pod-configmaps-b2456f9d-b867-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137273634s
STEP: Saw pod success
Aug  6 16:32:02.523: INFO: Pod "pod-configmaps-b2456f9d-b867-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:32:02.545: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-configmaps-b2456f9d-b867-11e9-8d18-525400524259 container env-test: <nil>
STEP: delete the pod
Aug  6 16:32:02.601: INFO: Waiting for pod pod-configmaps-b2456f9d-b867-11e9-8d18-525400524259 to disappear
Aug  6 16:32:02.624: INFO: Pod pod-configmaps-b2456f9d-b867-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:32:02.624: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sglsd" for this suite.
Aug  6 16:32:08.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:32:10.133: INFO: namespace: e2e-tests-configmap-sglsd, resource: packagemanifests, items remaining: 3
Aug  6 16:32:10.991: INFO: namespace: e2e-tests-configmap-sglsd, resource: bindings, ignored listing per whitelist
Aug  6 16:32:11.186: INFO: namespace: e2e-tests-configmap-sglsd no longer exists
Aug  6 16:32:11.209: INFO: namespace: e2e-tests-configmap-sglsd, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:32:11.231: INFO: namespace e2e-tests-configmap-sglsd deletion completed in 8.53607255s

• [SLOW TEST:20.269 seconds]
[sig-node] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:32:11.232: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug  6 16:32:12.677: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-dkg2t,SelfLink:/api/v1/namespaces/e2e-tests-watch-dkg2t/configmaps/e2e-watch-test-watch-closed,UID:b0e3e5cd-b867-11e9-9dff-0e6de702691c,ResourceVersion:145922,Generation:0,CreationTimestamp:2019-08-06 16:31:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug  6 16:32:12.677: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-dkg2t,SelfLink:/api/v1/namespaces/e2e-tests-watch-dkg2t/configmaps/e2e-watch-test-watch-closed,UID:b0e3e5cd-b867-11e9-9dff-0e6de702691c,ResourceVersion:145926,Generation:0,CreationTimestamp:2019-08-06 16:31:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug  6 16:32:12.775: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-dkg2t,SelfLink:/api/v1/namespaces/e2e-tests-watch-dkg2t/configmaps/e2e-watch-test-watch-closed,UID:b0e3e5cd-b867-11e9-9dff-0e6de702691c,ResourceVersion:145928,Generation:0,CreationTimestamp:2019-08-06 16:31:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug  6 16:32:12.775: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-dkg2t,SelfLink:/api/v1/namespaces/e2e-tests-watch-dkg2t/configmaps/e2e-watch-test-watch-closed,UID:b0e3e5cd-b867-11e9-9dff-0e6de702691c,ResourceVersion:145929,Generation:0,CreationTimestamp:2019-08-06 16:31:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:32:12.775: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-dkg2t" for this suite.
Aug  6 16:32:18.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:32:19.747: INFO: namespace: e2e-tests-watch-dkg2t, resource: bindings, ignored listing per whitelist
Aug  6 16:32:20.483: INFO: namespace: e2e-tests-watch-dkg2t, resource: packagemanifests, items remaining: 3
Aug  6 16:32:21.308: INFO: namespace: e2e-tests-watch-dkg2t no longer exists
Aug  6 16:32:21.330: INFO: namespace: e2e-tests-watch-dkg2t, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:32:21.353: INFO: namespace e2e-tests-watch-dkg2t deletion completed in 8.534822512s

• [SLOW TEST:10.120 seconds]
[sig-api-machinery] Watchers
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:32:21.353: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-c45fae89-b867-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 16:32:22.781: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c4632ab1-b867-11e9-8d18-525400524259" in namespace "e2e-tests-projected-ksxdt" to be "success or failure"
Aug  6 16:32:22.803: INFO: Pod "pod-projected-configmaps-c4632ab1-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.235438ms
Aug  6 16:32:24.828: INFO: Pod "pod-projected-configmaps-c4632ab1-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046946579s
Aug  6 16:32:26.852: INFO: Pod "pod-projected-configmaps-c4632ab1-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071762371s
Aug  6 16:32:28.876: INFO: Pod "pod-projected-configmaps-c4632ab1-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.095024485s
Aug  6 16:32:30.899: INFO: Pod "pod-projected-configmaps-c4632ab1-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.11799979s
Aug  6 16:32:32.922: INFO: Pod "pod-projected-configmaps-c4632ab1-b867-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.141277408s
STEP: Saw pod success
Aug  6 16:32:32.922: INFO: Pod "pod-projected-configmaps-c4632ab1-b867-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:32:32.945: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-configmaps-c4632ab1-b867-11e9-8d18-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 16:32:32.999: INFO: Waiting for pod pod-projected-configmaps-c4632ab1-b867-11e9-8d18-525400524259 to disappear
Aug  6 16:32:33.021: INFO: Pod pod-projected-configmaps-c4632ab1-b867-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:32:33.021: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ksxdt" for this suite.
Aug  6 16:32:39.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:32:41.137: INFO: namespace: e2e-tests-projected-ksxdt, resource: packagemanifests, items remaining: 3
Aug  6 16:32:41.533: INFO: namespace: e2e-tests-projected-ksxdt, resource: bindings, ignored listing per whitelist
Aug  6 16:32:41.576: INFO: namespace: e2e-tests-projected-ksxdt no longer exists
Aug  6 16:32:41.599: INFO: namespace: e2e-tests-projected-ksxdt, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:32:41.621: INFO: namespace e2e-tests-projected-ksxdt deletion completed in 8.537823635s

• [SLOW TEST:20.268 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:32:41.621: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 16:32:43.014: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0721887-b867-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-mmqbm" to be "success or failure"
Aug  6 16:32:43.039: INFO: Pod "downwardapi-volume-d0721887-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 25.728497ms
Aug  6 16:32:45.063: INFO: Pod "downwardapi-volume-d0721887-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049156685s
Aug  6 16:32:47.086: INFO: Pod "downwardapi-volume-d0721887-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071949412s
Aug  6 16:32:49.109: INFO: Pod "downwardapi-volume-d0721887-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.095225491s
Aug  6 16:32:51.132: INFO: Pod "downwardapi-volume-d0721887-b867-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.118150503s
Aug  6 16:32:53.155: INFO: Pod "downwardapi-volume-d0721887-b867-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.141462033s
STEP: Saw pod success
Aug  6 16:32:53.155: INFO: Pod "downwardapi-volume-d0721887-b867-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:32:53.178: INFO: Trying to get logs from node ip-10-0-128-44.ec2.internal pod downwardapi-volume-d0721887-b867-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 16:32:53.234: INFO: Waiting for pod downwardapi-volume-d0721887-b867-11e9-8d18-525400524259 to disappear
Aug  6 16:32:53.256: INFO: Pod downwardapi-volume-d0721887-b867-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:32:53.256: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mmqbm" for this suite.
Aug  6 16:32:59.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:33:01.223: INFO: namespace: e2e-tests-downward-api-mmqbm, resource: bindings, ignored listing per whitelist
Aug  6 16:33:01.713: INFO: namespace: e2e-tests-downward-api-mmqbm, resource: packagemanifests, items remaining: 3
Aug  6 16:33:01.801: INFO: namespace: e2e-tests-downward-api-mmqbm no longer exists
Aug  6 16:33:01.824: INFO: namespace: e2e-tests-downward-api-mmqbm, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:33:01.846: INFO: namespace e2e-tests-downward-api-mmqbm deletion completed in 8.52710974s

• [SLOW TEST:20.225 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:33:01.848: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0806 16:33:33.441134    4087 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug  6 16:33:33.441: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:33:33.441: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-kpl6q" for this suite.
Aug  6 16:33:39.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:33:40.939: INFO: namespace: e2e-tests-gc-kpl6q, resource: packagemanifests, items remaining: 3
Aug  6 16:33:41.092: INFO: namespace: e2e-tests-gc-kpl6q, resource: bindings, ignored listing per whitelist
Aug  6 16:33:41.981: INFO: namespace: e2e-tests-gc-kpl6q no longer exists
Aug  6 16:33:42.004: INFO: namespace: e2e-tests-gc-kpl6q, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:33:42.027: INFO: namespace e2e-tests-gc-kpl6q deletion completed in 8.543880113s

• [SLOW TEST:40.180 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:33:42.028: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:33:43.446: INFO: (0) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 28.356728ms)
Aug  6 16:33:43.470: INFO: (1) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 23.794235ms)
Aug  6 16:33:43.492: INFO: (2) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.325159ms)
Aug  6 16:33:43.515: INFO: (3) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 23.017233ms)
Aug  6 16:33:43.539: INFO: (4) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 23.177668ms)
Aug  6 16:33:43.561: INFO: (5) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.614795ms)
Aug  6 16:33:43.584: INFO: (6) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.725481ms)
Aug  6 16:33:43.608: INFO: (7) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 23.552352ms)
Aug  6 16:33:43.630: INFO: (8) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.726706ms)
Aug  6 16:33:43.654: INFO: (9) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 23.670788ms)
Aug  6 16:33:43.679: INFO: (10) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 24.384906ms)
Aug  6 16:33:43.702: INFO: (11) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 23.087033ms)
Aug  6 16:33:43.724: INFO: (12) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.61621ms)
Aug  6 16:33:43.747: INFO: (13) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.222809ms)
Aug  6 16:33:43.770: INFO: (14) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 23.38802ms)
Aug  6 16:33:43.793: INFO: (15) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.795693ms)
Aug  6 16:33:43.816: INFO: (16) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.707774ms)
Aug  6 16:33:43.838: INFO: (17) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.73355ms)
Aug  6 16:33:43.861: INFO: (18) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.42358ms)
Aug  6 16:33:43.884: INFO: (19) /api/v1/nodes/ip-10-0-128-44.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.898387ms)
[AfterEach] version v1
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:33:43.884: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-4s4f2" for this suite.
Aug  6 16:33:49.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:33:50.571: INFO: namespace: e2e-tests-proxy-4s4f2, resource: bindings, ignored listing per whitelist
Aug  6 16:33:51.080: INFO: namespace: e2e-tests-proxy-4s4f2, resource: packagemanifests, items remaining: 3
Aug  6 16:33:51.919: INFO: namespace: e2e-tests-proxy-4s4f2 no longer exists
Aug  6 16:33:51.941: INFO: namespace: e2e-tests-proxy-4s4f2, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:33:51.963: INFO: namespace e2e-tests-proxy-4s4f2 deletion completed in 8.036757404s

• [SLOW TEST:9.936 seconds]
[sig-network] Proxy
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:33:51.964: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-fa62b96f-b867-11e9-8d18-525400524259
Aug  6 16:33:53.397: INFO: Pod name my-hostname-basic-fa62b96f-b867-11e9-8d18-525400524259: Found 0 pods out of 1
Aug  6 16:33:58.420: INFO: Pod name my-hostname-basic-fa62b96f-b867-11e9-8d18-525400524259: Found 1 pods out of 1
Aug  6 16:33:58.420: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-fa62b96f-b867-11e9-8d18-525400524259" are running
Aug  6 16:34:02.465: INFO: Pod "my-hostname-basic-fa62b96f-b867-11e9-8d18-525400524259-5zkch" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-06 16:33:30 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-06 16:33:30 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-fa62b96f-b867-11e9-8d18-525400524259]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-06 16:33:30 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-fa62b96f-b867-11e9-8d18-525400524259]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-06 16:33:30 +0000 UTC Reason: Message:}])
Aug  6 16:34:02.465: INFO: Trying to dial the pod
Aug  6 16:34:07.536: INFO: Controller my-hostname-basic-fa62b96f-b867-11e9-8d18-525400524259: Got expected result from replica 1 [my-hostname-basic-fa62b96f-b867-11e9-8d18-525400524259-5zkch]: "my-hostname-basic-fa62b96f-b867-11e9-8d18-525400524259-5zkch", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:34:07.536: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-pgs9d" for this suite.
Aug  6 16:34:13.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:34:15.645: INFO: namespace: e2e-tests-replication-controller-pgs9d, resource: packagemanifests, items remaining: 3
Aug  6 16:34:15.778: INFO: namespace: e2e-tests-replication-controller-pgs9d, resource: bindings, ignored listing per whitelist
Aug  6 16:34:16.094: INFO: namespace: e2e-tests-replication-controller-pgs9d no longer exists
Aug  6 16:34:16.117: INFO: namespace: e2e-tests-replication-controller-pgs9d, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:34:16.138: INFO: namespace e2e-tests-replication-controller-pgs9d deletion completed in 8.539679593s

• [SLOW TEST:24.175 seconds]
[sig-apps] ReplicationController
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:34:16.139: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:34:17.494: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:34:27.694: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-j5kjp" for this suite.
Aug  6 16:35:09.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:35:10.761: INFO: namespace: e2e-tests-pods-j5kjp, resource: bindings, ignored listing per whitelist
Aug  6 16:35:10.813: INFO: namespace: e2e-tests-pods-j5kjp, resource: packagemanifests, items remaining: 3
Aug  6 16:35:12.256: INFO: namespace: e2e-tests-pods-j5kjp no longer exists
Aug  6 16:35:12.281: INFO: namespace: e2e-tests-pods-j5kjp, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:35:12.305: INFO: namespace e2e-tests-pods-j5kjp deletion completed in 44.547833806s

• [SLOW TEST:56.166 seconds]
[k8s.io] Pods
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:35:12.306: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 16:35:13.720: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a46bd1a-b868-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-cd2zg" to be "success or failure"
Aug  6 16:35:13.742: INFO: Pod "downwardapi-volume-2a46bd1a-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.452996ms
Aug  6 16:35:15.767: INFO: Pod "downwardapi-volume-2a46bd1a-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047540747s
Aug  6 16:35:17.791: INFO: Pod "downwardapi-volume-2a46bd1a-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071561617s
Aug  6 16:35:19.816: INFO: Pod "downwardapi-volume-2a46bd1a-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096753828s
Aug  6 16:35:21.843: INFO: Pod "downwardapi-volume-2a46bd1a-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.123431381s
Aug  6 16:35:23.866: INFO: Pod "downwardapi-volume-2a46bd1a-b868-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.146598617s
STEP: Saw pod success
Aug  6 16:35:23.866: INFO: Pod "downwardapi-volume-2a46bd1a-b868-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:35:23.890: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-2a46bd1a-b868-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 16:35:23.946: INFO: Waiting for pod downwardapi-volume-2a46bd1a-b868-11e9-8d18-525400524259 to disappear
Aug  6 16:35:23.968: INFO: Pod downwardapi-volume-2a46bd1a-b868-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:35:23.968: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cd2zg" for this suite.
Aug  6 16:35:30.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:35:31.153: INFO: namespace: e2e-tests-downward-api-cd2zg, resource: packagemanifests, items remaining: 3
Aug  6 16:35:32.289: INFO: namespace: e2e-tests-downward-api-cd2zg, resource: bindings, ignored listing per whitelist
Aug  6 16:35:32.531: INFO: namespace: e2e-tests-downward-api-cd2zg no longer exists
Aug  6 16:35:32.555: INFO: namespace: e2e-tests-downward-api-cd2zg, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:35:32.577: INFO: namespace e2e-tests-downward-api-cd2zg deletion completed in 8.545039399s

• [SLOW TEST:20.271 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:35:32.578: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-365b8764-b868-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 16:35:34.055: INFO: Waiting up to 5m0s for pod "pod-configmaps-366509d2-b868-11e9-8d18-525400524259" in namespace "e2e-tests-configmap-qppwl" to be "success or failure"
Aug  6 16:35:34.077: INFO: Pod "pod-configmaps-366509d2-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.678832ms
Aug  6 16:35:36.100: INFO: Pod "pod-configmaps-366509d2-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045743772s
Aug  6 16:35:38.124: INFO: Pod "pod-configmaps-366509d2-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069178497s
Aug  6 16:35:40.151: INFO: Pod "pod-configmaps-366509d2-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096083552s
Aug  6 16:35:42.176: INFO: Pod "pod-configmaps-366509d2-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.121482155s
Aug  6 16:35:44.199: INFO: Pod "pod-configmaps-366509d2-b868-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.144744045s
STEP: Saw pod success
Aug  6 16:35:44.200: INFO: Pod "pod-configmaps-366509d2-b868-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:35:44.222: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-configmaps-366509d2-b868-11e9-8d18-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 16:35:44.290: INFO: Waiting for pod pod-configmaps-366509d2-b868-11e9-8d18-525400524259 to disappear
Aug  6 16:35:44.312: INFO: Pod pod-configmaps-366509d2-b868-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:35:44.312: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-qppwl" for this suite.
Aug  6 16:35:50.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:35:51.334: INFO: namespace: e2e-tests-configmap-qppwl, resource: bindings, ignored listing per whitelist
Aug  6 16:35:52.005: INFO: namespace: e2e-tests-configmap-qppwl, resource: packagemanifests, items remaining: 3
Aug  6 16:35:52.877: INFO: namespace: e2e-tests-configmap-qppwl no longer exists
Aug  6 16:35:52.901: INFO: namespace: e2e-tests-configmap-qppwl, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:35:52.923: INFO: namespace e2e-tests-configmap-qppwl deletion completed in 8.547785766s

• [SLOW TEST:20.345 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:35:52.924: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-zdt9z
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-zdt9z to expose endpoints map[]
Aug  6 16:35:54.346: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-zdt9z exposes endpoints map[] (22.793685ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-zdt9z
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-zdt9z to expose endpoints map[pod1:[80]]
Aug  6 16:35:58.610: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.228719873s elapsed, will retry)
Aug  6 16:36:03.838: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-zdt9z exposes endpoints map[pod1:[80]] (9.457184195s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-zdt9z
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-zdt9z to expose endpoints map[pod1:[80] pod2:[80]]
Aug  6 16:36:08.206: INFO: Unexpected endpoints: found map[351003bc-b868-11e9-9dff-0e6de702691c:[80]], expected map[pod1:[80] pod2:[80]] (4.338344202s elapsed, will retry)
Aug  6 16:36:13.550: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-zdt9z exposes endpoints map[pod1:[80] pod2:[80]] (9.682327432s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-zdt9z
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-zdt9z to expose endpoints map[pod2:[80]]
Aug  6 16:36:13.628: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-zdt9z exposes endpoints map[pod2:[80]] (44.601728ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-zdt9z
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-zdt9z to expose endpoints map[]
Aug  6 16:36:13.676: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-zdt9z exposes endpoints map[] (21.956401ms elapsed)
[AfterEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:36:13.713: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-zdt9z" for this suite.
Aug  6 16:36:35.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:36:37.928: INFO: namespace: e2e-tests-services-zdt9z, resource: packagemanifests, items remaining: 3
Aug  6 16:36:38.128: INFO: namespace: e2e-tests-services-zdt9z, resource: bindings, ignored listing per whitelist
Aug  6 16:36:38.285: INFO: namespace: e2e-tests-services-zdt9z no longer exists
Aug  6 16:36:38.309: INFO: namespace: e2e-tests-services-zdt9z, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:36:38.332: INFO: namespace e2e-tests-services-zdt9z deletion completed in 24.553486044s
[AfterEach] [sig-network] Services
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:45.408 seconds]
[sig-network] Services
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:36:38.333: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Aug  6 16:36:39.705: INFO: namespace e2e-tests-kubectl-29fw2
Aug  6 16:36:39.705: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance create -f - --namespace=e2e-tests-kubectl-29fw2'
Aug  6 16:36:41.933: INFO: stderr: ""
Aug  6 16:36:41.933: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug  6 16:36:42.959: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 16:36:42.959: INFO: Found 0 / 1
Aug  6 16:36:43.956: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 16:36:43.957: INFO: Found 0 / 1
Aug  6 16:36:44.958: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 16:36:44.958: INFO: Found 0 / 1
Aug  6 16:36:45.957: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 16:36:45.957: INFO: Found 0 / 1
Aug  6 16:36:46.956: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 16:36:46.957: INFO: Found 0 / 1
Aug  6 16:36:47.957: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 16:36:47.957: INFO: Found 0 / 1
Aug  6 16:36:48.956: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 16:36:48.956: INFO: Found 0 / 1
Aug  6 16:36:49.958: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 16:36:49.958: INFO: Found 0 / 1
Aug  6 16:36:50.957: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 16:36:50.957: INFO: Found 1 / 1
Aug  6 16:36:50.957: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug  6 16:36:50.980: INFO: Selector matched 1 pods for map[app:redis]
Aug  6 16:36:50.980: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug  6 16:36:50.980: INFO: wait on redis-master startup in e2e-tests-kubectl-29fw2 
Aug  6 16:36:50.980: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance logs redis-master-2hnxm redis-master --namespace=e2e-tests-kubectl-29fw2'
Aug  6 16:36:51.172: INFO: stderr: ""
Aug  6 16:36:51.172: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Aug 16:36:27.633 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Aug 16:36:27.633 # Server started, Redis version 3.2.12\n1:M 06 Aug 16:36:27.633 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Aug 16:36:27.633 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Aug  6 16:36:51.172: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-29fw2'
Aug  6 16:36:51.378: INFO: stderr: ""
Aug  6 16:36:51.378: INFO: stdout: "service/rm2 exposed\n"
Aug  6 16:36:51.401: INFO: Service rm2 in namespace e2e-tests-kubectl-29fw2 found.
STEP: exposing service
Aug  6 16:36:53.446: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-29fw2'
Aug  6 16:36:53.660: INFO: stderr: ""
Aug  6 16:36:53.660: INFO: stdout: "service/rm3 exposed\n"
Aug  6 16:36:53.682: INFO: Service rm3 in namespace e2e-tests-kubectl-29fw2 found.
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:36:55.728: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-29fw2" for this suite.
Aug  6 16:37:17.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:37:18.905: INFO: namespace: e2e-tests-kubectl-29fw2, resource: bindings, ignored listing per whitelist
Aug  6 16:37:20.245: INFO: namespace: e2e-tests-kubectl-29fw2, resource: packagemanifests, items remaining: 3
Aug  6 16:37:20.291: INFO: namespace: e2e-tests-kubectl-29fw2 no longer exists
Aug  6 16:37:20.314: INFO: namespace: e2e-tests-kubectl-29fw2, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:37:20.337: INFO: namespace e2e-tests-kubectl-29fw2 deletion completed in 24.545447463s

• [SLOW TEST:42.004 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:37:20.337: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-p2ftj A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-p2ftj;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-p2ftj A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-p2ftj;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-p2ftj.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-p2ftj.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-p2ftj.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-p2ftj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-p2ftj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-p2ftj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-p2ftj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-p2ftj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-p2ftj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-p2ftj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-p2ftj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-p2ftj.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-p2ftj.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 243.79.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.79.243_udp@PTR;check="$$(dig +tcp +noall +answer +search 243.79.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.79.243_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-p2ftj A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-p2ftj;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-p2ftj A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-p2ftj;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-p2ftj.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-p2ftj.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-p2ftj.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-p2ftj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-p2ftj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-p2ftj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-p2ftj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-p2ftj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-p2ftj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-p2ftj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-p2ftj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-p2ftj.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-p2ftj.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 243.79.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.79.243_udp@PTR;check="$$(dig +tcp +noall +answer +search 243.79.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.79.243_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug  6 16:37:31.883: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-p2ftj/dns-test-769c1722-b868-11e9-8d18-525400524259: the server could not find the requested resource (get pods dns-test-769c1722-b868-11e9-8d18-525400524259)
Aug  6 16:37:31.906: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-p2ftj/dns-test-769c1722-b868-11e9-8d18-525400524259: the server could not find the requested resource (get pods dns-test-769c1722-b868-11e9-8d18-525400524259)
Aug  6 16:37:31.929: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-p2ftj from pod e2e-tests-dns-p2ftj/dns-test-769c1722-b868-11e9-8d18-525400524259: the server could not find the requested resource (get pods dns-test-769c1722-b868-11e9-8d18-525400524259)
Aug  6 16:37:31.975: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-p2ftj.svc from pod e2e-tests-dns-p2ftj/dns-test-769c1722-b868-11e9-8d18-525400524259: the server could not find the requested resource (get pods dns-test-769c1722-b868-11e9-8d18-525400524259)
Aug  6 16:37:32.202: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-p2ftj/dns-test-769c1722-b868-11e9-8d18-525400524259: the server could not find the requested resource (get pods dns-test-769c1722-b868-11e9-8d18-525400524259)
Aug  6 16:37:32.248: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-p2ftj from pod e2e-tests-dns-p2ftj/dns-test-769c1722-b868-11e9-8d18-525400524259: the server could not find the requested resource (get pods dns-test-769c1722-b868-11e9-8d18-525400524259)
Aug  6 16:37:32.498: INFO: Lookups using e2e-tests-dns-p2ftj/dns-test-769c1722-b868-11e9-8d18-525400524259 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-p2ftj wheezy_udp@dns-test-service.e2e-tests-dns-p2ftj.svc jessie_udp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-p2ftj]

Aug  6 16:37:38.136: INFO: DNS probes using e2e-tests-dns-p2ftj/dns-test-769c1722-b868-11e9-8d18-525400524259 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:37:38.239: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-p2ftj" for this suite.
Aug  6 16:37:44.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:37:45.653: INFO: namespace: e2e-tests-dns-p2ftj, resource: packagemanifests, items remaining: 3
Aug  6 16:37:46.276: INFO: namespace: e2e-tests-dns-p2ftj, resource: bindings, ignored listing per whitelist
Aug  6 16:37:46.786: INFO: namespace: e2e-tests-dns-p2ftj no longer exists
Aug  6 16:37:46.809: INFO: namespace: e2e-tests-dns-p2ftj, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:37:46.831: INFO: namespace e2e-tests-dns-p2ftj deletion completed in 8.549428472s

• [SLOW TEST:26.494 seconds]
[sig-network] DNS
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:37:46.832: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug  6 16:38:08.465: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  6 16:38:08.487: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  6 16:38:10.487: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  6 16:38:10.510: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  6 16:38:12.487: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  6 16:38:12.511: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  6 16:38:14.487: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  6 16:38:14.510: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  6 16:38:16.487: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  6 16:38:16.510: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  6 16:38:18.487: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  6 16:38:18.511: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  6 16:38:20.487: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  6 16:38:20.511: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  6 16:38:22.487: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  6 16:38:22.510: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  6 16:38:24.487: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  6 16:38:24.511: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  6 16:38:26.487: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  6 16:38:26.510: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:38:26.510: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-t2n8k" for this suite.
Aug  6 16:38:48.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:38:49.991: INFO: namespace: e2e-tests-container-lifecycle-hook-t2n8k, resource: bindings, ignored listing per whitelist
Aug  6 16:38:51.018: INFO: namespace: e2e-tests-container-lifecycle-hook-t2n8k, resource: packagemanifests, items remaining: 3
Aug  6 16:38:51.193: INFO: namespace: e2e-tests-container-lifecycle-hook-t2n8k no longer exists
Aug  6 16:38:51.216: INFO: namespace: e2e-tests-container-lifecycle-hook-t2n8k, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:38:51.238: INFO: namespace e2e-tests-container-lifecycle-hook-t2n8k deletion completed in 24.664502833s

• [SLOW TEST:64.407 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:38:51.240: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 16:38:52.626: INFO: Waiting up to 5m0s for pod "downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-n9k7x" to be "success or failure"
Aug  6 16:38:52.652: INFO: Pod "downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 25.963314ms
Aug  6 16:38:54.675: INFO: Pod "downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049039582s
Aug  6 16:38:56.699: INFO: Pod "downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07324691s
Aug  6 16:38:58.722: INFO: Pod "downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096439434s
Aug  6 16:39:00.745: INFO: Pod "downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.119267534s
Aug  6 16:39:02.768: INFO: Pod "downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 10.142244456s
Aug  6 16:39:04.793: INFO: Pod "downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.167220566s
STEP: Saw pod success
Aug  6 16:39:04.793: INFO: Pod "downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:39:04.816: INFO: Trying to get logs from node ip-10-0-135-96.ec2.internal pod downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 16:39:04.876: INFO: Waiting for pod downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259 to disappear
Aug  6 16:39:04.898: INFO: Pod downwardapi-volume-acc01af0-b868-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:39:04.898: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-n9k7x" for this suite.
Aug  6 16:39:11.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:39:12.317: INFO: namespace: e2e-tests-downward-api-n9k7x, resource: bindings, ignored listing per whitelist
Aug  6 16:39:12.568: INFO: namespace: e2e-tests-downward-api-n9k7x, resource: packagemanifests, items remaining: 3
Aug  6 16:39:13.454: INFO: namespace: e2e-tests-downward-api-n9k7x no longer exists
Aug  6 16:39:13.477: INFO: namespace: e2e-tests-downward-api-n9k7x, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:39:13.499: INFO: namespace e2e-tests-downward-api-n9k7x deletion completed in 8.537773802s

• [SLOW TEST:22.259 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:39:13.499: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:39:14.977: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ac9c4ad3-b868-11e9-9dff-0e6de702691c", Controller:(*bool)(0xc00222cf46), BlockOwnerDeletion:(*bool)(0xc00222cf47)}}
Aug  6 16:39:15.001: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ac92f040-b868-11e9-9dff-0e6de702691c", Controller:(*bool)(0xc0034b23b2), BlockOwnerDeletion:(*bool)(0xc0034b23b3)}}
Aug  6 16:39:15.029: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ac97d74d-b868-11e9-9dff-0e6de702691c", Controller:(*bool)(0xc003513a42), BlockOwnerDeletion:(*bool)(0xc003513a43)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:39:20.079: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-zz6kb" for this suite.
Aug  6 16:39:26.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:39:26.988: INFO: namespace: e2e-tests-gc-zz6kb, resource: bindings, ignored listing per whitelist
Aug  6 16:39:27.327: INFO: namespace: e2e-tests-gc-zz6kb, resource: packagemanifests, items remaining: 3
Aug  6 16:39:28.635: INFO: namespace: e2e-tests-gc-zz6kb no longer exists
Aug  6 16:39:28.659: INFO: namespace: e2e-tests-gc-zz6kb, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:39:28.681: INFO: namespace e2e-tests-gc-zz6kb deletion completed in 8.539381925s

• [SLOW TEST:15.182 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:39:28.681: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-z5tx
STEP: Creating a pod to test atomic-volume-subpath
Aug  6 16:39:30.179: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-z5tx" in namespace "e2e-tests-subpath-qq4wn" to be "success or failure"
Aug  6 16:39:30.201: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Pending", Reason="", readiness=false. Elapsed: 22.087832ms
Aug  6 16:39:32.224: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045054509s
Aug  6 16:39:34.247: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068278015s
Aug  6 16:39:36.270: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091232919s
Aug  6 16:39:38.293: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114823793s
Aug  6 16:39:40.317: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Running", Reason="", readiness=false. Elapsed: 10.138080928s
Aug  6 16:39:42.340: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Running", Reason="", readiness=false. Elapsed: 12.161314928s
Aug  6 16:39:44.363: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Running", Reason="", readiness=false. Elapsed: 14.184627942s
Aug  6 16:39:46.386: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Running", Reason="", readiness=false. Elapsed: 16.207635251s
Aug  6 16:39:48.409: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Running", Reason="", readiness=false. Elapsed: 18.230750154s
Aug  6 16:39:50.433: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Running", Reason="", readiness=false. Elapsed: 20.254060707s
Aug  6 16:39:52.456: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Running", Reason="", readiness=false. Elapsed: 22.27714804s
Aug  6 16:39:54.479: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Running", Reason="", readiness=false. Elapsed: 24.30062293s
Aug  6 16:39:56.503: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Running", Reason="", readiness=false. Elapsed: 26.324230762s
Aug  6 16:39:58.526: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Running", Reason="", readiness=false. Elapsed: 28.347249542s
Aug  6 16:40:00.549: INFO: Pod "pod-subpath-test-configmap-z5tx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.370206275s
STEP: Saw pod success
Aug  6 16:40:00.549: INFO: Pod "pod-subpath-test-configmap-z5tx" satisfied condition "success or failure"
Aug  6 16:40:00.571: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-subpath-test-configmap-z5tx container test-container-subpath-configmap-z5tx: <nil>
STEP: delete the pod
Aug  6 16:40:00.626: INFO: Waiting for pod pod-subpath-test-configmap-z5tx to disappear
Aug  6 16:40:00.650: INFO: Pod pod-subpath-test-configmap-z5tx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-z5tx
Aug  6 16:40:00.650: INFO: Deleting pod "pod-subpath-test-configmap-z5tx" in namespace "e2e-tests-subpath-qq4wn"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:40:00.674: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-qq4wn" for this suite.
Aug  6 16:40:06.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:40:08.076: INFO: namespace: e2e-tests-subpath-qq4wn, resource: packagemanifests, items remaining: 3
Aug  6 16:40:08.918: INFO: namespace: e2e-tests-subpath-qq4wn, resource: bindings, ignored listing per whitelist
Aug  6 16:40:09.230: INFO: namespace: e2e-tests-subpath-qq4wn no longer exists
Aug  6 16:40:09.254: INFO: namespace: e2e-tests-subpath-qq4wn, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:40:09.276: INFO: namespace e2e-tests-subpath-qq4wn deletion completed in 8.53549927s

• [SLOW TEST:40.595 seconds]
[sig-storage] Subpath
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:40:09.276: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-db45b62d-b868-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 16:40:10.694: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-db494cc0-b868-11e9-8d18-525400524259" in namespace "e2e-tests-projected-k8xwq" to be "success or failure"
Aug  6 16:40:10.717: INFO: Pod "pod-projected-secrets-db494cc0-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.091115ms
Aug  6 16:40:12.740: INFO: Pod "pod-projected-secrets-db494cc0-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046046799s
Aug  6 16:40:14.763: INFO: Pod "pod-projected-secrets-db494cc0-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069031983s
Aug  6 16:40:16.786: INFO: Pod "pod-projected-secrets-db494cc0-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091745605s
Aug  6 16:40:18.809: INFO: Pod "pod-projected-secrets-db494cc0-b868-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114779508s
Aug  6 16:40:20.832: INFO: Pod "pod-projected-secrets-db494cc0-b868-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137745253s
STEP: Saw pod success
Aug  6 16:40:20.832: INFO: Pod "pod-projected-secrets-db494cc0-b868-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:40:20.854: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-projected-secrets-db494cc0-b868-11e9-8d18-525400524259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug  6 16:40:20.909: INFO: Waiting for pod pod-projected-secrets-db494cc0-b868-11e9-8d18-525400524259 to disappear
Aug  6 16:40:20.932: INFO: Pod pod-projected-secrets-db494cc0-b868-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:40:20.932: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k8xwq" for this suite.
Aug  6 16:40:27.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:40:27.662: INFO: namespace: e2e-tests-projected-k8xwq, resource: bindings, ignored listing per whitelist
Aug  6 16:40:28.596: INFO: namespace: e2e-tests-projected-k8xwq, resource: packagemanifests, items remaining: 3
Aug  6 16:40:29.481: INFO: namespace: e2e-tests-projected-k8xwq no longer exists
Aug  6 16:40:29.504: INFO: namespace: e2e-tests-projected-k8xwq, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:40:29.526: INFO: namespace e2e-tests-projected-k8xwq deletion completed in 8.532203285s

• [SLOW TEST:20.250 seconds]
[sig-storage] Projected secret
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:40:29.527: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:40:30.947: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-e7607105-b868-11e9-8d18-525400524259
STEP: Creating configMap with name cm-test-opt-upd-e7607140-b868-11e9-8d18-525400524259
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e7607105-b868-11e9-8d18-525400524259
STEP: Updating configmap cm-test-opt-upd-e7607140-b868-11e9-8d18-525400524259
STEP: Creating configMap with name cm-test-opt-create-e7607152-b868-11e9-8d18-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:41:50.757: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hnh5d" for this suite.
Aug  6 16:42:12.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:42:13.765: INFO: namespace: e2e-tests-projected-hnh5d, resource: bindings, ignored listing per whitelist
Aug  6 16:42:15.124: INFO: namespace: e2e-tests-projected-hnh5d, resource: packagemanifests, items remaining: 3
Aug  6 16:42:15.299: INFO: namespace: e2e-tests-projected-hnh5d no longer exists
Aug  6 16:42:15.322: INFO: namespace: e2e-tests-projected-hnh5d, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:42:15.344: INFO: namespace e2e-tests-projected-hnh5d deletion completed in 24.543701976s

• [SLOW TEST:105.818 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:42:15.344: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-hrwhc
Aug  6 16:42:26.780: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-hrwhc
STEP: checking the pod's current state and verifying that restartCount is present
Aug  6 16:42:26.803: INFO: Initial restart count of pod liveness-http is 0
Aug  6 16:42:47.059: INFO: Restart count of pod e2e-tests-container-probe-hrwhc/liveness-http is now 1 (20.255870596s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:42:47.089: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-hrwhc" for this suite.
Aug  6 16:42:53.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:42:55.225: INFO: namespace: e2e-tests-container-probe-hrwhc, resource: bindings, ignored listing per whitelist
Aug  6 16:42:55.492: INFO: namespace: e2e-tests-container-probe-hrwhc, resource: packagemanifests, items remaining: 3
Aug  6 16:42:55.647: INFO: namespace: e2e-tests-container-probe-hrwhc no longer exists
Aug  6 16:42:55.673: INFO: namespace: e2e-tests-container-probe-hrwhc, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:42:55.695: INFO: namespace e2e-tests-container-probe-hrwhc deletion completed in 8.543928342s

• [SLOW TEST:40.351 seconds]
[k8s.io] Probing container
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:42:55.696: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-3e76dbd6-b869-11e9-8d18-525400524259
STEP: Creating a pod to test consume configMaps
Aug  6 16:42:57.111: INFO: Waiting up to 5m0s for pod "pod-configmaps-3e7a6892-b869-11e9-8d18-525400524259" in namespace "e2e-tests-configmap-6775t" to be "success or failure"
Aug  6 16:42:57.133: INFO: Pod "pod-configmaps-3e7a6892-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.446941ms
Aug  6 16:42:59.156: INFO: Pod "pod-configmaps-3e7a6892-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044882107s
Aug  6 16:43:01.179: INFO: Pod "pod-configmaps-3e7a6892-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06772294s
Aug  6 16:43:03.202: INFO: Pod "pod-configmaps-3e7a6892-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090971325s
Aug  6 16:43:05.226: INFO: Pod "pod-configmaps-3e7a6892-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114809474s
Aug  6 16:43:07.249: INFO: Pod "pod-configmaps-3e7a6892-b869-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137810041s
STEP: Saw pod success
Aug  6 16:43:07.249: INFO: Pod "pod-configmaps-3e7a6892-b869-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:43:07.271: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-configmaps-3e7a6892-b869-11e9-8d18-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  6 16:43:07.326: INFO: Waiting for pod pod-configmaps-3e7a6892-b869-11e9-8d18-525400524259 to disappear
Aug  6 16:43:07.348: INFO: Pod pod-configmaps-3e7a6892-b869-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:43:07.348: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-6775t" for this suite.
Aug  6 16:43:13.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:43:14.307: INFO: namespace: e2e-tests-configmap-6775t, resource: packagemanifests, items remaining: 3
Aug  6 16:43:14.462: INFO: namespace: e2e-tests-configmap-6775t, resource: bindings, ignored listing per whitelist
Aug  6 16:43:15.921: INFO: namespace: e2e-tests-configmap-6775t no longer exists
Aug  6 16:43:15.944: INFO: namespace: e2e-tests-configmap-6775t, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:43:15.966: INFO: namespace e2e-tests-configmap-6775t deletion completed in 8.554573191s

• [SLOW TEST:20.270 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:43:15.967: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug  6 16:43:17.359: INFO: Waiting up to 5m0s for pod "pod-4a8c5eb3-b869-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-n6km9" to be "success or failure"
Aug  6 16:43:17.382: INFO: Pod "pod-4a8c5eb3-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.584787ms
Aug  6 16:43:19.405: INFO: Pod "pod-4a8c5eb3-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045799906s
Aug  6 16:43:21.429: INFO: Pod "pod-4a8c5eb3-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069023479s
Aug  6 16:43:23.452: INFO: Pod "pod-4a8c5eb3-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092481351s
Aug  6 16:43:25.475: INFO: Pod "pod-4a8c5eb3-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115693259s
Aug  6 16:43:27.498: INFO: Pod "pod-4a8c5eb3-b869-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.138350806s
STEP: Saw pod success
Aug  6 16:43:27.498: INFO: Pod "pod-4a8c5eb3-b869-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:43:27.520: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-4a8c5eb3-b869-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 16:43:27.577: INFO: Waiting for pod pod-4a8c5eb3-b869-11e9-8d18-525400524259 to disappear
Aug  6 16:43:27.600: INFO: Pod pod-4a8c5eb3-b869-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:43:27.600: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-n6km9" for this suite.
Aug  6 16:43:33.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:43:35.409: INFO: namespace: e2e-tests-emptydir-n6km9, resource: packagemanifests, items remaining: 3
Aug  6 16:43:35.431: INFO: namespace: e2e-tests-emptydir-n6km9, resource: bindings, ignored listing per whitelist
Aug  6 16:43:36.216: INFO: namespace: e2e-tests-emptydir-n6km9 no longer exists
Aug  6 16:43:36.241: INFO: namespace: e2e-tests-emptydir-n6km9, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:43:36.264: INFO: namespace e2e-tests-emptydir-n6km9 deletion completed in 8.601712488s

• [SLOW TEST:20.298 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:43:36.265: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-56ac5c9d-b869-11e9-8d18-525400524259
STEP: Creating secret with name secret-projected-all-test-volume-56ac5c8d-b869-11e9-8d18-525400524259
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug  6 16:43:37.751: INFO: Waiting up to 5m0s for pod "projected-volume-56ac5c58-b869-11e9-8d18-525400524259" in namespace "e2e-tests-projected-dzkqw" to be "success or failure"
Aug  6 16:43:37.777: INFO: Pod "projected-volume-56ac5c58-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 25.310777ms
Aug  6 16:43:39.800: INFO: Pod "projected-volume-56ac5c58-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048549822s
Aug  6 16:43:41.823: INFO: Pod "projected-volume-56ac5c58-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071602574s
Aug  6 16:43:43.846: INFO: Pod "projected-volume-56ac5c58-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094553744s
Aug  6 16:43:45.870: INFO: Pod "projected-volume-56ac5c58-b869-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.11914571s
Aug  6 16:43:47.893: INFO: Pod "projected-volume-56ac5c58-b869-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.142124848s
STEP: Saw pod success
Aug  6 16:43:47.893: INFO: Pod "projected-volume-56ac5c58-b869-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:43:47.916: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod projected-volume-56ac5c58-b869-11e9-8d18-525400524259 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug  6 16:43:47.970: INFO: Waiting for pod projected-volume-56ac5c58-b869-11e9-8d18-525400524259 to disappear
Aug  6 16:43:47.992: INFO: Pod projected-volume-56ac5c58-b869-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected combined
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:43:47.992: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dzkqw" for this suite.
Aug  6 16:43:54.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:43:55.907: INFO: namespace: e2e-tests-projected-dzkqw, resource: packagemanifests, items remaining: 3
Aug  6 16:43:56.377: INFO: namespace: e2e-tests-projected-dzkqw, resource: bindings, ignored listing per whitelist
Aug  6 16:43:56.555: INFO: namespace: e2e-tests-projected-dzkqw no longer exists
Aug  6 16:43:56.578: INFO: namespace: e2e-tests-projected-dzkqw, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:43:56.600: INFO: namespace e2e-tests-projected-dzkqw deletion completed in 8.544326699s

• [SLOW TEST:20.335 seconds]
[sig-storage] Projected combined
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:43:56.601: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-nrbfw
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Aug  6 16:43:58.027: INFO: Found 1 stateful pods, waiting for 3
Aug  6 16:44:08.052: INFO: Found 2 stateful pods, waiting for 3
Aug  6 16:44:18.051: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:44:18.051: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:44:18.051: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug  6 16:44:28.051: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:44:28.051: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:44:28.051: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug  6 16:44:28.176: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug  6 16:44:28.278: INFO: Updating stateful set ss2
Aug  6 16:44:28.322: INFO: Waiting for Pod e2e-tests-statefulset-nrbfw/ss2-2 to have revision ss2-76875fdd44 update revision ss2-55568f78cf
STEP: Restoring Pods to the correct revision when they are deleted
Aug  6 16:44:38.461: INFO: Found 2 stateful pods, waiting for 3
Aug  6 16:44:48.484: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:44:48.484: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:44:48.484: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug  6 16:44:58.484: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:44:58.484: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:44:58.484: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug  6 16:44:58.585: INFO: Updating stateful set ss2
Aug  6 16:44:58.629: INFO: Waiting for Pod e2e-tests-statefulset-nrbfw/ss2-1 to have revision ss2-76875fdd44 update revision ss2-55568f78cf
Aug  6 16:45:08.731: INFO: Updating stateful set ss2
Aug  6 16:45:08.779: INFO: Waiting for StatefulSet e2e-tests-statefulset-nrbfw/ss2 to complete update
Aug  6 16:45:08.779: INFO: Waiting for Pod e2e-tests-statefulset-nrbfw/ss2-0 to have revision ss2-76875fdd44 update revision ss2-55568f78cf
Aug  6 16:45:18.825: INFO: Waiting for StatefulSet e2e-tests-statefulset-nrbfw/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug  6 16:45:28.827: INFO: Deleting all statefulset in ns e2e-tests-statefulset-nrbfw
Aug  6 16:45:28.850: INFO: Scaling statefulset ss2 to 0
Aug  6 16:45:58.941: INFO: Waiting for statefulset status.replicas updated to 0
Aug  6 16:45:58.964: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:45:59.033: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-nrbfw" for this suite.
Aug  6 16:46:05.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:46:05.928: INFO: namespace: e2e-tests-statefulset-nrbfw, resource: packagemanifests, items remaining: 3
Aug  6 16:46:06.973: INFO: namespace: e2e-tests-statefulset-nrbfw, resource: bindings, ignored listing per whitelist
Aug  6 16:46:07.597: INFO: namespace: e2e-tests-statefulset-nrbfw no longer exists
Aug  6 16:46:07.620: INFO: namespace: e2e-tests-statefulset-nrbfw, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:46:07.642: INFO: namespace e2e-tests-statefulset-nrbfw deletion completed in 8.546457381s

• [SLOW TEST:131.042 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:46:07.643: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:46:17.225: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-5sc9f" for this suite.
Aug  6 16:46:23.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:46:24.543: INFO: namespace: e2e-tests-emptydir-wrapper-5sc9f, resource: packagemanifests, items remaining: 3
Aug  6 16:46:25.325: INFO: namespace: e2e-tests-emptydir-wrapper-5sc9f, resource: bindings, ignored listing per whitelist
Aug  6 16:46:25.789: INFO: namespace: e2e-tests-emptydir-wrapper-5sc9f no longer exists
Aug  6 16:46:25.813: INFO: namespace: e2e-tests-emptydir-wrapper-5sc9f, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:46:25.835: INFO: namespace e2e-tests-emptydir-wrapper-5sc9f deletion completed in 8.545521852s

• [SLOW TEST:18.192 seconds]
[sig-storage] EmptyDir wrapper volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:46:25.836: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-kqsgr in namespace e2e-tests-proxy-zsg5d
I0806 16:46:27.256684    4087 runners.go:184] Created replication controller with name: proxy-service-kqsgr, namespace: e2e-tests-proxy-zsg5d, replica count: 1
I0806 16:46:28.307945    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 16:46:29.308403    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 16:46:30.308629    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 16:46:31.308901    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 16:46:32.309244    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 16:46:33.309602    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 16:46:34.310056    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 16:46:35.310336    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 16:46:36.310589    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 16:46:37.311552    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0806 16:46:38.311897    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0806 16:46:39.312212    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0806 16:46:40.312570    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0806 16:46:41.312775    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0806 16:46:42.313123    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0806 16:46:43.313426    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0806 16:46:44.315155    4087 runners.go:184] proxy-service-kqsgr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug  6 16:46:44.339: INFO: setup took 17.135454274s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug  6 16:46:44.365: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 25.169ms)
Aug  6 16:46:44.365: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 25.064246ms)
Aug  6 16:46:44.365: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 25.190139ms)
Aug  6 16:46:44.365: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 25.343018ms)
Aug  6 16:46:44.365: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 25.314955ms)
Aug  6 16:46:44.369: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 29.379624ms)
Aug  6 16:46:44.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 44.801289ms)
Aug  6 16:46:44.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 44.663939ms)
Aug  6 16:46:44.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 44.808341ms)
Aug  6 16:46:44.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 44.708915ms)
Aug  6 16:46:44.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 44.694455ms)
Aug  6 16:46:44.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 44.766424ms)
Aug  6 16:46:44.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 44.779916ms)
Aug  6 16:46:44.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 44.772898ms)
Aug  6 16:46:44.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 44.762314ms)
Aug  6 16:46:44.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 44.864358ms)
Aug  6 16:46:44.408: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 23.35629ms)
Aug  6 16:46:44.409: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 24.5547ms)
Aug  6 16:46:44.409: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 24.559954ms)
Aug  6 16:46:44.409: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.546408ms)
Aug  6 16:46:44.409: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 24.645246ms)
Aug  6 16:46:44.409: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.608873ms)
Aug  6 16:46:44.409: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.701438ms)
Aug  6 16:46:44.409: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 24.913016ms)
Aug  6 16:46:44.410: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.886134ms)
Aug  6 16:46:44.410: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 24.883116ms)
Aug  6 16:46:44.428: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 43.120609ms)
Aug  6 16:46:44.428: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 43.193074ms)
Aug  6 16:46:44.428: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 43.279117ms)
Aug  6 16:46:44.428: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 43.223846ms)
Aug  6 16:46:44.428: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 43.15036ms)
Aug  6 16:46:44.429: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 44.128201ms)
Aug  6 16:46:44.455: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 26.056291ms)
Aug  6 16:46:44.455: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 26.075849ms)
Aug  6 16:46:44.455: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 26.277843ms)
Aug  6 16:46:44.455: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 26.18651ms)
Aug  6 16:46:44.455: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 26.196506ms)
Aug  6 16:46:44.455: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 26.318525ms)
Aug  6 16:46:44.455: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 26.307397ms)
Aug  6 16:46:44.455: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 26.607308ms)
Aug  6 16:46:44.455: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 26.767951ms)
Aug  6 16:46:44.455: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 26.647574ms)
Aug  6 16:46:44.456: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 26.839044ms)
Aug  6 16:46:44.475: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 46.065253ms)
Aug  6 16:46:44.475: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 46.035466ms)
Aug  6 16:46:44.475: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 46.085276ms)
Aug  6 16:46:44.475: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 46.109725ms)
Aug  6 16:46:44.475: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 46.029073ms)
Aug  6 16:46:44.498: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 23.512204ms)
Aug  6 16:46:44.499: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 23.653486ms)
Aug  6 16:46:44.500: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 24.39841ms)
Aug  6 16:46:44.500: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 24.444468ms)
Aug  6 16:46:44.500: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 24.773219ms)
Aug  6 16:46:44.500: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.952969ms)
Aug  6 16:46:44.500: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 24.908861ms)
Aug  6 16:46:44.500: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.99046ms)
Aug  6 16:46:44.500: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.922839ms)
Aug  6 16:46:44.500: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 24.953809ms)
Aug  6 16:46:44.501: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 25.831385ms)
Aug  6 16:46:44.501: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 25.944782ms)
Aug  6 16:46:44.502: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 26.705558ms)
Aug  6 16:46:44.502: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 26.56086ms)
Aug  6 16:46:44.518: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 43.230453ms)
Aug  6 16:46:44.518: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 43.235835ms)
Aug  6 16:46:44.542: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 23.22115ms)
Aug  6 16:46:44.543: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.710352ms)
Aug  6 16:46:44.543: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 25.044579ms)
Aug  6 16:46:44.543: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 25.051152ms)
Aug  6 16:46:44.544: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 25.020594ms)
Aug  6 16:46:44.544: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 25.079369ms)
Aug  6 16:46:44.544: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 25.049367ms)
Aug  6 16:46:44.544: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 25.561864ms)
Aug  6 16:46:44.544: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 25.580787ms)
Aug  6 16:46:44.544: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 25.824911ms)
Aug  6 16:46:44.544: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 25.77563ms)
Aug  6 16:46:44.545: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 26.740417ms)
Aug  6 16:46:44.561: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 42.836041ms)
Aug  6 16:46:44.561: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 43.006215ms)
Aug  6 16:46:44.561: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 42.918741ms)
Aug  6 16:46:44.561: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 42.857032ms)
Aug  6 16:46:44.584: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 22.591387ms)
Aug  6 16:46:44.584: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 22.820976ms)
Aug  6 16:46:44.586: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.289081ms)
Aug  6 16:46:44.586: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.354911ms)
Aug  6 16:46:44.587: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 24.953231ms)
Aug  6 16:46:44.587: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 24.974458ms)
Aug  6 16:46:44.587: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 25.310397ms)
Aug  6 16:46:44.587: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 25.301797ms)
Aug  6 16:46:44.587: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 25.739305ms)
Aug  6 16:46:44.588: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 25.818528ms)
Aug  6 16:46:44.588: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 26.029177ms)
Aug  6 16:46:44.588: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 26.138974ms)
Aug  6 16:46:44.589: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 26.829528ms)
Aug  6 16:46:44.604: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 42.311135ms)
Aug  6 16:46:44.604: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 42.413843ms)
Aug  6 16:46:44.604: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 42.37265ms)
Aug  6 16:46:44.628: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 23.488577ms)
Aug  6 16:46:44.630: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 25.182193ms)
Aug  6 16:46:44.630: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 25.155301ms)
Aug  6 16:46:44.630: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 25.209184ms)
Aug  6 16:46:44.630: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 25.194756ms)
Aug  6 16:46:44.630: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 25.423392ms)
Aug  6 16:46:44.630: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 25.520019ms)
Aug  6 16:46:44.630: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 25.506408ms)
Aug  6 16:46:44.630: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 25.537387ms)
Aug  6 16:46:44.630: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 25.666667ms)
Aug  6 16:46:44.630: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 26.075517ms)
Aug  6 16:46:44.630: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 26.097593ms)
Aug  6 16:46:44.632: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 27.33544ms)
Aug  6 16:46:44.648: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 43.090448ms)
Aug  6 16:46:44.648: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 43.160121ms)
Aug  6 16:46:44.648: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 43.206656ms)
Aug  6 16:46:44.671: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 23.039094ms)
Aug  6 16:46:44.671: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 23.271788ms)
Aug  6 16:46:44.672: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.476945ms)
Aug  6 16:46:44.672: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 24.454254ms)
Aug  6 16:46:44.672: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 24.479029ms)
Aug  6 16:46:44.672: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 24.541205ms)
Aug  6 16:46:44.672: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.585526ms)
Aug  6 16:46:44.672: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 24.658396ms)
Aug  6 16:46:44.673: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.778612ms)
Aug  6 16:46:44.673: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 25.197612ms)
Aug  6 16:46:44.673: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 25.246884ms)
Aug  6 16:46:44.674: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 26.168523ms)
Aug  6 16:46:44.674: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 26.205698ms)
Aug  6 16:46:44.674: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 26.694545ms)
Aug  6 16:46:44.691: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 42.759816ms)
Aug  6 16:46:44.691: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 42.961049ms)
Aug  6 16:46:44.714: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 23.640119ms)
Aug  6 16:46:44.714: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 23.669696ms)
Aug  6 16:46:44.719: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 27.725644ms)
Aug  6 16:46:44.719: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 27.894565ms)
Aug  6 16:46:44.719: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 27.952503ms)
Aug  6 16:46:44.719: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 28.012762ms)
Aug  6 16:46:44.719: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 28.047557ms)
Aug  6 16:46:44.719: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 28.093944ms)
Aug  6 16:46:44.719: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 28.086459ms)
Aug  6 16:46:44.719: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 28.36894ms)
Aug  6 16:46:44.719: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 28.593164ms)
Aug  6 16:46:44.721: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 29.869283ms)
Aug  6 16:46:44.721: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 29.844309ms)
Aug  6 16:46:44.721: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 29.892047ms)
Aug  6 16:46:44.734: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 43.306854ms)
Aug  6 16:46:44.734: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 43.439414ms)
Aug  6 16:46:44.758: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 23.579721ms)
Aug  6 16:46:44.758: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 23.630433ms)
Aug  6 16:46:44.758: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 23.753562ms)
Aug  6 16:46:44.759: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 24.160365ms)
Aug  6 16:46:44.759: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 24.344265ms)
Aug  6 16:46:44.759: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.338706ms)
Aug  6 16:46:44.760: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 25.192735ms)
Aug  6 16:46:44.760: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 25.438627ms)
Aug  6 16:46:44.760: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 25.492133ms)
Aug  6 16:46:44.760: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 25.388242ms)
Aug  6 16:46:44.761: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 26.239269ms)
Aug  6 16:46:44.761: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 26.258522ms)
Aug  6 16:46:44.761: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 26.454162ms)
Aug  6 16:46:44.761: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 26.264351ms)
Aug  6 16:46:44.761: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 26.459455ms)
Aug  6 16:46:44.761: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 26.872191ms)
Aug  6 16:46:44.785: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 23.813538ms)
Aug  6 16:46:44.785: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 23.763136ms)
Aug  6 16:46:44.786: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.329617ms)
Aug  6 16:46:44.786: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 24.404172ms)
Aug  6 16:46:44.786: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.446742ms)
Aug  6 16:46:44.786: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 24.406844ms)
Aug  6 16:46:44.786: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 24.625368ms)
Aug  6 16:46:44.787: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 25.269489ms)
Aug  6 16:46:44.787: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 25.316584ms)
Aug  6 16:46:44.787: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 25.636613ms)
Aug  6 16:46:44.788: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 26.200958ms)
Aug  6 16:46:44.788: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 26.183402ms)
Aug  6 16:46:44.788: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 26.352454ms)
Aug  6 16:46:44.788: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 26.478819ms)
Aug  6 16:46:44.788: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 26.481829ms)
Aug  6 16:46:44.788: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 26.814051ms)
Aug  6 16:46:44.811: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 23.162231ms)
Aug  6 16:46:44.813: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.609811ms)
Aug  6 16:46:44.813: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 24.682704ms)
Aug  6 16:46:44.813: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 24.682181ms)
Aug  6 16:46:44.813: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 24.733795ms)
Aug  6 16:46:44.813: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 24.923399ms)
Aug  6 16:46:44.813: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 24.958789ms)
Aug  6 16:46:44.814: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 25.220395ms)
Aug  6 16:46:44.814: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 25.220552ms)
Aug  6 16:46:44.814: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 25.246409ms)
Aug  6 16:46:44.814: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 25.397582ms)
Aug  6 16:46:44.815: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 26.559335ms)
Aug  6 16:46:44.815: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 27.056061ms)
Aug  6 16:46:44.816: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 27.120415ms)
Aug  6 16:46:44.816: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 27.213836ms)
Aug  6 16:46:44.816: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 27.266018ms)
Aug  6 16:46:44.840: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 24.220459ms)
Aug  6 16:46:44.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 25.986277ms)
Aug  6 16:46:44.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 26.134005ms)
Aug  6 16:46:44.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 26.150652ms)
Aug  6 16:46:44.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 26.048288ms)
Aug  6 16:46:44.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 26.019753ms)
Aug  6 16:46:44.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 26.051409ms)
Aug  6 16:46:44.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 26.02369ms)
Aug  6 16:46:44.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 26.073142ms)
Aug  6 16:46:44.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 26.068869ms)
Aug  6 16:46:44.842: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 26.135106ms)
Aug  6 16:46:44.843: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 26.93912ms)
Aug  6 16:46:44.843: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 27.152999ms)
Aug  6 16:46:44.843: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 27.788848ms)
Aug  6 16:46:44.843: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 27.806394ms)
Aug  6 16:46:44.844: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 27.800609ms)
Aug  6 16:46:44.867: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 23.6161ms)
Aug  6 16:46:44.868: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.860111ms)
Aug  6 16:46:44.869: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 24.862189ms)
Aug  6 16:46:44.869: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 25.010944ms)
Aug  6 16:46:44.869: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 25.105291ms)
Aug  6 16:46:44.869: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 25.066209ms)
Aug  6 16:46:44.869: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 25.033932ms)
Aug  6 16:46:44.869: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 25.70754ms)
Aug  6 16:46:44.869: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 25.7516ms)
Aug  6 16:46:44.870: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 25.929912ms)
Aug  6 16:46:44.870: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 26.1731ms)
Aug  6 16:46:44.870: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 26.383874ms)
Aug  6 16:46:44.870: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 26.584152ms)
Aug  6 16:46:44.870: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 26.760615ms)
Aug  6 16:46:44.870: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 26.759575ms)
Aug  6 16:46:44.887: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 43.335978ms)
Aug  6 16:46:44.914: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 27.316746ms)
Aug  6 16:46:44.915: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 27.343409ms)
Aug  6 16:46:44.915: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 27.429001ms)
Aug  6 16:46:44.915: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 27.656499ms)
Aug  6 16:46:44.915: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 27.695529ms)
Aug  6 16:46:44.915: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 27.676343ms)
Aug  6 16:46:44.915: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 27.756277ms)
Aug  6 16:46:44.915: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 27.718548ms)
Aug  6 16:46:44.915: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 27.768958ms)
Aug  6 16:46:44.915: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 27.865127ms)
Aug  6 16:46:44.916: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 29.230784ms)
Aug  6 16:46:44.917: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 29.950181ms)
Aug  6 16:46:44.917: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 30.230794ms)
Aug  6 16:46:44.917: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 30.385141ms)
Aug  6 16:46:44.917: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 30.403233ms)
Aug  6 16:46:44.917: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 30.350001ms)
Aug  6 16:46:44.941: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 23.282214ms)
Aug  6 16:46:44.941: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 23.595419ms)
Aug  6 16:46:44.942: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.369226ms)
Aug  6 16:46:44.942: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 24.742263ms)
Aug  6 16:46:44.942: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 24.723365ms)
Aug  6 16:46:44.942: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.820596ms)
Aug  6 16:46:44.943: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 24.684431ms)
Aug  6 16:46:44.943: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.935555ms)
Aug  6 16:46:44.943: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.69425ms)
Aug  6 16:46:44.943: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 25.213288ms)
Aug  6 16:46:44.944: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 26.389886ms)
Aug  6 16:46:44.945: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 26.840591ms)
Aug  6 16:46:44.945: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 26.637374ms)
Aug  6 16:46:44.945: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 26.928865ms)
Aug  6 16:46:44.945: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 26.865795ms)
Aug  6 16:46:44.945: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 26.626572ms)
Aug  6 16:46:44.968: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 23.290219ms)
Aug  6 16:46:44.969: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.22939ms)
Aug  6 16:46:44.969: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 24.142974ms)
Aug  6 16:46:44.969: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 24.361038ms)
Aug  6 16:46:44.970: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 24.683749ms)
Aug  6 16:46:44.970: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 24.534542ms)
Aug  6 16:46:44.970: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.672774ms)
Aug  6 16:46:44.970: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 24.563075ms)
Aug  6 16:46:44.970: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 24.539697ms)
Aug  6 16:46:44.970: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.722074ms)
Aug  6 16:46:44.971: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 25.717807ms)
Aug  6 16:46:44.972: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 26.750508ms)
Aug  6 16:46:44.972: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 26.932604ms)
Aug  6 16:46:44.972: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 26.901508ms)
Aug  6 16:46:44.972: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 26.936ms)
Aug  6 16:46:44.972: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 27.023043ms)
Aug  6 16:46:44.995: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 22.773068ms)
Aug  6 16:46:44.995: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 22.800281ms)
Aug  6 16:46:44.995: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 23.147021ms)
Aug  6 16:46:44.996: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.47266ms)
Aug  6 16:46:44.997: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.560092ms)
Aug  6 16:46:44.997: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 24.484367ms)
Aug  6 16:46:44.997: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 24.51492ms)
Aug  6 16:46:44.997: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 24.584796ms)
Aug  6 16:46:44.997: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 25.321727ms)
Aug  6 16:46:44.997: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 25.25402ms)
Aug  6 16:46:44.997: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 25.363258ms)
Aug  6 16:46:44.997: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 25.484283ms)
Aug  6 16:46:44.998: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 25.451304ms)
Aug  6 16:46:44.999: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 26.768935ms)
Aug  6 16:46:44.999: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 26.776543ms)
Aug  6 16:46:45.014: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 42.436937ms)
Aug  6 16:46:45.038: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 23.204078ms)
Aug  6 16:46:45.038: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 23.283412ms)
Aug  6 16:46:45.038: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 23.244913ms)
Aug  6 16:46:45.038: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 23.430721ms)
Aug  6 16:46:45.040: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 24.99496ms)
Aug  6 16:46:45.040: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 25.064435ms)
Aug  6 16:46:45.040: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 25.041007ms)
Aug  6 16:46:45.040: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 25.081015ms)
Aug  6 16:46:45.040: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 25.744994ms)
Aug  6 16:46:45.040: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 25.665647ms)
Aug  6 16:46:45.041: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 25.949678ms)
Aug  6 16:46:45.041: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 25.918038ms)
Aug  6 16:46:45.041: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 26.470949ms)
Aug  6 16:46:45.041: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 26.521446ms)
Aug  6 16:46:45.042: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 27.189326ms)
Aug  6 16:46:45.042: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 27.757788ms)
Aug  6 16:46:45.066: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 23.823498ms)
Aug  6 16:46:45.066: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:443/proxy/... (200; 23.81724ms)
Aug  6 16:46:45.066: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 23.781146ms)
Aug  6 16:46:45.067: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:462/proxy/: tls qux (200; 24.847506ms)
Aug  6 16:46:45.067: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg/proxy/rewriteme"... (200; 24.857668ms)
Aug  6 16:46:45.067: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/https:proxy-service-kqsgr-9fqlg:460/proxy/: tls baz (200; 24.929748ms)
Aug  6 16:46:45.068: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:160/proxy/: foo (200; 24.94906ms)
Aug  6 16:46:45.068: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:1080/proxy/rewri... (200; 24.993632ms)
Aug  6 16:46:45.068: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/http:proxy-service-kqsgr-9fqlg:1080/proxy/... (200; 25.039203ms)
Aug  6 16:46:45.068: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/pods/proxy-service-kqsgr-9fqlg:162/proxy/: bar (200; 25.05488ms)
Aug  6 16:46:45.068: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname1/proxy/: foo (200; 25.343631ms)
Aug  6 16:46:45.069: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/http:proxy-service-kqsgr:portname2/proxy/: bar (200; 26.094033ms)
Aug  6 16:46:45.069: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname1/proxy/: tls baz (200; 26.269917ms)
Aug  6 16:46:45.069: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname1/proxy/: foo (200; 26.27735ms)
Aug  6 16:46:45.069: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/proxy-service-kqsgr:portname2/proxy/: bar (200; 26.444793ms)
Aug  6 16:46:45.069: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-zsg5d/services/https:proxy-service-kqsgr:tlsportname2/proxy/: tls qux (200; 26.627141ms)
STEP: deleting ReplicationController proxy-service-kqsgr in namespace e2e-tests-proxy-zsg5d, will wait for the garbage collector to delete the pods
Aug  6 16:46:45.167: INFO: Deleting ReplicationController proxy-service-kqsgr took: 25.778648ms
Aug  6 16:46:45.268: INFO: Terminating ReplicationController proxy-service-kqsgr pods took: 100.250715ms
[AfterEach] version v1
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:46:49.368: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-zsg5d" for this suite.
Aug  6 16:46:55.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:46:56.550: INFO: namespace: e2e-tests-proxy-zsg5d, resource: packagemanifests, items remaining: 3
Aug  6 16:46:56.814: INFO: namespace: e2e-tests-proxy-zsg5d, resource: bindings, ignored listing per whitelist
Aug  6 16:46:57.446: INFO: namespace: e2e-tests-proxy-zsg5d no longer exists
Aug  6 16:46:57.473: INFO: namespace: e2e-tests-proxy-zsg5d, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:46:57.496: INFO: namespace e2e-tests-proxy-zsg5d deletion completed in 8.063620476s

• [SLOW TEST:31.660 seconds]
[sig-network] Proxy
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:46:57.496: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-645lq
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Aug  6 16:46:58.929: INFO: Found 1 stateful pods, waiting for 3
Aug  6 16:47:08.958: INFO: Found 2 stateful pods, waiting for 3
Aug  6 16:47:18.955: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:47:18.955: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:47:18.955: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug  6 16:47:28.956: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:47:28.957: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:47:28.957: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug  6 16:47:29.026: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-645lq ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  6 16:47:29.462: INFO: stderr: ""
Aug  6 16:47:29.462: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  6 16:47:29.462: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug  6 16:47:39.612: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug  6 16:47:39.680: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-645lq ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  6 16:47:40.026: INFO: stderr: ""
Aug  6 16:47:40.026: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  6 16:47:40.026: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  6 16:47:50.166: INFO: Waiting for StatefulSet e2e-tests-statefulset-645lq/ss2 to complete update
Aug  6 16:47:50.166: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-0 to have revision ss2-76875fdd44 update revision ss2-55568f78cf
Aug  6 16:47:50.166: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-1 to have revision ss2-76875fdd44 update revision ss2-55568f78cf
Aug  6 16:47:50.166: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-2 to have revision ss2-76875fdd44 update revision ss2-55568f78cf
Aug  6 16:48:00.214: INFO: Waiting for StatefulSet e2e-tests-statefulset-645lq/ss2 to complete update
Aug  6 16:48:00.214: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-0 to have revision ss2-76875fdd44 update revision ss2-55568f78cf
Aug  6 16:48:00.214: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-1 to have revision ss2-76875fdd44 update revision ss2-55568f78cf
Aug  6 16:48:10.213: INFO: Waiting for StatefulSet e2e-tests-statefulset-645lq/ss2 to complete update
Aug  6 16:48:10.213: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-0 to have revision ss2-76875fdd44 update revision ss2-55568f78cf
Aug  6 16:48:20.214: INFO: Waiting for StatefulSet e2e-tests-statefulset-645lq/ss2 to complete update
STEP: Rolling back to a previous revision
Aug  6 16:48:30.215: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-645lq ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  6 16:48:30.643: INFO: stderr: ""
Aug  6 16:48:30.643: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  6 16:48:30.643: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  6 16:48:40.798: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug  6 16:48:40.866: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance exec --namespace=e2e-tests-statefulset-645lq ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  6 16:48:41.238: INFO: stderr: ""
Aug  6 16:48:41.238: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  6 16:48:41.238: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  6 16:48:51.375: INFO: Waiting for StatefulSet e2e-tests-statefulset-645lq/ss2 to complete update
Aug  6 16:48:51.375: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-0 to have revision ss2-55568f78cf update revision ss2-76875fdd44
Aug  6 16:48:51.375: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-1 to have revision ss2-55568f78cf update revision ss2-76875fdd44
Aug  6 16:48:51.375: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-2 to have revision ss2-55568f78cf update revision ss2-76875fdd44
Aug  6 16:49:01.421: INFO: Waiting for StatefulSet e2e-tests-statefulset-645lq/ss2 to complete update
Aug  6 16:49:01.421: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-0 to have revision ss2-55568f78cf update revision ss2-76875fdd44
Aug  6 16:49:01.421: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-1 to have revision ss2-55568f78cf update revision ss2-76875fdd44
Aug  6 16:49:11.420: INFO: Waiting for StatefulSet e2e-tests-statefulset-645lq/ss2 to complete update
Aug  6 16:49:11.420: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-0 to have revision ss2-55568f78cf update revision ss2-76875fdd44
Aug  6 16:49:11.420: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-1 to have revision ss2-55568f78cf update revision ss2-76875fdd44
Aug  6 16:49:21.421: INFO: Waiting for StatefulSet e2e-tests-statefulset-645lq/ss2 to complete update
Aug  6 16:49:21.421: INFO: Waiting for Pod e2e-tests-statefulset-645lq/ss2-0 to have revision ss2-55568f78cf update revision ss2-76875fdd44
Aug  6 16:49:31.420: INFO: Waiting for StatefulSet e2e-tests-statefulset-645lq/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug  6 16:49:41.421: INFO: Deleting all statefulset in ns e2e-tests-statefulset-645lq
Aug  6 16:49:41.443: INFO: Scaling statefulset ss2 to 0
Aug  6 16:50:01.535: INFO: Waiting for statefulset status.replicas updated to 0
Aug  6 16:50:01.557: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:50:01.636: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-645lq" for this suite.
Aug  6 16:50:07.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:50:09.806: INFO: namespace: e2e-tests-statefulset-645lq, resource: bindings, ignored listing per whitelist
Aug  6 16:50:09.945: INFO: namespace: e2e-tests-statefulset-645lq, resource: packagemanifests, items remaining: 3
Aug  6 16:50:10.165: INFO: namespace: e2e-tests-statefulset-645lq no longer exists
Aug  6 16:50:10.189: INFO: namespace: e2e-tests-statefulset-645lq, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:50:10.211: INFO: namespace e2e-tests-statefulset-645lq deletion completed in 8.531590974s

• [SLOW TEST:192.715 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:50:10.211: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  6 16:50:11.587: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-frhns'
Aug  6 16:50:13.114: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug  6 16:50:13.114: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Aug  6 16:50:13.140: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-frhns'
Aug  6 16:50:13.317: INFO: stderr: ""
Aug  6 16:50:13.317: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:50:13.317: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-frhns" for this suite.
Aug  6 16:50:35.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:50:36.176: INFO: namespace: e2e-tests-kubectl-frhns, resource: packagemanifests, items remaining: 3
Aug  6 16:50:36.824: INFO: namespace: e2e-tests-kubectl-frhns, resource: bindings, ignored listing per whitelist
Aug  6 16:50:37.900: INFO: namespace: e2e-tests-kubectl-frhns no longer exists
Aug  6 16:50:37.923: INFO: namespace: e2e-tests-kubectl-frhns, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:50:37.945: INFO: namespace e2e-tests-kubectl-frhns deletion completed in 24.564817611s

• [SLOW TEST:27.734 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:50:37.946: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug  6 16:50:49.517: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:50:49.590: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-kx6nf" for this suite.
Aug  6 16:51:11.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:51:12.409: INFO: namespace: e2e-tests-replicaset-kx6nf, resource: bindings, ignored listing per whitelist
Aug  6 16:51:13.765: INFO: namespace: e2e-tests-replicaset-kx6nf, resource: packagemanifests, items remaining: 3
Aug  6 16:51:14.147: INFO: namespace: e2e-tests-replicaset-kx6nf no longer exists
Aug  6 16:51:14.169: INFO: namespace: e2e-tests-replicaset-kx6nf, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:51:14.192: INFO: namespace e2e-tests-replicaset-kx6nf deletion completed in 24.539172349s

• [SLOW TEST:36.245 seconds]
[sig-apps] ReplicaSet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:51:14.192: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W0806 16:51:15.830515    4087 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug  6 16:51:15.830: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:51:15.830: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-hxq9x" for this suite.
Aug  6 16:51:21.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:51:22.925: INFO: namespace: e2e-tests-gc-hxq9x, resource: bindings, ignored listing per whitelist
Aug  6 16:51:24.089: INFO: namespace: e2e-tests-gc-hxq9x, resource: packagemanifests, items remaining: 3
Aug  6 16:51:24.444: INFO: namespace: e2e-tests-gc-hxq9x no longer exists
Aug  6 16:51:24.466: INFO: namespace: e2e-tests-gc-hxq9x, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:51:24.489: INFO: namespace e2e-tests-gc-hxq9x deletion completed in 8.615714385s

• [SLOW TEST:10.297 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:51:24.490: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug  6 16:51:25.902: INFO: Waiting up to 5m0s for pod "pod-6dbdd154-b86a-11e9-8d18-525400524259" in namespace "e2e-tests-emptydir-5cjcq" to be "success or failure"
Aug  6 16:51:25.925: INFO: Pod "pod-6dbdd154-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 22.978443ms
Aug  6 16:51:27.948: INFO: Pod "pod-6dbdd154-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045926238s
Aug  6 16:51:29.971: INFO: Pod "pod-6dbdd154-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069002068s
Aug  6 16:51:31.994: INFO: Pod "pod-6dbdd154-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092324059s
Aug  6 16:51:34.074: INFO: Pod "pod-6dbdd154-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.171778743s
Aug  6 16:51:36.100: INFO: Pod "pod-6dbdd154-b86a-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.198078663s
STEP: Saw pod success
Aug  6 16:51:36.100: INFO: Pod "pod-6dbdd154-b86a-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:51:36.125: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-6dbdd154-b86a-11e9-8d18-525400524259 container test-container: <nil>
STEP: delete the pod
Aug  6 16:51:36.194: INFO: Waiting for pod pod-6dbdd154-b86a-11e9-8d18-525400524259 to disappear
Aug  6 16:51:36.216: INFO: Pod pod-6dbdd154-b86a-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:51:36.216: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5cjcq" for this suite.
Aug  6 16:51:42.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:51:44.147: INFO: namespace: e2e-tests-emptydir-5cjcq, resource: packagemanifests, items remaining: 3
Aug  6 16:51:44.587: INFO: namespace: e2e-tests-emptydir-5cjcq, resource: bindings, ignored listing per whitelist
Aug  6 16:51:44.763: INFO: namespace: e2e-tests-emptydir-5cjcq no longer exists
Aug  6 16:51:44.786: INFO: namespace: e2e-tests-emptydir-5cjcq, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:51:44.808: INFO: namespace e2e-tests-emptydir-5cjcq deletion completed in 8.529400856s

• [SLOW TEST:20.319 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:51:44.809: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:51:46.169: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:51:54.414: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-xs4hb" for this suite.
Aug  6 16:52:32.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:52:33.403: INFO: namespace: e2e-tests-pods-xs4hb, resource: bindings, ignored listing per whitelist
Aug  6 16:52:34.618: INFO: namespace: e2e-tests-pods-xs4hb, resource: packagemanifests, items remaining: 3
Aug  6 16:52:34.972: INFO: namespace: e2e-tests-pods-xs4hb no longer exists
Aug  6 16:52:35.015: INFO: namespace: e2e-tests-pods-xs4hb, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:52:35.037: INFO: namespace e2e-tests-pods-xs4hb deletion completed in 40.560180562s

• [SLOW TEST:50.229 seconds]
[k8s.io] Pods
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:52:35.038: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 16:52:36.463: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97cc1ed9-b86a-11e9-8d18-525400524259" in namespace "e2e-tests-projected-qf6rf" to be "success or failure"
Aug  6 16:52:36.487: INFO: Pod "downwardapi-volume-97cc1ed9-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.619901ms
Aug  6 16:52:38.510: INFO: Pod "downwardapi-volume-97cc1ed9-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047339738s
Aug  6 16:52:40.534: INFO: Pod "downwardapi-volume-97cc1ed9-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071399067s
Aug  6 16:52:42.557: INFO: Pod "downwardapi-volume-97cc1ed9-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094199594s
Aug  6 16:52:44.580: INFO: Pod "downwardapi-volume-97cc1ed9-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117048837s
Aug  6 16:52:46.603: INFO: Pod "downwardapi-volume-97cc1ed9-b86a-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.140061993s
STEP: Saw pod success
Aug  6 16:52:46.603: INFO: Pod "downwardapi-volume-97cc1ed9-b86a-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:52:46.625: INFO: Trying to get logs from node ip-10-0-128-44.ec2.internal pod downwardapi-volume-97cc1ed9-b86a-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 16:52:46.683: INFO: Waiting for pod downwardapi-volume-97cc1ed9-b86a-11e9-8d18-525400524259 to disappear
Aug  6 16:52:46.706: INFO: Pod downwardapi-volume-97cc1ed9-b86a-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:52:46.706: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qf6rf" for this suite.
Aug  6 16:52:52.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:52:54.695: INFO: namespace: e2e-tests-projected-qf6rf, resource: packagemanifests, items remaining: 3
Aug  6 16:52:55.005: INFO: namespace: e2e-tests-projected-qf6rf, resource: bindings, ignored listing per whitelist
Aug  6 16:52:55.247: INFO: namespace: e2e-tests-projected-qf6rf no longer exists
Aug  6 16:52:55.270: INFO: namespace: e2e-tests-projected-qf6rf, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:52:55.292: INFO: namespace e2e-tests-projected-qf6rf deletion completed in 8.52225351s

• [SLOW TEST:20.254 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:52:55.292: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  6 16:52:56.693: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-vzbjx'
Aug  6 16:52:56.864: INFO: stderr: ""
Aug  6 16:52:56.864: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Aug  6 16:52:56.886: INFO: Running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-vzbjx'
Aug  6 16:52:59.312: INFO: stderr: ""
Aug  6 16:52:59.312: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:52:59.312: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vzbjx" for this suite.
Aug  6 16:53:05.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:53:06.138: INFO: namespace: e2e-tests-kubectl-vzbjx, resource: packagemanifests, items remaining: 3
Aug  6 16:53:07.722: INFO: namespace: e2e-tests-kubectl-vzbjx, resource: bindings, ignored listing per whitelist
Aug  6 16:53:07.899: INFO: namespace: e2e-tests-kubectl-vzbjx no longer exists
Aug  6 16:53:07.941: INFO: namespace: e2e-tests-kubectl-vzbjx, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:53:07.963: INFO: namespace e2e-tests-kubectl-vzbjx deletion completed in 8.568000661s

• [SLOW TEST:12.671 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:53:07.963: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Aug  6 16:53:09.444: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab7513cc-b86a-11e9-8d18-525400524259" in namespace "e2e-tests-downward-api-6vxxz" to be "success or failure"
Aug  6 16:53:09.467: INFO: Pod "downwardapi-volume-ab7513cc-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.670863ms
Aug  6 16:53:11.490: INFO: Pod "downwardapi-volume-ab7513cc-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046820708s
Aug  6 16:53:13.514: INFO: Pod "downwardapi-volume-ab7513cc-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069881995s
Aug  6 16:53:15.537: INFO: Pod "downwardapi-volume-ab7513cc-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093020145s
Aug  6 16:53:17.560: INFO: Pod "downwardapi-volume-ab7513cc-b86a-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.115962463s
STEP: Saw pod success
Aug  6 16:53:17.560: INFO: Pod "downwardapi-volume-ab7513cc-b86a-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:53:17.582: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod downwardapi-volume-ab7513cc-b86a-11e9-8d18-525400524259 container client-container: <nil>
STEP: delete the pod
Aug  6 16:53:17.638: INFO: Waiting for pod downwardapi-volume-ab7513cc-b86a-11e9-8d18-525400524259 to disappear
Aug  6 16:53:17.660: INFO: Pod downwardapi-volume-ab7513cc-b86a-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:53:17.660: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6vxxz" for this suite.
Aug  6 16:53:23.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:53:24.610: INFO: namespace: e2e-tests-downward-api-6vxxz, resource: bindings, ignored listing per whitelist
Aug  6 16:53:25.617: INFO: namespace: e2e-tests-downward-api-6vxxz, resource: packagemanifests, items remaining: 3
Aug  6 16:53:26.261: INFO: namespace: e2e-tests-downward-api-6vxxz no longer exists
Aug  6 16:53:26.304: INFO: namespace: e2e-tests-downward-api-6vxxz, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:53:26.327: INFO: namespace e2e-tests-downward-api-6vxxz deletion completed in 8.604104312s

• [SLOW TEST:18.364 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:53:26.327: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:54:27.742: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-vjc57" for this suite.
Aug  6 16:54:49.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:54:52.033: INFO: namespace: e2e-tests-container-probe-vjc57, resource: packagemanifests, items remaining: 3
Aug  6 16:54:52.055: INFO: namespace: e2e-tests-container-probe-vjc57, resource: bindings, ignored listing per whitelist
Aug  6 16:54:52.347: INFO: namespace: e2e-tests-container-probe-vjc57 no longer exists
Aug  6 16:54:52.390: INFO: namespace: e2e-tests-container-probe-vjc57, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:54:52.412: INFO: namespace e2e-tests-container-probe-vjc57 deletion completed in 24.587198315s

• [SLOW TEST:86.085 seconds]
[k8s.io] Probing container
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:54:52.413: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:54:53.784: INFO: Creating ReplicaSet my-hostname-basic-e9aad785-b86a-11e9-8d18-525400524259
Aug  6 16:54:53.859: INFO: Pod name my-hostname-basic-e9aad785-b86a-11e9-8d18-525400524259: Found 1 pods out of 1
Aug  6 16:54:53.859: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e9aad785-b86a-11e9-8d18-525400524259" is running
Aug  6 16:55:03.905: INFO: Pod "my-hostname-basic-e9aad785-b86a-11e9-8d18-525400524259-86jb5" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-06 16:54:31 +0000 UTC Reason: Message:}])
Aug  6 16:55:03.905: INFO: Trying to dial the pod
Aug  6 16:55:08.976: INFO: Controller my-hostname-basic-e9aad785-b86a-11e9-8d18-525400524259: Got expected result from replica 1 [my-hostname-basic-e9aad785-b86a-11e9-8d18-525400524259-86jb5]: "my-hostname-basic-e9aad785-b86a-11e9-8d18-525400524259-86jb5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:55:08.976: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-zhtls" for this suite.
Aug  6 16:55:15.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:55:16.107: INFO: namespace: e2e-tests-replicaset-zhtls, resource: packagemanifests, items remaining: 3
Aug  6 16:55:16.261: INFO: namespace: e2e-tests-replicaset-zhtls, resource: bindings, ignored listing per whitelist
Aug  6 16:55:17.557: INFO: namespace: e2e-tests-replicaset-zhtls no longer exists
Aug  6 16:55:17.600: INFO: namespace: e2e-tests-replicaset-zhtls, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:55:17.622: INFO: namespace e2e-tests-replicaset-zhtls deletion completed in 8.563725718s

• [SLOW TEST:25.209 seconds]
[sig-apps] ReplicaSet
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:55:17.623: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-f8b07f2e-b86a-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 16:55:19.050: INFO: Waiting up to 5m0s for pod "pod-secrets-f8b408bb-b86a-11e9-8d18-525400524259" in namespace "e2e-tests-secrets-725wc" to be "success or failure"
Aug  6 16:55:19.075: INFO: Pod "pod-secrets-f8b408bb-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 25.411146ms
Aug  6 16:55:21.099: INFO: Pod "pod-secrets-f8b408bb-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048801494s
Aug  6 16:55:23.122: INFO: Pod "pod-secrets-f8b408bb-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072351855s
Aug  6 16:55:25.145: INFO: Pod "pod-secrets-f8b408bb-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0954103s
Aug  6 16:55:27.169: INFO: Pod "pod-secrets-f8b408bb-b86a-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.11892836s
Aug  6 16:55:29.192: INFO: Pod "pod-secrets-f8b408bb-b86a-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.141998587s
STEP: Saw pod success
Aug  6 16:55:29.192: INFO: Pod "pod-secrets-f8b408bb-b86a-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:55:29.214: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-secrets-f8b408bb-b86a-11e9-8d18-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Aug  6 16:55:29.269: INFO: Waiting for pod pod-secrets-f8b408bb-b86a-11e9-8d18-525400524259 to disappear
Aug  6 16:55:29.291: INFO: Pod pod-secrets-f8b408bb-b86a-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:55:29.291: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-725wc" for this suite.
Aug  6 16:55:35.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:55:36.828: INFO: namespace: e2e-tests-secrets-725wc, resource: packagemanifests, items remaining: 3
Aug  6 16:55:37.004: INFO: namespace: e2e-tests-secrets-725wc, resource: bindings, ignored listing per whitelist
Aug  6 16:55:37.843: INFO: namespace: e2e-tests-secrets-725wc no longer exists
Aug  6 16:55:37.865: INFO: namespace: e2e-tests-secrets-725wc, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:55:37.890: INFO: namespace e2e-tests-secrets-725wc deletion completed in 8.536759308s

• [SLOW TEST:20.268 seconds]
[sig-storage] Secrets
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:55:37.891: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-04c414f5-b86b-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 16:55:39.303: INFO: Waiting up to 5m0s for pod "pod-secrets-04c7dd11-b86b-11e9-8d18-525400524259" in namespace "e2e-tests-secrets-7gf7b" to be "success or failure"
Aug  6 16:55:39.326: INFO: Pod "pod-secrets-04c7dd11-b86b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.183772ms
Aug  6 16:55:41.350: INFO: Pod "pod-secrets-04c7dd11-b86b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047108698s
Aug  6 16:55:43.373: INFO: Pod "pod-secrets-04c7dd11-b86b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070104408s
Aug  6 16:55:45.398: INFO: Pod "pod-secrets-04c7dd11-b86b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.095105203s
Aug  6 16:55:47.421: INFO: Pod "pod-secrets-04c7dd11-b86b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.118463212s
Aug  6 16:55:49.447: INFO: Pod "pod-secrets-04c7dd11-b86b-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.143892929s
STEP: Saw pod success
Aug  6 16:55:49.447: INFO: Pod "pod-secrets-04c7dd11-b86b-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:55:49.469: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-secrets-04c7dd11-b86b-11e9-8d18-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Aug  6 16:55:49.528: INFO: Waiting for pod pod-secrets-04c7dd11-b86b-11e9-8d18-525400524259 to disappear
Aug  6 16:55:49.550: INFO: Pod pod-secrets-04c7dd11-b86b-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:55:49.550: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7gf7b" for this suite.
Aug  6 16:55:55.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:55:56.538: INFO: namespace: e2e-tests-secrets-7gf7b, resource: packagemanifests, items remaining: 3
Aug  6 16:55:57.648: INFO: namespace: e2e-tests-secrets-7gf7b, resource: bindings, ignored listing per whitelist
Aug  6 16:55:58.108: INFO: namespace: e2e-tests-secrets-7gf7b no longer exists
Aug  6 16:55:58.132: INFO: namespace: e2e-tests-secrets-7gf7b, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:55:58.155: INFO: namespace e2e-tests-secrets-7gf7b deletion completed in 8.541432648s

• [SLOW TEST:20.264 seconds]
[sig-storage] Secrets
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:55:58.155: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Aug  6 16:55:59.502: INFO: Asynchronously running '/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance proxy --unix-socket=/tmp/kubectl-proxy-unix042368652/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:55:59.578: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sw4g7" for this suite.
Aug  6 16:56:05.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:56:07.202: INFO: namespace: e2e-tests-kubectl-sw4g7, resource: bindings, ignored listing per whitelist
Aug  6 16:56:07.607: INFO: namespace: e2e-tests-kubectl-sw4g7, resource: packagemanifests, items remaining: 3
Aug  6 16:56:08.113: INFO: namespace: e2e-tests-kubectl-sw4g7 no longer exists
Aug  6 16:56:08.136: INFO: namespace: e2e-tests-kubectl-sw4g7, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:56:08.158: INFO: namespace e2e-tests-kubectl-sw4g7 deletion completed in 8.537333847s

• [SLOW TEST:10.003 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:56:08.159: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:56:09.598: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug  6 16:56:19.644: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug  6 16:56:27.829: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-m7qr5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-m7qr5/deployments/test-cleanup-deployment,UID:0f6b1f8f-b86b-11e9-9dff-0e6de702691c,ResourceVersion:160021,Generation:1,CreationTimestamp:2019-08-06 16:55:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-06 16:55:57 +0000 UTC 2019-08-06 16:55:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-06 16:56:05 +0000 UTC 2019-08-06 16:55:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-6c849fcf55" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug  6 16:56:27.851: INFO: New ReplicaSet "test-cleanup-deployment-6c849fcf55" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6c849fcf55,GenerateName:,Namespace:e2e-tests-deployment-m7qr5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-m7qr5/replicasets/test-cleanup-deployment-6c849fcf55,UID:0f6c71c3-b86b-11e9-9dff-0e6de702691c,ResourceVersion:160010,Generation:1,CreationTimestamp:2019-08-06 16:55:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6c849fcf55,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 0f6b1f8f-b86b-11e9-9dff-0e6de702691c 0xc0029fff87 0xc0029fff88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6c849fcf55,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6c849fcf55,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug  6 16:56:27.875: INFO: Pod "test-cleanup-deployment-6c849fcf55-68h5v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6c849fcf55-68h5v,GenerateName:test-cleanup-deployment-6c849fcf55-,Namespace:e2e-tests-deployment-m7qr5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-m7qr5/pods/test-cleanup-deployment-6c849fcf55-68h5v,UID:0f6da521-b86b-11e9-9dff-0e6de702691c,ResourceVersion:160009,Generation:0,CreationTimestamp:2019-08-06 16:55:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6c849fcf55,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.231"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-6c849fcf55 0f6c71c3-b86b-11e9-9dff-0e6de702691c 0xc001e2e697 0xc001e2e698}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-drw7s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-drw7s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-drw7s true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-134.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-9t6lf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001e2e700} {node.kubernetes.io/not-ready Exists  NoExecute 0xc001e2e720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 16:55:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 16:56:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 16:56:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-06 16:55:57 +0000 UTC  }],Message:,Reason:,HostIP:10.0.130.134,PodIP:10.131.0.231,StartTime:2019-08-06 16:55:57 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-06 16:56:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://c1336a849c39123e0bcc849d468613f5f04b6eef55b5fbbd486df83f98e7fe2f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:56:27.875: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-m7qr5" for this suite.
Aug  6 16:56:34.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:56:35.794: INFO: namespace: e2e-tests-deployment-m7qr5, resource: packagemanifests, items remaining: 3
Aug  6 16:56:36.149: INFO: namespace: e2e-tests-deployment-m7qr5, resource: bindings, ignored listing per whitelist
Aug  6 16:56:36.433: INFO: namespace: e2e-tests-deployment-m7qr5 no longer exists
Aug  6 16:56:36.457: INFO: namespace: e2e-tests-deployment-m7qr5, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:56:36.479: INFO: namespace e2e-tests-deployment-m7qr5 deletion completed in 8.540600709s

• [SLOW TEST:28.320 seconds]
[sig-apps] Deployment
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:56:36.479: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Aug  6 16:56:38.030: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug  6 16:56:38.098: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:38.098: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:38.098: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:38.120: INFO: Number of nodes with available pods: 0
Aug  6 16:56:38.120: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:56:39.185: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:39.185: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:39.185: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:39.207: INFO: Number of nodes with available pods: 0
Aug  6 16:56:39.207: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:56:40.185: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:40.185: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:40.185: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:40.211: INFO: Number of nodes with available pods: 0
Aug  6 16:56:40.211: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:56:41.184: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:41.184: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:41.184: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:41.209: INFO: Number of nodes with available pods: 0
Aug  6 16:56:41.209: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:56:42.183: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:42.183: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:42.183: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:42.206: INFO: Number of nodes with available pods: 0
Aug  6 16:56:42.206: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:56:43.185: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:43.185: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:43.185: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:43.208: INFO: Number of nodes with available pods: 0
Aug  6 16:56:43.208: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:56:44.185: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:44.185: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:44.185: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:44.208: INFO: Number of nodes with available pods: 0
Aug  6 16:56:44.208: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:56:45.184: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:45.184: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:45.184: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:45.206: INFO: Number of nodes with available pods: 0
Aug  6 16:56:45.207: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:56:46.185: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:46.185: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:46.185: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:46.207: INFO: Number of nodes with available pods: 1
Aug  6 16:56:46.207: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:56:47.192: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:47.192: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:47.192: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:47.215: INFO: Number of nodes with available pods: 4
Aug  6 16:56:47.215: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug  6 16:56:47.395: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:47.395: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:47.395: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:47.395: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:47.441: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:47.441: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:47.441: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:48.465: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:48.465: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:48.465: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:48.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:48.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:48.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:48.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:49.465: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:49.465: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:49.465: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:49.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:49.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:49.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:49.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:50.465: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:50.465: INFO: Pod daemon-set-5nshl is not available
Aug  6 16:56:50.465: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:50.465: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:50.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:50.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:50.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:50.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:51.464: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:51.464: INFO: Pod daemon-set-5nshl is not available
Aug  6 16:56:51.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:51.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:51.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:51.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:51.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:51.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:52.464: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:52.464: INFO: Pod daemon-set-5nshl is not available
Aug  6 16:56:52.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:52.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:52.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:52.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:52.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:52.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:53.466: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:53.466: INFO: Pod daemon-set-5nshl is not available
Aug  6 16:56:53.466: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:53.466: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:53.466: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:53.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:53.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:53.510: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:54.467: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:54.467: INFO: Pod daemon-set-5nshl is not available
Aug  6 16:56:54.467: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:54.467: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:54.467: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:54.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:54.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:54.510: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:55.491: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:55.491: INFO: Pod daemon-set-5nshl is not available
Aug  6 16:56:55.491: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:55.491: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:55.491: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:55.534: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:55.534: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:55.534: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:56.464: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:56.464: INFO: Pod daemon-set-5nshl is not available
Aug  6 16:56:56.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:56.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:56.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:56.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:56.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:56.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:57.465: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:57.465: INFO: Pod daemon-set-5nshl is not available
Aug  6 16:56:57.465: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:57.465: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:57.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:57.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:57.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:57.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:58.466: INFO: Wrong image for pod: daemon-set-5nshl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:58.466: INFO: Pod daemon-set-5nshl is not available
Aug  6 16:56:58.466: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:58.466: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:58.466: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:58.525: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:58.525: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:58.525: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:59.465: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:59.465: INFO: Pod daemon-set-lkpp5 is not available
Aug  6 16:56:59.465: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:59.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:56:59.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:59.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:56:59.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:00.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:00.464: INFO: Pod daemon-set-lkpp5 is not available
Aug  6 16:57:00.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:00.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:00.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:00.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:00.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:01.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:01.464: INFO: Pod daemon-set-lkpp5 is not available
Aug  6 16:57:01.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:01.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:01.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:01.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:01.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:02.465: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:02.465: INFO: Pod daemon-set-lkpp5 is not available
Aug  6 16:57:02.465: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:02.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:02.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:02.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:02.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:03.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:03.464: INFO: Pod daemon-set-lkpp5 is not available
Aug  6 16:57:03.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:03.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:03.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:03.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:03.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:04.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:04.464: INFO: Pod daemon-set-lkpp5 is not available
Aug  6 16:57:04.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:04.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:04.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:04.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:04.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:05.466: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:05.466: INFO: Pod daemon-set-lkpp5 is not available
Aug  6 16:57:05.466: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:05.466: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:05.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:05.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:05.510: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:06.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:06.464: INFO: Pod daemon-set-lkpp5 is not available
Aug  6 16:57:06.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:06.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:06.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:06.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:06.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:07.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:07.464: INFO: Pod daemon-set-lkpp5 is not available
Aug  6 16:57:07.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:07.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:07.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:07.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:07.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:08.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:08.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:08.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:08.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:08.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:08.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:09.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:09.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:09.464: INFO: Pod daemon-set-q5l5p is not available
Aug  6 16:57:09.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:09.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:09.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:09.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:10.465: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:10.465: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:10.465: INFO: Pod daemon-set-q5l5p is not available
Aug  6 16:57:10.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:10.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:10.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:10.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:11.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:11.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:11.464: INFO: Pod daemon-set-q5l5p is not available
Aug  6 16:57:11.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:11.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:11.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:11.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:12.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:12.464: INFO: Wrong image for pod: daemon-set-q5l5p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:12.464: INFO: Pod daemon-set-q5l5p is not available
Aug  6 16:57:12.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:12.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:12.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:12.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:13.465: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:13.465: INFO: Pod daemon-set-ppct9 is not available
Aug  6 16:57:13.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:13.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:13.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:13.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:14.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:14.464: INFO: Pod daemon-set-ppct9 is not available
Aug  6 16:57:14.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:14.518: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:14.518: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:14.518: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:15.467: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:15.467: INFO: Pod daemon-set-ppct9 is not available
Aug  6 16:57:15.467: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:15.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:15.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:15.511: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:16.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:16.464: INFO: Pod daemon-set-ppct9 is not available
Aug  6 16:57:16.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:16.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:16.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:16.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:17.465: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:17.465: INFO: Pod daemon-set-ppct9 is not available
Aug  6 16:57:17.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:17.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:17.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:17.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:18.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:18.464: INFO: Pod daemon-set-ppct9 is not available
Aug  6 16:57:18.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:18.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:18.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:18.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:19.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:19.464: INFO: Pod daemon-set-ppct9 is not available
Aug  6 16:57:19.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:19.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:19.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:19.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:20.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:20.464: INFO: Pod daemon-set-ppct9 is not available
Aug  6 16:57:20.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:20.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:20.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:20.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:21.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:21.464: INFO: Pod daemon-set-ppct9 is not available
Aug  6 16:57:21.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:21.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:21.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:21.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:22.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:22.464: INFO: Pod daemon-set-ppct9 is not available
Aug  6 16:57:22.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:22.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:22.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:22.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:23.466: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:23.466: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:23.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:23.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:23.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:24.465: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:24.465: INFO: Pod daemon-set-bgn74 is not available
Aug  6 16:57:24.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:24.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:24.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:24.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:25.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:25.464: INFO: Pod daemon-set-bgn74 is not available
Aug  6 16:57:25.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:25.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:25.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:25.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:26.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:26.464: INFO: Pod daemon-set-bgn74 is not available
Aug  6 16:57:26.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:26.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:26.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:26.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:27.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:27.464: INFO: Pod daemon-set-bgn74 is not available
Aug  6 16:57:27.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:27.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:27.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:27.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:28.465: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:28.465: INFO: Pod daemon-set-bgn74 is not available
Aug  6 16:57:28.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:28.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:28.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:28.510: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:29.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:29.464: INFO: Pod daemon-set-bgn74 is not available
Aug  6 16:57:29.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:29.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:29.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:29.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:30.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:30.464: INFO: Pod daemon-set-bgn74 is not available
Aug  6 16:57:30.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:30.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:30.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:30.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:31.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:31.465: INFO: Pod daemon-set-bgn74 is not available
Aug  6 16:57:31.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:31.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:31.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:31.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:32.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:32.464: INFO: Pod daemon-set-bgn74 is not available
Aug  6 16:57:32.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:32.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:32.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:32.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:33.464: INFO: Wrong image for pod: daemon-set-bgn74. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:33.464: INFO: Pod daemon-set-bgn74 is not available
Aug  6 16:57:33.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:33.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:33.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:33.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:34.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:34.464: INFO: Pod daemon-set-vr98d is not available
Aug  6 16:57:34.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:34.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:34.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:35.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:35.466: INFO: Pod daemon-set-vr98d is not available
Aug  6 16:57:35.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:35.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:35.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:36.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:36.464: INFO: Pod daemon-set-vr98d is not available
Aug  6 16:57:36.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:36.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:36.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:37.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:37.464: INFO: Pod daemon-set-vr98d is not available
Aug  6 16:57:37.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:37.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:37.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:38.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:38.464: INFO: Pod daemon-set-vr98d is not available
Aug  6 16:57:38.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:38.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:38.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:39.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:39.465: INFO: Pod daemon-set-vr98d is not available
Aug  6 16:57:39.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:39.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:39.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:40.466: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:40.466: INFO: Pod daemon-set-vr98d is not available
Aug  6 16:57:40.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:40.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:40.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:41.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:41.466: INFO: Pod daemon-set-vr98d is not available
Aug  6 16:57:41.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:41.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:41.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:42.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:42.464: INFO: Pod daemon-set-vr98d is not available
Aug  6 16:57:42.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:42.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:42.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:43.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:43.465: INFO: Pod daemon-set-vr98d is not available
Aug  6 16:57:43.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:43.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:43.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:44.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:44.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:44.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:44.509: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:45.466: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:45.466: INFO: Pod daemon-set-qprqj is not available
Aug  6 16:57:45.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:45.509: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:45.510: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:46.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:46.465: INFO: Pod daemon-set-qprqj is not available
Aug  6 16:57:46.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:46.510: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:46.510: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:47.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:47.464: INFO: Pod daemon-set-qprqj is not available
Aug  6 16:57:47.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:47.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:47.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:48.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:48.464: INFO: Pod daemon-set-qprqj is not available
Aug  6 16:57:48.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:48.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:48.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:49.465: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:49.465: INFO: Pod daemon-set-qprqj is not available
Aug  6 16:57:49.511: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:49.511: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:49.511: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:50.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:50.464: INFO: Pod daemon-set-qprqj is not available
Aug  6 16:57:50.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:50.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:50.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:51.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:51.464: INFO: Pod daemon-set-qprqj is not available
Aug  6 16:57:51.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:51.508: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:51.508: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:52.464: INFO: Wrong image for pod: daemon-set-qprqj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  6 16:57:52.464: INFO: Pod daemon-set-qprqj is not available
Aug  6 16:57:52.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:52.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:52.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:53.464: INFO: Pod daemon-set-t8k9l is not available
Aug  6 16:57:53.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:53.507: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:53.507: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Aug  6 16:57:53.550: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:53.550: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:53.550: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:53.573: INFO: Number of nodes with available pods: 3
Aug  6 16:57:53.573: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:57:54.637: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:54.637: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:54.637: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:54.660: INFO: Number of nodes with available pods: 3
Aug  6 16:57:54.660: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:57:55.637: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:55.637: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:55.637: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:55.659: INFO: Number of nodes with available pods: 3
Aug  6 16:57:55.659: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:57:56.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:56.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:56.637: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:56.660: INFO: Number of nodes with available pods: 3
Aug  6 16:57:56.660: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:57:57.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:57.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:57.636: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:57.659: INFO: Number of nodes with available pods: 3
Aug  6 16:57:57.659: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:57:58.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:58.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:58.636: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:58.659: INFO: Number of nodes with available pods: 3
Aug  6 16:57:58.659: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:57:59.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:59.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:59.636: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:57:59.659: INFO: Number of nodes with available pods: 3
Aug  6 16:57:59.659: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:58:00.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:00.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:00.636: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:00.659: INFO: Number of nodes with available pods: 3
Aug  6 16:58:00.659: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:58:01.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:01.636: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:01.636: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:01.660: INFO: Number of nodes with available pods: 3
Aug  6 16:58:01.660: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:58:02.637: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:02.637: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:02.637: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:02.659: INFO: Number of nodes with available pods: 3
Aug  6 16:58:02.659: INFO: Node ip-10-0-128-44.ec2.internal is running more than one daemon pod
Aug  6 16:58:03.640: INFO: DaemonSet pods can't tolerate node ip-10-0-128-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:03.640: INFO: DaemonSet pods can't tolerate node ip-10-0-128-5.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:03.640: INFO: DaemonSet pods can't tolerate node ip-10-0-132-157.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug  6 16:58:03.663: INFO: Number of nodes with available pods: 4
Aug  6 16:58:03.663: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-4gwc6, will wait for the garbage collector to delete the pods
Aug  6 16:58:03.877: INFO: Deleting DaemonSet.extensions daemon-set took: 26.40077ms
Aug  6 16:58:03.977: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.235797ms
Aug  6 16:58:13.399: INFO: Number of nodes with available pods: 0
Aug  6 16:58:13.399: INFO: Number of running nodes: 0, number of available pods: 0
Aug  6 16:58:13.422: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-4gwc6/daemonsets","resourceVersion":"160966"},"items":null}

Aug  6 16:58:13.444: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-4gwc6/pods","resourceVersion":"160966"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:58:13.596: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-4gwc6" for this suite.
Aug  6 16:58:19.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:58:20.643: INFO: namespace: e2e-tests-daemonsets-4gwc6, resource: bindings, ignored listing per whitelist
Aug  6 16:58:21.591: INFO: namespace: e2e-tests-daemonsets-4gwc6, resource: packagemanifests, items remaining: 3
Aug  6 16:58:22.123: INFO: namespace: e2e-tests-daemonsets-4gwc6 no longer exists
Aug  6 16:58:22.145: INFO: namespace: e2e-tests-daemonsets-4gwc6, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:58:22.168: INFO: namespace e2e-tests-daemonsets-4gwc6 deletion completed in 8.548672384s

• [SLOW TEST:105.688 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:58:22.169: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug  6 16:58:23.653: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-a,UID:594acd5b-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161177,Generation:0,CreationTimestamp:2019-08-06 16:58:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug  6 16:58:23.653: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-a,UID:594acd5b-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161177,Generation:0,CreationTimestamp:2019-08-06 16:58:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug  6 16:58:33.699: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-a,UID:594acd5b-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161271,Generation:0,CreationTimestamp:2019-08-06 16:58:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug  6 16:58:33.699: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-a,UID:594acd5b-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161271,Generation:0,CreationTimestamp:2019-08-06 16:58:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug  6 16:58:43.746: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-a,UID:594acd5b-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161359,Generation:0,CreationTimestamp:2019-08-06 16:58:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug  6 16:58:43.746: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-a,UID:594acd5b-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161359,Generation:0,CreationTimestamp:2019-08-06 16:58:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug  6 16:58:53.772: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-a,UID:594acd5b-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161447,Generation:0,CreationTimestamp:2019-08-06 16:58:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug  6 16:58:53.772: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-a,UID:594acd5b-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161447,Generation:0,CreationTimestamp:2019-08-06 16:58:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug  6 16:59:03.798: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-b,UID:713807fb-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161502,Generation:0,CreationTimestamp:2019-08-06 16:58:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug  6 16:59:03.798: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-b,UID:713807fb-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161502,Generation:0,CreationTimestamp:2019-08-06 16:58:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug  6 16:59:13.824: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-b,UID:713807fb-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161551,Generation:0,CreationTimestamp:2019-08-06 16:58:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug  6 16:59:13.824: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-ph597,SelfLink:/api/v1/namespaces/e2e-tests-watch-ph597/configmaps/e2e-watch-test-configmap-b,UID:713807fb-b86b-11e9-9dff-0e6de702691c,ResourceVersion:161551,Generation:0,CreationTimestamp:2019-08-06 16:58:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:59:23.825: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-ph597" for this suite.
Aug  6 16:59:29.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:59:30.609: INFO: namespace: e2e-tests-watch-ph597, resource: packagemanifests, items remaining: 3
Aug  6 16:59:32.170: INFO: namespace: e2e-tests-watch-ph597, resource: bindings, ignored listing per whitelist
Aug  6 16:59:32.366: INFO: namespace: e2e-tests-watch-ph597 no longer exists
Aug  6 16:59:32.389: INFO: namespace: e2e-tests-watch-ph597, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:59:32.411: INFO: namespace e2e-tests-watch-ph597 deletion completed in 8.523197561s

• [SLOW TEST:70.243 seconds]
[sig-api-machinery] Watchers
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Aug  6 16:59:32.412: INFO: >>> kubeConfig: /home/jeder/osd_41_conformance/kubeconfig.je-cncf-conformance
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-908e4bff-b86b-11e9-8d18-525400524259
STEP: Creating a pod to test consume secrets
Aug  6 16:59:33.836: INFO: Waiting up to 5m0s for pod "pod-secrets-9091e248-b86b-11e9-8d18-525400524259" in namespace "e2e-tests-secrets-p9hsn" to be "success or failure"
Aug  6 16:59:33.859: INFO: Pod "pod-secrets-9091e248-b86b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 23.262483ms
Aug  6 16:59:35.882: INFO: Pod "pod-secrets-9091e248-b86b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046760137s
Aug  6 16:59:37.906: INFO: Pod "pod-secrets-9091e248-b86b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070031012s
Aug  6 16:59:39.929: INFO: Pod "pod-secrets-9091e248-b86b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09307177s
Aug  6 16:59:41.951: INFO: Pod "pod-secrets-9091e248-b86b-11e9-8d18-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115752184s
Aug  6 16:59:43.975: INFO: Pod "pod-secrets-9091e248-b86b-11e9-8d18-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.138928382s
STEP: Saw pod success
Aug  6 16:59:43.975: INFO: Pod "pod-secrets-9091e248-b86b-11e9-8d18-525400524259" satisfied condition "success or failure"
Aug  6 16:59:43.997: INFO: Trying to get logs from node ip-10-0-130-134.ec2.internal pod pod-secrets-9091e248-b86b-11e9-8d18-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Aug  6 16:59:44.054: INFO: Waiting for pod pod-secrets-9091e248-b86b-11e9-8d18-525400524259 to disappear
Aug  6 16:59:44.076: INFO: Pod pod-secrets-9091e248-b86b-11e9-8d18-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Aug  6 16:59:44.076: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-p9hsn" for this suite.
Aug  6 16:59:50.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  6 16:59:51.752: INFO: namespace: e2e-tests-secrets-p9hsn, resource: packagemanifests, items remaining: 3
Aug  6 16:59:52.195: INFO: namespace: e2e-tests-secrets-p9hsn, resource: bindings, ignored listing per whitelist
Aug  6 16:59:52.640: INFO: namespace: e2e-tests-secrets-p9hsn no longer exists
Aug  6 16:59:52.663: INFO: namespace: e2e-tests-secrets-p9hsn, total namespaces: 50, active: 50, terminating: 0
Aug  6 16:59:52.686: INFO: namespace e2e-tests-secrets-p9hsn deletion completed in 8.546682328s

• [SLOW TEST:20.274 seconds]
[sig-storage] Secrets
/home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_41_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSAug  6 16:59:52.686: INFO: Running AfterSuite actions on all nodes
Aug  6 16:59:52.686: INFO: Running AfterSuite actions on node 1
Aug  6 16:59:52.687: INFO: Dumping logs locally to: /home/jeder/osd_41_conformance/origin/_output/scripts/conformance-k8s.sh.1.13/artifacts
Aug  6 16:59:52.687: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

Ran 199 of 2162 Specs in 7731.507 seconds
SUCCESS! -- 199 Passed | 0 Failed | 0 Pending | 1963 Skipped PASS
