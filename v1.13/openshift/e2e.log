May  2 02:08:23.394: INFO: The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
I0502 02:08:23.394343   12606 e2e.go:224] Starting e2e run "29c5aa06-6c7f-11e9-97f0-0a58ac103caa" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1556762902 - Will randomize all specs
Will run 201 of 2162 specs

May  2 02:08:23.455: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 02:08:23.459: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
May  2 02:08:23.539: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May  2 02:08:23.599: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May  2 02:08:23.599: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
May  2 02:08:23.599: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May  2 02:08:23.619: INFO: e2e test version: v1.13.6-beta.0.64+300fedeffa6d25
May  2 02:08:23.633: INFO: kube-apiserver version: v1.13.4+b1e7641
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:08:23.633: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
May  2 02:08:24.613: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-dn96
STEP: Creating a pod to test atomic-volume-subpath
May  2 02:08:24.708: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dn96" in namespace "e2e-tests-subpath-bg6ld" to be "success or failure"
May  2 02:08:24.723: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Pending", Reason="", readiness=false. Elapsed: 15.524705ms
May  2 02:08:26.748: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040208853s
May  2 02:08:28.771: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06284603s
May  2 02:08:30.789: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080839171s
May  2 02:08:32.805: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097482394s
May  2 02:08:34.824: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Pending", Reason="", readiness=false. Elapsed: 10.116249145s
May  2 02:08:36.842: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Pending", Reason="", readiness=false. Elapsed: 12.134470861s
May  2 02:08:38.872: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Pending", Reason="", readiness=false. Elapsed: 14.163866277s
May  2 02:08:40.888: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Running", Reason="", readiness=false. Elapsed: 16.180024957s
May  2 02:08:42.904: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Running", Reason="", readiness=false. Elapsed: 18.196425806s
May  2 02:08:44.920: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Running", Reason="", readiness=false. Elapsed: 20.212700202s
May  2 02:08:46.937: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Running", Reason="", readiness=false. Elapsed: 22.229447439s
May  2 02:08:48.953: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Running", Reason="", readiness=false. Elapsed: 24.245191475s
May  2 02:08:50.969: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Running", Reason="", readiness=false. Elapsed: 26.261104713s
May  2 02:08:52.985: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Running", Reason="", readiness=false. Elapsed: 28.277214942s
May  2 02:08:55.001: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Running", Reason="", readiness=false. Elapsed: 30.293682234s
May  2 02:08:57.018: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Running", Reason="", readiness=false. Elapsed: 32.309876128s
May  2 02:08:59.034: INFO: Pod "pod-subpath-test-downwardapi-dn96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.325784124s
STEP: Saw pod success
May  2 02:08:59.034: INFO: Pod "pod-subpath-test-downwardapi-dn96" satisfied condition "success or failure"
May  2 02:08:59.049: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-subpath-test-downwardapi-dn96 container test-container-subpath-downwardapi-dn96: <nil>
STEP: delete the pod
May  2 02:08:59.119: INFO: Waiting for pod pod-subpath-test-downwardapi-dn96 to disappear
May  2 02:08:59.135: INFO: Pod pod-subpath-test-downwardapi-dn96 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-dn96
May  2 02:08:59.135: INFO: Deleting pod "pod-subpath-test-downwardapi-dn96" in namespace "e2e-tests-subpath-bg6ld"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:08:59.150: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-bg6ld" for this suite.
May  2 02:09:05.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:09:05.910: INFO: namespace: e2e-tests-subpath-bg6ld, resource: packagemanifests, items remaining: 1
May  2 02:09:07.078: INFO: namespace: e2e-tests-subpath-bg6ld, resource: bindings, ignored listing per whitelist
May  2 02:09:07.263: INFO: namespace: e2e-tests-subpath-bg6ld no longer exists
May  2 02:09:07.281: INFO: namespace: e2e-tests-subpath-bg6ld, total namespaces: 47, active: 47, terminating: 0
May  2 02:09:07.296: INFO: namespace e2e-tests-subpath-bg6ld deletion completed in 8.116485066s

â€¢ [SLOW TEST:43.663 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:09:07.296: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 02:09:08.221: INFO: Creating deployment "nginx-deployment"
May  2 02:09:08.239: INFO: Waiting for observed generation 1
May  2 02:09:10.275: INFO: Waiting for all required pods to come up
May  2 02:09:10.305: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May  2 02:09:20.352: INFO: Waiting for deployment "nginx-deployment" to complete
May  2 02:09:20.384: INFO: Updating deployment "nginx-deployment" with a non-existent image
May  2 02:09:20.434: INFO: Updating deployment nginx-deployment
May  2 02:09:20.434: INFO: Waiting for observed generation 2
May  2 02:09:22.468: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May  2 02:09:22.484: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May  2 02:09:22.501: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May  2 02:09:22.552: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May  2 02:09:22.552: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May  2 02:09:22.567: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May  2 02:09:22.600: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May  2 02:09:22.600: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May  2 02:09:22.635: INFO: Updating deployment nginx-deployment
May  2 02:09:22.635: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May  2 02:09:22.669: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May  2 02:09:24.703: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May  2 02:09:24.737: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-68z5s,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-68z5s/deployments/nginx-deployment,UID:44caae0e-6c7f-11e9-9e44-12f3365d453a,ResourceVersion:19133,Generation:3,CreationTimestamp:2019-05-02 02:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-05-02 02:09:22 +0000 UTC 2019-05-02 02:09:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-02 02:09:22 +0000 UTC 2019-05-02 02:09:08 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

May  2 02:09:24.755: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-68z5s,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-68z5s/replicasets/nginx-deployment-65bbdb5f8,UID:4c0fea9f-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19132,Generation:3,CreationTimestamp:2019-05-02 02:09:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 44caae0e-6c7f-11e9-9e44-12f3365d453a 0xc001975327 0xc001975328}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  2 02:09:24.755: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May  2 02:09:24.755: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-68z5s,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-68z5s/replicasets/nginx-deployment-555b55d965,UID:44cc8ed6-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19088,Generation:3,CreationTimestamp:2019-05-02 02:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 44caae0e-6c7f-11e9-9e44-12f3365d453a 0xc001975267 0xc001975268}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May  2 02:09:24.799: INFO: Pod "nginx-deployment-555b55d965-24h5x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-24h5x,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-24h5x,UID:4d6b0f8d-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19090,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001975c67 0xc001975c68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-140-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001975cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001975cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.140.17,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.799: INFO: Pod "nginx-deployment-555b55d965-2dfcq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2dfcq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-2dfcq,UID:44d38ed7-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:18946,Generation:0,CreationTimestamp:2019-05-02 02:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.11"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001975dc0 0xc001975dc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001975e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001975e40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:10.129.2.11,StartTime:2019-05-02 02:09:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-02 02:09:19 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://a7cd993a37a23ce6a4abdc891d487482ea2015ea0211e50d58e268a9524f44dd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.799: INFO: Pod "nginx-deployment-555b55d965-4qxcd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4qxcd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-4qxcd,UID:4d6b9ec3-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19105,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001975f20 0xc001975f21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001975f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001975fa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.799: INFO: Pod "nginx-deployment-555b55d965-5b9lv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5b9lv,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-5b9lv,UID:44d35fc2-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:18962,Generation:0,CreationTimestamp:2019-05-02 02:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.11"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001290077 0xc001290078}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-140-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012900e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001290100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.140.17,PodIP:10.128.2.11,StartTime:2019-05-02 02:09:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-02 02:09:19 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://b4ee46e32335f16c49bb360df67f9a9a49f33c70c413791b1960af4507df5a48}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.799: INFO: Pod "nginx-deployment-555b55d965-5x8q8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5x8q8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-5x8q8,UID:4d671bd7-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19082,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc0012901e0 0xc0012901e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001290240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001290260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.800: INFO: Pod "nginx-deployment-555b55d965-7wn9h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-7wn9h,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-7wn9h,UID:44d10d7f-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:18929,Generation:0,CreationTimestamp:2019-05-02 02:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.13"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001290337 0xc001290338}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012903a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012903c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:10.131.0.13,StartTime:2019-05-02 02:09:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-02 02:09:18 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://00cdead5cf70258c6e33f63a1d463c13f383e7b63f3daa5c01221e91eb51bbbf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.800: INFO: Pod "nginx-deployment-555b55d965-9dffj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9dffj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-9dffj,UID:4d66cb39-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19071,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc0012904a0 0xc0012904a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-140-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001290500} {node.kubernetes.io/not-ready Exists  NoExecute 0xc001290520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.140.17,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.800: INFO: Pod "nginx-deployment-555b55d965-9sbv5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9sbv5,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-9sbv5,UID:4d6c1a78-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19102,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc0012905f0 0xc0012905f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-140-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001290650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001290670}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.140.17,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.800: INFO: Pod "nginx-deployment-555b55d965-dc5p2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dc5p2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-dc5p2,UID:44d60844-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:18943,Generation:0,CreationTimestamp:2019-05-02 02:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.14"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001290740 0xc001290741}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012907a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012907c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:10.129.2.14,StartTime:2019-05-02 02:09:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-02 02:09:19 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://1cba312a142efe8ad75466a06154eebd9222cc9e926e24c092991f2ca83f52f6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.800: INFO: Pod "nginx-deployment-555b55d965-ddvxh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ddvxh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-ddvxh,UID:4d6b460e-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19136,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc0012908a0 0xc0012908a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001290900} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001290920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.800: INFO: Pod "nginx-deployment-555b55d965-f69x8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-f69x8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-f69x8,UID:44d9fb10-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:18932,Generation:0,CreationTimestamp:2019-05-02 02:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.15"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc0012909f0 0xc0012909f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001290a50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001290a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:10.131.0.15,StartTime:2019-05-02 02:09:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-02 02:09:18 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://bac28cc16fbb92beb28033694c21c1e78cc907a4e36648d7a893bcffcb538e81}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.800: INFO: Pod "nginx-deployment-555b55d965-md4hr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-md4hr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-md4hr,UID:4d6726a9-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19091,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001290b50 0xc001290b51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001290bb0} {node.kubernetes.io/not-ready Exists  NoExecute 0xc001290bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.800: INFO: Pod "nginx-deployment-555b55d965-mv5xq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mv5xq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-mv5xq,UID:4d6b59ef-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19099,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001290ca0 0xc001290ca1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001290d00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001290d20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.800: INFO: Pod "nginx-deployment-555b55d965-scshj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-scshj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-scshj,UID:4d6358e0-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19057,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001290df0 0xc001290df1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001290e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001290e70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.801: INFO: Pod "nginx-deployment-555b55d965-t8r54" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-t8r54,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-t8r54,UID:4d615b1d-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19045,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001290f40 0xc001290f41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-140-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001290fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001290fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.140.17,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.801: INFO: Pod "nginx-deployment-555b55d965-vk62c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-vk62c,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-vk62c,UID:44d62276-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:18937,Generation:0,CreationTimestamp:2019-05-02 02:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.14"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001291090 0xc001291091}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012910f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001291110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:10.131.0.14,StartTime:2019-05-02 02:09:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-02 02:09:18 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://e463926806d325538b03f01f8352fd7f8288cab366965056d441cf4c98c0c615}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.801: INFO: Pod "nginx-deployment-555b55d965-w9f4c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-w9f4c,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-w9f4c,UID:44d66a7e-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:18952,Generation:0,CreationTimestamp:2019-05-02 02:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.12"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc0012911f0 0xc0012911f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001291250} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001291270}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:10.129.2.12,StartTime:2019-05-02 02:09:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-02 02:09:19 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://b98bd501bb124d354478ce4f3a5b862a167bc07e9b5023968820e06b75232075}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.801: INFO: Pod "nginx-deployment-555b55d965-x7t8t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-x7t8t,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-x7t8t,UID:4d63b73f-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19061,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001291350 0xc001291351}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-140-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001291450} {node.kubernetes.io/not-ready Exists  NoExecute 0xc001291470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.140.17,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.801: INFO: Pod "nginx-deployment-555b55d965-xbxrf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-xbxrf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-xbxrf,UID:4d675b31-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19097,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001291540 0xc001291541}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012915a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012915c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.801: INFO: Pod "nginx-deployment-555b55d965-zbbj4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zbbj4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-555b55d965-zbbj4,UID:44da5e04-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:18949,Generation:0,CreationTimestamp:2019-05-02 02:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.13"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 44cc8ed6-6c7f-11e9-8dad-121ea440cb2c 0xc001291697 0xc001291698}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001291700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001291720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:10.129.2.13,StartTime:2019-05-02 02:09:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-02 02:09:19 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://e05e615add79cf2dab7a9988123978c73f0501bbde54ccafdeb184e5e1165a79}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.801: INFO: Pod "nginx-deployment-65bbdb5f8-4fv8b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-4fv8b,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-4fv8b,UID:4c22f939-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19014,Generation:0,CreationTimestamp:2019-05-02 02:09:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc001291800 0xc001291801}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001291870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001291890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:,StartTime:2019-05-02 02:09:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.801: INFO: Pod "nginx-deployment-65bbdb5f8-7cl8x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-7cl8x,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-7cl8x,UID:4c18c7b2-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:18999,Generation:0,CreationTimestamp:2019-05-02 02:09:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc001291970 0xc001291971}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012919e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001291a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:,StartTime:2019-05-02 02:09:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.801: INFO: Pod "nginx-deployment-65bbdb5f8-7q8sw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-7q8sw,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-7q8sw,UID:4d73cfc4-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19146,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc001291ae0 0xc001291ae1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001291b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001291b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.801: INFO: Pod "nginx-deployment-65bbdb5f8-cntl8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-cntl8,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-cntl8,UID:4d7a1916-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19140,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc001291c50 0xc001291c51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001291cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001291ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.802: INFO: Pod "nginx-deployment-65bbdb5f8-hvmr2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-hvmr2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-hvmr2,UID:4c193499-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19000,Generation:0,CreationTimestamp:2019-05-02 02:09:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc001291dc0 0xc001291dc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-140-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001291e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001291e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  }],Message:,Reason:,HostIP:10.0.140.17,PodIP:,StartTime:2019-05-02 02:09:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.802: INFO: Pod "nginx-deployment-65bbdb5f8-jznp2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-jznp2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-jznp2,UID:4c1f0d71-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19006,Generation:0,CreationTimestamp:2019-05-02 02:09:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc001291f30 0xc001291f31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001291fa0} {node.kubernetes.io/not-ready Exists  NoExecute 0xc001291fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:,StartTime:2019-05-02 02:09:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.802: INFO: Pod "nginx-deployment-65bbdb5f8-lh995" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-lh995,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-lh995,UID:4d74007c-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19155,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc0016480b0 0xc0016480b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-140-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001648130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001648150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.140.17,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.802: INFO: Pod "nginx-deployment-65bbdb5f8-mbqrv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-mbqrv,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-mbqrv,UID:4d63be98-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19070,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc001648230 0xc001648231}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016482a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016482c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.802: INFO: Pod "nginx-deployment-65bbdb5f8-njbz2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-njbz2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-njbz2,UID:4d6ab1f8-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19113,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc0016483a0 0xc0016483a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001648410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001648430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.802: INFO: Pod "nginx-deployment-65bbdb5f8-pfp89" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-pfp89,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-pfp89,UID:4d73deb3-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19154,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc001648520 0xc001648521}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-153-32.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016485a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016485c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.153.32,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.802: INFO: Pod "nginx-deployment-65bbdb5f8-q6tf4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-q6tf4,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-q6tf4,UID:4d73cd13-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19149,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc0016486b0 0xc0016486b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-140-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001648720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001648740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.140.17,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.802: INFO: Pod "nginx-deployment-65bbdb5f8-rvwd7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rvwd7,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-rvwd7,UID:4d6b4947-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:19143,Generation:0,CreationTimestamp:2019-05-02 02:09:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc001648820 0xc001648821}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-140-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001648890} {node.kubernetes.io/not-ready Exists  NoExecute 0xc0016488b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.140.17,PodIP:,StartTime:2019-05-02 02:09:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  2 02:09:24.802: INFO: Pod "nginx-deployment-65bbdb5f8-zb9jv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-zb9jv,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-68z5s,SelfLink:/api/v1/namespaces/e2e-tests-deployment-68z5s/pods/nginx-deployment-65bbdb5f8-zb9jv,UID:4c12e8b7-6c7f-11e9-8dad-121ea440cb2c,ResourceVersion:18989,Generation:0,CreationTimestamp:2019-05-02 02:09:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4c0fea9f-6c7f-11e9-8dad-121ea440cb2c 0xc001648990 0xc001648991}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r9hq8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9hq8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r9hq8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-8tcvf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001648a00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001648a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:09:20 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:,StartTime:2019-05-02 02:09:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:09:24.802: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-68z5s" for this suite.
May  2 02:09:32.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:09:33.451: INFO: namespace: e2e-tests-deployment-68z5s, resource: bindings, ignored listing per whitelist
May  2 02:09:34.220: INFO: namespace: e2e-tests-deployment-68z5s, resource: packagemanifests, items remaining: 1
May  2 02:09:34.901: INFO: namespace: e2e-tests-deployment-68z5s no longer exists
May  2 02:09:34.918: INFO: namespace: e2e-tests-deployment-68z5s, total namespaces: 47, active: 47, terminating: 0
May  2 02:09:34.934: INFO: namespace e2e-tests-deployment-68z5s deletion completed in 10.115294679s

â€¢ [SLOW TEST:27.638 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:09:34.934: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-55439c92-6c7f-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 02:09:35.909: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-554646b3-6c7f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-8knwd" to be "success or failure"
May  2 02:09:35.925: INFO: Pod "pod-projected-secrets-554646b3-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.019427ms
May  2 02:09:37.941: INFO: Pod "pod-projected-secrets-554646b3-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031379476s
May  2 02:09:39.957: INFO: Pod "pod-projected-secrets-554646b3-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047146349s
May  2 02:09:41.973: INFO: Pod "pod-projected-secrets-554646b3-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063230613s
May  2 02:09:43.994: INFO: Pod "pod-projected-secrets-554646b3-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08484093s
May  2 02:09:46.010: INFO: Pod "pod-projected-secrets-554646b3-6c7f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.100728083s
STEP: Saw pod success
May  2 02:09:46.010: INFO: Pod "pod-projected-secrets-554646b3-6c7f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:09:46.026: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-secrets-554646b3-6c7f-11e9-97f0-0a58ac103caa container projected-secret-volume-test: <nil>
STEP: delete the pod
May  2 02:09:46.068: INFO: Waiting for pod pod-projected-secrets-554646b3-6c7f-11e9-97f0-0a58ac103caa to disappear
May  2 02:09:46.083: INFO: Pod pod-projected-secrets-554646b3-6c7f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:09:46.083: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8knwd" for this suite.
May  2 02:09:52.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:09:52.712: INFO: namespace: e2e-tests-projected-8knwd, resource: bindings, ignored listing per whitelist
May  2 02:09:53.838: INFO: namespace: e2e-tests-projected-8knwd, resource: packagemanifests, items remaining: 1
May  2 02:09:54.194: INFO: namespace: e2e-tests-projected-8knwd no longer exists
May  2 02:09:54.211: INFO: namespace: e2e-tests-projected-8knwd, total namespaces: 47, active: 47, terminating: 0
May  2 02:09:54.226: INFO: namespace e2e-tests-projected-8knwd deletion completed in 8.113814635s

â€¢ [SLOW TEST:19.292 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:09:54.227: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-60c24c3a-6c7f-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 02:09:55.195: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-kt44c" to be "success or failure"
May  2 02:09:55.210: INFO: Pod "pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.377736ms
May  2 02:09:57.226: INFO: Pod "pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03151682s
May  2 02:09:59.243: INFO: Pod "pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047592033s
May  2 02:10:01.259: INFO: Pod "pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063640441s
May  2 02:10:03.276: INFO: Pod "pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081387676s
May  2 02:10:05.292: INFO: Pod "pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.097428661s
May  2 02:10:07.308: INFO: Pod "pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.113367417s
STEP: Saw pod success
May  2 02:10:07.308: INFO: Pod "pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:10:07.323: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa container secret-volume-test: <nil>
STEP: delete the pod
May  2 02:10:07.367: INFO: Waiting for pod pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa to disappear
May  2 02:10:07.382: INFO: Pod pod-projected-secrets-60c53831-6c7f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:10:07.382: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kt44c" for this suite.
May  2 02:10:13.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:10:14.175: INFO: namespace: e2e-tests-projected-kt44c, resource: packagemanifests, items remaining: 1
May  2 02:10:14.780: INFO: namespace: e2e-tests-projected-kt44c, resource: bindings, ignored listing per whitelist
May  2 02:10:15.491: INFO: namespace: e2e-tests-projected-kt44c no longer exists
May  2 02:10:15.508: INFO: namespace: e2e-tests-projected-kt44c, total namespaces: 47, active: 47, terminating: 0
May  2 02:10:15.524: INFO: namespace e2e-tests-projected-kt44c deletion completed in 8.112419902s

â€¢ [SLOW TEST:21.297 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:10:15.524: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May  2 02:10:16.445: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-788f2'
May  2 02:10:16.641: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  2 02:10:16.641: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
May  2 02:10:16.659: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-788f2'
May  2 02:10:16.823: INFO: stderr: ""
May  2 02:10:16.823: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:10:16.824: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-788f2" for this suite.
May  2 02:10:38.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:10:39.709: INFO: namespace: e2e-tests-kubectl-788f2, resource: bindings, ignored listing per whitelist
May  2 02:10:39.761: INFO: namespace: e2e-tests-kubectl-788f2, resource: packagemanifests, items remaining: 1
May  2 02:10:40.923: INFO: namespace: e2e-tests-kubectl-788f2 no longer exists
May  2 02:10:40.940: INFO: namespace: e2e-tests-kubectl-788f2, total namespaces: 47, active: 47, terminating: 0
May  2 02:10:40.955: INFO: namespace e2e-tests-kubectl-788f2 deletion completed in 24.114962255s

â€¢ [SLOW TEST:25.431 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:10:40.955: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 02:10:41.878: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:10:52.048: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-hp9qg" for this suite.
May  2 02:11:40.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:11:40.679: INFO: namespace: e2e-tests-pods-hp9qg, resource: bindings, ignored listing per whitelist
May  2 02:11:41.458: INFO: namespace: e2e-tests-pods-hp9qg, resource: packagemanifests, items remaining: 1
May  2 02:11:42.160: INFO: namespace: e2e-tests-pods-hp9qg no longer exists
May  2 02:11:42.177: INFO: namespace: e2e-tests-pods-hp9qg, total namespaces: 47, active: 47, terminating: 0
May  2 02:11:42.192: INFO: namespace e2e-tests-pods-hp9qg deletion completed in 50.114361974s

â€¢ [SLOW TEST:61.237 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:11:42.193: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
May  2 02:11:43.145: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-k9tc6" to be "success or failure"
May  2 02:11:43.160: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 15.384152ms
May  2 02:11:45.177: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031764657s
May  2 02:11:47.658: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.513477568s
May  2 02:11:49.676: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.530853373s
May  2 02:11:51.692: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.547616952s
May  2 02:11:53.709: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.563836171s
STEP: Saw pod success
May  2 02:11:53.709: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May  2 02:11:53.724: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May  2 02:11:53.766: INFO: Waiting for pod pod-host-path-test to disappear
May  2 02:11:53.783: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:11:53.783: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-k9tc6" for this suite.
May  2 02:11:59.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:12:01.402: INFO: namespace: e2e-tests-hostpath-k9tc6, resource: packagemanifests, items remaining: 1
May  2 02:12:01.731: INFO: namespace: e2e-tests-hostpath-k9tc6, resource: bindings, ignored listing per whitelist
May  2 02:12:01.895: INFO: namespace: e2e-tests-hostpath-k9tc6 no longer exists
May  2 02:12:01.912: INFO: namespace: e2e-tests-hostpath-k9tc6, total namespaces: 47, active: 47, terminating: 0
May  2 02:12:01.927: INFO: namespace e2e-tests-hostpath-k9tc6 deletion completed in 8.114202286s

â€¢ [SLOW TEST:19.735 seconds]
[sig-storage] HostPath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:12:01.928: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
May  2 02:12:02.899: INFO: Waiting up to 5m0s for pod "pod-ace34323-6c7f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-2cqfm" to be "success or failure"
May  2 02:12:02.915: INFO: Pod "pod-ace34323-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.177509ms
May  2 02:12:04.931: INFO: Pod "pod-ace34323-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031338551s
May  2 02:12:06.947: INFO: Pod "pod-ace34323-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047626161s
May  2 02:12:08.963: INFO: Pod "pod-ace34323-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063951942s
May  2 02:12:10.979: INFO: Pod "pod-ace34323-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.079963489s
May  2 02:12:12.995: INFO: Pod "pod-ace34323-6c7f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.096022666s
May  2 02:12:15.012: INFO: Pod "pod-ace34323-6c7f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.112159605s
STEP: Saw pod success
May  2 02:12:15.012: INFO: Pod "pod-ace34323-6c7f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:12:15.029: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-ace34323-6c7f-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 02:12:15.070: INFO: Waiting for pod pod-ace34323-6c7f-11e9-97f0-0a58ac103caa to disappear
May  2 02:12:15.086: INFO: Pod pod-ace34323-6c7f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:12:15.086: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2cqfm" for this suite.
May  2 02:12:21.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:12:21.764: INFO: namespace: e2e-tests-emptydir-2cqfm, resource: bindings, ignored listing per whitelist
May  2 02:12:22.305: INFO: namespace: e2e-tests-emptydir-2cqfm, resource: packagemanifests, items remaining: 1
May  2 02:12:23.197: INFO: namespace: e2e-tests-emptydir-2cqfm no longer exists
May  2 02:12:23.214: INFO: namespace: e2e-tests-emptydir-2cqfm, total namespaces: 47, active: 47, terminating: 0
May  2 02:12:23.229: INFO: namespace e2e-tests-emptydir-2cqfm deletion completed in 8.113483144s

â€¢ [SLOW TEST:21.301 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:12:23.229: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-zsnp
STEP: Creating a pod to test atomic-volume-subpath
May  2 02:12:24.239: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zsnp" in namespace "e2e-tests-subpath-2ghzc" to be "success or failure"
May  2 02:12:24.255: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Pending", Reason="", readiness=false. Elapsed: 15.540427ms
May  2 02:12:26.277: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038479137s
May  2 02:12:28.294: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054769702s
May  2 02:12:30.310: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070692767s
May  2 02:12:32.327: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087948839s
May  2 02:12:34.345: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Pending", Reason="", readiness=false. Elapsed: 10.10570037s
May  2 02:12:36.365: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Running", Reason="", readiness=false. Elapsed: 12.125647257s
May  2 02:12:38.389: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Running", Reason="", readiness=false. Elapsed: 14.149981663s
May  2 02:12:40.413: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Running", Reason="", readiness=false. Elapsed: 16.173641407s
May  2 02:12:42.434: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Running", Reason="", readiness=false. Elapsed: 18.19463689s
May  2 02:12:44.464: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Running", Reason="", readiness=false. Elapsed: 20.224923938s
May  2 02:12:46.480: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Running", Reason="", readiness=false. Elapsed: 22.240732597s
May  2 02:12:48.496: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Running", Reason="", readiness=false. Elapsed: 24.257325431s
May  2 02:12:50.512: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Running", Reason="", readiness=false. Elapsed: 26.273176619s
May  2 02:12:52.528: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Running", Reason="", readiness=false. Elapsed: 28.289302624s
May  2 02:12:54.544: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Running", Reason="", readiness=false. Elapsed: 30.305215066s
May  2 02:12:56.560: INFO: Pod "pod-subpath-test-projected-zsnp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.321112457s
STEP: Saw pod success
May  2 02:12:56.560: INFO: Pod "pod-subpath-test-projected-zsnp" satisfied condition "success or failure"
May  2 02:12:56.575: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-subpath-test-projected-zsnp container test-container-subpath-projected-zsnp: <nil>
STEP: delete the pod
May  2 02:12:56.618: INFO: Waiting for pod pod-subpath-test-projected-zsnp to disappear
May  2 02:12:56.633: INFO: Pod pod-subpath-test-projected-zsnp no longer exists
STEP: Deleting pod pod-subpath-test-projected-zsnp
May  2 02:12:56.633: INFO: Deleting pod "pod-subpath-test-projected-zsnp" in namespace "e2e-tests-subpath-2ghzc"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:12:56.648: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-2ghzc" for this suite.
May  2 02:13:02.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:13:03.497: INFO: namespace: e2e-tests-subpath-2ghzc, resource: bindings, ignored listing per whitelist
May  2 02:13:03.611: INFO: namespace: e2e-tests-subpath-2ghzc, resource: packagemanifests, items remaining: 1
May  2 02:13:04.759: INFO: namespace: e2e-tests-subpath-2ghzc no longer exists
May  2 02:13:04.777: INFO: namespace: e2e-tests-subpath-2ghzc, total namespaces: 47, active: 47, terminating: 0
May  2 02:13:04.792: INFO: namespace e2e-tests-subpath-2ghzc deletion completed in 8.115034972s

â€¢ [SLOW TEST:41.563 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:13:04.792: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-khljj
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-khljj
STEP: Deleting pre-stop pod
May  2 02:13:32.888: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:13:32.908: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-khljj" for this suite.
May  2 02:14:12.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:14:13.761: INFO: namespace: e2e-tests-prestop-khljj, resource: packagemanifests, items remaining: 1
May  2 02:14:14.853: INFO: namespace: e2e-tests-prestop-khljj, resource: bindings, ignored listing per whitelist
May  2 02:14:15.020: INFO: namespace: e2e-tests-prestop-khljj no longer exists
May  2 02:14:15.037: INFO: namespace: e2e-tests-prestop-khljj, total namespaces: 47, active: 47, terminating: 0
May  2 02:14:15.052: INFO: namespace e2e-tests-prestop-khljj deletion completed in 42.114947565s

â€¢ [SLOW TEST:70.259 seconds]
[k8s.io] [sig-node] PreStop
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:14:15.052: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
May  2 02:14:16.569: INFO: created pod pod-service-account-defaultsa
May  2 02:14:16.569: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May  2 02:14:16.592: INFO: created pod pod-service-account-mountsa
May  2 02:14:16.592: INFO: pod pod-service-account-mountsa service account token volume mount: true
May  2 02:14:16.614: INFO: created pod pod-service-account-nomountsa
May  2 02:14:16.614: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May  2 02:14:16.638: INFO: created pod pod-service-account-defaultsa-mountspec
May  2 02:14:16.638: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May  2 02:14:16.662: INFO: created pod pod-service-account-mountsa-mountspec
May  2 02:14:16.662: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May  2 02:14:16.684: INFO: created pod pod-service-account-nomountsa-mountspec
May  2 02:14:16.684: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May  2 02:14:16.707: INFO: created pod pod-service-account-defaultsa-nomountspec
May  2 02:14:16.707: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May  2 02:14:16.730: INFO: created pod pod-service-account-mountsa-nomountspec
May  2 02:14:16.730: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May  2 02:14:16.755: INFO: created pod pod-service-account-nomountsa-nomountspec
May  2 02:14:16.755: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:14:16.755: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-l9kgm" for this suite.
May  2 02:14:40.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:14:42.193: INFO: namespace: e2e-tests-svcaccounts-l9kgm, resource: bindings, ignored listing per whitelist
May  2 02:14:42.413: INFO: namespace: e2e-tests-svcaccounts-l9kgm, resource: packagemanifests, items remaining: 1
May  2 02:14:42.867: INFO: namespace: e2e-tests-svcaccounts-l9kgm no longer exists
May  2 02:14:42.884: INFO: namespace: e2e-tests-svcaccounts-l9kgm, total namespaces: 47, active: 47, terminating: 0
May  2 02:14:42.899: INFO: namespace e2e-tests-svcaccounts-l9kgm deletion completed in 26.115506363s

â€¢ [SLOW TEST:27.847 seconds]
[sig-auth] ServiceAccounts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:14:42.900: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 02:14:43.857: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:14:44.540: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-mmqnj" for this suite.
May  2 02:14:50.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:14:51.267: INFO: namespace: e2e-tests-custom-resource-definition-mmqnj, resource: packagemanifests, items remaining: 1
May  2 02:14:51.994: INFO: namespace: e2e-tests-custom-resource-definition-mmqnj, resource: bindings, ignored listing per whitelist
May  2 02:14:52.653: INFO: namespace: e2e-tests-custom-resource-definition-mmqnj no longer exists
May  2 02:14:52.670: INFO: namespace: e2e-tests-custom-resource-definition-mmqnj, total namespaces: 47, active: 47, terminating: 0
May  2 02:14:52.685: INFO: namespace e2e-tests-custom-resource-definition-mmqnj deletion completed in 8.116264576s

â€¢ [SLOW TEST:9.786 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:14:52.685: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May  2 02:14:53.619: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:15:07.517: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-fnjv9" for this suite.
May  2 02:15:29.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:15:30.230: INFO: namespace: e2e-tests-init-container-fnjv9, resource: bindings, ignored listing per whitelist
May  2 02:15:31.616: INFO: namespace: e2e-tests-init-container-fnjv9, resource: packagemanifests, items remaining: 1
May  2 02:15:31.632: INFO: namespace: e2e-tests-init-container-fnjv9 no longer exists
May  2 02:15:31.649: INFO: namespace: e2e-tests-init-container-fnjv9, total namespaces: 47, active: 47, terminating: 0
May  2 02:15:31.664: INFO: namespace e2e-tests-init-container-fnjv9 deletion completed in 24.117662583s

â€¢ [SLOW TEST:38.978 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:15:31.664: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:15:49.745: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-lh9ph" for this suite.
May  2 02:16:13.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:16:14.475: INFO: namespace: e2e-tests-replication-controller-lh9ph, resource: packagemanifests, items remaining: 1
May  2 02:16:14.827: INFO: namespace: e2e-tests-replication-controller-lh9ph, resource: bindings, ignored listing per whitelist
May  2 02:16:15.860: INFO: namespace: e2e-tests-replication-controller-lh9ph no longer exists
May  2 02:16:15.877: INFO: namespace: e2e-tests-replication-controller-lh9ph, total namespaces: 47, active: 47, terminating: 0
May  2 02:16:15.893: INFO: namespace e2e-tests-replication-controller-lh9ph deletion completed in 26.118451359s

â€¢ [SLOW TEST:44.229 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:16:15.893: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-443fcd58-6c80-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 02:16:16.858: INFO: Waiting up to 5m0s for pod "pod-secrets-444295dc-6c80-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-secrets-wtmcz" to be "success or failure"
May  2 02:16:16.873: INFO: Pod "pod-secrets-444295dc-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.091867ms
May  2 02:16:18.889: INFO: Pod "pod-secrets-444295dc-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0308645s
May  2 02:16:20.905: INFO: Pod "pod-secrets-444295dc-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047093767s
May  2 02:16:22.921: INFO: Pod "pod-secrets-444295dc-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063088604s
May  2 02:16:24.938: INFO: Pod "pod-secrets-444295dc-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.079489617s
May  2 02:16:26.953: INFO: Pod "pod-secrets-444295dc-6c80-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.095397193s
STEP: Saw pod success
May  2 02:16:26.954: INFO: Pod "pod-secrets-444295dc-6c80-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:16:26.969: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-secrets-444295dc-6c80-11e9-97f0-0a58ac103caa container secret-volume-test: <nil>
STEP: delete the pod
May  2 02:16:27.014: INFO: Waiting for pod pod-secrets-444295dc-6c80-11e9-97f0-0a58ac103caa to disappear
May  2 02:16:27.029: INFO: Pod pod-secrets-444295dc-6c80-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:16:27.029: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-wtmcz" for this suite.
May  2 02:16:33.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:16:34.476: INFO: namespace: e2e-tests-secrets-wtmcz, resource: packagemanifests, items remaining: 1
May  2 02:16:34.603: INFO: namespace: e2e-tests-secrets-wtmcz, resource: bindings, ignored listing per whitelist
May  2 02:16:35.140: INFO: namespace: e2e-tests-secrets-wtmcz no longer exists
May  2 02:16:35.157: INFO: namespace: e2e-tests-secrets-wtmcz, total namespaces: 47, active: 47, terminating: 0
May  2 02:16:35.172: INFO: namespace e2e-tests-secrets-wtmcz deletion completed in 8.112834714s

â€¢ [SLOW TEST:19.279 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:16:35.172: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 02:16:36.135: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  2 02:16:46.169: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May  2 02:16:58.307: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-7jcxg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7jcxg/deployments/test-cleanup-deployment,UID:55c48fd7-6c80-11e9-9e44-12f3365d453a,ResourceVersion:22757,Generation:1,CreationTimestamp:2019-05-02 02:16:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-02 02:16:46 +0000 UTC 2019-05-02 02:16:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-02 02:16:57 +0000 UTC 2019-05-02 02:16:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-7dbbfcf846" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May  2 02:16:58.324: INFO: New ReplicaSet "test-cleanup-deployment-7dbbfcf846" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846,GenerateName:,Namespace:e2e-tests-deployment-7jcxg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7jcxg/replicasets/test-cleanup-deployment-7dbbfcf846,UID:55c65aac-6c80-11e9-8dad-121ea440cb2c,ResourceVersion:22746,Generation:1,CreationTimestamp:2019-05-02 02:16:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 55c48fd7-6c80-11e9-9e44-12f3365d453a 0xc00050da27 0xc00050da28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May  2 02:16:58.340: INFO: Pod "test-cleanup-deployment-7dbbfcf846-hcxk6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846-hcxk6,GenerateName:test-cleanup-deployment-7dbbfcf846-,Namespace:e2e-tests-deployment-7jcxg,SelfLink:/api/v1/namespaces/e2e-tests-deployment-7jcxg/pods/test-cleanup-deployment-7dbbfcf846-hcxk6,UID:55c774a1-6c80-11e9-8dad-121ea440cb2c,ResourceVersion:22745,Generation:0,CreationTimestamp:2019-05-02 02:16:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.37"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-7dbbfcf846 55c65aac-6c80-11e9-8dad-121ea440cb2c 0xc0018944e7 0xc0018944e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zct57 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zct57,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-zct57 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-86lhg}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001894590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018945b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:16:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:16:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:16:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:16:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:10.131.0.37,StartTime:2019-05-02 02:16:46 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-02 02:16:56 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://75b0b9eb7e29d6e9131c76e02f2f0528e062292082a3a8644390af51f21d3bd7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:16:58.340: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-7jcxg" for this suite.
May  2 02:17:04.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:17:05.774: INFO: namespace: e2e-tests-deployment-7jcxg, resource: bindings, ignored listing per whitelist
May  2 02:17:06.139: INFO: namespace: e2e-tests-deployment-7jcxg, resource: packagemanifests, items remaining: 1
May  2 02:17:06.451: INFO: namespace: e2e-tests-deployment-7jcxg no longer exists
May  2 02:17:06.469: INFO: namespace: e2e-tests-deployment-7jcxg, total namespaces: 47, active: 47, terminating: 0
May  2 02:17:06.485: INFO: namespace e2e-tests-deployment-7jcxg deletion completed in 8.11572993s

â€¢ [SLOW TEST:31.313 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:17:06.485: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May  2 02:17:07.469: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-a,UID:626f68a0-6c80-11e9-9e44-12f3365d453a,ResourceVersion:22879,Generation:0,CreationTimestamp:2019-05-02 02:17:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  2 02:17:07.469: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-a,UID:626f68a0-6c80-11e9-9e44-12f3365d453a,ResourceVersion:22879,Generation:0,CreationTimestamp:2019-05-02 02:17:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May  2 02:17:17.509: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-a,UID:626f68a0-6c80-11e9-9e44-12f3365d453a,ResourceVersion:22928,Generation:0,CreationTimestamp:2019-05-02 02:17:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May  2 02:17:17.509: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-a,UID:626f68a0-6c80-11e9-9e44-12f3365d453a,ResourceVersion:22928,Generation:0,CreationTimestamp:2019-05-02 02:17:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May  2 02:17:27.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-a,UID:626f68a0-6c80-11e9-9e44-12f3365d453a,ResourceVersion:22975,Generation:0,CreationTimestamp:2019-05-02 02:17:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  2 02:17:27.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-a,UID:626f68a0-6c80-11e9-9e44-12f3365d453a,ResourceVersion:22975,Generation:0,CreationTimestamp:2019-05-02 02:17:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May  2 02:17:37.569: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-a,UID:626f68a0-6c80-11e9-9e44-12f3365d453a,ResourceVersion:23022,Generation:0,CreationTimestamp:2019-05-02 02:17:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  2 02:17:37.569: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-a,UID:626f68a0-6c80-11e9-9e44-12f3365d453a,ResourceVersion:23022,Generation:0,CreationTimestamp:2019-05-02 02:17:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May  2 02:17:47.591: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-b,UID:7a59235c-6c80-11e9-9e44-12f3365d453a,ResourceVersion:23067,Generation:0,CreationTimestamp:2019-05-02 02:17:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  2 02:17:47.591: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-b,UID:7a59235c-6c80-11e9-9e44-12f3365d453a,ResourceVersion:23067,Generation:0,CreationTimestamp:2019-05-02 02:17:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May  2 02:17:57.614: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-b,UID:7a59235c-6c80-11e9-9e44-12f3365d453a,ResourceVersion:23116,Generation:0,CreationTimestamp:2019-05-02 02:17:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  2 02:17:57.614: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-4zn5b,SelfLink:/api/v1/namespaces/e2e-tests-watch-4zn5b/configmaps/e2e-watch-test-configmap-b,UID:7a59235c-6c80-11e9-9e44-12f3365d453a,ResourceVersion:23116,Generation:0,CreationTimestamp:2019-05-02 02:17:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:18:07.615: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-4zn5b" for this suite.
May  2 02:18:13.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:18:15.097: INFO: namespace: e2e-tests-watch-4zn5b, resource: bindings, ignored listing per whitelist
May  2 02:18:15.192: INFO: namespace: e2e-tests-watch-4zn5b, resource: packagemanifests, items remaining: 1
May  2 02:18:15.732: INFO: namespace: e2e-tests-watch-4zn5b no longer exists
May  2 02:18:15.749: INFO: namespace: e2e-tests-watch-4zn5b, total namespaces: 47, active: 47, terminating: 0
May  2 02:18:15.767: INFO: namespace e2e-tests-watch-4zn5b deletion completed in 8.121062093s

â€¢ [SLOW TEST:69.282 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:18:15.767: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-zq6g
STEP: Creating a pod to test atomic-volume-subpath
May  2 02:18:16.749: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zq6g" in namespace "e2e-tests-subpath-flxsc" to be "success or failure"
May  2 02:18:16.764: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Pending", Reason="", readiness=false. Elapsed: 15.225849ms
May  2 02:18:18.780: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031530536s
May  2 02:18:20.796: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047639182s
May  2 02:18:22.813: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063862313s
May  2 02:18:24.829: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Pending", Reason="", readiness=false. Elapsed: 8.079998036s
May  2 02:18:26.845: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Running", Reason="", readiness=false. Elapsed: 10.096235773s
May  2 02:18:28.862: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Running", Reason="", readiness=false. Elapsed: 12.112896342s
May  2 02:18:30.878: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Running", Reason="", readiness=false. Elapsed: 14.129135084s
May  2 02:18:32.895: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Running", Reason="", readiness=false. Elapsed: 16.14629552s
May  2 02:18:34.913: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Running", Reason="", readiness=false. Elapsed: 18.163809006s
May  2 02:18:36.929: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Running", Reason="", readiness=false. Elapsed: 20.179851906s
May  2 02:18:38.947: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Running", Reason="", readiness=false. Elapsed: 22.197850085s
May  2 02:18:40.964: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Running", Reason="", readiness=false. Elapsed: 24.214858543s
May  2 02:18:42.980: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Running", Reason="", readiness=false. Elapsed: 26.231348333s
May  2 02:18:44.996: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Running", Reason="", readiness=false. Elapsed: 28.247454101s
May  2 02:18:47.013: INFO: Pod "pod-subpath-test-configmap-zq6g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.264124899s
STEP: Saw pod success
May  2 02:18:47.013: INFO: Pod "pod-subpath-test-configmap-zq6g" satisfied condition "success or failure"
May  2 02:18:47.028: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-subpath-test-configmap-zq6g container test-container-subpath-configmap-zq6g: <nil>
STEP: delete the pod
May  2 02:18:47.071: INFO: Waiting for pod pod-subpath-test-configmap-zq6g to disappear
May  2 02:18:47.086: INFO: Pod pod-subpath-test-configmap-zq6g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zq6g
May  2 02:18:47.086: INFO: Deleting pod "pod-subpath-test-configmap-zq6g" in namespace "e2e-tests-subpath-flxsc"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:18:47.102: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-flxsc" for this suite.
May  2 02:18:53.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:18:54.234: INFO: namespace: e2e-tests-subpath-flxsc, resource: bindings, ignored listing per whitelist
May  2 02:18:54.840: INFO: namespace: e2e-tests-subpath-flxsc, resource: packagemanifests, items remaining: 1
May  2 02:18:55.223: INFO: namespace: e2e-tests-subpath-flxsc no longer exists
May  2 02:18:55.241: INFO: namespace: e2e-tests-subpath-flxsc, total namespaces: 47, active: 47, terminating: 0
May  2 02:18:55.256: INFO: namespace e2e-tests-subpath-flxsc deletion completed in 8.12474929s

â€¢ [SLOW TEST:39.490 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:18:55.257: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:19:56.236: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-9pdzj" for this suite.
May  2 02:20:18.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:20:19.366: INFO: namespace: e2e-tests-container-probe-9pdzj, resource: packagemanifests, items remaining: 1
May  2 02:20:20.230: INFO: namespace: e2e-tests-container-probe-9pdzj, resource: bindings, ignored listing per whitelist
May  2 02:20:20.347: INFO: namespace: e2e-tests-container-probe-9pdzj no longer exists
May  2 02:20:20.364: INFO: namespace: e2e-tests-container-probe-9pdzj, total namespaces: 47, active: 47, terminating: 0
May  2 02:20:20.379: INFO: namespace e2e-tests-container-probe-9pdzj deletion completed in 24.114337476s

â€¢ [SLOW TEST:85.122 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:20:20.379: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-d5fc4496-6c80-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 02:20:21.366: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d5ff1ba3-6c80-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-n7zp2" to be "success or failure"
May  2 02:20:21.382: INFO: Pod "pod-projected-configmaps-d5ff1ba3-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.06369ms
May  2 02:20:23.398: INFO: Pod "pod-projected-configmaps-d5ff1ba3-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032143241s
May  2 02:20:25.414: INFO: Pod "pod-projected-configmaps-d5ff1ba3-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048253483s
May  2 02:20:27.431: INFO: Pod "pod-projected-configmaps-d5ff1ba3-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064573021s
May  2 02:20:29.447: INFO: Pod "pod-projected-configmaps-d5ff1ba3-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080578653s
May  2 02:20:31.463: INFO: Pod "pod-projected-configmaps-d5ff1ba3-6c80-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.0967952s
STEP: Saw pod success
May  2 02:20:31.463: INFO: Pod "pod-projected-configmaps-d5ff1ba3-6c80-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:20:31.479: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-configmaps-d5ff1ba3-6c80-11e9-97f0-0a58ac103caa container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  2 02:20:31.523: INFO: Waiting for pod pod-projected-configmaps-d5ff1ba3-6c80-11e9-97f0-0a58ac103caa to disappear
May  2 02:20:31.539: INFO: Pod pod-projected-configmaps-d5ff1ba3-6c80-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:20:31.539: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n7zp2" for this suite.
May  2 02:20:37.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:20:38.366: INFO: namespace: e2e-tests-projected-n7zp2, resource: packagemanifests, items remaining: 1
May  2 02:20:39.217: INFO: namespace: e2e-tests-projected-n7zp2, resource: bindings, ignored listing per whitelist
May  2 02:20:39.650: INFO: namespace: e2e-tests-projected-n7zp2 no longer exists
May  2 02:20:39.668: INFO: namespace: e2e-tests-projected-n7zp2, total namespaces: 47, active: 47, terminating: 0
May  2 02:20:39.683: INFO: namespace e2e-tests-projected-n7zp2 deletion completed in 8.114985655s

â€¢ [SLOW TEST:19.304 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:20:39.683: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 02:20:40.641: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e17bcc79-6c80-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-kx882" to be "success or failure"
May  2 02:20:40.658: INFO: Pod "downwardapi-volume-e17bcc79-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.915225ms
May  2 02:20:42.674: INFO: Pod "downwardapi-volume-e17bcc79-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033110254s
May  2 02:20:44.690: INFO: Pod "downwardapi-volume-e17bcc79-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049046872s
May  2 02:20:46.706: INFO: Pod "downwardapi-volume-e17bcc79-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065469095s
May  2 02:20:48.723: INFO: Pod "downwardapi-volume-e17bcc79-6c80-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081732231s
May  2 02:20:50.739: INFO: Pod "downwardapi-volume-e17bcc79-6c80-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098017631s
STEP: Saw pod success
May  2 02:20:50.739: INFO: Pod "downwardapi-volume-e17bcc79-6c80-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:20:50.754: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-e17bcc79-6c80-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 02:20:50.797: INFO: Waiting for pod downwardapi-volume-e17bcc79-6c80-11e9-97f0-0a58ac103caa to disappear
May  2 02:20:50.812: INFO: Pod downwardapi-volume-e17bcc79-6c80-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:20:50.812: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kx882" for this suite.
May  2 02:20:56.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:20:57.580: INFO: namespace: e2e-tests-downward-api-kx882, resource: bindings, ignored listing per whitelist
May  2 02:20:58.229: INFO: namespace: e2e-tests-downward-api-kx882, resource: packagemanifests, items remaining: 1
May  2 02:20:58.924: INFO: namespace: e2e-tests-downward-api-kx882 no longer exists
May  2 02:20:58.944: INFO: namespace: e2e-tests-downward-api-kx882, total namespaces: 47, active: 47, terminating: 0
May  2 02:20:58.960: INFO: namespace e2e-tests-downward-api-kx882 deletion completed in 8.118343147s

â€¢ [SLOW TEST:19.276 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:20:58.960: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-cs8fr
May  2 02:21:09.944: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-cs8fr
STEP: checking the pod's current state and verifying that restartCount is present
May  2 02:21:09.959: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:25:11.933: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cs8fr" for this suite.
May  2 02:25:18.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:25:18.581: INFO: namespace: e2e-tests-container-probe-cs8fr, resource: bindings, ignored listing per whitelist
May  2 02:25:19.325: INFO: namespace: e2e-tests-container-probe-cs8fr, resource: packagemanifests, items remaining: 1
May  2 02:25:20.047: INFO: namespace: e2e-tests-container-probe-cs8fr no longer exists
May  2 02:25:20.064: INFO: namespace: e2e-tests-container-probe-cs8fr, total namespaces: 47, active: 47, terminating: 0
May  2 02:25:20.080: INFO: namespace e2e-tests-container-probe-cs8fr deletion completed in 8.116556367s

â€¢ [SLOW TEST:261.120 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:25:20.080: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 02:25:21.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88a15502-6c81-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-kf8tl" to be "success or failure"
May  2 02:25:21.080: INFO: Pod "downwardapi-volume-88a15502-6c81-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.718654ms
May  2 02:25:23.097: INFO: Pod "downwardapi-volume-88a15502-6c81-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032213161s
May  2 02:25:25.114: INFO: Pod "downwardapi-volume-88a15502-6c81-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049378487s
May  2 02:25:27.130: INFO: Pod "downwardapi-volume-88a15502-6c81-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065866438s
May  2 02:25:29.146: INFO: Pod "downwardapi-volume-88a15502-6c81-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081822971s
May  2 02:25:31.166: INFO: Pod "downwardapi-volume-88a15502-6c81-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.10134681s
STEP: Saw pod success
May  2 02:25:31.166: INFO: Pod "downwardapi-volume-88a15502-6c81-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:25:31.184: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-88a15502-6c81-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 02:25:31.230: INFO: Waiting for pod downwardapi-volume-88a15502-6c81-11e9-97f0-0a58ac103caa to disappear
May  2 02:25:31.246: INFO: Pod downwardapi-volume-88a15502-6c81-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:25:31.246: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kf8tl" for this suite.
May  2 02:25:37.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:25:37.971: INFO: namespace: e2e-tests-downward-api-kf8tl, resource: bindings, ignored listing per whitelist
May  2 02:25:38.286: INFO: namespace: e2e-tests-downward-api-kf8tl, resource: packagemanifests, items remaining: 1
May  2 02:25:39.359: INFO: namespace: e2e-tests-downward-api-kf8tl no longer exists
May  2 02:25:39.377: INFO: namespace: e2e-tests-downward-api-kf8tl, total namespaces: 47, active: 47, terminating: 0
May  2 02:25:39.393: INFO: namespace e2e-tests-downward-api-kf8tl deletion completed in 8.11667087s

â€¢ [SLOW TEST:19.313 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:25:39.393: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May  2 02:25:52.416: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-94205c8c-6c81-11e9-97f0-0a58ac103caa,GenerateName:,Namespace:e2e-tests-events-jxt7z,SelfLink:/api/v1/namespaces/e2e-tests-events-jxt7z/pods/send-events-94205c8c-6c81-11e9-97f0-0a58ac103caa,UID:9422fa4a-6c81-11e9-9e44-12f3365d453a,ResourceVersion:25900,Generation:0,CreationTimestamp:2019-05-02 02:25:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 324067241,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.44"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: anyuid,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-btchk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-btchk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-btchk true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c26,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-smqbd}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002059630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002059650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:25:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:25:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:25:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:25:40 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:10.131.0.44,StartTime:2019-05-02 02:25:40 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-02 02:25:50 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:53c28beabd3509fb5b1d1185b2962e8204384cef7562982d8b216b71292aabf9 cri-o://cfe786b61970f09ce1904a51405e1042f51ff65881389dd9a22258f1ce6c7459}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May  2 02:25:54.432: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May  2 02:25:56.449: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:25:56.468: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-jxt7z" for this suite.
May  2 02:26:40.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:26:41.259: INFO: namespace: e2e-tests-events-jxt7z, resource: packagemanifests, items remaining: 1
May  2 02:26:41.825: INFO: namespace: e2e-tests-events-jxt7z, resource: bindings, ignored listing per whitelist
May  2 02:26:42.580: INFO: namespace: e2e-tests-events-jxt7z no longer exists
May  2 02:26:42.597: INFO: namespace: e2e-tests-events-jxt7z, total namespaces: 47, active: 47, terminating: 0
May  2 02:26:42.612: INFO: namespace e2e-tests-events-jxt7z deletion completed in 46.115095445s

â€¢ [SLOW TEST:63.220 seconds]
[k8s.io] [sig-node] Events
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:26:42.613: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May  2 02:26:43.596: INFO: Pod name pod-release: Found 0 pods out of 1
May  2 02:26:48.613: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:26:48.670: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-w9zsm" for this suite.
May  2 02:26:54.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:26:56.272: INFO: namespace: e2e-tests-replication-controller-w9zsm, resource: packagemanifests, items remaining: 1
May  2 02:26:56.352: INFO: namespace: e2e-tests-replication-controller-w9zsm, resource: bindings, ignored listing per whitelist
May  2 02:26:56.784: INFO: namespace: e2e-tests-replication-controller-w9zsm no longer exists
May  2 02:26:56.801: INFO: namespace: e2e-tests-replication-controller-w9zsm, total namespaces: 47, active: 47, terminating: 0
May  2 02:26:56.817: INFO: namespace e2e-tests-replication-controller-w9zsm deletion completed in 8.116016936s

â€¢ [SLOW TEST:14.204 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:26:56.817: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-t7bxc
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  2 02:26:57.734: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
May  2 02:27:36.094: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.47:8080/dial?request=hostName&protocol=udp&host=10.131.0.46&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-t7bxc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 02:27:36.094: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 02:27:36.294: INFO: Waiting for endpoints: map[]
May  2 02:27:36.310: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.47:8080/dial?request=hostName&protocol=udp&host=10.129.2.20&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-t7bxc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 02:27:36.310: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 02:27:36.486: INFO: Waiting for endpoints: map[]
May  2 02:27:36.502: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.47:8080/dial?request=hostName&protocol=udp&host=10.128.2.17&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-t7bxc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 02:27:36.502: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 02:27:36.721: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:27:36.721: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-t7bxc" for this suite.
May  2 02:28:00.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:28:01.617: INFO: namespace: e2e-tests-pod-network-test-t7bxc, resource: packagemanifests, items remaining: 1
May  2 02:28:02.488: INFO: namespace: e2e-tests-pod-network-test-t7bxc, resource: bindings, ignored listing per whitelist
May  2 02:28:02.837: INFO: namespace: e2e-tests-pod-network-test-t7bxc no longer exists
May  2 02:28:02.854: INFO: namespace: e2e-tests-pod-network-test-t7bxc, total namespaces: 47, active: 47, terminating: 0
May  2 02:28:02.869: INFO: namespace e2e-tests-pod-network-test-t7bxc deletion completed in 26.118102702s

â€¢ [SLOW TEST:66.052 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:28:02.869: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-e9a559b5-6c81-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 02:28:03.845: INFO: Waiting up to 5m0s for pod "pod-secrets-e9a80b5c-6c81-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-secrets-4l6dj" to be "success or failure"
May  2 02:28:03.861: INFO: Pod "pod-secrets-e9a80b5c-6c81-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.303121ms
May  2 02:28:05.878: INFO: Pod "pod-secrets-e9a80b5c-6c81-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032780048s
May  2 02:28:07.894: INFO: Pod "pod-secrets-e9a80b5c-6c81-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049122369s
May  2 02:28:09.910: INFO: Pod "pod-secrets-e9a80b5c-6c81-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065386553s
May  2 02:28:11.927: INFO: Pod "pod-secrets-e9a80b5c-6c81-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08154403s
May  2 02:28:13.944: INFO: Pod "pod-secrets-e9a80b5c-6c81-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098402987s
STEP: Saw pod success
May  2 02:28:13.944: INFO: Pod "pod-secrets-e9a80b5c-6c81-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:28:13.959: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-secrets-e9a80b5c-6c81-11e9-97f0-0a58ac103caa container secret-volume-test: <nil>
STEP: delete the pod
May  2 02:28:14.002: INFO: Waiting for pod pod-secrets-e9a80b5c-6c81-11e9-97f0-0a58ac103caa to disappear
May  2 02:28:14.017: INFO: Pod pod-secrets-e9a80b5c-6c81-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:28:14.017: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4l6dj" for this suite.
May  2 02:28:20.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:28:20.951: INFO: namespace: e2e-tests-secrets-4l6dj, resource: bindings, ignored listing per whitelist
May  2 02:28:21.788: INFO: namespace: e2e-tests-secrets-4l6dj, resource: packagemanifests, items remaining: 1
May  2 02:28:22.128: INFO: namespace: e2e-tests-secrets-4l6dj no longer exists
May  2 02:28:22.145: INFO: namespace: e2e-tests-secrets-4l6dj, total namespaces: 47, active: 47, terminating: 0
May  2 02:28:22.161: INFO: namespace e2e-tests-secrets-4l6dj deletion completed in 8.114347549s

â€¢ [SLOW TEST:19.292 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:28:22.161: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-rlmww
May  2 02:28:33.160: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-rlmww
STEP: checking the pod's current state and verifying that restartCount is present
May  2 02:28:33.175: INFO: Initial restart count of pod liveness-http is 0
May  2 02:28:51.338: INFO: Restart count of pod e2e-tests-container-probe-rlmww/liveness-http is now 1 (18.162761372s elapsed)
May  2 02:29:11.502: INFO: Restart count of pod e2e-tests-container-probe-rlmww/liveness-http is now 2 (38.326460111s elapsed)
May  2 02:29:31.670: INFO: Restart count of pod e2e-tests-container-probe-rlmww/liveness-http is now 3 (58.494915132s elapsed)
May  2 02:29:51.838: INFO: Restart count of pod e2e-tests-container-probe-rlmww/liveness-http is now 4 (1m18.662376176s elapsed)
May  2 02:30:52.338: INFO: Restart count of pod e2e-tests-container-probe-rlmww/liveness-http is now 5 (2m19.162208669s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:30:52.361: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-rlmww" for this suite.
May  2 02:30:58.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:30:59.363: INFO: namespace: e2e-tests-container-probe-rlmww, resource: packagemanifests, items remaining: 1
May  2 02:30:59.479: INFO: namespace: e2e-tests-container-probe-rlmww, resource: bindings, ignored listing per whitelist
May  2 02:31:00.482: INFO: namespace: e2e-tests-container-probe-rlmww no longer exists
May  2 02:31:00.499: INFO: namespace: e2e-tests-container-probe-rlmww, total namespaces: 47, active: 47, terminating: 0
May  2 02:31:00.514: INFO: namespace e2e-tests-container-probe-rlmww deletion completed in 8.122103866s

â€¢ [SLOW TEST:158.353 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:31:00.514: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
May  2 02:31:01.448: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig cluster-info'
May  2 02:31:01.776: INFO: stderr: ""
May  2 02:31:01.776: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.ci-op-5702fvi7-fb0a9.origin-ci-int-aws.dev.rhcloud.com:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:31:01.776: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-khd5w" for this suite.
May  2 02:31:07.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:31:08.627: INFO: namespace: e2e-tests-kubectl-khd5w, resource: packagemanifests, items remaining: 1
May  2 02:31:09.029: INFO: namespace: e2e-tests-kubectl-khd5w, resource: bindings, ignored listing per whitelist
May  2 02:31:09.889: INFO: namespace: e2e-tests-kubectl-khd5w no longer exists
May  2 02:31:09.906: INFO: namespace: e2e-tests-kubectl-khd5w, total namespaces: 47, active: 47, terminating: 0
May  2 02:31:09.921: INFO: namespace e2e-tests-kubectl-khd5w deletion completed in 8.11464679s

â€¢ [SLOW TEST:9.406 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:31:09.921: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May  2 02:31:10.886: INFO: Waiting up to 5m0s for pod "downward-api-59243b58-6c82-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-tcxlt" to be "success or failure"
May  2 02:31:10.902: INFO: Pod "downward-api-59243b58-6c82-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.574023ms
May  2 02:31:12.919: INFO: Pod "downward-api-59243b58-6c82-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032213121s
May  2 02:31:14.935: INFO: Pod "downward-api-59243b58-6c82-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048664555s
May  2 02:31:16.952: INFO: Pod "downward-api-59243b58-6c82-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065422065s
May  2 02:31:18.970: INFO: Pod "downward-api-59243b58-6c82-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083530133s
May  2 02:31:20.987: INFO: Pod "downward-api-59243b58-6c82-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.100323236s
STEP: Saw pod success
May  2 02:31:20.987: INFO: Pod "downward-api-59243b58-6c82-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:31:21.002: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downward-api-59243b58-6c82-11e9-97f0-0a58ac103caa container dapi-container: <nil>
STEP: delete the pod
May  2 02:31:21.045: INFO: Waiting for pod downward-api-59243b58-6c82-11e9-97f0-0a58ac103caa to disappear
May  2 02:31:21.061: INFO: Pod downward-api-59243b58-6c82-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:31:21.061: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tcxlt" for this suite.
May  2 02:31:27.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:31:28.436: INFO: namespace: e2e-tests-downward-api-tcxlt, resource: packagemanifests, items remaining: 1
May  2 02:31:28.764: INFO: namespace: e2e-tests-downward-api-tcxlt, resource: bindings, ignored listing per whitelist
May  2 02:31:29.175: INFO: namespace: e2e-tests-downward-api-tcxlt no longer exists
May  2 02:31:29.194: INFO: namespace: e2e-tests-downward-api-tcxlt, total namespaces: 47, active: 47, terminating: 0
May  2 02:31:29.209: INFO: namespace e2e-tests-downward-api-tcxlt deletion completed in 8.11915629s

â€¢ [SLOW TEST:19.288 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:31:29.210: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  2 02:31:30.280: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:30.280: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:30.280: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:30.296: INFO: Number of nodes with available pods: 0
May  2 02:31:30.296: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:31:31.327: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:31.327: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:31.327: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:31.345: INFO: Number of nodes with available pods: 0
May  2 02:31:31.345: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:31:32.332: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:32.332: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:32.332: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:32.348: INFO: Number of nodes with available pods: 0
May  2 02:31:32.349: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:31:33.332: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:33.332: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:33.332: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:33.357: INFO: Number of nodes with available pods: 0
May  2 02:31:33.357: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:31:34.329: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:34.329: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:34.330: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:34.346: INFO: Number of nodes with available pods: 0
May  2 02:31:34.346: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:31:35.329: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:35.329: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:35.329: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:35.345: INFO: Number of nodes with available pods: 0
May  2 02:31:35.345: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:31:36.341: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:36.341: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:36.341: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:36.358: INFO: Number of nodes with available pods: 0
May  2 02:31:36.358: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:31:37.327: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:37.328: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:37.328: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:37.347: INFO: Number of nodes with available pods: 0
May  2 02:31:37.347: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:31:38.339: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:38.339: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:38.339: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:38.360: INFO: Number of nodes with available pods: 1
May  2 02:31:38.360: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:31:39.341: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:39.341: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:39.341: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:39.358: INFO: Number of nodes with available pods: 3
May  2 02:31:39.358: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May  2 02:31:39.440: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:39.440: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:39.440: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:39.468: INFO: Number of nodes with available pods: 2
May  2 02:31:39.468: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:40.512: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:40.512: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:40.512: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:40.532: INFO: Number of nodes with available pods: 2
May  2 02:31:40.532: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:41.501: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:41.501: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:41.501: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:41.521: INFO: Number of nodes with available pods: 2
May  2 02:31:41.521: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:42.501: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:42.501: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:42.501: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:42.517: INFO: Number of nodes with available pods: 2
May  2 02:31:42.518: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:43.498: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:43.498: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:43.498: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:43.514: INFO: Number of nodes with available pods: 2
May  2 02:31:43.514: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:44.499: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:44.499: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:44.499: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:44.515: INFO: Number of nodes with available pods: 2
May  2 02:31:44.515: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:45.500: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:45.500: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:45.500: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:45.517: INFO: Number of nodes with available pods: 2
May  2 02:31:45.517: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:46.511: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:46.511: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:46.511: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:46.527: INFO: Number of nodes with available pods: 2
May  2 02:31:46.527: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:47.504: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:47.504: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:47.504: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:47.522: INFO: Number of nodes with available pods: 2
May  2 02:31:47.522: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:48.505: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:48.505: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:48.505: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:48.527: INFO: Number of nodes with available pods: 2
May  2 02:31:48.527: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:49.500: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:49.500: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:49.500: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:49.516: INFO: Number of nodes with available pods: 2
May  2 02:31:49.516: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:50.497: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:50.497: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:50.497: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:50.514: INFO: Number of nodes with available pods: 2
May  2 02:31:50.514: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:51.498: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:51.498: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:51.498: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:51.515: INFO: Number of nodes with available pods: 2
May  2 02:31:51.515: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:52.497: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:52.497: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:52.497: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:52.513: INFO: Number of nodes with available pods: 2
May  2 02:31:52.513: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 02:31:53.499: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:53.499: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:53.499: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:31:53.515: INFO: Number of nodes with available pods: 3
May  2 02:31:53.515: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-qfhg2, will wait for the garbage collector to delete the pods
May  2 02:31:53.618: INFO: Deleting DaemonSet.extensions daemon-set took: 20.236074ms
May  2 02:31:53.719: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.401209ms
May  2 02:32:04.934: INFO: Number of nodes with available pods: 0
May  2 02:32:04.935: INFO: Number of running nodes: 0, number of available pods: 0
May  2 02:32:04.952: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-qfhg2/daemonsets","resourceVersion":"28371"},"items":null}

May  2 02:32:04.968: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-qfhg2/pods","resourceVersion":"28371"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:32:05.043: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-qfhg2" for this suite.
May  2 02:32:11.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:32:12.357: INFO: namespace: e2e-tests-daemonsets-qfhg2, resource: packagemanifests, items remaining: 1
May  2 02:32:12.940: INFO: namespace: e2e-tests-daemonsets-qfhg2, resource: bindings, ignored listing per whitelist
May  2 02:32:13.156: INFO: namespace: e2e-tests-daemonsets-qfhg2 no longer exists
May  2 02:32:13.174: INFO: namespace: e2e-tests-daemonsets-qfhg2, total namespaces: 47, active: 47, terminating: 0
May  2 02:32:13.189: INFO: namespace e2e-tests-daemonsets-qfhg2 deletion completed in 8.116579197s

â€¢ [SLOW TEST:43.980 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:32:13.189: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  2 02:32:14.255: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:14.255: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:14.255: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:14.273: INFO: Number of nodes with available pods: 0
May  2 02:32:14.273: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:15.303: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:15.303: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:15.303: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:15.319: INFO: Number of nodes with available pods: 0
May  2 02:32:15.319: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:16.303: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:16.303: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:16.303: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:16.319: INFO: Number of nodes with available pods: 0
May  2 02:32:16.319: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:17.303: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:17.303: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:17.303: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:17.319: INFO: Number of nodes with available pods: 0
May  2 02:32:17.319: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:18.303: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:18.303: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:18.304: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:18.319: INFO: Number of nodes with available pods: 0
May  2 02:32:18.319: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:19.303: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:19.303: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:19.303: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:19.319: INFO: Number of nodes with available pods: 0
May  2 02:32:19.319: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:20.303: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:20.303: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:20.303: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:20.319: INFO: Number of nodes with available pods: 0
May  2 02:32:20.319: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:21.304: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:21.304: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:21.304: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:21.320: INFO: Number of nodes with available pods: 0
May  2 02:32:21.320: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:22.303: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:22.303: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:22.304: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:22.320: INFO: Number of nodes with available pods: 1
May  2 02:32:22.320: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:23.303: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:23.303: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:23.303: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:23.319: INFO: Number of nodes with available pods: 2
May  2 02:32:23.319: INFO: Node ip-10-0-153-32.ec2.internal is running more than one daemon pod
May  2 02:32:24.303: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:24.303: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:24.303: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:24.319: INFO: Number of nodes with available pods: 3
May  2 02:32:24.319: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May  2 02:32:24.386: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:24.386: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:24.386: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:24.403: INFO: Number of nodes with available pods: 2
May  2 02:32:24.403: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:25.433: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:25.433: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:25.433: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:25.448: INFO: Number of nodes with available pods: 2
May  2 02:32:25.448: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:26.435: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:26.435: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:26.435: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:26.451: INFO: Number of nodes with available pods: 2
May  2 02:32:26.451: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:27.433: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:27.433: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:27.433: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:27.449: INFO: Number of nodes with available pods: 2
May  2 02:32:27.449: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:28.433: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:28.433: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:28.433: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:28.449: INFO: Number of nodes with available pods: 2
May  2 02:32:28.449: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:29.433: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:29.433: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:29.433: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:29.449: INFO: Number of nodes with available pods: 2
May  2 02:32:29.449: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:30.434: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:30.434: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:30.434: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:30.450: INFO: Number of nodes with available pods: 2
May  2 02:32:30.450: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:31.433: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:31.433: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:31.433: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:31.450: INFO: Number of nodes with available pods: 2
May  2 02:32:31.450: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:32.433: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:32.433: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:32.433: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:32.449: INFO: Number of nodes with available pods: 2
May  2 02:32:32.449: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 02:32:33.433: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:33.433: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:33.433: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 02:32:33.449: INFO: Number of nodes with available pods: 3
May  2 02:32:33.449: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-p2d9b, will wait for the garbage collector to delete the pods
May  2 02:32:33.566: INFO: Deleting DaemonSet.extensions daemon-set took: 20.131291ms
May  2 02:32:33.667: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.20114ms
May  2 02:32:44.882: INFO: Number of nodes with available pods: 0
May  2 02:32:44.882: INFO: Number of running nodes: 0, number of available pods: 0
May  2 02:32:44.898: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-p2d9b/daemonsets","resourceVersion":"28702"},"items":null}

May  2 02:32:44.913: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-p2d9b/pods","resourceVersion":"28702"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:32:44.988: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-p2d9b" for this suite.
May  2 02:32:51.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:32:52.149: INFO: namespace: e2e-tests-daemonsets-p2d9b, resource: packagemanifests, items remaining: 1
May  2 02:32:52.519: INFO: namespace: e2e-tests-daemonsets-p2d9b, resource: bindings, ignored listing per whitelist
May  2 02:32:53.101: INFO: namespace: e2e-tests-daemonsets-p2d9b no longer exists
May  2 02:32:53.118: INFO: namespace: e2e-tests-daemonsets-p2d9b, total namespaces: 47, active: 47, terminating: 0
May  2 02:32:53.134: INFO: namespace e2e-tests-daemonsets-p2d9b deletion completed in 8.117026575s

â€¢ [SLOW TEST:39.945 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:32:53.134: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0502 02:33:04.335995   12606 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  2 02:33:04.336: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:33:04.336: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-sdgbc" for this suite.
May  2 02:33:10.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:33:11.316: INFO: namespace: e2e-tests-gc-sdgbc, resource: bindings, ignored listing per whitelist
May  2 02:33:11.510: INFO: namespace: e2e-tests-gc-sdgbc, resource: packagemanifests, items remaining: 1
May  2 02:33:12.433: INFO: namespace: e2e-tests-gc-sdgbc no longer exists
May  2 02:33:12.451: INFO: namespace: e2e-tests-gc-sdgbc, total namespaces: 47, active: 47, terminating: 0
May  2 02:33:12.466: INFO: namespace e2e-tests-gc-sdgbc deletion completed in 8.1142002s

â€¢ [SLOW TEST:19.332 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:33:12.467: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 02:33:13.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a22f5e81-6c82-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-79znb" to be "success or failure"
May  2 02:33:13.453: INFO: Pod "downwardapi-volume-a22f5e81-6c82-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 18.559054ms
May  2 02:33:15.469: INFO: Pod "downwardapi-volume-a22f5e81-6c82-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035178437s
May  2 02:33:17.486: INFO: Pod "downwardapi-volume-a22f5e81-6c82-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05190694s
May  2 02:33:19.503: INFO: Pod "downwardapi-volume-a22f5e81-6c82-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06899239s
May  2 02:33:21.519: INFO: Pod "downwardapi-volume-a22f5e81-6c82-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085186507s
May  2 02:33:23.536: INFO: Pod "downwardapi-volume-a22f5e81-6c82-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.101773479s
STEP: Saw pod success
May  2 02:33:23.536: INFO: Pod "downwardapi-volume-a22f5e81-6c82-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:33:23.552: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-a22f5e81-6c82-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 02:33:23.597: INFO: Waiting for pod downwardapi-volume-a22f5e81-6c82-11e9-97f0-0a58ac103caa to disappear
May  2 02:33:23.612: INFO: Pod downwardapi-volume-a22f5e81-6c82-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:33:23.612: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-79znb" for this suite.
May  2 02:33:29.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:33:30.597: INFO: namespace: e2e-tests-downward-api-79znb, resource: bindings, ignored listing per whitelist
May  2 02:33:31.233: INFO: namespace: e2e-tests-downward-api-79znb, resource: packagemanifests, items remaining: 1
May  2 02:33:31.726: INFO: namespace: e2e-tests-downward-api-79znb no longer exists
May  2 02:33:31.743: INFO: namespace: e2e-tests-downward-api-79znb, total namespaces: 47, active: 47, terminating: 0
May  2 02:33:31.759: INFO: namespace e2e-tests-downward-api-79znb deletion completed in 8.116930016s

â€¢ [SLOW TEST:19.292 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:33:31.759: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May  2 02:33:52.876: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  2 02:33:52.891: INFO: Pod pod-with-poststart-http-hook still exists
May  2 02:33:54.891: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  2 02:33:54.907: INFO: Pod pod-with-poststart-http-hook still exists
May  2 02:33:56.891: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  2 02:33:56.907: INFO: Pod pod-with-poststart-http-hook still exists
May  2 02:33:58.891: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  2 02:33:58.907: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:33:58.907: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-8fql6" for this suite.
May  2 02:34:20.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:34:21.625: INFO: namespace: e2e-tests-container-lifecycle-hook-8fql6, resource: bindings, ignored listing per whitelist
May  2 02:34:22.011: INFO: namespace: e2e-tests-container-lifecycle-hook-8fql6, resource: packagemanifests, items remaining: 1
May  2 02:34:23.029: INFO: namespace: e2e-tests-container-lifecycle-hook-8fql6 no longer exists
May  2 02:34:23.048: INFO: namespace: e2e-tests-container-lifecycle-hook-8fql6, total namespaces: 47, active: 47, terminating: 0
May  2 02:34:23.063: INFO: namespace e2e-tests-container-lifecycle-hook-8fql6 deletion completed in 24.126371387s

â€¢ [SLOW TEST:51.304 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:34:23.063: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-b8zcm
[It] Should recreate evicted statefulset [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-b8zcm
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-b8zcm
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-b8zcm
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-b8zcm
May  2 02:34:36.114: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-b8zcm, name: ss-0, uid: d244406a-6c82-11e9-8dad-121ea440cb2c, status phase: Pending. Waiting for statefulset controller to delete.
May  2 02:34:36.467: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-b8zcm, name: ss-0, uid: d244406a-6c82-11e9-8dad-121ea440cb2c, status phase: Failed. Waiting for statefulset controller to delete.
May  2 02:34:36.473: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-b8zcm, name: ss-0, uid: d244406a-6c82-11e9-8dad-121ea440cb2c, status phase: Failed. Waiting for statefulset controller to delete.
May  2 02:34:36.478: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-b8zcm
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-b8zcm
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-b8zcm and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May  2 02:34:48.616: INFO: Deleting all statefulset in ns e2e-tests-statefulset-b8zcm
May  2 02:34:48.633: INFO: Scaling statefulset ss to 0
May  2 02:34:58.701: INFO: Waiting for statefulset status.replicas updated to 0
May  2 02:34:58.718: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:34:58.773: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-b8zcm" for this suite.
May  2 02:35:04.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:35:06.188: INFO: namespace: e2e-tests-statefulset-b8zcm, resource: bindings, ignored listing per whitelist
May  2 02:35:06.238: INFO: namespace: e2e-tests-statefulset-b8zcm, resource: packagemanifests, items remaining: 1
May  2 02:35:06.891: INFO: namespace: e2e-tests-statefulset-b8zcm no longer exists
May  2 02:35:06.908: INFO: namespace: e2e-tests-statefulset-b8zcm, total namespaces: 47, active: 47, terminating: 0
May  2 02:35:06.924: INFO: namespace e2e-tests-statefulset-b8zcm deletion completed in 8.12049402s

â€¢ [SLOW TEST:43.861 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:35:06.924: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 02:35:07.894: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-e66cac89-6c82-11e9-97f0-0a58ac103caa
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:35:18.059: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hv5vq" for this suite.
May  2 02:35:40.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:35:40.706: INFO: namespace: e2e-tests-configmap-hv5vq, resource: bindings, ignored listing per whitelist
May  2 02:35:41.465: INFO: namespace: e2e-tests-configmap-hv5vq, resource: packagemanifests, items remaining: 1
May  2 02:35:42.172: INFO: namespace: e2e-tests-configmap-hv5vq no longer exists
May  2 02:35:42.190: INFO: namespace: e2e-tests-configmap-hv5vq, total namespaces: 47, active: 47, terminating: 0
May  2 02:35:42.205: INFO: namespace e2e-tests-configmap-hv5vq deletion completed in 24.116021358s

â€¢ [SLOW TEST:35.281 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:35:42.205: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
May  2 02:35:43.722: INFO: Waiting up to 5m0s for pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp" in namespace "e2e-tests-svcaccounts-m6r98" to be "success or failure"
May  2 02:35:43.738: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013039ms
May  2 02:35:45.755: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032248278s
May  2 02:35:47.771: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048557083s
May  2 02:35:49.787: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06466369s
May  2 02:35:51.805: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082617118s
May  2 02:35:53.821: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp": Phase="Pending", Reason="", readiness=false. Elapsed: 10.099131548s
May  2 02:35:55.837: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.115115614s
STEP: Saw pod success
May  2 02:35:55.837: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp" satisfied condition "success or failure"
May  2 02:35:55.853: INFO: Trying to get logs from node ip-10-0-153-32.ec2.internal pod pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp container token-test: <nil>
STEP: delete the pod
May  2 02:35:55.915: INFO: Waiting for pod pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp to disappear
May  2 02:35:55.930: INFO: Pod pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-vsglp no longer exists
STEP: Creating a pod to test consume service account root CA
May  2 02:35:55.954: INFO: Waiting up to 5m0s for pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l" in namespace "e2e-tests-svcaccounts-m6r98" to be "success or failure"
May  2 02:35:55.969: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l": Phase="Pending", Reason="", readiness=false. Elapsed: 15.287416ms
May  2 02:35:57.986: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032441565s
May  2 02:36:00.003: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049297918s
May  2 02:36:02.021: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066958335s
May  2 02:36:04.038: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l": Phase="Pending", Reason="", readiness=false. Elapsed: 8.084477521s
May  2 02:36:06.056: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l": Phase="Pending", Reason="", readiness=false. Elapsed: 10.102304887s
May  2 02:36:08.074: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l": Phase="Pending", Reason="", readiness=false. Elapsed: 12.11997535s
May  2 02:36:10.091: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.13703406s
STEP: Saw pod success
May  2 02:36:10.091: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l" satisfied condition "success or failure"
May  2 02:36:10.107: INFO: Trying to get logs from node ip-10-0-140-17.ec2.internal pod pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l container root-ca-test: <nil>
STEP: delete the pod
May  2 02:36:10.182: INFO: Waiting for pod pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l to disappear
May  2 02:36:10.197: INFO: Pod pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-n4q4l no longer exists
STEP: Creating a pod to test consume service account namespace
May  2 02:36:10.222: INFO: Waiting up to 5m0s for pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p" in namespace "e2e-tests-svcaccounts-m6r98" to be "success or failure"
May  2 02:36:10.237: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p": Phase="Pending", Reason="", readiness=false. Elapsed: 15.314436ms
May  2 02:36:12.253: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031473752s
May  2 02:36:14.270: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048551037s
May  2 02:36:16.287: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065486865s
May  2 02:36:18.304: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081961869s
May  2 02:36:20.320: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p": Phase="Pending", Reason="", readiness=false. Elapsed: 10.098128872s
May  2 02:36:22.337: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.114991138s
STEP: Saw pod success
May  2 02:36:22.338: INFO: Pod "pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p" satisfied condition "success or failure"
May  2 02:36:22.354: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p container namespace-test: <nil>
STEP: delete the pod
May  2 02:36:22.409: INFO: Waiting for pod pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p to disappear
May  2 02:36:22.424: INFO: Pod pod-service-account-fbc3acb5-6c82-11e9-97f0-0a58ac103caa-thr4p no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:36:22.424: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-m6r98" for this suite.
May  2 02:36:28.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:36:29.138: INFO: namespace: e2e-tests-svcaccounts-m6r98, resource: bindings, ignored listing per whitelist
May  2 02:36:30.181: INFO: namespace: e2e-tests-svcaccounts-m6r98, resource: packagemanifests, items remaining: 1
May  2 02:36:30.537: INFO: namespace: e2e-tests-svcaccounts-m6r98 no longer exists
May  2 02:36:30.554: INFO: namespace: e2e-tests-svcaccounts-m6r98, total namespaces: 47, active: 47, terminating: 0
May  2 02:36:30.570: INFO: namespace e2e-tests-svcaccounts-m6r98 deletion completed in 8.115764805s

â€¢ [SLOW TEST:48.364 seconds]
[sig-auth] ServiceAccounts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:36:30.570: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-nfbxz
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  2 02:36:31.496: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
May  2 02:37:07.863: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.128.2.24 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-nfbxz PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 02:37:07.863: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 02:37:09.098: INFO: Found all expected endpoints: [netserver-0]
May  2 02:37:09.115: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.131.0.63 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-nfbxz PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 02:37:09.115: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 02:37:10.279: INFO: Found all expected endpoints: [netserver-1]
May  2 02:37:10.295: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.129.2.25 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-nfbxz PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 02:37:10.295: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 02:37:11.464: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:37:11.465: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-nfbxz" for this suite.
May  2 02:37:35.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:37:37.097: INFO: namespace: e2e-tests-pod-network-test-nfbxz, resource: packagemanifests, items remaining: 1
May  2 02:37:37.097: INFO: namespace: e2e-tests-pod-network-test-nfbxz, resource: bindings, ignored listing per whitelist
May  2 02:37:37.592: INFO: namespace: e2e-tests-pod-network-test-nfbxz no longer exists
May  2 02:37:37.610: INFO: namespace: e2e-tests-pod-network-test-nfbxz, total namespaces: 47, active: 47, terminating: 0
May  2 02:37:37.628: INFO: namespace e2e-tests-pod-network-test-nfbxz deletion completed in 26.133159137s

â€¢ [SLOW TEST:67.058 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:37:37.628: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
May  2 02:37:38.593: INFO: Waiting up to 5m0s for pod "var-expansion-403b336a-6c83-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-var-expansion-86f87" to be "success or failure"
May  2 02:37:38.609: INFO: Pod "var-expansion-403b336a-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.573087ms
May  2 02:37:40.626: INFO: Pod "var-expansion-403b336a-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033027018s
May  2 02:37:42.643: INFO: Pod "var-expansion-403b336a-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049496172s
May  2 02:37:44.660: INFO: Pod "var-expansion-403b336a-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066414742s
May  2 02:37:46.676: INFO: Pod "var-expansion-403b336a-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082717682s
May  2 02:37:48.693: INFO: Pod "var-expansion-403b336a-6c83-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.099598825s
STEP: Saw pod success
May  2 02:37:48.694: INFO: Pod "var-expansion-403b336a-6c83-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:37:48.709: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod var-expansion-403b336a-6c83-11e9-97f0-0a58ac103caa container dapi-container: <nil>
STEP: delete the pod
May  2 02:37:48.752: INFO: Waiting for pod var-expansion-403b336a-6c83-11e9-97f0-0a58ac103caa to disappear
May  2 02:37:48.767: INFO: Pod var-expansion-403b336a-6c83-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:37:48.767: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-86f87" for this suite.
May  2 02:37:54.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:37:55.512: INFO: namespace: e2e-tests-var-expansion-86f87, resource: packagemanifests, items remaining: 1
May  2 02:37:56.535: INFO: namespace: e2e-tests-var-expansion-86f87, resource: bindings, ignored listing per whitelist
May  2 02:37:56.883: INFO: namespace: e2e-tests-var-expansion-86f87 no longer exists
May  2 02:37:56.901: INFO: namespace: e2e-tests-var-expansion-86f87, total namespaces: 47, active: 47, terminating: 0
May  2 02:37:56.916: INFO: namespace e2e-tests-var-expansion-86f87 deletion completed in 8.120018898s

â€¢ [SLOW TEST:19.288 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:37:56.917: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 02:37:57.881: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4bbeaa3a-6c83-11e9-97f0-0a58ac103caa
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-4bbeaa3a-6c83-11e9-97f0-0a58ac103caa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:39:10.698: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mztd5" for this suite.
May  2 02:39:34.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:39:35.544: INFO: namespace: e2e-tests-projected-mztd5, resource: packagemanifests, items remaining: 1
May  2 02:39:36.054: INFO: namespace: e2e-tests-projected-mztd5, resource: bindings, ignored listing per whitelist
May  2 02:39:36.825: INFO: namespace: e2e-tests-projected-mztd5 no longer exists
May  2 02:39:36.842: INFO: namespace: e2e-tests-projected-mztd5, total namespaces: 47, active: 47, terminating: 0
May  2 02:39:36.858: INFO: namespace e2e-tests-projected-mztd5 deletion completed in 26.116383248s

â€¢ [SLOW TEST:99.941 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:39:36.858: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
May  2 02:39:37.832: INFO: Waiting up to 5m0s for pod "pod-874ddfb0-6c83-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-cn4j8" to be "success or failure"
May  2 02:39:37.848: INFO: Pod "pod-874ddfb0-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.497622ms
May  2 02:39:39.864: INFO: Pod "pod-874ddfb0-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031759035s
May  2 02:39:41.880: INFO: Pod "pod-874ddfb0-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048215267s
May  2 02:39:43.897: INFO: Pod "pod-874ddfb0-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064328315s
May  2 02:39:45.912: INFO: Pod "pod-874ddfb0-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.079990428s
May  2 02:39:47.928: INFO: Pod "pod-874ddfb0-6c83-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.096038106s
STEP: Saw pod success
May  2 02:39:47.928: INFO: Pod "pod-874ddfb0-6c83-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:39:47.944: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-874ddfb0-6c83-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 02:39:47.985: INFO: Waiting for pod pod-874ddfb0-6c83-11e9-97f0-0a58ac103caa to disappear
May  2 02:39:48.000: INFO: Pod pod-874ddfb0-6c83-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:39:48.000: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cn4j8" for this suite.
May  2 02:39:54.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:39:55.036: INFO: namespace: e2e-tests-emptydir-cn4j8, resource: bindings, ignored listing per whitelist
May  2 02:39:55.414: INFO: namespace: e2e-tests-emptydir-cn4j8, resource: packagemanifests, items remaining: 1
May  2 02:39:56.112: INFO: namespace: e2e-tests-emptydir-cn4j8 no longer exists
May  2 02:39:56.129: INFO: namespace: e2e-tests-emptydir-cn4j8, total namespaces: 47, active: 47, terminating: 0
May  2 02:39:56.145: INFO: namespace e2e-tests-emptydir-cn4j8 deletion completed in 8.115761618s

â€¢ [SLOW TEST:19.287 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:39:56.145: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
May  2 02:39:57.082: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-n4kdw'
May  2 02:39:57.699: INFO: stderr: ""
May  2 02:39:57.699: INFO: stdout: "pod/pause created\n"
May  2 02:39:57.699: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May  2 02:39:57.699: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-n4kdw" to be "running and ready"
May  2 02:39:57.715: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.938049ms
May  2 02:39:59.732: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032608908s
May  2 02:40:01.748: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049005956s
May  2 02:40:03.764: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065228824s
May  2 02:40:05.781: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081879271s
May  2 02:40:07.797: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 10.097710109s
May  2 02:40:07.797: INFO: Pod "pause" satisfied condition "running and ready"
May  2 02:40:07.797: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
May  2 02:40:07.797: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-n4kdw'
May  2 02:40:07.988: INFO: stderr: ""
May  2 02:40:07.988: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May  2 02:40:07.988: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-n4kdw'
May  2 02:40:08.137: INFO: stderr: ""
May  2 02:40:08.137: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   testing-label-value\n"
STEP: removing the label testing-label of a pod
May  2 02:40:08.137: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig label pods pause testing-label- --namespace=e2e-tests-kubectl-n4kdw'
May  2 02:40:08.311: INFO: stderr: ""
May  2 02:40:08.311: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May  2 02:40:08.311: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-n4kdw'
May  2 02:40:08.463: INFO: stderr: ""
May  2 02:40:08.463: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
May  2 02:40:08.463: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-n4kdw'
May  2 02:40:08.630: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  2 02:40:08.630: INFO: stdout: "pod \"pause\" force deleted\n"
May  2 02:40:08.630: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-n4kdw'
May  2 02:40:08.799: INFO: stderr: "No resources found.\n"
May  2 02:40:08.799: INFO: stdout: ""
May  2 02:40:08.799: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=pause --namespace=e2e-tests-kubectl-n4kdw -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  2 02:40:08.942: INFO: stderr: ""
May  2 02:40:08.942: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:40:08.942: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-n4kdw" for this suite.
May  2 02:40:15.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:40:15.817: INFO: namespace: e2e-tests-kubectl-n4kdw, resource: bindings, ignored listing per whitelist
May  2 02:40:15.885: INFO: namespace: e2e-tests-kubectl-n4kdw, resource: packagemanifests, items remaining: 1
May  2 02:40:17.055: INFO: namespace: e2e-tests-kubectl-n4kdw no longer exists
May  2 02:40:17.072: INFO: namespace: e2e-tests-kubectl-n4kdw, total namespaces: 47, active: 47, terminating: 0
May  2 02:40:17.087: INFO: namespace e2e-tests-kubectl-n4kdw deletion completed in 8.114337367s

â€¢ [SLOW TEST:20.942 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:40:17.087: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
May  2 02:40:18.013: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:40:18.143: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-kxb8p" for this suite.
May  2 02:40:24.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:40:25.261: INFO: namespace: e2e-tests-kubectl-kxb8p, resource: packagemanifests, items remaining: 1
May  2 02:40:25.613: INFO: namespace: e2e-tests-kubectl-kxb8p, resource: bindings, ignored listing per whitelist
May  2 02:40:26.244: INFO: namespace: e2e-tests-kubectl-kxb8p no longer exists
May  2 02:40:26.261: INFO: namespace: e2e-tests-kubectl-kxb8p, total namespaces: 47, active: 47, terminating: 0
May  2 02:40:26.279: INFO: namespace e2e-tests-kubectl-kxb8p deletion completed in 8.11886511s

â€¢ [SLOW TEST:9.192 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:40:26.279: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
May  2 02:40:27.243: INFO: Waiting up to 5m0s for pod "var-expansion-a4c171f9-6c83-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-var-expansion-r4x6z" to be "success or failure"
May  2 02:40:27.259: INFO: Pod "var-expansion-a4c171f9-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.251616ms
May  2 02:40:29.275: INFO: Pod "var-expansion-a4c171f9-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03161699s
May  2 02:40:31.291: INFO: Pod "var-expansion-a4c171f9-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047897731s
May  2 02:40:33.307: INFO: Pod "var-expansion-a4c171f9-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063881397s
May  2 02:40:35.324: INFO: Pod "var-expansion-a4c171f9-6c83-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.080378362s
STEP: Saw pod success
May  2 02:40:35.324: INFO: Pod "var-expansion-a4c171f9-6c83-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:40:35.339: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod var-expansion-a4c171f9-6c83-11e9-97f0-0a58ac103caa container dapi-container: <nil>
STEP: delete the pod
May  2 02:40:35.381: INFO: Waiting for pod var-expansion-a4c171f9-6c83-11e9-97f0-0a58ac103caa to disappear
May  2 02:40:35.397: INFO: Pod var-expansion-a4c171f9-6c83-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:40:35.397: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-r4x6z" for this suite.
May  2 02:40:41.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:40:42.814: INFO: namespace: e2e-tests-var-expansion-r4x6z, resource: bindings, ignored listing per whitelist
May  2 02:40:43.128: INFO: namespace: e2e-tests-var-expansion-r4x6z, resource: packagemanifests, items remaining: 1
May  2 02:40:43.510: INFO: namespace: e2e-tests-var-expansion-r4x6z no longer exists
May  2 02:40:43.528: INFO: namespace: e2e-tests-var-expansion-r4x6z, total namespaces: 47, active: 47, terminating: 0
May  2 02:40:43.543: INFO: namespace e2e-tests-var-expansion-r4x6z deletion completed in 8.117124581s

â€¢ [SLOW TEST:17.264 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:40:43.543: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-af0b3632-6c83-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 02:40:44.521: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af0deefb-6c83-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-22zcd" to be "success or failure"
May  2 02:40:44.537: INFO: Pod "pod-projected-configmaps-af0deefb-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.722737ms
May  2 02:40:46.554: INFO: Pod "pod-projected-configmaps-af0deefb-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032186712s
May  2 02:40:48.570: INFO: Pod "pod-projected-configmaps-af0deefb-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048249677s
May  2 02:40:50.586: INFO: Pod "pod-projected-configmaps-af0deefb-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064472372s
May  2 02:40:52.602: INFO: Pod "pod-projected-configmaps-af0deefb-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080791141s
May  2 02:40:54.619: INFO: Pod "pod-projected-configmaps-af0deefb-6c83-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097202129s
STEP: Saw pod success
May  2 02:40:54.619: INFO: Pod "pod-projected-configmaps-af0deefb-6c83-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:40:54.634: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-configmaps-af0deefb-6c83-11e9-97f0-0a58ac103caa container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  2 02:40:54.678: INFO: Waiting for pod pod-projected-configmaps-af0deefb-6c83-11e9-97f0-0a58ac103caa to disappear
May  2 02:40:54.694: INFO: Pod pod-projected-configmaps-af0deefb-6c83-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:40:54.694: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-22zcd" for this suite.
May  2 02:41:00.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:41:02.132: INFO: namespace: e2e-tests-projected-22zcd, resource: bindings, ignored listing per whitelist
May  2 02:41:02.301: INFO: namespace: e2e-tests-projected-22zcd, resource: packagemanifests, items remaining: 1
May  2 02:41:02.810: INFO: namespace: e2e-tests-projected-22zcd no longer exists
May  2 02:41:02.826: INFO: namespace: e2e-tests-projected-22zcd, total namespaces: 47, active: 47, terminating: 0
May  2 02:41:02.842: INFO: namespace e2e-tests-projected-22zcd deletion completed in 8.118975896s

â€¢ [SLOW TEST:19.299 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:41:02.842: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 02:41:03.818: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba8ea192-6c83-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-fn5cs" to be "success or failure"
May  2 02:41:03.834: INFO: Pod "downwardapi-volume-ba8ea192-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.673141ms
May  2 02:41:05.850: INFO: Pod "downwardapi-volume-ba8ea192-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031888213s
May  2 02:41:07.866: INFO: Pod "downwardapi-volume-ba8ea192-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048067873s
May  2 02:41:09.883: INFO: Pod "downwardapi-volume-ba8ea192-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064678186s
May  2 02:41:11.899: INFO: Pod "downwardapi-volume-ba8ea192-6c83-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08062516s
May  2 02:41:13.915: INFO: Pod "downwardapi-volume-ba8ea192-6c83-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.0967512s
STEP: Saw pod success
May  2 02:41:13.915: INFO: Pod "downwardapi-volume-ba8ea192-6c83-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:41:13.930: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-ba8ea192-6c83-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 02:41:13.975: INFO: Waiting for pod downwardapi-volume-ba8ea192-6c83-11e9-97f0-0a58ac103caa to disappear
May  2 02:41:13.990: INFO: Pod downwardapi-volume-ba8ea192-6c83-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:41:13.990: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fn5cs" for this suite.
May  2 02:41:20.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:41:20.942: INFO: namespace: e2e-tests-projected-fn5cs, resource: bindings, ignored listing per whitelist
May  2 02:41:21.741: INFO: namespace: e2e-tests-projected-fn5cs, resource: packagemanifests, items remaining: 1
May  2 02:41:22.116: INFO: namespace: e2e-tests-projected-fn5cs no longer exists
May  2 02:41:22.133: INFO: namespace: e2e-tests-projected-fn5cs, total namespaces: 47, active: 47, terminating: 0
May  2 02:41:22.149: INFO: namespace e2e-tests-projected-fn5cs deletion completed in 8.116601505s

â€¢ [SLOW TEST:19.307 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:41:22.149: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May  2 02:41:33.749: INFO: Successfully updated pod "labelsupdatec612eea7-6c83-11e9-97f0-0a58ac103caa"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:41:35.791: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-c8xs4" for this suite.
May  2 02:41:57.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:41:58.838: INFO: namespace: e2e-tests-projected-c8xs4, resource: bindings, ignored listing per whitelist
May  2 02:41:58.996: INFO: namespace: e2e-tests-projected-c8xs4, resource: packagemanifests, items remaining: 1
May  2 02:41:59.905: INFO: namespace: e2e-tests-projected-c8xs4 no longer exists
May  2 02:41:59.921: INFO: namespace: e2e-tests-projected-c8xs4, total namespaces: 47, active: 47, terminating: 0
May  2 02:41:59.937: INFO: namespace e2e-tests-projected-c8xs4 deletion completed in 24.115027272s

â€¢ [SLOW TEST:37.787 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:41:59.937: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 02:42:00.888: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-dc96a19a-6c83-11e9-97f0-0a58ac103caa
STEP: Creating secret with name s-test-opt-upd-dc96a254-6c83-11e9-97f0-0a58ac103caa
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-dc96a19a-6c83-11e9-97f0-0a58ac103caa
STEP: Updating secret s-test-opt-upd-dc96a254-6c83-11e9-97f0-0a58ac103caa
STEP: Creating secret with name s-test-opt-create-dc96a26c-6c83-11e9-97f0-0a58ac103caa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:43:36.034: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jrf7q" for this suite.
May  2 02:44:00.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:44:00.842: INFO: namespace: e2e-tests-projected-jrf7q, resource: bindings, ignored listing per whitelist
May  2 02:44:02.032: INFO: namespace: e2e-tests-projected-jrf7q, resource: packagemanifests, items remaining: 1
May  2 02:44:02.146: INFO: namespace: e2e-tests-projected-jrf7q no longer exists
May  2 02:44:02.163: INFO: namespace: e2e-tests-projected-jrf7q, total namespaces: 47, active: 47, terminating: 0
May  2 02:44:02.180: INFO: namespace e2e-tests-projected-jrf7q deletion completed in 26.116672743s

â€¢ [SLOW TEST:122.244 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:44:02.181: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-25703906-6c84-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 02:44:03.157: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-25734ebc-6c84-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-bq96c" to be "success or failure"
May  2 02:44:03.172: INFO: Pod "pod-projected-configmaps-25734ebc-6c84-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.408026ms
May  2 02:44:05.188: INFO: Pod "pod-projected-configmaps-25734ebc-6c84-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031860707s
May  2 02:44:07.205: INFO: Pod "pod-projected-configmaps-25734ebc-6c84-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04813139s
May  2 02:44:09.221: INFO: Pod "pod-projected-configmaps-25734ebc-6c84-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064716512s
May  2 02:44:11.238: INFO: Pod "pod-projected-configmaps-25734ebc-6c84-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080932407s
May  2 02:44:13.254: INFO: Pod "pod-projected-configmaps-25734ebc-6c84-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.09721925s
STEP: Saw pod success
May  2 02:44:13.254: INFO: Pod "pod-projected-configmaps-25734ebc-6c84-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:44:13.269: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-configmaps-25734ebc-6c84-11e9-97f0-0a58ac103caa container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  2 02:44:13.311: INFO: Waiting for pod pod-projected-configmaps-25734ebc-6c84-11e9-97f0-0a58ac103caa to disappear
May  2 02:44:13.327: INFO: Pod pod-projected-configmaps-25734ebc-6c84-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:44:13.327: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bq96c" for this suite.
May  2 02:44:19.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:44:20.139: INFO: namespace: e2e-tests-projected-bq96c, resource: packagemanifests, items remaining: 1
May  2 02:44:20.376: INFO: namespace: e2e-tests-projected-bq96c, resource: bindings, ignored listing per whitelist
May  2 02:44:21.440: INFO: namespace: e2e-tests-projected-bq96c no longer exists
May  2 02:44:21.457: INFO: namespace: e2e-tests-projected-bq96c, total namespaces: 47, active: 47, terminating: 0
May  2 02:44:21.472: INFO: namespace e2e-tests-projected-bq96c deletion completed in 8.115529055s

â€¢ [SLOW TEST:19.292 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:44:21.473: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-fpjn7
May  2 02:44:32.465: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-fpjn7
STEP: checking the pod's current state and verifying that restartCount is present
May  2 02:44:32.481: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:48:34.465: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-fpjn7" for this suite.
May  2 02:48:40.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:48:42.165: INFO: namespace: e2e-tests-container-probe-fpjn7, resource: packagemanifests, items remaining: 1
May  2 02:48:42.196: INFO: namespace: e2e-tests-container-probe-fpjn7, resource: bindings, ignored listing per whitelist
May  2 02:48:42.578: INFO: namespace: e2e-tests-container-probe-fpjn7 no longer exists
May  2 02:48:42.595: INFO: namespace: e2e-tests-container-probe-fpjn7, total namespaces: 47, active: 47, terminating: 0
May  2 02:48:42.610: INFO: namespace e2e-tests-container-probe-fpjn7 deletion completed in 8.115502803s

â€¢ [SLOW TEST:261.137 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:48:42.610: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 02:48:43.573: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-cc9b8d20-6c84-11e9-97f0-0a58ac103caa
STEP: Creating secret with name s-test-opt-upd-cc9b8d85-6c84-11e9-97f0-0a58ac103caa
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-cc9b8d20-6c84-11e9-97f0-0a58ac103caa
STEP: Updating secret s-test-opt-upd-cc9b8d85-6c84-11e9-97f0-0a58ac103caa
STEP: Creating secret with name s-test-opt-create-cc9b8d9d-6c84-11e9-97f0-0a58ac103caa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:50:16.692: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2d9c5" for this suite.
May  2 02:50:40.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:50:41.454: INFO: namespace: e2e-tests-secrets-2d9c5, resource: bindings, ignored listing per whitelist
May  2 02:50:41.876: INFO: namespace: e2e-tests-secrets-2d9c5, resource: packagemanifests, items remaining: 1
May  2 02:50:42.821: INFO: namespace: e2e-tests-secrets-2d9c5 no longer exists
May  2 02:50:42.838: INFO: namespace: e2e-tests-secrets-2d9c5, total namespaces: 47, active: 47, terminating: 0
May  2 02:50:42.853: INFO: namespace e2e-tests-secrets-2d9c5 deletion completed in 26.12952174s

â€¢ [SLOW TEST:120.243 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:50:42.853: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0502 02:51:23.954577   12606 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  2 02:51:23.954: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:51:23.954: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-524dk" for this suite.
May  2 02:51:30.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:51:30.876: INFO: namespace: e2e-tests-gc-524dk, resource: packagemanifests, items remaining: 1
May  2 02:51:30.922: INFO: namespace: e2e-tests-gc-524dk, resource: bindings, ignored listing per whitelist
May  2 02:51:32.052: INFO: namespace: e2e-tests-gc-524dk no longer exists
May  2 02:51:32.069: INFO: namespace: e2e-tests-gc-524dk, total namespaces: 47, active: 47, terminating: 0
May  2 02:51:32.087: INFO: namespace e2e-tests-gc-524dk deletion completed in 8.115888574s

â€¢ [SLOW TEST:49.233 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:51:32.087: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May  2 02:51:53.221: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  2 02:51:53.237: INFO: Pod pod-with-poststart-exec-hook still exists
May  2 02:51:55.237: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  2 02:51:55.254: INFO: Pod pod-with-poststart-exec-hook still exists
May  2 02:51:57.237: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  2 02:51:57.253: INFO: Pod pod-with-poststart-exec-hook still exists
May  2 02:51:59.237: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  2 02:51:59.253: INFO: Pod pod-with-poststart-exec-hook still exists
May  2 02:52:01.237: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  2 02:52:01.253: INFO: Pod pod-with-poststart-exec-hook still exists
May  2 02:52:03.237: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  2 02:52:03.253: INFO: Pod pod-with-poststart-exec-hook still exists
May  2 02:52:05.237: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  2 02:52:05.253: INFO: Pod pod-with-poststart-exec-hook still exists
May  2 02:52:07.237: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  2 02:52:07.253: INFO: Pod pod-with-poststart-exec-hook still exists
May  2 02:52:09.237: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  2 02:52:09.253: INFO: Pod pod-with-poststart-exec-hook still exists
May  2 02:52:11.237: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  2 02:52:11.255: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:52:11.255: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-h7xqw" for this suite.
May  2 02:52:33.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:52:34.634: INFO: namespace: e2e-tests-container-lifecycle-hook-h7xqw, resource: bindings, ignored listing per whitelist
May  2 02:52:35.042: INFO: namespace: e2e-tests-container-lifecycle-hook-h7xqw, resource: packagemanifests, items remaining: 1
May  2 02:52:35.369: INFO: namespace: e2e-tests-container-lifecycle-hook-h7xqw no longer exists
May  2 02:52:35.387: INFO: namespace: e2e-tests-container-lifecycle-hook-h7xqw, total namespaces: 47, active: 47, terminating: 0
May  2 02:52:35.402: INFO: namespace e2e-tests-container-lifecycle-hook-h7xqw deletion completed in 24.117495673s

â€¢ [SLOW TEST:63.315 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:52:35.402: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-2qgxw
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-2qgxw
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-2qgxw
May  2 02:52:36.388: INFO: Found 0 stateful pods, waiting for 1
May  2 02:52:46.406: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May  2 02:52:46.422: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-2qgxw ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  2 02:52:46.801: INFO: stderr: ""
May  2 02:52:46.801: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  2 02:52:46.801: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  2 02:52:46.817: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  2 02:52:56.834: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  2 02:52:56.834: INFO: Waiting for statefulset status.replicas updated to 0
May  2 02:52:56.904: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
May  2 02:52:56.904: INFO: ss-0  ip-10-0-135-216.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:36 +0000 UTC  }]
May  2 02:52:56.904: INFO: ss-1  ip-10-0-153-32.ec2.internal   Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:56 +0000 UTC  }]
May  2 02:52:56.904: INFO: 
May  2 02:52:56.904: INFO: StatefulSet ss has not reached scale 3, at 2
May  2 02:52:57.920: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982709518s
May  2 02:52:58.937: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.966223591s
May  2 02:52:59.953: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.949473484s
May  2 02:53:00.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.932955893s
May  2 02:53:01.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.91602339s
May  2 02:53:03.004: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.899398191s
May  2 02:53:04.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.881495516s
May  2 02:53:05.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.864359821s
May  2 02:53:06.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 847.709166ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-2qgxw
May  2 02:53:07.072: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-2qgxw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 02:53:07.382: INFO: stderr: ""
May  2 02:53:07.382: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  2 02:53:07.382: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  2 02:53:07.382: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-2qgxw ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 02:53:07.688: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
May  2 02:53:07.688: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  2 02:53:07.688: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  2 02:53:07.689: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-2qgxw ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 02:53:08.074: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
May  2 02:53:08.074: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  2 02:53:08.074: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  2 02:53:08.090: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  2 02:53:08.090: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  2 02:53:08.090: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May  2 02:53:08.106: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-2qgxw ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  2 02:53:08.410: INFO: stderr: ""
May  2 02:53:08.410: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  2 02:53:08.410: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  2 02:53:08.410: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-2qgxw ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  2 02:53:08.718: INFO: stderr: ""
May  2 02:53:08.718: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  2 02:53:08.718: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  2 02:53:08.718: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-2qgxw ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  2 02:53:09.034: INFO: stderr: ""
May  2 02:53:09.034: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  2 02:53:09.034: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  2 02:53:09.034: INFO: Waiting for statefulset status.replicas updated to 0
May  2 02:53:09.050: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May  2 02:53:19.084: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  2 02:53:19.084: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  2 02:53:19.084: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  2 02:53:19.135: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
May  2 02:53:19.135: INFO: ss-0  ip-10-0-135-216.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:36 +0000 UTC  }]
May  2 02:53:19.135: INFO: ss-1  ip-10-0-153-32.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:56 +0000 UTC  }]
May  2 02:53:19.135: INFO: ss-2  ip-10-0-140-17.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:56 +0000 UTC  }]
May  2 02:53:19.135: INFO: 
May  2 02:53:19.135: INFO: StatefulSet ss has not reached scale 0, at 3
May  2 02:53:20.154: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
May  2 02:53:20.154: INFO: ss-0  ip-10-0-135-216.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:36 +0000 UTC  }]
May  2 02:53:20.154: INFO: ss-1  ip-10-0-153-32.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:56 +0000 UTC  }]
May  2 02:53:20.154: INFO: ss-2  ip-10-0-140-17.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:56 +0000 UTC  }]
May  2 02:53:20.154: INFO: 
May  2 02:53:20.154: INFO: StatefulSet ss has not reached scale 0, at 3
May  2 02:53:21.171: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
May  2 02:53:21.171: INFO: ss-0  ip-10-0-135-216.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:53:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 02:52:36 +0000 UTC  }]
May  2 02:53:21.171: INFO: 
May  2 02:53:21.171: INFO: StatefulSet ss has not reached scale 0, at 1
May  2 02:53:22.187: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.948340783s
May  2 02:53:23.203: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.932153243s
May  2 02:53:24.219: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.916075777s
May  2 02:53:25.236: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.899965743s
May  2 02:53:26.252: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.883449183s
May  2 02:53:27.269: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.866899247s
May  2 02:53:28.285: INFO: Verifying statefulset ss doesn't scale past 0 for another 850.359853ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-2qgxw
May  2 02:53:29.301: INFO: Scaling statefulset ss to 0
May  2 02:53:29.351: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May  2 02:53:29.368: INFO: Deleting all statefulset in ns e2e-tests-statefulset-2qgxw
May  2 02:53:29.385: INFO: Scaling statefulset ss to 0
May  2 02:53:29.435: INFO: Waiting for statefulset status.replicas updated to 0
May  2 02:53:29.451: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:53:29.505: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-2qgxw" for this suite.
May  2 02:53:35.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:53:36.754: INFO: namespace: e2e-tests-statefulset-2qgxw, resource: bindings, ignored listing per whitelist
May  2 02:53:36.912: INFO: namespace: e2e-tests-statefulset-2qgxw, resource: packagemanifests, items remaining: 1
May  2 02:53:37.622: INFO: namespace: e2e-tests-statefulset-2qgxw no longer exists
May  2 02:53:37.638: INFO: namespace: e2e-tests-statefulset-2qgxw, total namespaces: 47, active: 47, terminating: 0
May  2 02:53:37.654: INFO: namespace e2e-tests-statefulset-2qgxw deletion completed in 8.119165628s

â€¢ [SLOW TEST:62.252 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:53:37.654: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May  2 02:53:49.239: INFO: Successfully updated pod "annotationupdate7c718381-6c85-11e9-97f0-0a58ac103caa"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:53:51.281: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jpff7" for this suite.
May  2 02:54:13.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:54:13.911: INFO: namespace: e2e-tests-projected-jpff7, resource: bindings, ignored listing per whitelist
May  2 02:54:15.020: INFO: namespace: e2e-tests-projected-jpff7, resource: packagemanifests, items remaining: 1
May  2 02:54:15.393: INFO: namespace: e2e-tests-projected-jpff7 no longer exists
May  2 02:54:15.410: INFO: namespace: e2e-tests-projected-jpff7, total namespaces: 47, active: 47, terminating: 0
May  2 02:54:15.426: INFO: namespace e2e-tests-projected-jpff7 deletion completed in 24.115211164s

â€¢ [SLOW TEST:37.772 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:54:15.426: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:54:26.473: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-jpwfd" for this suite.
May  2 02:55:10.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:55:11.172: INFO: namespace: e2e-tests-kubelet-test-jpwfd, resource: packagemanifests, items remaining: 1
May  2 02:55:11.411: INFO: namespace: e2e-tests-kubelet-test-jpwfd, resource: bindings, ignored listing per whitelist
May  2 02:55:12.588: INFO: namespace: e2e-tests-kubelet-test-jpwfd no longer exists
May  2 02:55:12.605: INFO: namespace: e2e-tests-kubelet-test-jpwfd, total namespaces: 47, active: 47, terminating: 0
May  2 02:55:12.620: INFO: namespace e2e-tests-kubelet-test-jpwfd deletion completed in 46.117501872s

â€¢ [SLOW TEST:57.194 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:55:12.621: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 02:55:13.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b50eece2-6c85-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-vjglk" to be "success or failure"
May  2 02:55:13.606: INFO: Pod "downwardapi-volume-b50eece2-6c85-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 17.041925ms
May  2 02:55:15.624: INFO: Pod "downwardapi-volume-b50eece2-6c85-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035068077s
May  2 02:55:17.641: INFO: Pod "downwardapi-volume-b50eece2-6c85-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052262086s
May  2 02:55:19.657: INFO: Pod "downwardapi-volume-b50eece2-6c85-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068648132s
May  2 02:55:21.673: INFO: Pod "downwardapi-volume-b50eece2-6c85-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.084992638s
May  2 02:55:23.690: INFO: Pod "downwardapi-volume-b50eece2-6c85-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.101277109s
STEP: Saw pod success
May  2 02:55:23.690: INFO: Pod "downwardapi-volume-b50eece2-6c85-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:55:23.706: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-b50eece2-6c85-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 02:55:23.747: INFO: Waiting for pod downwardapi-volume-b50eece2-6c85-11e9-97f0-0a58ac103caa to disappear
May  2 02:55:23.762: INFO: Pod downwardapi-volume-b50eece2-6c85-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:55:23.763: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vjglk" for this suite.
May  2 02:55:29.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:55:30.522: INFO: namespace: e2e-tests-projected-vjglk, resource: packagemanifests, items remaining: 1
May  2 02:55:30.584: INFO: namespace: e2e-tests-projected-vjglk, resource: bindings, ignored listing per whitelist
May  2 02:55:31.875: INFO: namespace: e2e-tests-projected-vjglk no longer exists
May  2 02:55:31.892: INFO: namespace: e2e-tests-projected-vjglk, total namespaces: 47, active: 47, terminating: 0
May  2 02:55:31.907: INFO: namespace e2e-tests-projected-vjglk deletion completed in 8.114763648s

â€¢ [SLOW TEST:19.286 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:55:31.907: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-l2hhw
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  2 02:55:32.874: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
May  2 02:56:11.249: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.31:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-l2hhw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 02:56:11.249: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 02:56:11.511: INFO: Found all expected endpoints: [netserver-0]
May  2 02:56:11.527: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.0.84:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-l2hhw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 02:56:11.527: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 02:56:11.735: INFO: Found all expected endpoints: [netserver-1]
May  2 02:56:11.751: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.31:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-l2hhw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 02:56:11.751: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 02:56:11.922: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:56:11.922: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-l2hhw" for this suite.
May  2 02:56:36.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:56:36.976: INFO: namespace: e2e-tests-pod-network-test-l2hhw, resource: packagemanifests, items remaining: 1
May  2 02:56:37.104: INFO: namespace: e2e-tests-pod-network-test-l2hhw, resource: bindings, ignored listing per whitelist
May  2 02:56:38.038: INFO: namespace: e2e-tests-pod-network-test-l2hhw no longer exists
May  2 02:56:38.055: INFO: namespace: e2e-tests-pod-network-test-l2hhw, total namespaces: 47, active: 47, terminating: 0
May  2 02:56:38.070: INFO: namespace e2e-tests-pod-network-test-l2hhw deletion completed in 26.118280215s

â€¢ [SLOW TEST:66.163 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:56:38.070: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May  2 02:56:39.005: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-f6w2g'
May  2 02:56:40.047: INFO: stderr: ""
May  2 02:56:40.047: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
May  2 02:56:40.063: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-f6w2g'
May  2 02:56:48.569: INFO: stderr: ""
May  2 02:56:48.569: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:56:48.569: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-f6w2g" for this suite.
May  2 02:56:54.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:56:55.602: INFO: namespace: e2e-tests-kubectl-f6w2g, resource: packagemanifests, items remaining: 1
May  2 02:56:55.701: INFO: namespace: e2e-tests-kubectl-f6w2g, resource: bindings, ignored listing per whitelist
May  2 02:56:56.686: INFO: namespace: e2e-tests-kubectl-f6w2g no longer exists
May  2 02:56:56.703: INFO: namespace: e2e-tests-kubectl-f6w2g, total namespaces: 47, active: 47, terminating: 0
May  2 02:56:56.719: INFO: namespace e2e-tests-kubectl-f6w2g deletion completed in 8.119748278s

â€¢ [SLOW TEST:18.648 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:56:56.719: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
May  2 02:56:57.679: INFO: Waiting up to 5m0s for pod "pod-f319f7ed-6c85-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-sqjrc" to be "success or failure"
May  2 02:56:57.694: INFO: Pod "pod-f319f7ed-6c85-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.692767ms
May  2 02:56:59.711: INFO: Pod "pod-f319f7ed-6c85-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031973593s
May  2 02:57:01.727: INFO: Pod "pod-f319f7ed-6c85-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048279656s
May  2 02:57:03.743: INFO: Pod "pod-f319f7ed-6c85-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064705802s
May  2 02:57:05.760: INFO: Pod "pod-f319f7ed-6c85-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080980997s
May  2 02:57:07.776: INFO: Pod "pod-f319f7ed-6c85-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097627731s
STEP: Saw pod success
May  2 02:57:07.776: INFO: Pod "pod-f319f7ed-6c85-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 02:57:07.792: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-f319f7ed-6c85-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 02:57:07.837: INFO: Waiting for pod pod-f319f7ed-6c85-11e9-97f0-0a58ac103caa to disappear
May  2 02:57:07.852: INFO: Pod pod-f319f7ed-6c85-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:57:07.852: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sqjrc" for this suite.
May  2 02:57:13.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:57:14.723: INFO: namespace: e2e-tests-emptydir-sqjrc, resource: bindings, ignored listing per whitelist
May  2 02:57:15.905: INFO: namespace: e2e-tests-emptydir-sqjrc, resource: packagemanifests, items remaining: 1
May  2 02:57:15.969: INFO: namespace: e2e-tests-emptydir-sqjrc no longer exists
May  2 02:57:15.985: INFO: namespace: e2e-tests-emptydir-sqjrc, total namespaces: 47, active: 47, terminating: 0
May  2 02:57:16.001: INFO: namespace e2e-tests-emptydir-sqjrc deletion completed in 8.117796726s

â€¢ [SLOW TEST:19.282 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:57:16.001: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May  2 02:57:17.045: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tt8sp,SelfLink:/api/v1/namespaces/e2e-tests-watch-tt8sp/configmaps/e2e-watch-test-label-changed,UID:fe9d4b05-6c85-11e9-9e44-12f3365d453a,ResourceVersion:38826,Generation:0,CreationTimestamp:2019-05-02 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  2 02:57:17.045: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tt8sp,SelfLink:/api/v1/namespaces/e2e-tests-watch-tt8sp/configmaps/e2e-watch-test-label-changed,UID:fe9d4b05-6c85-11e9-9e44-12f3365d453a,ResourceVersion:38828,Generation:0,CreationTimestamp:2019-05-02 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May  2 02:57:17.045: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tt8sp,SelfLink:/api/v1/namespaces/e2e-tests-watch-tt8sp/configmaps/e2e-watch-test-label-changed,UID:fe9d4b05-6c85-11e9-9e44-12f3365d453a,ResourceVersion:38830,Generation:0,CreationTimestamp:2019-05-02 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May  2 02:57:27.172: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tt8sp,SelfLink:/api/v1/namespaces/e2e-tests-watch-tt8sp/configmaps/e2e-watch-test-label-changed,UID:fe9d4b05-6c85-11e9-9e44-12f3365d453a,ResourceVersion:38879,Generation:0,CreationTimestamp:2019-05-02 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  2 02:57:27.172: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tt8sp,SelfLink:/api/v1/namespaces/e2e-tests-watch-tt8sp/configmaps/e2e-watch-test-label-changed,UID:fe9d4b05-6c85-11e9-9e44-12f3365d453a,ResourceVersion:38880,Generation:0,CreationTimestamp:2019-05-02 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May  2 02:57:27.172: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tt8sp,SelfLink:/api/v1/namespaces/e2e-tests-watch-tt8sp/configmaps/e2e-watch-test-label-changed,UID:fe9d4b05-6c85-11e9-9e44-12f3365d453a,ResourceVersion:38882,Generation:0,CreationTimestamp:2019-05-02 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:57:27.173: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-tt8sp" for this suite.
May  2 02:57:33.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:57:34.566: INFO: namespace: e2e-tests-watch-tt8sp, resource: packagemanifests, items remaining: 1
May  2 02:57:34.694: INFO: namespace: e2e-tests-watch-tt8sp, resource: bindings, ignored listing per whitelist
May  2 02:57:35.285: INFO: namespace: e2e-tests-watch-tt8sp no longer exists
May  2 02:57:35.302: INFO: namespace: e2e-tests-watch-tt8sp, total namespaces: 47, active: 47, terminating: 0
May  2 02:57:35.317: INFO: namespace e2e-tests-watch-tt8sp deletion completed in 8.114516673s

â€¢ [SLOW TEST:19.316 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:57:35.317: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-wqchg
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
May  2 02:57:36.317: INFO: Found 0 stateful pods, waiting for 3
May  2 02:57:46.334: INFO: Found 2 stateful pods, waiting for 3
May  2 02:57:56.335: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  2 02:57:56.335: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  2 02:57:56.335: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May  2 02:58:06.334: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  2 02:58:06.334: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  2 02:58:06.334: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May  2 02:58:06.430: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May  2 02:58:06.507: INFO: Updating stateful set ss2
May  2 02:58:06.542: INFO: Waiting for Pod e2e-tests-statefulset-wqchg/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May  2 02:58:16.643: INFO: Found 2 stateful pods, waiting for 3
May  2 02:58:26.660: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  2 02:58:26.660: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  2 02:58:26.660: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May  2 02:58:36.660: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  2 02:58:36.660: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  2 02:58:36.660: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May  2 02:58:36.736: INFO: Updating stateful set ss2
May  2 02:58:36.771: INFO: Waiting for Pod e2e-tests-statefulset-wqchg/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May  2 02:58:46.850: INFO: Updating stateful set ss2
May  2 02:58:46.883: INFO: Waiting for StatefulSet e2e-tests-statefulset-wqchg/ss2 to complete update
May  2 02:58:46.883: INFO: Waiting for Pod e2e-tests-statefulset-wqchg/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May  2 02:58:56.916: INFO: Waiting for StatefulSet e2e-tests-statefulset-wqchg/ss2 to complete update
May  2 02:58:56.916: INFO: Waiting for Pod e2e-tests-statefulset-wqchg/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May  2 02:59:06.917: INFO: Waiting for StatefulSet e2e-tests-statefulset-wqchg/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May  2 02:59:16.917: INFO: Deleting all statefulset in ns e2e-tests-statefulset-wqchg
May  2 02:59:16.933: INFO: Scaling statefulset ss2 to 0
May  2 02:59:37.002: INFO: Waiting for statefulset status.replicas updated to 0
May  2 02:59:37.019: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:59:37.073: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-wqchg" for this suite.
May  2 02:59:43.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:59:43.719: INFO: namespace: e2e-tests-statefulset-wqchg, resource: bindings, ignored listing per whitelist
May  2 02:59:44.653: INFO: namespace: e2e-tests-statefulset-wqchg, resource: packagemanifests, items remaining: 1
May  2 02:59:45.186: INFO: namespace: e2e-tests-statefulset-wqchg no longer exists
May  2 02:59:45.203: INFO: namespace: e2e-tests-statefulset-wqchg, total namespaces: 47, active: 47, terminating: 0
May  2 02:59:45.218: INFO: namespace e2e-tests-statefulset-wqchg deletion completed in 8.1152524s

â€¢ [SLOW TEST:129.901 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:59:45.218: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 1 pods
STEP: Gathering metrics
W0502 02:59:46.860424   12606 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  2 02:59:46.860: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 02:59:46.860: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-mcntr" for this suite.
May  2 02:59:52.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 02:59:53.602: INFO: namespace: e2e-tests-gc-mcntr, resource: bindings, ignored listing per whitelist
May  2 02:59:53.657: INFO: namespace: e2e-tests-gc-mcntr, resource: packagemanifests, items remaining: 1
May  2 02:59:54.958: INFO: namespace: e2e-tests-gc-mcntr no longer exists
May  2 02:59:54.975: INFO: namespace: e2e-tests-gc-mcntr, total namespaces: 47, active: 47, terminating: 0
May  2 02:59:54.990: INFO: namespace e2e-tests-gc-mcntr deletion completed in 8.113481759s

â€¢ [SLOW TEST:9.772 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 02:59:54.990: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-5d5d387d-6c86-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 02:59:55.977: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d601356-6c86-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-rgs7b" to be "success or failure"
May  2 02:59:55.993: INFO: Pod "pod-projected-configmaps-5d601356-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.658607ms
May  2 02:59:58.009: INFO: Pod "pod-projected-configmaps-5d601356-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032334651s
May  2 03:00:00.026: INFO: Pod "pod-projected-configmaps-5d601356-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049007358s
May  2 03:00:02.045: INFO: Pod "pod-projected-configmaps-5d601356-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067740289s
May  2 03:00:04.061: INFO: Pod "pod-projected-configmaps-5d601356-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.084157937s
May  2 03:00:06.077: INFO: Pod "pod-projected-configmaps-5d601356-6c86-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.10021492s
STEP: Saw pod success
May  2 03:00:06.077: INFO: Pod "pod-projected-configmaps-5d601356-6c86-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:00:06.093: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-configmaps-5d601356-6c86-11e9-97f0-0a58ac103caa container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  2 03:00:06.137: INFO: Waiting for pod pod-projected-configmaps-5d601356-6c86-11e9-97f0-0a58ac103caa to disappear
May  2 03:00:06.152: INFO: Pod pod-projected-configmaps-5d601356-6c86-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:00:06.152: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rgs7b" for this suite.
May  2 03:00:12.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:00:12.983: INFO: namespace: e2e-tests-projected-rgs7b, resource: packagemanifests, items remaining: 1
May  2 03:00:13.578: INFO: namespace: e2e-tests-projected-rgs7b, resource: bindings, ignored listing per whitelist
May  2 03:00:14.268: INFO: namespace: e2e-tests-projected-rgs7b no longer exists
May  2 03:00:14.287: INFO: namespace: e2e-tests-projected-rgs7b, total namespaces: 47, active: 47, terminating: 0
May  2 03:00:14.303: INFO: namespace e2e-tests-projected-rgs7b deletion completed in 8.121522973s

â€¢ [SLOW TEST:19.313 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:00:14.303: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May  2 03:00:15.244: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:00:25.290: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-srr2p" for this suite.
May  2 03:00:31.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:00:32.048: INFO: namespace: e2e-tests-init-container-srr2p, resource: packagemanifests, items remaining: 1
May  2 03:00:32.869: INFO: namespace: e2e-tests-init-container-srr2p, resource: bindings, ignored listing per whitelist
May  2 03:00:33.402: INFO: namespace: e2e-tests-init-container-srr2p no longer exists
May  2 03:00:33.418: INFO: namespace: e2e-tests-init-container-srr2p, total namespaces: 47, active: 47, terminating: 0
May  2 03:00:33.433: INFO: namespace e2e-tests-init-container-srr2p deletion completed in 8.113796483s

â€¢ [SLOW TEST:19.130 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:00:33.434: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:00:34.399: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-744ae3a8-6c86-11e9-97f0-0a58ac103caa
STEP: Creating configMap with name cm-test-opt-upd-744ae3eb-6c86-11e9-97f0-0a58ac103caa
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-744ae3a8-6c86-11e9-97f0-0a58ac103caa
STEP: Updating configmap cm-test-opt-upd-744ae3eb-6c86-11e9-97f0-0a58ac103caa
STEP: Creating configMap with name cm-test-opt-create-744ae47b-6c86-11e9-97f0-0a58ac103caa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:02:15.638: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7tpns" for this suite.
May  2 03:02:39.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:02:40.268: INFO: namespace: e2e-tests-projected-7tpns, resource: bindings, ignored listing per whitelist
May  2 03:02:40.781: INFO: namespace: e2e-tests-projected-7tpns, resource: packagemanifests, items remaining: 1
May  2 03:02:41.751: INFO: namespace: e2e-tests-projected-7tpns no longer exists
May  2 03:02:41.768: INFO: namespace: e2e-tests-projected-7tpns, total namespaces: 47, active: 47, terminating: 0
May  2 03:02:41.784: INFO: namespace e2e-tests-projected-7tpns deletion completed in 26.116657348s

â€¢ [SLOW TEST:128.350 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:02:41.784: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-wtdqv/secret-test-c0c6480f-6c86-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 03:02:42.759: INFO: Waiting up to 5m0s for pod "pod-configmaps-c0c8eb3a-6c86-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-secrets-wtdqv" to be "success or failure"
May  2 03:02:42.774: INFO: Pod "pod-configmaps-c0c8eb3a-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.639018ms
May  2 03:02:44.792: INFO: Pod "pod-configmaps-c0c8eb3a-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033079965s
May  2 03:02:46.809: INFO: Pod "pod-configmaps-c0c8eb3a-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049897246s
May  2 03:02:48.826: INFO: Pod "pod-configmaps-c0c8eb3a-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067020627s
May  2 03:02:50.842: INFO: Pod "pod-configmaps-c0c8eb3a-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083171492s
May  2 03:02:52.858: INFO: Pod "pod-configmaps-c0c8eb3a-6c86-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.09913518s
STEP: Saw pod success
May  2 03:02:52.858: INFO: Pod "pod-configmaps-c0c8eb3a-6c86-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:02:52.874: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-configmaps-c0c8eb3a-6c86-11e9-97f0-0a58ac103caa container env-test: <nil>
STEP: delete the pod
May  2 03:02:52.919: INFO: Waiting for pod pod-configmaps-c0c8eb3a-6c86-11e9-97f0-0a58ac103caa to disappear
May  2 03:02:52.934: INFO: Pod pod-configmaps-c0c8eb3a-6c86-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:02:52.934: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-wtdqv" for this suite.
May  2 03:02:59.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:03:00.833: INFO: namespace: e2e-tests-secrets-wtdqv, resource: bindings, ignored listing per whitelist
May  2 03:03:00.936: INFO: namespace: e2e-tests-secrets-wtdqv, resource: packagemanifests, items remaining: 1
May  2 03:03:01.049: INFO: namespace: e2e-tests-secrets-wtdqv no longer exists
May  2 03:03:01.066: INFO: namespace: e2e-tests-secrets-wtdqv, total namespaces: 47, active: 47, terminating: 0
May  2 03:03:01.081: INFO: namespace e2e-tests-secrets-wtdqv deletion completed in 8.116546961s

â€¢ [SLOW TEST:19.297 seconds]
[sig-api-machinery] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:03:01.081: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 03:03:02.072: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc4bb3f8-6c86-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-7lwml" to be "success or failure"
May  2 03:03:02.088: INFO: Pod "downwardapi-volume-cc4bb3f8-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.93637ms
May  2 03:03:04.109: INFO: Pod "downwardapi-volume-cc4bb3f8-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036554909s
May  2 03:03:06.126: INFO: Pod "downwardapi-volume-cc4bb3f8-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053443973s
May  2 03:03:08.142: INFO: Pod "downwardapi-volume-cc4bb3f8-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069677145s
May  2 03:03:10.158: INFO: Pod "downwardapi-volume-cc4bb3f8-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085822076s
May  2 03:03:12.175: INFO: Pod "downwardapi-volume-cc4bb3f8-6c86-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.102420216s
STEP: Saw pod success
May  2 03:03:12.175: INFO: Pod "downwardapi-volume-cc4bb3f8-6c86-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:03:12.190: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-cc4bb3f8-6c86-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 03:03:12.233: INFO: Waiting for pod downwardapi-volume-cc4bb3f8-6c86-11e9-97f0-0a58ac103caa to disappear
May  2 03:03:12.249: INFO: Pod downwardapi-volume-cc4bb3f8-6c86-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:03:12.249: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7lwml" for this suite.
May  2 03:03:18.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:03:19.189: INFO: namespace: e2e-tests-projected-7lwml, resource: packagemanifests, items remaining: 1
May  2 03:03:19.451: INFO: namespace: e2e-tests-projected-7lwml, resource: bindings, ignored listing per whitelist
May  2 03:03:20.362: INFO: namespace: e2e-tests-projected-7lwml no longer exists
May  2 03:03:20.379: INFO: namespace: e2e-tests-projected-7lwml, total namespaces: 47, active: 47, terminating: 0
May  2 03:03:20.395: INFO: namespace e2e-tests-projected-7lwml deletion completed in 8.115625817s

â€¢ [SLOW TEST:19.314 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:03:20.395: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 03:03:21.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7c9e75a-6c86-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-7r8x8" to be "success or failure"
May  2 03:03:21.372: INFO: Pod "downwardapi-volume-d7c9e75a-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.83952ms
May  2 03:03:23.389: INFO: Pod "downwardapi-volume-d7c9e75a-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033384662s
May  2 03:03:25.405: INFO: Pod "downwardapi-volume-d7c9e75a-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050015036s
May  2 03:03:27.422: INFO: Pod "downwardapi-volume-d7c9e75a-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066514261s
May  2 03:03:29.438: INFO: Pod "downwardapi-volume-d7c9e75a-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08265218s
May  2 03:03:31.454: INFO: Pod "downwardapi-volume-d7c9e75a-6c86-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098773728s
STEP: Saw pod success
May  2 03:03:31.454: INFO: Pod "downwardapi-volume-d7c9e75a-6c86-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:03:31.470: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-d7c9e75a-6c86-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 03:03:31.513: INFO: Waiting for pod downwardapi-volume-d7c9e75a-6c86-11e9-97f0-0a58ac103caa to disappear
May  2 03:03:31.528: INFO: Pod downwardapi-volume-d7c9e75a-6c86-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:03:31.528: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7r8x8" for this suite.
May  2 03:03:37.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:03:38.243: INFO: namespace: e2e-tests-downward-api-7r8x8, resource: bindings, ignored listing per whitelist
May  2 03:03:38.406: INFO: namespace: e2e-tests-downward-api-7r8x8, resource: packagemanifests, items remaining: 1
May  2 03:03:39.641: INFO: namespace: e2e-tests-downward-api-7r8x8 no longer exists
May  2 03:03:39.657: INFO: namespace: e2e-tests-downward-api-7r8x8, total namespaces: 47, active: 47, terminating: 0
May  2 03:03:39.672: INFO: namespace e2e-tests-downward-api-7r8x8 deletion completed in 8.114581684s

â€¢ [SLOW TEST:19.278 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:03:39.673: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:03:40.711: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
May  2 03:03:40.747: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-tl4z8/daemonsets","resourceVersion":"41553"},"items":null}

May  2 03:03:40.762: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-tl4z8/pods","resourceVersion":"41554"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:03:40.824: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-tl4z8" for this suite.
May  2 03:03:46.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:03:47.442: INFO: namespace: e2e-tests-daemonsets-tl4z8, resource: bindings, ignored listing per whitelist
May  2 03:03:48.271: INFO: namespace: e2e-tests-daemonsets-tl4z8, resource: packagemanifests, items remaining: 1
May  2 03:03:48.924: INFO: namespace: e2e-tests-daemonsets-tl4z8 no longer exists
May  2 03:03:48.942: INFO: namespace: e2e-tests-daemonsets-tl4z8, total namespaces: 47, active: 47, terminating: 0
May  2 03:03:48.966: INFO: namespace e2e-tests-daemonsets-tl4z8 deletion completed in 8.125638984s

S [SKIPPING] [9.293 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  May  2 03:03:40.711: Requires at least 2 nodes (not -1)

  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:03:48.966: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-e8d2daee-6c86-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 03:03:49.950: INFO: Waiting up to 5m0s for pod "pod-configmaps-e8d5bc61-6c86-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-configmap-2cvfj" to be "success or failure"
May  2 03:03:49.966: INFO: Pod "pod-configmaps-e8d5bc61-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.037224ms
May  2 03:03:51.983: INFO: Pod "pod-configmaps-e8d5bc61-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033016196s
May  2 03:03:54.000: INFO: Pod "pod-configmaps-e8d5bc61-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050036457s
May  2 03:03:56.020: INFO: Pod "pod-configmaps-e8d5bc61-6c86-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070077679s
May  2 03:03:58.039: INFO: Pod "pod-configmaps-e8d5bc61-6c86-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.089099513s
STEP: Saw pod success
May  2 03:03:58.039: INFO: Pod "pod-configmaps-e8d5bc61-6c86-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:03:58.055: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-configmaps-e8d5bc61-6c86-11e9-97f0-0a58ac103caa container configmap-volume-test: <nil>
STEP: delete the pod
May  2 03:03:58.102: INFO: Waiting for pod pod-configmaps-e8d5bc61-6c86-11e9-97f0-0a58ac103caa to disappear
May  2 03:03:58.117: INFO: Pod pod-configmaps-e8d5bc61-6c86-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:03:58.117: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-2cvfj" for this suite.
May  2 03:04:04.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:04:05.173: INFO: namespace: e2e-tests-configmap-2cvfj, resource: bindings, ignored listing per whitelist
May  2 03:04:05.382: INFO: namespace: e2e-tests-configmap-2cvfj, resource: packagemanifests, items remaining: 1
May  2 03:04:06.258: INFO: namespace: e2e-tests-configmap-2cvfj no longer exists
May  2 03:04:06.279: INFO: namespace: e2e-tests-configmap-2cvfj, total namespaces: 47, active: 47, terminating: 0
May  2 03:04:06.296: INFO: namespace e2e-tests-configmap-2cvfj deletion completed in 8.144532627s

â€¢ [SLOW TEST:17.330 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:04:06.296: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May  2 03:04:07.394: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
May  2 03:04:07.448: INFO: Waiting for terminating namespaces to be deleted...
May  2 03:04:07.466: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-135-216.ec2.internal before test
May  2 03:04:07.503: INFO: ovs-ckrk2 from openshift-sdn started at 2019-05-02 01:49:53 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container openvswitch ready: true, restart count 0
May  2 03:04:07.503: INFO: machine-config-daemon-6kqb6 from openshift-machine-config-operator started at 2019-05-02 01:50:32 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container machine-config-daemon ready: true, restart count 0
May  2 03:04:07.503: INFO: node-ca-cvq2r from openshift-image-registry started at 2019-05-02 01:51:28 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container node-ca ready: true, restart count 0
May  2 03:04:07.503: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-05-02 01:52:08 +0000 UTC (3 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container alertmanager ready: true, restart count 0
May  2 03:04:07.503: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  2 03:04:07.503: INFO: 	Container config-reloader ready: true, restart count 0
May  2 03:04:07.503: INFO: prometheus-adapter-88656d548-swtj7 from openshift-monitoring started at 2019-05-02 01:52:10 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  2 03:04:07.503: INFO: node-exporter-jvhfx from openshift-monitoring started at 2019-05-02 01:51:03 +0000 UTC (2 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:04:07.503: INFO: 	Container node-exporter ready: true, restart count 0
May  2 03:04:07.503: INFO: multus-8jgrn from openshift-multus started at 2019-05-02 01:49:53 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container kube-multus ready: true, restart count 0
May  2 03:04:07.503: INFO: tuned-t9pcv from openshift-cluster-node-tuning-operator started at 2019-05-02 01:50:14 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container tuned ready: true, restart count 0
May  2 03:04:07.503: INFO: certified-operators-6f5774bb49-zqjw9 from openshift-marketplace started at 2019-05-02 01:50:35 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container certified-operators ready: true, restart count 0
May  2 03:04:07.503: INFO: router-default-8db6b5c76-pmbks from openshift-ingress started at 2019-05-02 01:50:55 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container router ready: true, restart count 0
May  2 03:04:07.503: INFO: kube-state-metrics-74d989d8d8-ffvw4 from openshift-monitoring started at 2019-05-02 01:50:34 +0000 UTC (3 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May  2 03:04:07.503: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May  2 03:04:07.503: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  2 03:04:07.503: INFO: community-operators-67554d9c5f-f9lqv from openshift-marketplace started at 2019-05-02 01:50:36 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container community-operators ready: true, restart count 0
May  2 03:04:07.503: INFO: sdn-vnzxb from openshift-sdn started at 2019-05-02 01:49:53 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container sdn ready: true, restart count 0
May  2 03:04:07.503: INFO: dns-default-j5c7f from openshift-dns started at 2019-05-02 01:49:53 +0000 UTC (2 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container dns ready: true, restart count 0
May  2 03:04:07.503: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  2 03:04:07.503: INFO: redhat-operators-8658cf87c-9pfhf from openshift-marketplace started at 2019-05-02 01:50:36 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.503: INFO: 	Container redhat-operators ready: true, restart count 0
May  2 03:04:07.503: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-140-17.ec2.internal before test
May  2 03:04:07.527: INFO: dns-default-l8vqv from openshift-dns started at 2019-05-02 01:49:57 +0000 UTC (2 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container dns ready: true, restart count 0
May  2 03:04:07.527: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  2 03:04:07.527: INFO: multus-ngggs from openshift-multus started at 2019-05-02 01:49:57 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container kube-multus ready: true, restart count 0
May  2 03:04:07.527: INFO: node-ca-9nbxx from openshift-image-registry started at 2019-05-02 01:51:28 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container node-ca ready: true, restart count 0
May  2 03:04:07.527: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-05-02 01:51:31 +0000 UTC (3 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container alertmanager ready: true, restart count 0
May  2 03:04:07.527: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  2 03:04:07.527: INFO: 	Container config-reloader ready: true, restart count 0
May  2 03:04:07.527: INFO: sdn-ww2h4 from openshift-sdn started at 2019-05-02 01:49:57 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container sdn ready: true, restart count 0
May  2 03:04:07.527: INFO: telemeter-client-7cccb458c9-j8xss from openshift-monitoring started at 2019-05-02 01:50:38 +0000 UTC (3 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:04:07.527: INFO: 	Container reload ready: true, restart count 0
May  2 03:04:07.527: INFO: 	Container telemeter-client ready: true, restart count 0
May  2 03:04:07.527: INFO: router-default-8db6b5c76-sn9q9 from openshift-ingress started at 2019-05-02 01:50:55 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container router ready: true, restart count 0
May  2 03:04:07.527: INFO: node-exporter-5ms2n from openshift-monitoring started at 2019-05-02 01:51:03 +0000 UTC (2 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:04:07.527: INFO: 	Container node-exporter ready: true, restart count 0
May  2 03:04:07.527: INFO: image-registry-7d85675b77-2xv8x from openshift-image-registry started at 2019-05-02 01:51:06 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container registry ready: true, restart count 0
May  2 03:04:07.527: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-05-02 01:52:12 +0000 UTC (6 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:04:07.527: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  2 03:04:07.527: INFO: 	Container prometheus ready: true, restart count 1
May  2 03:04:07.527: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May  2 03:04:07.527: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  2 03:04:07.527: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May  2 03:04:07.527: INFO: ovs-9m7kl from openshift-sdn started at 2019-05-02 01:49:57 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container openvswitch ready: true, restart count 0
May  2 03:04:07.527: INFO: tuned-l87zf from openshift-cluster-node-tuning-operator started at 2019-05-02 01:50:14 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container tuned ready: true, restart count 0
May  2 03:04:07.527: INFO: downloads-5b9759bd45-ffhzh from openshift-console started at 2019-05-02 01:50:08 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container download-server ready: true, restart count 0
May  2 03:04:07.527: INFO: machine-config-daemon-fvkb5 from openshift-machine-config-operator started at 2019-05-02 01:50:37 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.527: INFO: 	Container machine-config-daemon ready: true, restart count 0
May  2 03:04:07.527: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-153-32.ec2.internal before test
May  2 03:04:07.564: INFO: prometheus-adapter-88656d548-666vp from openshift-monitoring started at 2019-05-02 01:52:10 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  2 03:04:07.564: INFO: sdn-22xbv from openshift-sdn started at 2019-05-02 01:50:03 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container sdn ready: true, restart count 0
May  2 03:04:07.564: INFO: downloads-5b9759bd45-rw4d7 from openshift-console started at 2019-05-02 01:50:08 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container download-server ready: true, restart count 0
May  2 03:04:07.564: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-05-02 01:52:12 +0000 UTC (6 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:04:07.564: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  2 03:04:07.564: INFO: 	Container prometheus ready: true, restart count 1
May  2 03:04:07.564: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May  2 03:04:07.564: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  2 03:04:07.564: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May  2 03:04:07.564: INFO: multus-vt2rg from openshift-multus started at 2019-05-02 01:50:03 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container kube-multus ready: true, restart count 0
May  2 03:04:07.564: INFO: grafana-5ccd6d5857-65m9v from openshift-monitoring started at 2019-05-02 01:51:27 +0000 UTC (2 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container grafana ready: true, restart count 0
May  2 03:04:07.564: INFO: 	Container grafana-proxy ready: true, restart count 0
May  2 03:04:07.564: INFO: tuned-crx5s from openshift-cluster-node-tuning-operator started at 2019-05-02 01:50:14 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container tuned ready: true, restart count 0
May  2 03:04:07.564: INFO: prometheus-operator-f7c6b5c59-6wjgw from openshift-monitoring started at 2019-05-02 01:51:59 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container prometheus-operator ready: true, restart count 0
May  2 03:04:07.564: INFO: node-exporter-c4m2k from openshift-monitoring started at 2019-05-02 01:51:03 +0000 UTC (2 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:04:07.564: INFO: 	Container node-exporter ready: true, restart count 0
May  2 03:04:07.564: INFO: node-ca-vpzc4 from openshift-image-registry started at 2019-05-02 01:51:28 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container node-ca ready: true, restart count 0
May  2 03:04:07.564: INFO: dns-default-kqjfp from openshift-dns started at 2019-05-02 01:50:03 +0000 UTC (2 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container dns ready: true, restart count 0
May  2 03:04:07.564: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  2 03:04:07.564: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-05-02 01:51:47 +0000 UTC (3 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container alertmanager ready: true, restart count 0
May  2 03:04:07.564: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  2 03:04:07.564: INFO: 	Container config-reloader ready: true, restart count 0
May  2 03:04:07.564: INFO: ovs-zvrlj from openshift-sdn started at 2019-05-02 01:50:03 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container openvswitch ready: true, restart count 0
May  2 03:04:07.564: INFO: machine-config-daemon-clvrk from openshift-machine-config-operator started at 2019-05-02 01:51:01 +0000 UTC (1 container statuses recorded)
May  2 03:04:07.564: INFO: 	Container machine-config-daemon ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node ip-10-0-135-216.ec2.internal
STEP: verifying the node has the label node ip-10-0-140-17.ec2.internal
STEP: verifying the node has the label node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod tuned-crx5s requesting resource cpu=0m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod tuned-l87zf requesting resource cpu=0m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod tuned-t9pcv requesting resource cpu=0m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod downloads-5b9759bd45-ffhzh requesting resource cpu=0m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod downloads-5b9759bd45-rw4d7 requesting resource cpu=0m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod dns-default-j5c7f requesting resource cpu=110m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod dns-default-kqjfp requesting resource cpu=110m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod dns-default-l8vqv requesting resource cpu=110m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod image-registry-7d85675b77-2xv8x requesting resource cpu=100m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod node-ca-9nbxx requesting resource cpu=0m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod node-ca-cvq2r requesting resource cpu=0m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod node-ca-vpzc4 requesting resource cpu=0m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod router-default-8db6b5c76-pmbks requesting resource cpu=100m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod router-default-8db6b5c76-sn9q9 requesting resource cpu=100m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod machine-config-daemon-6kqb6 requesting resource cpu=20m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod machine-config-daemon-clvrk requesting resource cpu=20m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod machine-config-daemon-fvkb5 requesting resource cpu=20m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod certified-operators-6f5774bb49-zqjw9 requesting resource cpu=0m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod community-operators-67554d9c5f-f9lqv requesting resource cpu=0m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod redhat-operators-8658cf87c-9pfhf requesting resource cpu=0m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod alertmanager-main-0 requesting resource cpu=0m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod alertmanager-main-1 requesting resource cpu=0m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod alertmanager-main-2 requesting resource cpu=0m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod grafana-5ccd6d5857-65m9v requesting resource cpu=100m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod kube-state-metrics-74d989d8d8-ffvw4 requesting resource cpu=0m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod node-exporter-5ms2n requesting resource cpu=10m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod node-exporter-c4m2k requesting resource cpu=10m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod node-exporter-jvhfx requesting resource cpu=10m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod prometheus-adapter-88656d548-666vp requesting resource cpu=0m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod prometheus-adapter-88656d548-swtj7 requesting resource cpu=0m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod prometheus-k8s-0 requesting resource cpu=0m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod prometheus-k8s-1 requesting resource cpu=0m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod prometheus-operator-f7c6b5c59-6wjgw requesting resource cpu=0m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod telemeter-client-7cccb458c9-j8xss requesting resource cpu=10m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod multus-8jgrn requesting resource cpu=0m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod multus-ngggs requesting resource cpu=0m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod multus-vt2rg requesting resource cpu=0m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod ovs-9m7kl requesting resource cpu=200m on Node ip-10-0-140-17.ec2.internal
May  2 03:04:07.847: INFO: Pod ovs-ckrk2 requesting resource cpu=200m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod ovs-zvrlj requesting resource cpu=200m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod sdn-22xbv requesting resource cpu=100m on Node ip-10-0-153-32.ec2.internal
May  2 03:04:07.847: INFO: Pod sdn-vnzxb requesting resource cpu=100m on Node ip-10-0-135-216.ec2.internal
May  2 03:04:07.847: INFO: Pod sdn-ww2h4 requesting resource cpu=100m on Node ip-10-0-140-17.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3849145-6c86-11e9-97f0-0a58ac103caa.159abea782c0a6d2], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-2j7lw/filler-pod-f3849145-6c86-11e9-97f0-0a58ac103caa to ip-10-0-140-17.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3849145-6c86-11e9-97f0-0a58ac103caa.159abea948cecce2], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3849145-6c86-11e9-97f0-0a58ac103caa.159abea9b4a2a582], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3849145-6c86-11e9-97f0-0a58ac103caa.159abea9bed735d8], Reason = [Created], Message = [Created container filler-pod-f3849145-6c86-11e9-97f0-0a58ac103caa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3849145-6c86-11e9-97f0-0a58ac103caa.159abea9bff8b4fc], Reason = [Started], Message = [Started container filler-pod-f3849145-6c86-11e9-97f0-0a58ac103caa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f388c497-6c86-11e9-97f0-0a58ac103caa.159abea78435452a], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-2j7lw/filler-pod-f388c497-6c86-11e9-97f0-0a58ac103caa to ip-10-0-153-32.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f388c497-6c86-11e9-97f0-0a58ac103caa.159abea967d2b597], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f388c497-6c86-11e9-97f0-0a58ac103caa.159abea9dc1c4254], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f388c497-6c86-11e9-97f0-0a58ac103caa.159abea9e669ce2c], Reason = [Created], Message = [Created container filler-pod-f388c497-6c86-11e9-97f0-0a58ac103caa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f388c497-6c86-11e9-97f0-0a58ac103caa.159abea9e9730736], Reason = [Started], Message = [Started container filler-pod-f388c497-6c86-11e9-97f0-0a58ac103caa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f38d24ae-6c86-11e9-97f0-0a58ac103caa.159abea785e7d80d], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-2j7lw/filler-pod-f38d24ae-6c86-11e9-97f0-0a58ac103caa to ip-10-0-135-216.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f38d24ae-6c86-11e9-97f0-0a58ac103caa.159abea943daeb0c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f38d24ae-6c86-11e9-97f0-0a58ac103caa.159abea94e17bcfd], Reason = [Created], Message = [Created container filler-pod-f38d24ae-6c86-11e9-97f0-0a58ac103caa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f38d24ae-6c86-11e9-97f0-0a58ac103caa.159abea94f48eabc], Reason = [Started], Message = [Started container filler-pod-f38d24ae-6c86-11e9-97f0-0a58ac103caa]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.159abeaa58304e7f], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) were unschedulable.]
STEP: removing the label node off the node ip-10-0-140-17.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-153-32.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-135-216.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:04:21.218: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-2j7lw" for this suite.
May  2 03:04:27.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:04:28.167: INFO: namespace: e2e-tests-sched-pred-2j7lw, resource: packagemanifests, items remaining: 1
May  2 03:04:29.063: INFO: namespace: e2e-tests-sched-pred-2j7lw, resource: bindings, ignored listing per whitelist
May  2 03:04:29.335: INFO: namespace: e2e-tests-sched-pred-2j7lw no longer exists
May  2 03:04:29.353: INFO: namespace: e2e-tests-sched-pred-2j7lw, total namespaces: 47, active: 47, terminating: 0
May  2 03:04:29.368: INFO: namespace e2e-tests-sched-pred-2j7lw deletion completed in 8.12128026s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:23.072 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:04:29.368: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
May  2 03:04:30.299: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy --unix-socket=/tmp/kubectl-proxy-unix299348222/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:04:30.364: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2swpj" for this suite.
May  2 03:04:36.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:04:37.517: INFO: namespace: e2e-tests-kubectl-2swpj, resource: bindings, ignored listing per whitelist
May  2 03:04:37.598: INFO: namespace: e2e-tests-kubectl-2swpj, resource: packagemanifests, items remaining: 1
May  2 03:04:38.463: INFO: namespace: e2e-tests-kubectl-2swpj no longer exists
May  2 03:04:38.479: INFO: namespace: e2e-tests-kubectl-2swpj, total namespaces: 47, active: 47, terminating: 0
May  2 03:04:38.494: INFO: namespace e2e-tests-kubectl-2swpj deletion completed in 8.113305783s

â€¢ [SLOW TEST:9.126 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:04:38.494: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May  2 03:04:50.068: INFO: Successfully updated pod "pod-update-activedeadlineseconds-065811fa-6c87-11e9-97f0-0a58ac103caa"
May  2 03:04:50.068: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-065811fa-6c87-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-pods-tt9zj" to be "terminated due to deadline exceeded"
May  2 03:04:50.084: INFO: Pod "pod-update-activedeadlineseconds-065811fa-6c87-11e9-97f0-0a58ac103caa": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 15.573206ms
May  2 03:04:50.084: INFO: Pod "pod-update-activedeadlineseconds-065811fa-6c87-11e9-97f0-0a58ac103caa" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:04:50.084: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-tt9zj" for this suite.
May  2 03:04:56.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:04:56.762: INFO: namespace: e2e-tests-pods-tt9zj, resource: bindings, ignored listing per whitelist
May  2 03:04:57.571: INFO: namespace: e2e-tests-pods-tt9zj, resource: packagemanifests, items remaining: 1
May  2 03:04:58.197: INFO: namespace: e2e-tests-pods-tt9zj no longer exists
May  2 03:04:58.214: INFO: namespace: e2e-tests-pods-tt9zj, total namespaces: 47, active: 47, terminating: 0
May  2 03:04:58.229: INFO: namespace e2e-tests-pods-tt9zj deletion completed in 8.115691903s

â€¢ [SLOW TEST:19.734 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:04:58.229: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-qbdtv
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
May  2 03:04:59.210: INFO: Found 0 stateful pods, waiting for 3
May  2 03:05:09.227: INFO: Found 2 stateful pods, waiting for 3
May  2 03:05:19.227: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  2 03:05:19.227: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  2 03:05:19.227: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May  2 03:05:29.227: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  2 03:05:29.227: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  2 03:05:29.227: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May  2 03:05:29.276: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-qbdtv ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  2 03:05:29.629: INFO: stderr: ""
May  2 03:05:29.629: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  2 03:05:29.630: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May  2 03:05:39.740: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May  2 03:05:49.822: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-qbdtv ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:05:50.173: INFO: stderr: ""
May  2 03:05:50.173: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  2 03:05:50.173: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  2 03:06:00.272: INFO: Waiting for StatefulSet e2e-tests-statefulset-qbdtv/ss2 to complete update
May  2 03:06:00.273: INFO: Waiting for Pod e2e-tests-statefulset-qbdtv/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May  2 03:06:00.273: INFO: Waiting for Pod e2e-tests-statefulset-qbdtv/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May  2 03:06:10.307: INFO: Waiting for StatefulSet e2e-tests-statefulset-qbdtv/ss2 to complete update
May  2 03:06:10.307: INFO: Waiting for Pod e2e-tests-statefulset-qbdtv/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May  2 03:06:20.306: INFO: Waiting for StatefulSet e2e-tests-statefulset-qbdtv/ss2 to complete update
May  2 03:06:20.307: INFO: Waiting for Pod e2e-tests-statefulset-qbdtv/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May  2 03:06:30.307: INFO: Waiting for StatefulSet e2e-tests-statefulset-qbdtv/ss2 to complete update
STEP: Rolling back to a previous revision
May  2 03:06:40.307: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-qbdtv ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  2 03:06:40.658: INFO: stderr: ""
May  2 03:06:40.658: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  2 03:06:40.658: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  2 03:06:50.772: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May  2 03:06:50.821: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-qbdtv ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:06:51.156: INFO: stderr: ""
May  2 03:06:51.156: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  2 03:06:51.156: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  2 03:07:01.254: INFO: Waiting for StatefulSet e2e-tests-statefulset-qbdtv/ss2 to complete update
May  2 03:07:01.254: INFO: Waiting for Pod e2e-tests-statefulset-qbdtv/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May  2 03:07:01.254: INFO: Waiting for Pod e2e-tests-statefulset-qbdtv/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
May  2 03:07:11.287: INFO: Waiting for StatefulSet e2e-tests-statefulset-qbdtv/ss2 to complete update
May  2 03:07:11.288: INFO: Waiting for Pod e2e-tests-statefulset-qbdtv/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May  2 03:07:21.287: INFO: Waiting for StatefulSet e2e-tests-statefulset-qbdtv/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May  2 03:07:31.288: INFO: Deleting all statefulset in ns e2e-tests-statefulset-qbdtv
May  2 03:07:31.305: INFO: Scaling statefulset ss2 to 0
May  2 03:08:11.374: INFO: Waiting for statefulset status.replicas updated to 0
May  2 03:08:11.391: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:08:11.451: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-qbdtv" for this suite.
May  2 03:08:17.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:08:18.610: INFO: namespace: e2e-tests-statefulset-qbdtv, resource: packagemanifests, items remaining: 1
May  2 03:08:19.411: INFO: namespace: e2e-tests-statefulset-qbdtv, resource: bindings, ignored listing per whitelist
May  2 03:08:19.577: INFO: namespace: e2e-tests-statefulset-qbdtv no longer exists
May  2 03:08:19.594: INFO: namespace: e2e-tests-statefulset-qbdtv, total namespaces: 47, active: 47, terminating: 0
May  2 03:08:19.609: INFO: namespace e2e-tests-statefulset-qbdtv deletion completed in 8.115814991s

â€¢ [SLOW TEST:201.380 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:08:19.609: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
May  2 03:08:20.545: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=e2e-tests-kubectl-sds4c run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May  2 03:08:30.676: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May  2 03:08:30.676: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:08:32.708: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sds4c" for this suite.
May  2 03:08:40.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:08:41.457: INFO: namespace: e2e-tests-kubectl-sds4c, resource: bindings, ignored listing per whitelist
May  2 03:08:42.531: INFO: namespace: e2e-tests-kubectl-sds4c, resource: packagemanifests, items remaining: 1
May  2 03:08:42.822: INFO: namespace: e2e-tests-kubectl-sds4c no longer exists
May  2 03:08:42.839: INFO: namespace: e2e-tests-kubectl-sds4c, total namespaces: 47, active: 47, terminating: 0
May  2 03:08:42.856: INFO: namespace e2e-tests-kubectl-sds4c deletion completed in 10.118036164s

â€¢ [SLOW TEST:23.246 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:08:42.856: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
May  2 03:08:53.944: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-98012704-6c87-11e9-97f0-0a58ac103caa", GenerateName:"", Namespace:"e2e-tests-pods-8dvrd", SelfLink:"/api/v1/namespaces/e2e-tests-pods-8dvrd/pods/pod-submit-remove-98012704-6c87-11e9-97f0-0a58ac103caa", UID:"9807d0b1-6c87-11e9-9e44-12f3365d453a", ResourceVersion:"43873", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63692363323, loc:(*time.Location)(0x7b5bbe0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"810797544"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.102\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-nxtm4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0008955c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-nxtm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00063e320), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0029a7798), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-135-216.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002643ec0), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-p8rgv"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0029a77e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0029a7800)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0029a7808), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0029a780c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692363323, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692363332, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692363332, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692363323, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.135.216", PodIP:"10.131.0.102", StartTime:(*v1.Time)(0xc002868880), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0028688a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"docker.io/library/nginx:1.14-alpine", ImageID:"docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760", ContainerID:"cri-o://6ea5c29858ca59ce70dc529eef16e4c06bfe84c0d19c6db9536b2bbc94a5c581"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
May  2 03:08:59.013: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:08:59.029: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-8dvrd" for this suite.
May  2 03:09:05.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:09:05.677: INFO: namespace: e2e-tests-pods-8dvrd, resource: bindings, ignored listing per whitelist
May  2 03:09:06.865: INFO: namespace: e2e-tests-pods-8dvrd, resource: packagemanifests, items remaining: 1
May  2 03:09:07.131: INFO: namespace: e2e-tests-pods-8dvrd no longer exists
May  2 03:09:07.148: INFO: namespace: e2e-tests-pods-8dvrd, total namespaces: 47, active: 47, terminating: 0
May  2 03:09:07.163: INFO: namespace e2e-tests-pods-8dvrd deletion completed in 8.117496791s

â€¢ [SLOW TEST:24.307 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:09:07.163: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:09:55.293: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-k75gw" for this suite.
May  2 03:10:01.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:10:02.502: INFO: namespace: e2e-tests-container-runtime-k75gw, resource: bindings, ignored listing per whitelist
May  2 03:10:02.954: INFO: namespace: e2e-tests-container-runtime-k75gw, resource: packagemanifests, items remaining: 1
May  2 03:10:03.410: INFO: namespace: e2e-tests-container-runtime-k75gw no longer exists
May  2 03:10:03.427: INFO: namespace: e2e-tests-container-runtime-k75gw, total namespaces: 47, active: 47, terminating: 0
May  2 03:10:03.442: INFO: namespace e2e-tests-container-runtime-k75gw deletion completed in 8.118793465s

â€¢ [SLOW TEST:56.279 seconds]
[k8s.io] Container Runtime
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:10:03.442: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-74wdw
May  2 03:10:14.462: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-74wdw
STEP: checking the pod's current state and verifying that restartCount is present
May  2 03:10:14.478: INFO: Initial restart count of pod liveness-http is 0
May  2 03:10:34.674: INFO: Restart count of pod e2e-tests-container-probe-74wdw/liveness-http is now 1 (20.19609632s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:10:34.697: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-74wdw" for this suite.
May  2 03:10:40.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:10:42.193: INFO: namespace: e2e-tests-container-probe-74wdw, resource: packagemanifests, items remaining: 1
May  2 03:10:42.193: INFO: namespace: e2e-tests-container-probe-74wdw, resource: bindings, ignored listing per whitelist
May  2 03:10:42.813: INFO: namespace: e2e-tests-container-probe-74wdw no longer exists
May  2 03:10:42.830: INFO: namespace: e2e-tests-container-probe-74wdw, total namespaces: 47, active: 47, terminating: 0
May  2 03:10:42.846: INFO: namespace e2e-tests-container-probe-74wdw deletion completed in 8.117224186s

â€¢ [SLOW TEST:39.404 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:10:42.846: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May  2 03:10:55.998: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9bzc8 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 03:10:55.998: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 03:10:56.185: INFO: Exec stderr: ""
May  2 03:10:56.185: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9bzc8 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 03:10:56.185: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 03:10:56.352: INFO: Exec stderr: ""
May  2 03:10:56.352: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9bzc8 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 03:10:56.352: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 03:10:56.518: INFO: Exec stderr: ""
May  2 03:10:56.518: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9bzc8 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 03:10:56.518: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 03:10:56.679: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May  2 03:10:56.679: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9bzc8 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 03:10:56.679: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 03:10:56.878: INFO: Exec stderr: ""
May  2 03:10:56.878: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9bzc8 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 03:10:56.878: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 03:10:57.061: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May  2 03:10:57.061: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9bzc8 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 03:10:57.061: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 03:10:57.228: INFO: Exec stderr: ""
May  2 03:10:57.228: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9bzc8 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 03:10:57.228: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 03:10:57.430: INFO: Exec stderr: ""
May  2 03:10:57.431: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9bzc8 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 03:10:57.431: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 03:10:57.598: INFO: Exec stderr: ""
May  2 03:10:57.598: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-9bzc8 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 03:10:57.598: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 03:10:57.759: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:10:57.759: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-9bzc8" for this suite.
May  2 03:11:35.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:11:36.920: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-9bzc8, resource: bindings, ignored listing per whitelist
May  2 03:11:37.002: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-9bzc8, resource: packagemanifests, items remaining: 1
May  2 03:11:37.870: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-9bzc8 no longer exists
May  2 03:11:37.888: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-9bzc8, total namespaces: 47, active: 47, terminating: 0
May  2 03:11:37.903: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-9bzc8 deletion completed in 40.114502993s

â€¢ [SLOW TEST:55.057 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:11:37.903: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 03:11:38.868: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0053f222-6c88-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-bqxjp" to be "success or failure"
May  2 03:11:38.887: INFO: Pod "downwardapi-volume-0053f222-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 18.51149ms
May  2 03:11:40.903: INFO: Pod "downwardapi-volume-0053f222-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034811905s
May  2 03:11:42.920: INFO: Pod "downwardapi-volume-0053f222-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05194434s
May  2 03:11:44.936: INFO: Pod "downwardapi-volume-0053f222-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067892068s
May  2 03:11:46.952: INFO: Pod "downwardapi-volume-0053f222-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08406754s
May  2 03:11:48.976: INFO: Pod "downwardapi-volume-0053f222-6c88-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.108346114s
STEP: Saw pod success
May  2 03:11:48.977: INFO: Pod "downwardapi-volume-0053f222-6c88-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:11:48.993: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-0053f222-6c88-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 03:11:49.035: INFO: Waiting for pod downwardapi-volume-0053f222-6c88-11e9-97f0-0a58ac103caa to disappear
May  2 03:11:49.051: INFO: Pod downwardapi-volume-0053f222-6c88-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:11:49.052: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bqxjp" for this suite.
May  2 03:11:55.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:11:56.235: INFO: namespace: e2e-tests-projected-bqxjp, resource: bindings, ignored listing per whitelist
May  2 03:11:56.369: INFO: namespace: e2e-tests-projected-bqxjp, resource: packagemanifests, items remaining: 1
May  2 03:11:57.164: INFO: namespace: e2e-tests-projected-bqxjp no longer exists
May  2 03:11:57.182: INFO: namespace: e2e-tests-projected-bqxjp, total namespaces: 47, active: 47, terminating: 0
May  2 03:11:57.197: INFO: namespace e2e-tests-projected-bqxjp deletion completed in 8.114624611s

â€¢ [SLOW TEST:19.294 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:11:57.198: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-0bd61c84-6c88-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 03:11:58.190: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bd8f182-6c88-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-configmap-zpsj6" to be "success or failure"
May  2 03:11:58.210: INFO: Pod "pod-configmaps-0bd8f182-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 20.224583ms
May  2 03:12:00.227: INFO: Pod "pod-configmaps-0bd8f182-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037015813s
May  2 03:12:02.256: INFO: Pod "pod-configmaps-0bd8f182-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065422607s
May  2 03:12:04.274: INFO: Pod "pod-configmaps-0bd8f182-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083352769s
May  2 03:12:06.293: INFO: Pod "pod-configmaps-0bd8f182-6c88-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.103053402s
STEP: Saw pod success
May  2 03:12:06.293: INFO: Pod "pod-configmaps-0bd8f182-6c88-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:12:06.309: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-configmaps-0bd8f182-6c88-11e9-97f0-0a58ac103caa container configmap-volume-test: <nil>
STEP: delete the pod
May  2 03:12:06.357: INFO: Waiting for pod pod-configmaps-0bd8f182-6c88-11e9-97f0-0a58ac103caa to disappear
May  2 03:12:06.373: INFO: Pod pod-configmaps-0bd8f182-6c88-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:12:06.373: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zpsj6" for this suite.
May  2 03:12:12.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:12:13.493: INFO: namespace: e2e-tests-configmap-zpsj6, resource: packagemanifests, items remaining: 1
May  2 03:12:13.916: INFO: namespace: e2e-tests-configmap-zpsj6, resource: bindings, ignored listing per whitelist
May  2 03:12:14.516: INFO: namespace: e2e-tests-configmap-zpsj6 no longer exists
May  2 03:12:14.534: INFO: namespace: e2e-tests-configmap-zpsj6, total namespaces: 47, active: 47, terminating: 0
May  2 03:12:14.550: INFO: namespace e2e-tests-configmap-zpsj6 deletion completed in 8.145502503s

â€¢ [SLOW TEST:17.352 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:12:14.550: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:12:15.617: INFO: (0) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 19.78725ms)
May  2 03:12:15.634: INFO: (1) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.527097ms)
May  2 03:12:15.650: INFO: (2) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.362351ms)
May  2 03:12:15.667: INFO: (3) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.621964ms)
May  2 03:12:15.683: INFO: (4) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.37376ms)
May  2 03:12:15.701: INFO: (5) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 17.191744ms)
May  2 03:12:15.724: INFO: (6) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 23.052221ms)
May  2 03:12:15.743: INFO: (7) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 19.065938ms)
May  2 03:12:15.760: INFO: (8) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 17.19147ms)
May  2 03:12:15.778: INFO: (9) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 17.571826ms)
May  2 03:12:15.800: INFO: (10) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 21.889396ms)
May  2 03:12:15.817: INFO: (11) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.927352ms)
May  2 03:12:15.834: INFO: (12) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 17.916635ms)
May  2 03:12:15.851: INFO: (13) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.567192ms)
May  2 03:12:15.867: INFO: (14) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.103755ms)
May  2 03:12:15.885: INFO: (15) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 17.247616ms)
May  2 03:12:15.902: INFO: (16) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 17.05966ms)
May  2 03:12:15.919: INFO: (17) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.865315ms)
May  2 03:12:15.938: INFO: (18) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 18.946123ms)
May  2 03:12:15.954: INFO: (19) /api/v1/nodes/ip-10-0-135-216.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.001708ms)
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:12:15.954: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-qbsqt" for this suite.
May  2 03:12:22.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:12:22.222: INFO: namespace: e2e-tests-proxy-qbsqt, resource: bindings, ignored listing per whitelist
May  2 03:12:23.283: INFO: namespace: e2e-tests-proxy-qbsqt, resource: packagemanifests, items remaining: 1
May  2 03:12:23.395: INFO: namespace: e2e-tests-proxy-qbsqt no longer exists
May  2 03:12:23.413: INFO: namespace: e2e-tests-proxy-qbsqt, total namespaces: 47, active: 47, terminating: 0
May  2 03:12:23.432: INFO: namespace e2e-tests-proxy-qbsqt deletion completed in 7.460738019s

â€¢ [SLOW TEST:8.882 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:12:23.432: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-1b77b3e0-6c88-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 03:12:24.414: INFO: Waiting up to 5m0s for pod "pod-secrets-1b7a6135-6c88-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-secrets-glhdn" to be "success or failure"
May  2 03:12:24.429: INFO: Pod "pod-secrets-1b7a6135-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.827569ms
May  2 03:12:26.445: INFO: Pod "pod-secrets-1b7a6135-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031825059s
May  2 03:12:28.462: INFO: Pod "pod-secrets-1b7a6135-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048188791s
May  2 03:12:30.478: INFO: Pod "pod-secrets-1b7a6135-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064002004s
May  2 03:12:32.493: INFO: Pod "pod-secrets-1b7a6135-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.079771416s
May  2 03:12:34.509: INFO: Pod "pod-secrets-1b7a6135-6c88-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.09579211s
STEP: Saw pod success
May  2 03:12:34.509: INFO: Pod "pod-secrets-1b7a6135-6c88-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:12:34.525: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-secrets-1b7a6135-6c88-11e9-97f0-0a58ac103caa container secret-volume-test: <nil>
STEP: delete the pod
May  2 03:12:34.568: INFO: Waiting for pod pod-secrets-1b7a6135-6c88-11e9-97f0-0a58ac103caa to disappear
May  2 03:12:34.584: INFO: Pod pod-secrets-1b7a6135-6c88-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:12:34.584: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-glhdn" for this suite.
May  2 03:12:40.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:12:41.307: INFO: namespace: e2e-tests-secrets-glhdn, resource: bindings, ignored listing per whitelist
May  2 03:12:42.269: INFO: namespace: e2e-tests-secrets-glhdn, resource: packagemanifests, items remaining: 1
May  2 03:12:42.698: INFO: namespace: e2e-tests-secrets-glhdn no longer exists
May  2 03:12:42.715: INFO: namespace: e2e-tests-secrets-glhdn, total namespaces: 47, active: 47, terminating: 0
May  2 03:12:42.730: INFO: namespace e2e-tests-secrets-glhdn deletion completed in 8.114590309s

â€¢ [SLOW TEST:19.298 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:12:42.730: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-8hcwm/configmap-test-26f6d706-6c88-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 03:12:43.704: INFO: Waiting up to 5m0s for pod "pod-configmaps-26f9adeb-6c88-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-configmap-8hcwm" to be "success or failure"
May  2 03:12:43.720: INFO: Pod "pod-configmaps-26f9adeb-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.698449ms
May  2 03:12:45.736: INFO: Pod "pod-configmaps-26f9adeb-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03172221s
May  2 03:12:47.752: INFO: Pod "pod-configmaps-26f9adeb-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047834343s
May  2 03:12:49.769: INFO: Pod "pod-configmaps-26f9adeb-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064133995s
May  2 03:12:51.785: INFO: Pod "pod-configmaps-26f9adeb-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080246842s
May  2 03:12:53.801: INFO: Pod "pod-configmaps-26f9adeb-6c88-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.09652237s
STEP: Saw pod success
May  2 03:12:53.801: INFO: Pod "pod-configmaps-26f9adeb-6c88-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:12:53.816: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-configmaps-26f9adeb-6c88-11e9-97f0-0a58ac103caa container env-test: <nil>
STEP: delete the pod
May  2 03:12:53.858: INFO: Waiting for pod pod-configmaps-26f9adeb-6c88-11e9-97f0-0a58ac103caa to disappear
May  2 03:12:53.875: INFO: Pod pod-configmaps-26f9adeb-6c88-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:12:53.875: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8hcwm" for this suite.
May  2 03:12:59.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:13:01.193: INFO: namespace: e2e-tests-configmap-8hcwm, resource: bindings, ignored listing per whitelist
May  2 03:13:01.514: INFO: namespace: e2e-tests-configmap-8hcwm, resource: packagemanifests, items remaining: 1
May  2 03:13:01.990: INFO: namespace: e2e-tests-configmap-8hcwm no longer exists
May  2 03:13:02.007: INFO: namespace: e2e-tests-configmap-8hcwm, total namespaces: 47, active: 47, terminating: 0
May  2 03:13:02.022: INFO: namespace e2e-tests-configmap-8hcwm deletion completed in 8.114711471s

â€¢ [SLOW TEST:19.292 seconds]
[sig-node] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:13:02.022: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 03:13:03.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-327756ff-6c88-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-bql65" to be "success or failure"
May  2 03:13:03.054: INFO: Pod "downwardapi-volume-327756ff-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.324448ms
May  2 03:13:05.070: INFO: Pod "downwardapi-volume-327756ff-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032698208s
May  2 03:13:07.087: INFO: Pod "downwardapi-volume-327756ff-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049273926s
May  2 03:13:09.103: INFO: Pod "downwardapi-volume-327756ff-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06560319s
May  2 03:13:11.120: INFO: Pod "downwardapi-volume-327756ff-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082128271s
May  2 03:13:13.136: INFO: Pod "downwardapi-volume-327756ff-6c88-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098443103s
STEP: Saw pod success
May  2 03:13:13.136: INFO: Pod "downwardapi-volume-327756ff-6c88-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:13:13.151: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-327756ff-6c88-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 03:13:13.194: INFO: Waiting for pod downwardapi-volume-327756ff-6c88-11e9-97f0-0a58ac103caa to disappear
May  2 03:13:13.209: INFO: Pod downwardapi-volume-327756ff-6c88-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:13:13.210: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bql65" for this suite.
May  2 03:13:19.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:13:20.730: INFO: namespace: e2e-tests-projected-bql65, resource: bindings, ignored listing per whitelist
May  2 03:13:20.793: INFO: namespace: e2e-tests-projected-bql65, resource: packagemanifests, items remaining: 1
May  2 03:13:21.323: INFO: namespace: e2e-tests-projected-bql65 no longer exists
May  2 03:13:21.340: INFO: namespace: e2e-tests-projected-bql65, total namespaces: 47, active: 47, terminating: 0
May  2 03:13:21.355: INFO: namespace e2e-tests-projected-bql65 deletion completed in 8.115386421s

â€¢ [SLOW TEST:19.333 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:13:21.356: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:13:52.375: INFO: Container started at 2019-05-02 03:13:29 +0000 UTC, pod became ready at 2019-05-02 03:13:51 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:13:52.375: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-5rpt2" for this suite.
May  2 03:14:14.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:14:15.191: INFO: namespace: e2e-tests-container-probe-5rpt2, resource: packagemanifests, items remaining: 1
May  2 03:14:15.644: INFO: namespace: e2e-tests-container-probe-5rpt2, resource: bindings, ignored listing per whitelist
May  2 03:14:16.490: INFO: namespace: e2e-tests-container-probe-5rpt2 no longer exists
May  2 03:14:16.507: INFO: namespace: e2e-tests-container-probe-5rpt2, total namespaces: 47, active: 47, terminating: 0
May  2 03:14:16.522: INFO: namespace e2e-tests-container-probe-5rpt2 deletion completed in 24.116661873s

â€¢ [SLOW TEST:55.166 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:14:16.522: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-5ee2af9a-6c88-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 03:14:17.521: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5ee56686-6c88-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-pv8z6" to be "success or failure"
May  2 03:14:17.537: INFO: Pod "pod-projected-configmaps-5ee56686-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.936868ms
May  2 03:14:19.554: INFO: Pod "pod-projected-configmaps-5ee56686-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032397832s
May  2 03:14:21.570: INFO: Pod "pod-projected-configmaps-5ee56686-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049230761s
May  2 03:14:23.587: INFO: Pod "pod-projected-configmaps-5ee56686-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065990261s
May  2 03:14:25.603: INFO: Pod "pod-projected-configmaps-5ee56686-6c88-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082283445s
May  2 03:14:27.620: INFO: Pod "pod-projected-configmaps-5ee56686-6c88-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098359581s
STEP: Saw pod success
May  2 03:14:27.620: INFO: Pod "pod-projected-configmaps-5ee56686-6c88-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:14:27.635: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-configmaps-5ee56686-6c88-11e9-97f0-0a58ac103caa container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  2 03:14:27.679: INFO: Waiting for pod pod-projected-configmaps-5ee56686-6c88-11e9-97f0-0a58ac103caa to disappear
May  2 03:14:27.695: INFO: Pod pod-projected-configmaps-5ee56686-6c88-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:14:27.695: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pv8z6" for this suite.
May  2 03:14:33.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:14:34.665: INFO: namespace: e2e-tests-projected-pv8z6, resource: bindings, ignored listing per whitelist
May  2 03:14:35.178: INFO: namespace: e2e-tests-projected-pv8z6, resource: packagemanifests, items remaining: 1
May  2 03:14:35.811: INFO: namespace: e2e-tests-projected-pv8z6 no longer exists
May  2 03:14:35.828: INFO: namespace: e2e-tests-projected-pv8z6, total namespaces: 47, active: 47, terminating: 0
May  2 03:14:35.843: INFO: namespace e2e-tests-projected-pv8z6 deletion completed in 8.119212623s

â€¢ [SLOW TEST:19.321 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:14:35.843: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-z8lgv
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-z8lgv
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-z8lgv
May  2 03:14:36.856: INFO: Found 0 stateful pods, waiting for 1
May  2 03:14:46.872: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May  2 03:14:46.888: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  2 03:14:47.268: INFO: stderr: ""
May  2 03:14:47.268: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  2 03:14:47.268: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  2 03:14:47.284: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  2 03:14:57.300: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  2 03:14:57.300: INFO: Waiting for statefulset status.replicas updated to 0
May  2 03:14:57.368: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999743s
May  2 03:14:58.384: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.984362339s
May  2 03:14:59.401: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.967597225s
May  2 03:15:00.417: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.951034891s
May  2 03:15:01.434: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.934501201s
May  2 03:15:02.450: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.917907235s
May  2 03:15:03.467: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.901458768s
May  2 03:15:04.483: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.885164831s
May  2 03:15:05.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.868960271s
May  2 03:15:06.516: INFO: Verifying statefulset ss doesn't scale past 1 for another 852.211445ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-z8lgv
May  2 03:15:07.533: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:15:07.844: INFO: stderr: ""
May  2 03:15:07.844: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  2 03:15:07.844: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  2 03:15:07.860: INFO: Found 1 stateful pods, waiting for 3
May  2 03:15:17.876: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  2 03:15:17.876: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  2 03:15:17.876: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
May  2 03:15:27.877: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  2 03:15:27.877: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  2 03:15:27.877: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May  2 03:15:27.907: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  2 03:15:28.233: INFO: stderr: ""
May  2 03:15:28.233: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  2 03:15:28.233: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  2 03:15:28.233: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  2 03:15:28.577: INFO: stderr: ""
May  2 03:15:28.577: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  2 03:15:28.577: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  2 03:15:28.577: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  2 03:15:28.922: INFO: stderr: ""
May  2 03:15:28.922: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  2 03:15:28.922: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  2 03:15:28.922: INFO: Waiting for statefulset status.replicas updated to 0
May  2 03:15:28.941: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May  2 03:15:38.977: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  2 03:15:38.978: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  2 03:15:38.978: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  2 03:15:39.029: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999271s
May  2 03:15:40.045: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983410416s
May  2 03:15:41.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.967113262s
May  2 03:15:42.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.950874245s
May  2 03:15:43.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.934491447s
May  2 03:15:44.111: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.918067905s
May  2 03:15:45.127: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.901417869s
May  2 03:15:46.143: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.885000187s
May  2 03:15:47.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.868637858s
May  2 03:15:48.176: INFO: Verifying statefulset ss doesn't scale past 3 for another 852.228757ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-z8lgv
May  2 03:15:49.193: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:15:49.541: INFO: stderr: ""
May  2 03:15:49.541: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  2 03:15:49.541: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  2 03:15:49.541: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:15:49.852: INFO: stderr: ""
May  2 03:15:49.852: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  2 03:15:49.852: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  2 03:15:49.852: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:15:50.269: INFO: rc: 1
May  2 03:15:50.269: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: Internal error occurred: error executing command in container: container is not created or running
 [] <nil> 0xc00221aae0 exit status 1 <nil> <nil> true [0xc002082108 0xc002082120 0xc002082138] [0xc002082108 0xc002082120 0xc002082138] [0xc002082118 0xc002082130] [0x932ca0 0x932ca0] 0xc0029e9380 <nil>}:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container is not created or running

error:
exit status 1

May  2 03:16:00.269: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:16:00.409: INFO: rc: 1
May  2 03:16:00.409: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000aa2d50 exit status 1 <nil> <nil> true [0xc00106ab80 0xc00106ac10 0xc00106ac68] [0xc00106ab80 0xc00106ac10 0xc00106ac68] [0xc00106abf8 0xc00106ac30] [0x932ca0 0x932ca0] 0xc0026aba40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:16:10.410: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:16:10.540: INFO: rc: 1
May  2 03:16:10.540: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000aa35c0 exit status 1 <nil> <nil> true [0xc00106ac88 0xc00106acd8 0xc00106ad88] [0xc00106ac88 0xc00106acd8 0xc00106ad88] [0xc00106acb0 0xc00106ad60] [0x932ca0 0x932ca0] 0xc0023ae0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:16:20.540: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:16:20.678: INFO: rc: 1
May  2 03:16:20.678: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c129f0 exit status 1 <nil> <nil> true [0xc000174b60 0xc000174b80 0xc000174bb8] [0xc000174b60 0xc000174b80 0xc000174bb8] [0xc000174b78 0xc000174b90] [0x932ca0 0x932ca0] 0xc00263ef00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:16:30.679: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:16:30.808: INFO: rc: 1
May  2 03:16:30.808: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c13470 exit status 1 <nil> <nil> true [0xc000174bd0 0xc000174c00 0xc000174c38] [0xc000174bd0 0xc000174c00 0xc000174c38] [0xc000174bf8 0xc000174c18] [0x932ca0 0x932ca0] 0xc00263f2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:16:40.810: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:16:40.952: INFO: rc: 1
May  2 03:16:40.952: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00221b3e0 exit status 1 <nil> <nil> true [0xc002082140 0xc002082158 0xc002082170] [0xc002082140 0xc002082158 0xc002082170] [0xc002082150 0xc002082168] [0x932ca0 0x932ca0] 0xc0029e9620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:16:50.953: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:16:51.131: INFO: rc: 1
May  2 03:16:51.131: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f6a1b0 exit status 1 <nil> <nil> true [0xc00106add0 0xc00106ae18 0xc00106aeb8] [0xc00106add0 0xc00106ae18 0xc00106aeb8] [0xc00106ade8 0xc00106aea8] [0x932ca0 0x932ca0] 0xc0023ae3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:17:01.132: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:17:01.285: INFO: rc: 1
May  2 03:17:01.285: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0001d2990 exit status 1 <nil> <nil> true [0xc002082178 0xc002082190 0xc0020821a8] [0xc002082178 0xc002082190 0xc0020821a8] [0xc002082188 0xc0020821a0] [0x932ca0 0x932ca0] 0xc0029e9980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:17:11.286: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:17:11.427: INFO: rc: 1
May  2 03:17:11.427: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f6a9c0 exit status 1 <nil> <nil> true [0xc00106aec0 0xc00106af48 0xc00106afb0] [0xc00106aec0 0xc00106af48 0xc00106afb0] [0xc00106af18 0xc00106afa8] [0x932ca0 0x932ca0] 0xc0023ae660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:17:21.428: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:17:21.578: INFO: rc: 1
May  2 03:17:21.578: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0004735f0 exit status 1 <nil> <nil> true [0xc00097a000 0xc00097a068 0xc00097a0d8] [0xc00097a000 0xc00097a068 0xc00097a0d8] [0xc00097a060 0xc00097a0c0] [0x932ca0 0x932ca0] 0xc002155aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:17:31.578: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:17:31.718: INFO: rc: 1
May  2 03:17:31.718: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000aa24b0 exit status 1 <nil> <nil> true [0xc000174058 0xc0001740e8 0xc000174298] [0xc000174058 0xc0001740e8 0xc000174298] [0xc000174098 0xc000174258] [0x932ca0 0x932ca0] 0xc0026aa300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:17:41.718: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:17:41.871: INFO: rc: 1
May  2 03:17:41.871: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000aa2c00 exit status 1 <nil> <nil> true [0xc000174878 0xc0001748c8 0xc000174948] [0xc000174878 0xc0001748c8 0xc000174948] [0xc0001748b8 0xc000174900] [0x932ca0 0x932ca0] 0xc0026aa8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:17:51.872: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:17:52.024: INFO: rc: 1
May  2 03:17:52.024: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021c87b0 exit status 1 <nil> <nil> true [0xc00097a000 0xc00097a068 0xc00097a0d8] [0xc00097a000 0xc00097a068 0xc00097a0d8] [0xc00097a060 0xc00097a0c0] [0x932ca0 0x932ca0] 0xc002ce0240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:18:02.024: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:18:02.163: INFO: rc: 1
May  2 03:18:02.164: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000aa34a0 exit status 1 <nil> <nil> true [0xc000174958 0xc000174a38 0xc000174a78] [0xc000174958 0xc000174a38 0xc000174a78] [0xc000174970 0xc000174a68] [0x932ca0 0x932ca0] 0xc0026aade0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:18:12.164: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:18:12.306: INFO: rc: 1
May  2 03:18:12.306: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0017e4660 exit status 1 <nil> <nil> true [0xc002082000 0xc002082018 0xc002082030] [0xc002082000 0xc002082018 0xc002082030] [0xc002082010 0xc002082028] [0x932ca0 0x932ca0] 0xc00263e480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:18:22.306: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:18:22.437: INFO: rc: 1
May  2 03:18:22.437: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000aa3f80 exit status 1 <nil> <nil> true [0xc000174ab0 0xc000174af0 0xc000174b60] [0xc000174ab0 0xc000174af0 0xc000174b60] [0xc000174ae8 0xc000174b40] [0x932ca0 0x932ca0] 0xc0026ab200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:18:32.437: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:18:32.571: INFO: rc: 1
May  2 03:18:32.571: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0017e51a0 exit status 1 <nil> <nil> true [0xc002082038 0xc002082050 0xc002082068] [0xc002082038 0xc002082050 0xc002082068] [0xc002082048 0xc002082060] [0x932ca0 0x932ca0] 0xc00263e720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:18:42.571: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:18:42.799: INFO: rc: 1
May  2 03:18:42.799: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00221a630 exit status 1 <nil> <nil> true [0xc000174b70 0xc000174b88 0xc000174bd0] [0xc000174b70 0xc000174b88 0xc000174bd0] [0xc000174b80 0xc000174bb8] [0x932ca0 0x932ca0] 0xc0026ab680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:18:52.800: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:18:52.988: INFO: rc: 1
May  2 03:18:52.988: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021c8f00 exit status 1 <nil> <nil> true [0xc00097a0e8 0xc00097a130 0xc00097a178] [0xc00097a0e8 0xc00097a130 0xc00097a178] [0xc00097a128 0xc00097a158] [0x932ca0 0x932ca0] 0xc002ce04e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:19:02.988: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:19:03.128: INFO: rc: 1
May  2 03:19:03.128: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021c98c0 exit status 1 <nil> <nil> true [0xc00097a190 0xc00097a200 0xc00097a248] [0xc00097a190 0xc00097a200 0xc00097a248] [0xc00097a1e8 0xc00097a240] [0x932ca0 0x932ca0] 0xc002ce07e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:19:13.128: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:19:13.291: INFO: rc: 1
May  2 03:19:13.291: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001cb27b0 exit status 1 <nil> <nil> true [0xc00106a018 0xc00106a060 0xc00106a0b8] [0xc00106a018 0xc00106a060 0xc00106a0b8] [0xc00106a058 0xc00106a088] [0x932ca0 0x932ca0] 0xc002155aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:19:23.292: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:19:23.433: INFO: rc: 1
May  2 03:19:23.433: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a96000 exit status 1 <nil> <nil> true [0xc00097a270 0xc00097a2d0 0xc00097a360] [0xc00097a270 0xc00097a2d0 0xc00097a360] [0xc00097a2a8 0xc00097a338] [0x932ca0 0x932ca0] 0xc002ce0d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:19:33.433: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:19:33.580: INFO: rc: 1
May  2 03:19:33.580: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021c87e0 exit status 1 <nil> <nil> true [0xc00106a030 0xc00106a080 0xc00106a0c8] [0xc00106a030 0xc00106a080 0xc00106a0c8] [0xc00106a060 0xc00106a0b8] [0x932ca0 0x932ca0] 0xc002155aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:19:43.581: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:19:43.731: INFO: rc: 1
May  2 03:19:43.731: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000aa2960 exit status 1 <nil> <nil> true [0xc002082000 0xc002082018 0xc002082030] [0xc002082000 0xc002082018 0xc002082030] [0xc002082010 0xc002082028] [0x932ca0 0x932ca0] 0xc00263e480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:19:53.731: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:19:53.903: INFO: rc: 1
May  2 03:19:53.903: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001cb2270 exit status 1 <nil> <nil> true [0xc000174000 0xc000174098 0xc000174258] [0xc000174000 0xc000174098 0xc000174258] [0xc000174080 0xc000174140] [0x932ca0 0x932ca0] 0xc0026aa300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:20:03.903: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:20:04.035: INFO: rc: 1
May  2 03:20:04.035: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000aa32c0 exit status 1 <nil> <nil> true [0xc002082038 0xc002082050 0xc002082068] [0xc002082038 0xc002082050 0xc002082068] [0xc002082048 0xc002082060] [0x932ca0 0x932ca0] 0xc00263e720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:20:14.035: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:20:14.168: INFO: rc: 1
May  2 03:20:14.168: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000aa3dd0 exit status 1 <nil> <nil> true [0xc002082070 0xc002082088 0xc0020820a0] [0xc002082070 0xc002082088 0xc0020820a0] [0xc002082080 0xc002082098] [0x932ca0 0x932ca0] 0xc00263e9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:20:24.168: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:20:24.306: INFO: rc: 1
May  2 03:20:24.306: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0017e4630 exit status 1 <nil> <nil> true [0xc00097a000 0xc00097a068 0xc00097a0d8] [0xc00097a000 0xc00097a068 0xc00097a0d8] [0xc00097a060 0xc00097a0c0] [0x932ca0 0x932ca0] 0xc002ce0240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:20:34.307: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:20:34.441: INFO: rc: 1
May  2 03:20:34.442: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0017e51d0 exit status 1 <nil> <nil> true [0xc00097a0e8 0xc00097a130 0xc00097a178] [0xc00097a0e8 0xc00097a130 0xc00097a178] [0xc00097a128 0xc00097a158] [0x932ca0 0x932ca0] 0xc002ce04e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:20:44.443: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:20:44.582: INFO: rc: 1
May  2 03:20:44.582: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00221a150 exit status 1 <nil> <nil> true [0xc00097a190 0xc00097a200 0xc00097a248] [0xc00097a190 0xc00097a200 0xc00097a248] [0xc00097a1e8 0xc00097a240] [0x932ca0 0x932ca0] 0xc002ce07e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May  2 03:20:54.583: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-z8lgv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  2 03:20:54.742: INFO: rc: 1
May  2 03:20:54.742: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
May  2 03:20:54.742: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May  2 03:20:54.795: INFO: Deleting all statefulset in ns e2e-tests-statefulset-z8lgv
May  2 03:20:54.813: INFO: Scaling statefulset ss to 0
May  2 03:20:54.862: INFO: Waiting for statefulset status.replicas updated to 0
May  2 03:20:54.878: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:20:54.930: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-z8lgv" for this suite.
May  2 03:21:01.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:21:02.551: INFO: namespace: e2e-tests-statefulset-z8lgv, resource: packagemanifests, items remaining: 1
May  2 03:21:02.879: INFO: namespace: e2e-tests-statefulset-z8lgv, resource: bindings, ignored listing per whitelist
May  2 03:21:03.041: INFO: namespace: e2e-tests-statefulset-z8lgv no longer exists
May  2 03:21:03.058: INFO: namespace: e2e-tests-statefulset-z8lgv, total namespaces: 47, active: 47, terminating: 0
May  2 03:21:03.074: INFO: namespace e2e-tests-statefulset-z8lgv deletion completed in 8.11392935s

â€¢ [SLOW TEST:387.231 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:21:03.074: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
May  2 03:21:04.088: INFO: Waiting up to 5m0s for pod "pod-513a2fcf-6c89-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-c9q7c" to be "success or failure"
May  2 03:21:04.103: INFO: Pod "pod-513a2fcf-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.288162ms
May  2 03:21:06.120: INFO: Pod "pod-513a2fcf-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032366937s
May  2 03:21:08.137: INFO: Pod "pod-513a2fcf-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04895636s
May  2 03:21:10.153: INFO: Pod "pod-513a2fcf-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065571519s
May  2 03:21:12.169: INFO: Pod "pod-513a2fcf-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081438931s
May  2 03:21:14.185: INFO: Pod "pod-513a2fcf-6c89-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097587993s
STEP: Saw pod success
May  2 03:21:14.185: INFO: Pod "pod-513a2fcf-6c89-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:21:14.201: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-513a2fcf-6c89-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 03:21:14.247: INFO: Waiting for pod pod-513a2fcf-6c89-11e9-97f0-0a58ac103caa to disappear
May  2 03:21:14.262: INFO: Pod pod-513a2fcf-6c89-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:21:14.262: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-c9q7c" for this suite.
May  2 03:21:20.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:21:21.144: INFO: namespace: e2e-tests-emptydir-c9q7c, resource: packagemanifests, items remaining: 1
May  2 03:21:21.867: INFO: namespace: e2e-tests-emptydir-c9q7c, resource: bindings, ignored listing per whitelist
May  2 03:21:22.374: INFO: namespace: e2e-tests-emptydir-c9q7c no longer exists
May  2 03:21:22.391: INFO: namespace: e2e-tests-emptydir-c9q7c, total namespaces: 47, active: 47, terminating: 0
May  2 03:21:22.406: INFO: namespace e2e-tests-emptydir-c9q7c deletion completed in 8.113908337s

â€¢ [SLOW TEST:19.332 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:21:22.407: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May  2 03:21:23.347: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
May  2 03:21:23.380: INFO: Waiting for terminating namespaces to be deleted...
May  2 03:21:23.397: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-135-216.ec2.internal before test
May  2 03:21:23.418: INFO: tuned-t9pcv from openshift-cluster-node-tuning-operator started at 2019-05-02 01:50:14 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container tuned ready: true, restart count 0
May  2 03:21:23.418: INFO: certified-operators-6f5774bb49-zqjw9 from openshift-marketplace started at 2019-05-02 01:50:35 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container certified-operators ready: true, restart count 0
May  2 03:21:23.418: INFO: router-default-8db6b5c76-pmbks from openshift-ingress started at 2019-05-02 01:50:55 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container router ready: true, restart count 0
May  2 03:21:23.418: INFO: node-exporter-jvhfx from openshift-monitoring started at 2019-05-02 01:51:03 +0000 UTC (2 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:21:23.418: INFO: 	Container node-exporter ready: true, restart count 0
May  2 03:21:23.418: INFO: multus-8jgrn from openshift-multus started at 2019-05-02 01:49:53 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container kube-multus ready: true, restart count 0
May  2 03:21:23.418: INFO: community-operators-67554d9c5f-f9lqv from openshift-marketplace started at 2019-05-02 01:50:36 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container community-operators ready: true, restart count 0
May  2 03:21:23.418: INFO: kube-state-metrics-74d989d8d8-ffvw4 from openshift-monitoring started at 2019-05-02 01:50:34 +0000 UTC (3 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May  2 03:21:23.418: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May  2 03:21:23.418: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  2 03:21:23.418: INFO: dns-default-j5c7f from openshift-dns started at 2019-05-02 01:49:53 +0000 UTC (2 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container dns ready: true, restart count 0
May  2 03:21:23.418: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  2 03:21:23.418: INFO: redhat-operators-8658cf87c-9pfhf from openshift-marketplace started at 2019-05-02 01:50:36 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container redhat-operators ready: true, restart count 0
May  2 03:21:23.418: INFO: sdn-vnzxb from openshift-sdn started at 2019-05-02 01:49:53 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container sdn ready: true, restart count 0
May  2 03:21:23.418: INFO: machine-config-daemon-6kqb6 from openshift-machine-config-operator started at 2019-05-02 01:50:32 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container machine-config-daemon ready: true, restart count 0
May  2 03:21:23.418: INFO: node-ca-cvq2r from openshift-image-registry started at 2019-05-02 01:51:28 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container node-ca ready: true, restart count 0
May  2 03:21:23.418: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-05-02 01:52:08 +0000 UTC (3 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container alertmanager ready: true, restart count 0
May  2 03:21:23.418: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  2 03:21:23.418: INFO: 	Container config-reloader ready: true, restart count 0
May  2 03:21:23.418: INFO: prometheus-adapter-88656d548-swtj7 from openshift-monitoring started at 2019-05-02 01:52:10 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  2 03:21:23.418: INFO: ovs-ckrk2 from openshift-sdn started at 2019-05-02 01:49:53 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.418: INFO: 	Container openvswitch ready: true, restart count 0
May  2 03:21:23.418: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-140-17.ec2.internal before test
May  2 03:21:23.441: INFO: ovs-9m7kl from openshift-sdn started at 2019-05-02 01:49:57 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container openvswitch ready: true, restart count 0
May  2 03:21:23.441: INFO: tuned-l87zf from openshift-cluster-node-tuning-operator started at 2019-05-02 01:50:14 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container tuned ready: true, restart count 0
May  2 03:21:23.441: INFO: router-default-8db6b5c76-sn9q9 from openshift-ingress started at 2019-05-02 01:50:55 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container router ready: true, restart count 0
May  2 03:21:23.441: INFO: node-exporter-5ms2n from openshift-monitoring started at 2019-05-02 01:51:03 +0000 UTC (2 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:21:23.441: INFO: 	Container node-exporter ready: true, restart count 0
May  2 03:21:23.441: INFO: image-registry-7d85675b77-2xv8x from openshift-image-registry started at 2019-05-02 01:51:06 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container registry ready: true, restart count 0
May  2 03:21:23.441: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-05-02 01:52:12 +0000 UTC (6 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:21:23.441: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  2 03:21:23.441: INFO: 	Container prometheus ready: true, restart count 1
May  2 03:21:23.441: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May  2 03:21:23.441: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  2 03:21:23.441: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May  2 03:21:23.441: INFO: downloads-5b9759bd45-ffhzh from openshift-console started at 2019-05-02 01:50:08 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container download-server ready: true, restart count 0
May  2 03:21:23.441: INFO: machine-config-daemon-fvkb5 from openshift-machine-config-operator started at 2019-05-02 01:50:37 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container machine-config-daemon ready: true, restart count 0
May  2 03:21:23.441: INFO: dns-default-l8vqv from openshift-dns started at 2019-05-02 01:49:57 +0000 UTC (2 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container dns ready: true, restart count 0
May  2 03:21:23.441: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  2 03:21:23.441: INFO: multus-ngggs from openshift-multus started at 2019-05-02 01:49:57 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container kube-multus ready: true, restart count 0
May  2 03:21:23.441: INFO: sdn-ww2h4 from openshift-sdn started at 2019-05-02 01:49:57 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container sdn ready: true, restart count 0
May  2 03:21:23.441: INFO: telemeter-client-7cccb458c9-j8xss from openshift-monitoring started at 2019-05-02 01:50:38 +0000 UTC (3 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:21:23.441: INFO: 	Container reload ready: true, restart count 0
May  2 03:21:23.441: INFO: 	Container telemeter-client ready: true, restart count 0
May  2 03:21:23.441: INFO: node-ca-9nbxx from openshift-image-registry started at 2019-05-02 01:51:28 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.441: INFO: 	Container node-ca ready: true, restart count 0
May  2 03:21:23.441: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-05-02 01:51:31 +0000 UTC (3 container statuses recorded)
May  2 03:21:23.442: INFO: 	Container alertmanager ready: true, restart count 0
May  2 03:21:23.442: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  2 03:21:23.442: INFO: 	Container config-reloader ready: true, restart count 0
May  2 03:21:23.442: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-153-32.ec2.internal before test
May  2 03:21:23.465: INFO: multus-vt2rg from openshift-multus started at 2019-05-02 01:50:03 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container kube-multus ready: true, restart count 0
May  2 03:21:23.465: INFO: downloads-5b9759bd45-rw4d7 from openshift-console started at 2019-05-02 01:50:08 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container download-server ready: true, restart count 0
May  2 03:21:23.465: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-05-02 01:52:12 +0000 UTC (6 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:21:23.465: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  2 03:21:23.465: INFO: 	Container prometheus ready: true, restart count 1
May  2 03:21:23.465: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May  2 03:21:23.465: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  2 03:21:23.465: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May  2 03:21:23.465: INFO: tuned-crx5s from openshift-cluster-node-tuning-operator started at 2019-05-02 01:50:14 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container tuned ready: true, restart count 0
May  2 03:21:23.465: INFO: grafana-5ccd6d5857-65m9v from openshift-monitoring started at 2019-05-02 01:51:27 +0000 UTC (2 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container grafana ready: true, restart count 0
May  2 03:21:23.465: INFO: 	Container grafana-proxy ready: true, restart count 0
May  2 03:21:23.465: INFO: node-exporter-c4m2k from openshift-monitoring started at 2019-05-02 01:51:03 +0000 UTC (2 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:21:23.465: INFO: 	Container node-exporter ready: true, restart count 0
May  2 03:21:23.465: INFO: prometheus-operator-f7c6b5c59-6wjgw from openshift-monitoring started at 2019-05-02 01:51:59 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container prometheus-operator ready: true, restart count 0
May  2 03:21:23.465: INFO: dns-default-kqjfp from openshift-dns started at 2019-05-02 01:50:03 +0000 UTC (2 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container dns ready: true, restart count 0
May  2 03:21:23.465: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  2 03:21:23.465: INFO: node-ca-vpzc4 from openshift-image-registry started at 2019-05-02 01:51:28 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container node-ca ready: true, restart count 0
May  2 03:21:23.465: INFO: ovs-zvrlj from openshift-sdn started at 2019-05-02 01:50:03 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container openvswitch ready: true, restart count 0
May  2 03:21:23.465: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-05-02 01:51:47 +0000 UTC (3 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container alertmanager ready: true, restart count 0
May  2 03:21:23.465: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  2 03:21:23.465: INFO: 	Container config-reloader ready: true, restart count 0
May  2 03:21:23.465: INFO: machine-config-daemon-clvrk from openshift-machine-config-operator started at 2019-05-02 01:51:01 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container machine-config-daemon ready: true, restart count 0
May  2 03:21:23.465: INFO: sdn-22xbv from openshift-sdn started at 2019-05-02 01:50:03 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container sdn ready: true, restart count 0
May  2 03:21:23.465: INFO: prometheus-adapter-88656d548-666vp from openshift-monitoring started at 2019-05-02 01:52:10 +0000 UTC (1 container statuses recorded)
May  2 03:21:23.465: INFO: 	Container prometheus-adapter ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-62d0875c-6c89-11e9-97f0-0a58ac103caa 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-62d0875c-6c89-11e9-97f0-0a58ac103caa off the node ip-10-0-135-216.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-62d0875c-6c89-11e9-97f0-0a58ac103caa
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:21:43.726: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-fc4s7" for this suite.
May  2 03:22:05.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:22:06.441: INFO: namespace: e2e-tests-sched-pred-fc4s7, resource: bindings, ignored listing per whitelist
May  2 03:22:06.826: INFO: namespace: e2e-tests-sched-pred-fc4s7, resource: packagemanifests, items remaining: 1
May  2 03:22:07.838: INFO: namespace: e2e-tests-sched-pred-fc4s7 no longer exists
May  2 03:22:07.855: INFO: namespace: e2e-tests-sched-pred-fc4s7, total namespaces: 47, active: 47, terminating: 0
May  2 03:22:07.870: INFO: namespace e2e-tests-sched-pred-fc4s7 deletion completed in 24.114336201s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:45.463 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:22:07.870: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-77d1c018-6c89-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 03:22:08.855: INFO: Waiting up to 5m0s for pod "pod-configmaps-77d4d691-6c89-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-configmap-7vrrc" to be "success or failure"
May  2 03:22:08.871: INFO: Pod "pod-configmaps-77d4d691-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.625514ms
May  2 03:22:10.888: INFO: Pod "pod-configmaps-77d4d691-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033050196s
May  2 03:22:12.904: INFO: Pod "pod-configmaps-77d4d691-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049211583s
May  2 03:22:14.921: INFO: Pod "pod-configmaps-77d4d691-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065755224s
May  2 03:22:16.937: INFO: Pod "pod-configmaps-77d4d691-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081792957s
May  2 03:22:18.953: INFO: Pod "pod-configmaps-77d4d691-6c89-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098127588s
STEP: Saw pod success
May  2 03:22:18.953: INFO: Pod "pod-configmaps-77d4d691-6c89-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:22:18.977: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-configmaps-77d4d691-6c89-11e9-97f0-0a58ac103caa container configmap-volume-test: <nil>
STEP: delete the pod
May  2 03:22:19.019: INFO: Waiting for pod pod-configmaps-77d4d691-6c89-11e9-97f0-0a58ac103caa to disappear
May  2 03:22:19.034: INFO: Pod pod-configmaps-77d4d691-6c89-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:22:19.034: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7vrrc" for this suite.
May  2 03:22:25.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:22:25.984: INFO: namespace: e2e-tests-configmap-7vrrc, resource: packagemanifests, items remaining: 1
May  2 03:22:26.000: INFO: namespace: e2e-tests-configmap-7vrrc, resource: bindings, ignored listing per whitelist
May  2 03:22:27.147: INFO: namespace: e2e-tests-configmap-7vrrc no longer exists
May  2 03:22:27.164: INFO: namespace: e2e-tests-configmap-7vrrc, total namespaces: 47, active: 47, terminating: 0
May  2 03:22:27.179: INFO: namespace e2e-tests-configmap-7vrrc deletion completed in 8.115121116s

â€¢ [SLOW TEST:19.309 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:22:27.180: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-mfl9w in namespace e2e-tests-proxy-gsnwx
I0502 03:22:28.174397   12606 runners.go:184] Created replication controller with name: proxy-service-mfl9w, namespace: e2e-tests-proxy-gsnwx, replica count: 1
I0502 03:22:29.225006   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 03:22:30.225226   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 03:22:31.225461   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 03:22:32.225722   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 03:22:33.225916   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 03:22:34.226167   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 03:22:35.226400   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 03:22:36.226561   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 03:22:37.226795   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 03:22:38.227054   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 03:22:39.227290   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0502 03:22:40.227518   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0502 03:22:41.227800   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0502 03:22:42.228017   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0502 03:22:43.228250   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0502 03:22:44.228461   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0502 03:22:45.228685   12606 runners.go:184] proxy-service-mfl9w Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  2 03:22:45.245: INFO: setup took 17.111495937s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May  2 03:22:45.265: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.094402ms)
May  2 03:22:45.265: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.904971ms)
May  2 03:22:45.271: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 26.367871ms)
May  2 03:22:45.271: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 26.347312ms)
May  2 03:22:45.271: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 26.479738ms)
May  2 03:22:45.278: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 32.919043ms)
May  2 03:22:45.278: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 32.987289ms)
May  2 03:22:45.278: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 33.073189ms)
May  2 03:22:45.278: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 32.981931ms)
May  2 03:22:45.279: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 33.556746ms)
May  2 03:22:45.279: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 33.771978ms)
May  2 03:22:45.284: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 39.13929ms)
May  2 03:22:45.284: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 39.152814ms)
May  2 03:22:45.284: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 39.28044ms)
May  2 03:22:45.284: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 39.102117ms)
May  2 03:22:45.285: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 39.643441ms)
May  2 03:22:45.303: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 18.333783ms)
May  2 03:22:45.304: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 19.318757ms)
May  2 03:22:45.305: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.940991ms)
May  2 03:22:45.305: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 19.995714ms)
May  2 03:22:45.305: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 20.202897ms)
May  2 03:22:45.306: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 21.01212ms)
May  2 03:22:45.306: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 21.026972ms)
May  2 03:22:45.306: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 21.01308ms)
May  2 03:22:45.306: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 21.066067ms)
May  2 03:22:45.306: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 21.348377ms)
May  2 03:22:45.307: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 21.720715ms)
May  2 03:22:45.307: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 22.074094ms)
May  2 03:22:45.308: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 23.239242ms)
May  2 03:22:45.308: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 23.393165ms)
May  2 03:22:45.308: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 23.496598ms)
May  2 03:22:45.309: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 23.613914ms)
May  2 03:22:45.327: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 18.228687ms)
May  2 03:22:45.328: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 18.981724ms)
May  2 03:22:45.328: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 19.703691ms)
May  2 03:22:45.329: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 19.902283ms)
May  2 03:22:45.329: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 20.427861ms)
May  2 03:22:45.329: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 20.508593ms)
May  2 03:22:45.329: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 20.517614ms)
May  2 03:22:45.329: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.683839ms)
May  2 03:22:45.329: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 20.816039ms)
May  2 03:22:45.330: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 21.006979ms)
May  2 03:22:45.330: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.872516ms)
May  2 03:22:45.330: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 21.573267ms)
May  2 03:22:45.331: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 22.791024ms)
May  2 03:22:45.331: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 22.726939ms)
May  2 03:22:45.332: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 22.794846ms)
May  2 03:22:45.332: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 22.912808ms)
May  2 03:22:45.350: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 18.790935ms)
May  2 03:22:45.352: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 20.032859ms)
May  2 03:22:45.353: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.822252ms)
May  2 03:22:45.353: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 20.993878ms)
May  2 03:22:45.353: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 20.882304ms)
May  2 03:22:45.353: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 20.903136ms)
May  2 03:22:45.353: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 20.928748ms)
May  2 03:22:45.353: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 21.053615ms)
May  2 03:22:45.354: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 21.868469ms)
May  2 03:22:45.354: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 22.030799ms)
May  2 03:22:45.354: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 21.996299ms)
May  2 03:22:45.354: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 22.071139ms)
May  2 03:22:45.355: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 23.039207ms)
May  2 03:22:45.355: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 23.038873ms)
May  2 03:22:45.355: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 22.999404ms)
May  2 03:22:45.355: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 23.120875ms)
May  2 03:22:45.374: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 19.003734ms)
May  2 03:22:45.374: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.106809ms)
May  2 03:22:45.374: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 19.330712ms)
May  2 03:22:45.376: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 21.046061ms)
May  2 03:22:45.376: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 20.943108ms)
May  2 03:22:45.376: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 21.103798ms)
May  2 03:22:45.376: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 21.129088ms)
May  2 03:22:45.376: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 21.393632ms)
May  2 03:22:45.376: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 21.553544ms)
May  2 03:22:45.377: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 21.632876ms)
May  2 03:22:45.377: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 22.160187ms)
May  2 03:22:45.377: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 22.18936ms)
May  2 03:22:45.378: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 23.06136ms)
May  2 03:22:45.378: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 23.09826ms)
May  2 03:22:45.380: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 24.593137ms)
May  2 03:22:45.380: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 24.604028ms)
May  2 03:22:45.397: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 17.716447ms)
May  2 03:22:45.397: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 17.746239ms)
May  2 03:22:45.399: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 18.986258ms)
May  2 03:22:45.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 19.812578ms)
May  2 03:22:45.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 20.125538ms)
May  2 03:22:45.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 19.832363ms)
May  2 03:22:45.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.933951ms)
May  2 03:22:45.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 20.313561ms)
May  2 03:22:45.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 20.502209ms)
May  2 03:22:45.401: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.859659ms)
May  2 03:22:45.401: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 20.801124ms)
May  2 03:22:45.403: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 22.925545ms)
May  2 03:22:45.403: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 23.211566ms)
May  2 03:22:45.403: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 23.298627ms)
May  2 03:22:45.403: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 23.56711ms)
May  2 03:22:45.403: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 23.553652ms)
May  2 03:22:45.423: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.036626ms)
May  2 03:22:45.423: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 19.129701ms)
May  2 03:22:45.423: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 20.004753ms)
May  2 03:22:45.424: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 20.351644ms)
May  2 03:22:45.424: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 20.405446ms)
May  2 03:22:45.424: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 20.46911ms)
May  2 03:22:45.424: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 20.364291ms)
May  2 03:22:45.424: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 20.663722ms)
May  2 03:22:45.424: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 20.559312ms)
May  2 03:22:45.424: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 20.764764ms)
May  2 03:22:45.425: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 21.013011ms)
May  2 03:22:45.425: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 21.859101ms)
May  2 03:22:45.426: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 22.875476ms)
May  2 03:22:45.426: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 22.834367ms)
May  2 03:22:45.426: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 22.799098ms)
May  2 03:22:45.426: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 22.825644ms)
May  2 03:22:45.446: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 19.179141ms)
May  2 03:22:45.446: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 19.314095ms)
May  2 03:22:45.446: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.411659ms)
May  2 03:22:45.446: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 19.705399ms)
May  2 03:22:45.446: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 19.720959ms)
May  2 03:22:45.446: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 19.897572ms)
May  2 03:22:45.447: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 20.048655ms)
May  2 03:22:45.447: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 20.397723ms)
May  2 03:22:45.447: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 20.39689ms)
May  2 03:22:45.447: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.304674ms)
May  2 03:22:45.447: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 20.352414ms)
May  2 03:22:45.448: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 21.741941ms)
May  2 03:22:45.450: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 23.178394ms)
May  2 03:22:45.450: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 23.258207ms)
May  2 03:22:45.450: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 23.356483ms)
May  2 03:22:45.450: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 23.446544ms)
May  2 03:22:45.469: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 18.497154ms)
May  2 03:22:45.471: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 21.258792ms)
May  2 03:22:45.472: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 21.769965ms)
May  2 03:22:45.472: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 21.956928ms)
May  2 03:22:45.472: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 21.906283ms)
May  2 03:22:45.472: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 22.155004ms)
May  2 03:22:45.472: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 22.214362ms)
May  2 03:22:45.473: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 22.854301ms)
May  2 03:22:45.473: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 23.072155ms)
May  2 03:22:45.473: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 23.007088ms)
May  2 03:22:45.473: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 23.462215ms)
May  2 03:22:45.474: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 24.101037ms)
May  2 03:22:45.475: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 25.258235ms)
May  2 03:22:45.475: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 25.317094ms)
May  2 03:22:45.476: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 25.615307ms)
May  2 03:22:45.476: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 26.158623ms)
May  2 03:22:45.495: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 18.695354ms)
May  2 03:22:45.495: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.15061ms)
May  2 03:22:45.495: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 19.044061ms)
May  2 03:22:45.496: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.230731ms)
May  2 03:22:45.496: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 19.536253ms)
May  2 03:22:45.496: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 19.845761ms)
May  2 03:22:45.497: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 20.315855ms)
May  2 03:22:45.497: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 20.667245ms)
May  2 03:22:45.497: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 20.816414ms)
May  2 03:22:45.497: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 20.786526ms)
May  2 03:22:45.497: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 20.966175ms)
May  2 03:22:45.499: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 22.399582ms)
May  2 03:22:45.499: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 22.484326ms)
May  2 03:22:45.499: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 22.479784ms)
May  2 03:22:45.499: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 22.586803ms)
May  2 03:22:45.499: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 22.586228ms)
May  2 03:22:45.517: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 17.509291ms)
May  2 03:22:45.518: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.35646ms)
May  2 03:22:45.518: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 19.376365ms)
May  2 03:22:45.519: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.314679ms)
May  2 03:22:45.519: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 19.490027ms)
May  2 03:22:45.519: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 19.833719ms)
May  2 03:22:45.519: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 19.789824ms)
May  2 03:22:45.519: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 19.798146ms)
May  2 03:22:45.520: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 21.021164ms)
May  2 03:22:45.521: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 21.73363ms)
May  2 03:22:45.521: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 22.173075ms)
May  2 03:22:45.522: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 22.694035ms)
May  2 03:22:45.522: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 22.672795ms)
May  2 03:22:45.522: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 23.292889ms)
May  2 03:22:45.523: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 23.383824ms)
May  2 03:22:45.523: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 23.760155ms)
May  2 03:22:45.540: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 16.884044ms)
May  2 03:22:45.542: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 18.745883ms)
May  2 03:22:45.542: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 18.943283ms)
May  2 03:22:45.542: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 19.34965ms)
May  2 03:22:45.548: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 24.903359ms)
May  2 03:22:45.548: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 24.949827ms)
May  2 03:22:45.549: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 25.555806ms)
May  2 03:22:45.549: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 25.509319ms)
May  2 03:22:45.549: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 25.534618ms)
May  2 03:22:45.549: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 25.647848ms)
May  2 03:22:45.549: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 25.730842ms)
May  2 03:22:45.549: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 25.63542ms)
May  2 03:22:45.549: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 25.939762ms)
May  2 03:22:45.549: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 26.132317ms)
May  2 03:22:45.549: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 26.520927ms)
May  2 03:22:45.551: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 28.580769ms)
May  2 03:22:45.569: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 17.149213ms)
May  2 03:22:45.570: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 18.503911ms)
May  2 03:22:45.572: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 20.635661ms)
May  2 03:22:45.572: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.695738ms)
May  2 03:22:45.573: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 21.639911ms)
May  2 03:22:45.574: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 21.944474ms)
May  2 03:22:45.574: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 21.995197ms)
May  2 03:22:45.574: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 22.398242ms)
May  2 03:22:45.574: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 22.296978ms)
May  2 03:22:45.574: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 22.427783ms)
May  2 03:22:45.575: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 23.808627ms)
May  2 03:22:45.576: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 24.216115ms)
May  2 03:22:45.576: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 24.292834ms)
May  2 03:22:45.576: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 24.333518ms)
May  2 03:22:45.576: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 24.39523ms)
May  2 03:22:45.577: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 25.724226ms)
May  2 03:22:45.595: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 17.94008ms)
May  2 03:22:45.596: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 18.696163ms)
May  2 03:22:45.598: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 20.188151ms)
May  2 03:22:45.598: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 20.362083ms)
May  2 03:22:45.598: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 20.37976ms)
May  2 03:22:45.598: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 20.455909ms)
May  2 03:22:45.599: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 21.214382ms)
May  2 03:22:45.599: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 21.153007ms)
May  2 03:22:45.599: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 21.200455ms)
May  2 03:22:45.599: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 21.297249ms)
May  2 03:22:45.599: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 21.184325ms)
May  2 03:22:45.599: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 21.541641ms)
May  2 03:22:45.600: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 22.640504ms)
May  2 03:22:45.600: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 22.95413ms)
May  2 03:22:45.601: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 23.102618ms)
May  2 03:22:45.601: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 23.207158ms)
May  2 03:22:45.619: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 17.855167ms)
May  2 03:22:45.620: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 19.358568ms)
May  2 03:22:45.621: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 19.728925ms)
May  2 03:22:45.621: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 19.765373ms)
May  2 03:22:45.621: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.02816ms)
May  2 03:22:45.621: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 20.269325ms)
May  2 03:22:45.622: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 20.772957ms)
May  2 03:22:45.622: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.84438ms)
May  2 03:22:45.622: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 21.010711ms)
May  2 03:22:45.622: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 20.999712ms)
May  2 03:22:45.622: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 21.500147ms)
May  2 03:22:45.622: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 21.776094ms)
May  2 03:22:45.623: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 22.246701ms)
May  2 03:22:45.623: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 22.226004ms)
May  2 03:22:45.623: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 22.317893ms)
May  2 03:22:45.623: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 22.523203ms)
May  2 03:22:45.642: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 18.729787ms)
May  2 03:22:45.642: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 18.698308ms)
May  2 03:22:45.642: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 18.820013ms)
May  2 03:22:45.642: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 18.985197ms)
May  2 03:22:45.643: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.06885ms)
May  2 03:22:45.643: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 19.116496ms)
May  2 03:22:45.643: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 19.082925ms)
May  2 03:22:45.644: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 20.75212ms)
May  2 03:22:45.646: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 21.929335ms)
May  2 03:22:45.646: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 22.062982ms)
May  2 03:22:45.646: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 22.264672ms)
May  2 03:22:45.646: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 22.316618ms)
May  2 03:22:45.646: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 22.582993ms)
May  2 03:22:45.646: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 22.606213ms)
May  2 03:22:45.648: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 24.19186ms)
May  2 03:22:45.648: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 24.767802ms)
May  2 03:22:45.666: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 17.182024ms)
May  2 03:22:45.667: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 18.582919ms)
May  2 03:22:45.668: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 19.829128ms)
May  2 03:22:45.668: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 19.876136ms)
May  2 03:22:45.668: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 19.948658ms)
May  2 03:22:45.669: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 20.071999ms)
May  2 03:22:45.669: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 20.44996ms)
May  2 03:22:45.669: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 20.569738ms)
May  2 03:22:45.669: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 20.640056ms)
May  2 03:22:45.669: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.895591ms)
May  2 03:22:45.671: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 22.876584ms)
May  2 03:22:45.673: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 23.98357ms)
May  2 03:22:45.673: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 24.276805ms)
May  2 03:22:45.673: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 24.426851ms)
May  2 03:22:45.673: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 24.559222ms)
May  2 03:22:45.673: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 24.509564ms)
May  2 03:22:45.691: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 18.442942ms)
May  2 03:22:45.692: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.002112ms)
May  2 03:22:45.692: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 19.0499ms)
May  2 03:22:45.693: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 20.140331ms)
May  2 03:22:45.693: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 20.271036ms)
May  2 03:22:45.694: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 20.414181ms)
May  2 03:22:45.694: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 20.583516ms)
May  2 03:22:45.694: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 20.769424ms)
May  2 03:22:45.694: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.981131ms)
May  2 03:22:45.694: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 21.181666ms)
May  2 03:22:45.694: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 21.246295ms)
May  2 03:22:45.696: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 22.768104ms)
May  2 03:22:45.696: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 23.182143ms)
May  2 03:22:45.696: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 23.27829ms)
May  2 03:22:45.697: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 23.425653ms)
May  2 03:22:45.697: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 23.588031ms)
May  2 03:22:45.715: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 18.059618ms)
May  2 03:22:45.715: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 18.079289ms)
May  2 03:22:45.716: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 19.010121ms)
May  2 03:22:45.716: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 19.572382ms)
May  2 03:22:45.717: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 19.831293ms)
May  2 03:22:45.717: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 20.15372ms)
May  2 03:22:45.717: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 20.236788ms)
May  2 03:22:45.718: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 20.803673ms)
May  2 03:22:45.718: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 20.899666ms)
May  2 03:22:45.718: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 21.390084ms)
May  2 03:22:45.718: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 21.423757ms)
May  2 03:22:45.719: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 21.660045ms)
May  2 03:22:45.720: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 23.319484ms)
May  2 03:22:45.720: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 23.669172ms)
May  2 03:22:45.721: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 23.810402ms)
May  2 03:22:45.721: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 23.988871ms)
May  2 03:22:45.740: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:1080/proxy/... (200; 19.161267ms)
May  2 03:22:45.742: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:1080/proxy/rewri... (200; 20.526333ms)
May  2 03:22:45.742: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:462/proxy/: tls qux (200; 20.62534ms)
May  2 03:22:45.742: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 20.474587ms)
May  2 03:22:45.742: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp/proxy/rewriteme"... (200; 21.146318ms)
May  2 03:22:45.742: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 21.057483ms)
May  2 03:22:45.742: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/proxy-service-mfl9w-kjlcp:160/proxy/: foo (200; 21.199012ms)
May  2 03:22:45.742: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:443/proxy/... (200; 21.348507ms)
May  2 03:22:45.742: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/https:proxy-service-mfl9w-kjlcp:460/proxy/: tls baz (200; 21.29136ms)
May  2 03:22:45.742: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/pods/http:proxy-service-mfl9w-kjlcp:162/proxy/: bar (200; 21.390102ms)
May  2 03:22:45.743: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname2/proxy/: tls qux (200; 21.536039ms)
May  2 03:22:45.743: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname2/proxy/: bar (200; 21.543971ms)
May  2 03:22:45.744: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/proxy-service-mfl9w:portname1/proxy/: foo (200; 23.220506ms)
May  2 03:22:45.745: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname1/proxy/: foo (200; 23.484029ms)
May  2 03:22:45.745: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/https:proxy-service-mfl9w:tlsportname1/proxy/: tls baz (200; 23.43403ms)
May  2 03:22:45.745: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-gsnwx/services/http:proxy-service-mfl9w:portname2/proxy/: bar (200; 23.4297ms)
STEP: deleting ReplicationController proxy-service-mfl9w in namespace e2e-tests-proxy-gsnwx, will wait for the garbage collector to delete the pods
May  2 03:22:45.829: INFO: Deleting ReplicationController proxy-service-mfl9w took: 19.012185ms
May  2 03:22:45.929: INFO: Terminating ReplicationController proxy-service-mfl9w pods took: 100.364338ms
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:22:58.630: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-gsnwx" for this suite.
May  2 03:23:04.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:23:05.250: INFO: namespace: e2e-tests-proxy-gsnwx, resource: packagemanifests, items remaining: 1
May  2 03:23:05.443: INFO: namespace: e2e-tests-proxy-gsnwx, resource: bindings, ignored listing per whitelist
May  2 03:23:06.072: INFO: namespace: e2e-tests-proxy-gsnwx no longer exists
May  2 03:23:06.089: INFO: namespace: e2e-tests-proxy-gsnwx, total namespaces: 47, active: 47, terminating: 0
May  2 03:23:06.104: INFO: namespace e2e-tests-proxy-gsnwx deletion completed in 7.443992549s

â€¢ [SLOW TEST:38.925 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:23:06.105: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
May  2 03:23:17.249: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:23:42.390: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-794wg" for this suite.
May  2 03:23:48.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:23:49.346: INFO: namespace: e2e-tests-namespaces-794wg, resource: packagemanifests, items remaining: 1
May  2 03:23:49.606: INFO: namespace: e2e-tests-namespaces-794wg, resource: bindings, ignored listing per whitelist
May  2 03:23:50.506: INFO: namespace: e2e-tests-namespaces-794wg no longer exists
May  2 03:23:50.525: INFO: namespace: e2e-tests-namespaces-794wg, total namespaces: 48, active: 48, terminating: 0
May  2 03:23:50.540: INFO: namespace e2e-tests-namespaces-794wg deletion completed in 8.119836173s
STEP: Destroying namespace "e2e-tests-nsdeletetest-8nhsq" for this suite.
May  2 03:23:50.558: INFO: Namespace e2e-tests-nsdeletetest-8nhsq was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-blh5m" for this suite.
May  2 03:23:56.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:23:57.550: INFO: namespace: e2e-tests-nsdeletetest-blh5m, resource: packagemanifests, items remaining: 1
May  2 03:23:57.819: INFO: namespace: e2e-tests-nsdeletetest-blh5m, resource: bindings, ignored listing per whitelist
May  2 03:23:58.643: INFO: namespace: e2e-tests-nsdeletetest-blh5m no longer exists
May  2 03:23:58.660: INFO: namespace: e2e-tests-nsdeletetest-blh5m, total namespaces: 47, active: 47, terminating: 0
May  2 03:23:58.675: INFO: namespace e2e-tests-nsdeletetest-blh5m deletion completed in 8.117817343s

â€¢ [SLOW TEST:52.571 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:23:58.676: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-b9de560c-6c89-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 03:23:59.664: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b9e12ecf-6c89-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-rb4ql" to be "success or failure"
May  2 03:23:59.680: INFO: Pod "pod-projected-configmaps-b9e12ecf-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.751002ms
May  2 03:24:01.696: INFO: Pod "pod-projected-configmaps-b9e12ecf-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032038782s
May  2 03:24:03.713: INFO: Pod "pod-projected-configmaps-b9e12ecf-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048252937s
May  2 03:24:05.729: INFO: Pod "pod-projected-configmaps-b9e12ecf-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06438929s
May  2 03:24:07.745: INFO: Pod "pod-projected-configmaps-b9e12ecf-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080643772s
May  2 03:24:09.762: INFO: Pod "pod-projected-configmaps-b9e12ecf-6c89-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097401658s
STEP: Saw pod success
May  2 03:24:09.762: INFO: Pod "pod-projected-configmaps-b9e12ecf-6c89-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:24:09.778: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-configmaps-b9e12ecf-6c89-11e9-97f0-0a58ac103caa container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  2 03:24:09.822: INFO: Waiting for pod pod-projected-configmaps-b9e12ecf-6c89-11e9-97f0-0a58ac103caa to disappear
May  2 03:24:09.837: INFO: Pod pod-projected-configmaps-b9e12ecf-6c89-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:24:09.837: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rb4ql" for this suite.
May  2 03:24:15.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:24:16.790: INFO: namespace: e2e-tests-projected-rb4ql, resource: bindings, ignored listing per whitelist
May  2 03:24:17.102: INFO: namespace: e2e-tests-projected-rb4ql, resource: packagemanifests, items remaining: 1
May  2 03:24:17.950: INFO: namespace: e2e-tests-projected-rb4ql no longer exists
May  2 03:24:17.967: INFO: namespace: e2e-tests-projected-rb4ql, total namespaces: 47, active: 47, terminating: 0
May  2 03:24:17.983: INFO: namespace e2e-tests-projected-rb4ql deletion completed in 8.116117565s

â€¢ [SLOW TEST:19.307 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:24:17.983: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-c55fd364-6c89-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 03:24:18.967: INFO: Waiting up to 5m0s for pod "pod-configmaps-c5629948-6c89-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-configmap-bvbjx" to be "success or failure"
May  2 03:24:18.984: INFO: Pod "pod-configmaps-c5629948-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 17.621873ms
May  2 03:24:21.001: INFO: Pod "pod-configmaps-c5629948-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033939883s
May  2 03:24:23.017: INFO: Pod "pod-configmaps-c5629948-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050177593s
May  2 03:24:25.033: INFO: Pod "pod-configmaps-c5629948-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066311155s
May  2 03:24:27.049: INFO: Pod "pod-configmaps-c5629948-6c89-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082593963s
May  2 03:24:29.066: INFO: Pod "pod-configmaps-c5629948-6c89-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.09917933s
STEP: Saw pod success
May  2 03:24:29.066: INFO: Pod "pod-configmaps-c5629948-6c89-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:24:29.081: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-configmaps-c5629948-6c89-11e9-97f0-0a58ac103caa container configmap-volume-test: <nil>
STEP: delete the pod
May  2 03:24:29.124: INFO: Waiting for pod pod-configmaps-c5629948-6c89-11e9-97f0-0a58ac103caa to disappear
May  2 03:24:29.139: INFO: Pod pod-configmaps-c5629948-6c89-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:24:29.139: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bvbjx" for this suite.
May  2 03:24:35.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:24:35.799: INFO: namespace: e2e-tests-configmap-bvbjx, resource: bindings, ignored listing per whitelist
May  2 03:24:36.644: INFO: namespace: e2e-tests-configmap-bvbjx, resource: packagemanifests, items remaining: 1
May  2 03:24:37.266: INFO: namespace: e2e-tests-configmap-bvbjx no longer exists
May  2 03:24:37.283: INFO: namespace: e2e-tests-configmap-bvbjx, total namespaces: 47, active: 47, terminating: 0
May  2 03:24:37.299: INFO: namespace e2e-tests-configmap-bvbjx deletion completed in 8.117664369s

â€¢ [SLOW TEST:19.316 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:24:37.299: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:24:38.285: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-d0ea63a5-6c89-11e9-97f0-0a58ac103caa
STEP: Creating configMap with name cm-test-opt-upd-d0ea63e2-6c89-11e9-97f0-0a58ac103caa
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d0ea63a5-6c89-11e9-97f0-0a58ac103caa
STEP: Updating configmap cm-test-opt-upd-d0ea63e2-6c89-11e9-97f0-0a58ac103caa
STEP: Creating configMap with name cm-test-opt-create-d0ea63fc-6c89-11e9-97f0-0a58ac103caa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:25:59.279: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9zsr4" for this suite.
May  2 03:26:23.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:26:23.993: INFO: namespace: e2e-tests-configmap-9zsr4, resource: packagemanifests, items remaining: 1
May  2 03:26:24.137: INFO: namespace: e2e-tests-configmap-9zsr4, resource: bindings, ignored listing per whitelist
May  2 03:26:25.394: INFO: namespace: e2e-tests-configmap-9zsr4 no longer exists
May  2 03:26:25.412: INFO: namespace: e2e-tests-configmap-9zsr4, total namespaces: 47, active: 47, terminating: 0
May  2 03:26:25.427: INFO: namespace e2e-tests-configmap-9zsr4 deletion completed in 26.118190368s

â€¢ [SLOW TEST:108.128 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:26:25.428: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May  2 03:26:26.394: INFO: Waiting up to 5m0s for pod "downward-api-11543f1d-6c8a-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-xc6f2" to be "success or failure"
May  2 03:26:26.411: INFO: Pod "downward-api-11543f1d-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.468515ms
May  2 03:26:28.427: INFO: Pod "downward-api-11543f1d-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032350191s
May  2 03:26:30.443: INFO: Pod "downward-api-11543f1d-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04878867s
May  2 03:26:32.459: INFO: Pod "downward-api-11543f1d-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064837765s
May  2 03:26:34.476: INFO: Pod "downward-api-11543f1d-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081276755s
May  2 03:26:36.492: INFO: Pod "downward-api-11543f1d-6c8a-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.09788615s
STEP: Saw pod success
May  2 03:26:36.492: INFO: Pod "downward-api-11543f1d-6c8a-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:26:36.508: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downward-api-11543f1d-6c8a-11e9-97f0-0a58ac103caa container dapi-container: <nil>
STEP: delete the pod
May  2 03:26:36.552: INFO: Waiting for pod downward-api-11543f1d-6c8a-11e9-97f0-0a58ac103caa to disappear
May  2 03:26:36.567: INFO: Pod downward-api-11543f1d-6c8a-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:26:36.567: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xc6f2" for this suite.
May  2 03:26:42.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:26:43.777: INFO: namespace: e2e-tests-downward-api-xc6f2, resource: packagemanifests, items remaining: 1
May  2 03:26:43.809: INFO: namespace: e2e-tests-downward-api-xc6f2, resource: bindings, ignored listing per whitelist
May  2 03:26:44.695: INFO: namespace: e2e-tests-downward-api-xc6f2 no longer exists
May  2 03:26:44.712: INFO: namespace: e2e-tests-downward-api-xc6f2, total namespaces: 47, active: 47, terminating: 0
May  2 03:26:44.727: INFO: namespace e2e-tests-downward-api-xc6f2 deletion completed in 8.117417974s

â€¢ [SLOW TEST:19.299 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:26:44.727: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:26:45.698: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-jbktm" for this suite.
May  2 03:26:51.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:26:52.589: INFO: namespace: e2e-tests-services-jbktm, resource: bindings, ignored listing per whitelist
May  2 03:26:52.717: INFO: namespace: e2e-tests-services-jbktm, resource: packagemanifests, items remaining: 1
May  2 03:26:53.800: INFO: namespace: e2e-tests-services-jbktm no longer exists
May  2 03:26:53.817: INFO: namespace: e2e-tests-services-jbktm, total namespaces: 47, active: 47, terminating: 0
May  2 03:26:53.833: INFO: namespace e2e-tests-services-jbktm deletion completed in 8.117606384s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:9.106 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:26:53.833: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May  2 03:26:54.758: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:27:05.522: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-vrxk7" for this suite.
May  2 03:27:11.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:27:12.926: INFO: namespace: e2e-tests-init-container-vrxk7, resource: packagemanifests, items remaining: 1
May  2 03:27:13.005: INFO: namespace: e2e-tests-init-container-vrxk7, resource: bindings, ignored listing per whitelist
May  2 03:27:13.638: INFO: namespace: e2e-tests-init-container-vrxk7 no longer exists
May  2 03:27:13.655: INFO: namespace: e2e-tests-init-container-vrxk7, total namespaces: 47, active: 47, terminating: 0
May  2 03:27:13.670: INFO: namespace e2e-tests-init-container-vrxk7 deletion completed in 8.118115001s

â€¢ [SLOW TEST:19.837 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:27:13.670: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May  2 03:27:15.569: INFO: Pod name wrapped-volume-race-2ea227f6-6c8a-11e9-97f0-0a58ac103caa: Found 0 pods out of 5
May  2 03:27:20.601: INFO: Pod name wrapped-volume-race-2ea227f6-6c8a-11e9-97f0-0a58ac103caa: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2ea227f6-6c8a-11e9-97f0-0a58ac103caa in namespace e2e-tests-emptydir-wrapper-57mn4, will wait for the garbage collector to delete the pods
May  2 03:27:58.823: INFO: Deleting ReplicationController wrapped-volume-race-2ea227f6-6c8a-11e9-97f0-0a58ac103caa took: 23.151368ms
May  2 03:27:58.924: INFO: Terminating ReplicationController wrapped-volume-race-2ea227f6-6c8a-11e9-97f0-0a58ac103caa pods took: 100.34241ms
STEP: Creating RC which spawns configmap-volume pods
May  2 03:28:40.077: INFO: Pod name wrapped-volume-race-6100db75-6c8a-11e9-97f0-0a58ac103caa: Found 0 pods out of 5
May  2 03:28:45.109: INFO: Pod name wrapped-volume-race-6100db75-6c8a-11e9-97f0-0a58ac103caa: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6100db75-6c8a-11e9-97f0-0a58ac103caa in namespace e2e-tests-emptydir-wrapper-57mn4, will wait for the garbage collector to delete the pods
May  2 03:29:25.292: INFO: Deleting ReplicationController wrapped-volume-race-6100db75-6c8a-11e9-97f0-0a58ac103caa took: 20.997766ms
May  2 03:29:25.392: INFO: Terminating ReplicationController wrapped-volume-race-6100db75-6c8a-11e9-97f0-0a58ac103caa pods took: 100.306152ms
STEP: Creating RC which spawns configmap-volume pods
May  2 03:30:10.048: INFO: Pod name wrapped-volume-race-96a0f188-6c8a-11e9-97f0-0a58ac103caa: Found 0 pods out of 5
May  2 03:30:15.080: INFO: Pod name wrapped-volume-race-96a0f188-6c8a-11e9-97f0-0a58ac103caa: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-96a0f188-6c8a-11e9-97f0-0a58ac103caa in namespace e2e-tests-emptydir-wrapper-57mn4, will wait for the garbage collector to delete the pods
May  2 03:30:55.291: INFO: Deleting ReplicationController wrapped-volume-race-96a0f188-6c8a-11e9-97f0-0a58ac103caa took: 23.183251ms
May  2 03:30:55.392: INFO: Terminating ReplicationController wrapped-volume-race-96a0f188-6c8a-11e9-97f0-0a58ac103caa pods took: 100.319042ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:31:31.997: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-57mn4" for this suite.
May  2 03:31:40.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:31:40.986: INFO: namespace: e2e-tests-emptydir-wrapper-57mn4, resource: packagemanifests, items remaining: 1
May  2 03:31:41.406: INFO: namespace: e2e-tests-emptydir-wrapper-57mn4, resource: bindings, ignored listing per whitelist
May  2 03:31:42.107: INFO: namespace: e2e-tests-emptydir-wrapper-57mn4 no longer exists
May  2 03:31:42.124: INFO: namespace: e2e-tests-emptydir-wrapper-57mn4, total namespaces: 47, active: 47, terminating: 0
May  2 03:31:42.139: INFO: namespace e2e-tests-emptydir-wrapper-57mn4 deletion completed in 10.11296085s

â€¢ [SLOW TEST:268.470 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:31:42.140: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:31:53.250: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-wpt5v" for this suite.
May  2 03:31:59.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:32:00.244: INFO: namespace: e2e-tests-emptydir-wrapper-wpt5v, resource: bindings, ignored listing per whitelist
May  2 03:32:00.404: INFO: namespace: e2e-tests-emptydir-wrapper-wpt5v, resource: packagemanifests, items remaining: 1
May  2 03:32:01.365: INFO: namespace: e2e-tests-emptydir-wrapper-wpt5v no longer exists
May  2 03:32:01.384: INFO: namespace: e2e-tests-emptydir-wrapper-wpt5v, total namespaces: 47, active: 47, terminating: 0
May  2 03:32:01.400: INFO: namespace e2e-tests-emptydir-wrapper-wpt5v deletion completed in 8.119446809s

â€¢ [SLOW TEST:19.260 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:32:01.400: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May  2 03:32:10.997: INFO: Successfully updated pod "annotationupdated996b1e2-6c8a-11e9-97f0-0a58ac103caa"
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:32:13.039: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pzpxc" for this suite.
May  2 03:32:35.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:32:36.486: INFO: namespace: e2e-tests-downward-api-pzpxc, resource: bindings, ignored listing per whitelist
May  2 03:32:36.615: INFO: namespace: e2e-tests-downward-api-pzpxc, resource: packagemanifests, items remaining: 1
May  2 03:32:37.148: INFO: namespace: e2e-tests-downward-api-pzpxc no longer exists
May  2 03:32:37.165: INFO: namespace: e2e-tests-downward-api-pzpxc, total namespaces: 47, active: 47, terminating: 0
May  2 03:32:37.180: INFO: namespace e2e-tests-downward-api-pzpxc deletion completed in 24.112216253s

â€¢ [SLOW TEST:35.780 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:32:37.180: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-eeeb6893-6c8a-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 03:32:38.165: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eeee2b95-6c8a-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-cxx89" to be "success or failure"
May  2 03:32:38.180: INFO: Pod "pod-projected-secrets-eeee2b95-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.370083ms
May  2 03:32:40.196: INFO: Pod "pod-projected-secrets-eeee2b95-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031749243s
May  2 03:32:42.213: INFO: Pod "pod-projected-secrets-eeee2b95-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047975241s
May  2 03:32:44.230: INFO: Pod "pod-projected-secrets-eeee2b95-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065321274s
May  2 03:32:46.246: INFO: Pod "pod-projected-secrets-eeee2b95-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08154911s
May  2 03:32:48.262: INFO: Pod "pod-projected-secrets-eeee2b95-6c8a-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097715082s
STEP: Saw pod success
May  2 03:32:48.262: INFO: Pod "pod-projected-secrets-eeee2b95-6c8a-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:32:48.278: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-secrets-eeee2b95-6c8a-11e9-97f0-0a58ac103caa container projected-secret-volume-test: <nil>
STEP: delete the pod
May  2 03:32:48.321: INFO: Waiting for pod pod-projected-secrets-eeee2b95-6c8a-11e9-97f0-0a58ac103caa to disappear
May  2 03:32:48.336: INFO: Pod pod-projected-secrets-eeee2b95-6c8a-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:32:48.336: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cxx89" for this suite.
May  2 03:32:54.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:32:55.749: INFO: namespace: e2e-tests-projected-cxx89, resource: bindings, ignored listing per whitelist
May  2 03:32:55.833: INFO: namespace: e2e-tests-projected-cxx89, resource: packagemanifests, items remaining: 1
May  2 03:32:56.449: INFO: namespace: e2e-tests-projected-cxx89 no longer exists
May  2 03:32:56.466: INFO: namespace: e2e-tests-projected-cxx89, total namespaces: 47, active: 47, terminating: 0
May  2 03:32:56.481: INFO: namespace e2e-tests-projected-cxx89 deletion completed in 8.114713498s

â€¢ [SLOW TEST:19.301 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:32:56.481: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-fa7024ce-6c8a-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 03:32:57.490: INFO: Waiting up to 5m0s for pod "pod-secrets-fa72fff6-6c8a-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-secrets-hbbfp" to be "success or failure"
May  2 03:32:57.506: INFO: Pod "pod-secrets-fa72fff6-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.763287ms
May  2 03:32:59.522: INFO: Pod "pod-secrets-fa72fff6-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032528721s
May  2 03:33:01.539: INFO: Pod "pod-secrets-fa72fff6-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049191502s
May  2 03:33:03.555: INFO: Pod "pod-secrets-fa72fff6-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065548646s
May  2 03:33:05.572: INFO: Pod "pod-secrets-fa72fff6-6c8a-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082022097s
May  2 03:33:07.588: INFO: Pod "pod-secrets-fa72fff6-6c8a-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098548505s
STEP: Saw pod success
May  2 03:33:07.588: INFO: Pod "pod-secrets-fa72fff6-6c8a-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:33:07.604: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-secrets-fa72fff6-6c8a-11e9-97f0-0a58ac103caa container secret-volume-test: <nil>
STEP: delete the pod
May  2 03:33:07.648: INFO: Waiting for pod pod-secrets-fa72fff6-6c8a-11e9-97f0-0a58ac103caa to disappear
May  2 03:33:07.663: INFO: Pod pod-secrets-fa72fff6-6c8a-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:33:07.663: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hbbfp" for this suite.
May  2 03:33:13.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:33:14.379: INFO: namespace: e2e-tests-secrets-hbbfp, resource: bindings, ignored listing per whitelist
May  2 03:33:15.176: INFO: namespace: e2e-tests-secrets-hbbfp, resource: packagemanifests, items remaining: 1
May  2 03:33:15.779: INFO: namespace: e2e-tests-secrets-hbbfp no longer exists
May  2 03:33:15.796: INFO: namespace: e2e-tests-secrets-hbbfp, total namespaces: 47, active: 47, terminating: 0
May  2 03:33:15.812: INFO: namespace e2e-tests-secrets-hbbfp deletion completed in 8.119138586s

â€¢ [SLOW TEST:19.330 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:33:15.812: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-05f6be0d-6c8b-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 03:33:16.919: INFO: Waiting up to 5m0s for pod "pod-secrets-0607112c-6c8b-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-secrets-8ll6t" to be "success or failure"
May  2 03:33:16.936: INFO: Pod "pod-secrets-0607112c-6c8b-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 17.011782ms
May  2 03:33:18.952: INFO: Pod "pod-secrets-0607112c-6c8b-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033070825s
May  2 03:33:20.968: INFO: Pod "pod-secrets-0607112c-6c8b-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049219297s
May  2 03:33:22.984: INFO: Pod "pod-secrets-0607112c-6c8b-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06549315s
May  2 03:33:25.001: INFO: Pod "pod-secrets-0607112c-6c8b-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082083532s
May  2 03:33:27.017: INFO: Pod "pod-secrets-0607112c-6c8b-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098167689s
STEP: Saw pod success
May  2 03:33:27.017: INFO: Pod "pod-secrets-0607112c-6c8b-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:33:27.032: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-secrets-0607112c-6c8b-11e9-97f0-0a58ac103caa container secret-volume-test: <nil>
STEP: delete the pod
May  2 03:33:27.075: INFO: Waiting for pod pod-secrets-0607112c-6c8b-11e9-97f0-0a58ac103caa to disappear
May  2 03:33:27.090: INFO: Pod pod-secrets-0607112c-6c8b-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:33:27.090: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8ll6t" for this suite.
May  2 03:33:33.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:33:34.314: INFO: namespace: e2e-tests-secrets-8ll6t, resource: bindings, ignored listing per whitelist
May  2 03:33:35.151: INFO: namespace: e2e-tests-secrets-8ll6t, resource: packagemanifests, items remaining: 1
May  2 03:33:35.216: INFO: namespace: e2e-tests-secrets-8ll6t no longer exists
May  2 03:33:35.235: INFO: namespace: e2e-tests-secrets-8ll6t, total namespaces: 48, active: 48, terminating: 0
May  2 03:33:35.250: INFO: namespace e2e-tests-secrets-8ll6t deletion completed in 8.118054586s
STEP: Destroying namespace "e2e-tests-secret-namespace-f22dw" for this suite.
May  2 03:33:41.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:33:42.675: INFO: namespace: e2e-tests-secret-namespace-f22dw, resource: bindings, ignored listing per whitelist
May  2 03:33:42.884: INFO: namespace: e2e-tests-secret-namespace-f22dw, resource: packagemanifests, items remaining: 1
May  2 03:33:43.332: INFO: namespace: e2e-tests-secret-namespace-f22dw no longer exists
May  2 03:33:43.348: INFO: namespace: e2e-tests-secret-namespace-f22dw, total namespaces: 47, active: 47, terminating: 0
May  2 03:33:43.364: INFO: namespace e2e-tests-secret-namespace-f22dw deletion completed in 8.113268214s

â€¢ [SLOW TEST:27.552 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:33:43.364: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:33:44.412: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-m758g" for this suite.
May  2 03:33:50.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:33:51.628: INFO: namespace: e2e-tests-kubelet-test-m758g, resource: bindings, ignored listing per whitelist
May  2 03:33:52.123: INFO: namespace: e2e-tests-kubelet-test-m758g, resource: packagemanifests, items remaining: 1
May  2 03:33:52.511: INFO: namespace: e2e-tests-kubelet-test-m758g no longer exists
May  2 03:33:52.528: INFO: namespace: e2e-tests-kubelet-test-m758g, total namespaces: 47, active: 47, terminating: 0
May  2 03:33:52.543: INFO: namespace e2e-tests-kubelet-test-m758g deletion completed in 8.113299972s

â€¢ [SLOW TEST:9.179 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:33:52.543: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:33:53.464: INFO: Creating ReplicaSet my-hostname-basic-1bd3f370-6c8b-11e9-97f0-0a58ac103caa
May  2 03:33:53.500: INFO: Pod name my-hostname-basic-1bd3f370-6c8b-11e9-97f0-0a58ac103caa: Found 1 pods out of 1
May  2 03:33:53.500: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1bd3f370-6c8b-11e9-97f0-0a58ac103caa" is running
May  2 03:34:03.535: INFO: Pod "my-hostname-basic-1bd3f370-6c8b-11e9-97f0-0a58ac103caa-l48bw" is running (conditions: [])
May  2 03:34:03.536: INFO: Trying to dial the pod
May  2 03:34:08.587: INFO: Controller my-hostname-basic-1bd3f370-6c8b-11e9-97f0-0a58ac103caa: Got expected result from replica 1 [my-hostname-basic-1bd3f370-6c8b-11e9-97f0-0a58ac103caa-l48bw]: "my-hostname-basic-1bd3f370-6c8b-11e9-97f0-0a58ac103caa-l48bw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:34:08.587: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-2dgbd" for this suite.
May  2 03:34:14.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:34:15.811: INFO: namespace: e2e-tests-replicaset-2dgbd, resource: bindings, ignored listing per whitelist
May  2 03:34:15.847: INFO: namespace: e2e-tests-replicaset-2dgbd, resource: packagemanifests, items remaining: 1
May  2 03:34:16.699: INFO: namespace: e2e-tests-replicaset-2dgbd no longer exists
May  2 03:34:16.720: INFO: namespace: e2e-tests-replicaset-2dgbd, total namespaces: 47, active: 47, terminating: 0
May  2 03:34:16.736: INFO: namespace e2e-tests-replicaset-2dgbd deletion completed in 8.11931977s

â€¢ [SLOW TEST:24.192 seconds]
[sig-apps] ReplicaSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:34:16.736: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May  2 03:34:28.378: INFO: Successfully updated pod "pod-update-2a4aec28-6c8b-11e9-97f0-0a58ac103caa"
STEP: verifying the updated pod is in kubernetes
May  2 03:34:28.410: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:34:28.410: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-t6d4p" for this suite.
May  2 03:34:52.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:34:54.037: INFO: namespace: e2e-tests-pods-t6d4p, resource: bindings, ignored listing per whitelist
May  2 03:34:54.315: INFO: namespace: e2e-tests-pods-t6d4p, resource: packagemanifests, items remaining: 1
May  2 03:34:54.529: INFO: namespace: e2e-tests-pods-t6d4p no longer exists
May  2 03:34:54.545: INFO: namespace: e2e-tests-pods-t6d4p, total namespaces: 47, active: 47, terminating: 0
May  2 03:34:54.561: INFO: namespace e2e-tests-pods-t6d4p deletion completed in 26.115618668s

â€¢ [SLOW TEST:37.825 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:34:54.561: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May  2 03:35:06.128: INFO: Successfully updated pod "labelsupdate40cd199b-6c8b-11e9-97f0-0a58ac103caa"
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:35:08.169: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rc4rt" for this suite.
May  2 03:35:30.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:35:31.471: INFO: namespace: e2e-tests-downward-api-rc4rt, resource: packagemanifests, items remaining: 1
May  2 03:35:31.599: INFO: namespace: e2e-tests-downward-api-rc4rt, resource: bindings, ignored listing per whitelist
May  2 03:35:32.282: INFO: namespace: e2e-tests-downward-api-rc4rt no longer exists
May  2 03:35:32.301: INFO: namespace: e2e-tests-downward-api-rc4rt, total namespaces: 47, active: 47, terminating: 0
May  2 03:35:32.316: INFO: namespace e2e-tests-downward-api-rc4rt deletion completed in 24.117152698s

â€¢ [SLOW TEST:37.755 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:35:32.316: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-574fe916-6c8b-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 03:35:33.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-57532d75-6c8b-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-configmap-crc8q" to be "success or failure"
May  2 03:35:33.325: INFO: Pod "pod-configmaps-57532d75-6c8b-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.719612ms
May  2 03:35:35.342: INFO: Pod "pod-configmaps-57532d75-6c8b-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032961306s
May  2 03:35:37.358: INFO: Pod "pod-configmaps-57532d75-6c8b-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049375087s
May  2 03:35:39.375: INFO: Pod "pod-configmaps-57532d75-6c8b-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066068791s
May  2 03:35:41.392: INFO: Pod "pod-configmaps-57532d75-6c8b-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083343068s
May  2 03:35:43.409: INFO: Pod "pod-configmaps-57532d75-6c8b-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.100240401s
STEP: Saw pod success
May  2 03:35:43.409: INFO: Pod "pod-configmaps-57532d75-6c8b-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:35:43.425: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-configmaps-57532d75-6c8b-11e9-97f0-0a58ac103caa container configmap-volume-test: <nil>
STEP: delete the pod
May  2 03:35:43.467: INFO: Waiting for pod pod-configmaps-57532d75-6c8b-11e9-97f0-0a58ac103caa to disappear
May  2 03:35:43.482: INFO: Pod pod-configmaps-57532d75-6c8b-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:35:43.482: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-crc8q" for this suite.
May  2 03:35:49.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:35:50.739: INFO: namespace: e2e-tests-configmap-crc8q, resource: bindings, ignored listing per whitelist
May  2 03:35:50.889: INFO: namespace: e2e-tests-configmap-crc8q, resource: packagemanifests, items remaining: 1
May  2 03:35:51.596: INFO: namespace: e2e-tests-configmap-crc8q no longer exists
May  2 03:35:51.613: INFO: namespace: e2e-tests-configmap-crc8q, total namespaces: 47, active: 47, terminating: 0
May  2 03:35:51.628: INFO: namespace e2e-tests-configmap-crc8q deletion completed in 8.116336842s

â€¢ [SLOW TEST:19.312 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:35:51.629: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:35:52.655: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May  2 03:35:52.690: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:52.691: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:52.691: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:52.708: INFO: Number of nodes with available pods: 0
May  2 03:35:52.708: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:35:53.739: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:53.739: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:53.739: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:53.755: INFO: Number of nodes with available pods: 0
May  2 03:35:53.755: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:35:54.738: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:54.738: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:54.739: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:54.760: INFO: Number of nodes with available pods: 0
May  2 03:35:54.760: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:35:55.739: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:55.739: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:55.739: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:55.755: INFO: Number of nodes with available pods: 0
May  2 03:35:55.755: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:35:56.738: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:56.738: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:56.738: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:56.754: INFO: Number of nodes with available pods: 0
May  2 03:35:56.754: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:35:57.738: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:57.738: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:57.738: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:57.755: INFO: Number of nodes with available pods: 0
May  2 03:35:57.755: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:35:58.738: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:58.738: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:58.738: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:58.754: INFO: Number of nodes with available pods: 0
May  2 03:35:58.754: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:35:59.738: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:59.738: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:59.738: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:35:59.754: INFO: Number of nodes with available pods: 0
May  2 03:35:59.754: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:36:00.738: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:00.738: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:00.738: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:00.754: INFO: Number of nodes with available pods: 0
May  2 03:36:00.754: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:36:01.740: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:01.740: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:01.740: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:01.757: INFO: Number of nodes with available pods: 3
May  2 03:36:01.757: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May  2 03:36:01.896: INFO: Wrong image for pod: daemon-set-lnv52. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:01.896: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:01.896: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:01.912: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:01.912: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:01.912: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:02.929: INFO: Wrong image for pod: daemon-set-lnv52. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:02.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:02.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:02.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:02.959: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:02.959: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:03.929: INFO: Wrong image for pod: daemon-set-lnv52. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:03.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:03.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:03.959: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:03.959: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:03.959: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:04.928: INFO: Wrong image for pod: daemon-set-lnv52. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:04.928: INFO: Pod daemon-set-lnv52 is not available
May  2 03:36:04.928: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:04.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:04.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:04.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:04.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:05.929: INFO: Pod daemon-set-565js is not available
May  2 03:36:05.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:05.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:05.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:05.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:05.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:06.928: INFO: Pod daemon-set-565js is not available
May  2 03:36:06.928: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:06.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:06.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:06.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:06.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:07.929: INFO: Pod daemon-set-565js is not available
May  2 03:36:07.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:07.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:07.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:07.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:07.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:08.928: INFO: Pod daemon-set-565js is not available
May  2 03:36:08.928: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:08.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:08.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:08.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:08.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:09.929: INFO: Pod daemon-set-565js is not available
May  2 03:36:09.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:09.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:09.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:09.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:09.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:10.928: INFO: Pod daemon-set-565js is not available
May  2 03:36:10.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:10.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:10.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:10.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:10.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:11.929: INFO: Pod daemon-set-565js is not available
May  2 03:36:11.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:11.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:11.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:11.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:11.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:12.929: INFO: Pod daemon-set-565js is not available
May  2 03:36:12.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:12.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:12.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:12.959: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:12.959: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:13.929: INFO: Pod daemon-set-565js is not available
May  2 03:36:13.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:13.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:13.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:13.959: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:13.959: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:14.929: INFO: Pod daemon-set-565js is not available
May  2 03:36:14.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:14.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:14.960: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:14.960: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:14.960: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:15.929: INFO: Pod daemon-set-565js is not available
May  2 03:36:15.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:15.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:15.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:15.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:15.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:16.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:16.929: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:16.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:16.959: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:16.959: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:16.959: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:17.928: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:17.928: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:17.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:17.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:17.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:17.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:18.928: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:18.928: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:18.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:18.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:18.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:18.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:19.928: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:19.928: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:19.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:19.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:19.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:19.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:20.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:20.929: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:20.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:20.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:20.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:20.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:21.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:21.929: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:21.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:21.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:21.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:21.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:22.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:22.929: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:22.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:22.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:22.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:22.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:23.928: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:23.928: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:23.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:23.957: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:23.957: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:23.957: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:24.930: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:24.930: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:24.930: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:24.959: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:24.959: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:24.959: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:25.928: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:25.928: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:25.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:25.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:25.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:25.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:26.929: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:26.929: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:26.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:26.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:26.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:26.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:27.928: INFO: Wrong image for pod: daemon-set-qwstw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:27.928: INFO: Pod daemon-set-qwstw is not available
May  2 03:36:27.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:27.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:27.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:27.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:28.928: INFO: Pod daemon-set-f2c76 is not available
May  2 03:36:28.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:28.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:28.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:28.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:29.929: INFO: Pod daemon-set-f2c76 is not available
May  2 03:36:29.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:29.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:29.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:29.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:30.930: INFO: Pod daemon-set-f2c76 is not available
May  2 03:36:30.930: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:30.961: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:30.961: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:30.961: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:31.929: INFO: Pod daemon-set-f2c76 is not available
May  2 03:36:31.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:31.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:31.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:31.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:32.928: INFO: Pod daemon-set-f2c76 is not available
May  2 03:36:32.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:32.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:32.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:32.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:33.929: INFO: Pod daemon-set-f2c76 is not available
May  2 03:36:33.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:33.959: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:33.959: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:33.959: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:34.929: INFO: Pod daemon-set-f2c76 is not available
May  2 03:36:34.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:34.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:34.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:34.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:35.929: INFO: Pod daemon-set-f2c76 is not available
May  2 03:36:35.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:35.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:35.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:35.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:36.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:36.959: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:36.959: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:36.959: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:37.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:37.929: INFO: Pod daemon-set-vfzr9 is not available
May  2 03:36:37.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:37.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:37.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:38.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:38.929: INFO: Pod daemon-set-vfzr9 is not available
May  2 03:36:38.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:38.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:38.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:39.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:39.928: INFO: Pod daemon-set-vfzr9 is not available
May  2 03:36:39.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:39.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:39.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:40.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:40.929: INFO: Pod daemon-set-vfzr9 is not available
May  2 03:36:40.959: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:40.959: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:40.959: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:41.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:41.928: INFO: Pod daemon-set-vfzr9 is not available
May  2 03:36:41.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:41.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:41.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:42.929: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:42.929: INFO: Pod daemon-set-vfzr9 is not available
May  2 03:36:42.959: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:42.959: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:42.959: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:43.928: INFO: Wrong image for pod: daemon-set-vfzr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  2 03:36:43.928: INFO: Pod daemon-set-vfzr9 is not available
May  2 03:36:43.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:43.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:43.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:44.928: INFO: Pod daemon-set-xdvvn is not available
May  2 03:36:44.958: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:44.958: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:44.958: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May  2 03:36:44.974: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:44.974: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:44.974: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:44.990: INFO: Number of nodes with available pods: 2
May  2 03:36:44.990: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 03:36:46.020: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:46.020: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:46.020: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:46.036: INFO: Number of nodes with available pods: 2
May  2 03:36:46.036: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 03:36:47.020: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:47.020: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:47.020: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:47.036: INFO: Number of nodes with available pods: 2
May  2 03:36:47.036: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 03:36:48.020: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:48.020: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:48.020: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:48.036: INFO: Number of nodes with available pods: 2
May  2 03:36:48.036: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 03:36:49.020: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:49.020: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:49.020: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:49.036: INFO: Number of nodes with available pods: 2
May  2 03:36:49.036: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 03:36:50.021: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:50.021: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:50.021: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:50.036: INFO: Number of nodes with available pods: 2
May  2 03:36:50.036: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 03:36:51.020: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:51.020: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:51.020: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:51.036: INFO: Number of nodes with available pods: 2
May  2 03:36:51.036: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 03:36:52.020: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:52.020: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:52.020: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:52.036: INFO: Number of nodes with available pods: 2
May  2 03:36:52.036: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 03:36:53.020: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:53.020: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:53.020: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:53.036: INFO: Number of nodes with available pods: 2
May  2 03:36:53.036: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 03:36:54.020: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:54.020: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:54.020: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:54.036: INFO: Number of nodes with available pods: 2
May  2 03:36:54.036: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 03:36:55.021: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:55.021: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:55.021: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:55.037: INFO: Number of nodes with available pods: 2
May  2 03:36:55.037: INFO: Node ip-10-0-140-17.ec2.internal is running more than one daemon pod
May  2 03:36:56.022: INFO: DaemonSet pods can't tolerate node ip-10-0-132-8.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:56.022: INFO: DaemonSet pods can't tolerate node ip-10-0-137-95.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:56.022: INFO: DaemonSet pods can't tolerate node ip-10-0-151-117.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-05-02 02:03:44 +0000 UTC}], skip checking this node
May  2 03:36:56.039: INFO: Number of nodes with available pods: 3
May  2 03:36:56.039: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-s798m, will wait for the garbage collector to delete the pods
May  2 03:36:56.205: INFO: Deleting DaemonSet.extensions daemon-set took: 19.002209ms
May  2 03:36:56.305: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.274352ms
May  2 03:37:08.621: INFO: Number of nodes with available pods: 0
May  2 03:37:08.622: INFO: Number of running nodes: 0, number of available pods: 0
May  2 03:37:08.638: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-s798m/daemonsets","resourceVersion":"55460"},"items":null}

May  2 03:37:08.654: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-s798m/pods","resourceVersion":"55460"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:37:08.730: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-s798m" for this suite.
May  2 03:37:14.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:37:15.695: INFO: namespace: e2e-tests-daemonsets-s798m, resource: packagemanifests, items remaining: 1
May  2 03:37:16.789: INFO: namespace: e2e-tests-daemonsets-s798m, resource: bindings, ignored listing per whitelist
May  2 03:37:16.856: INFO: namespace: e2e-tests-daemonsets-s798m no longer exists
May  2 03:37:16.877: INFO: namespace: e2e-tests-daemonsets-s798m, total namespaces: 47, active: 47, terminating: 0
May  2 03:37:16.896: INFO: namespace e2e-tests-daemonsets-s798m deletion completed in 8.136047227s

â€¢ [SLOW TEST:85.268 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:37:16.896: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May  2 03:37:17.957: INFO: PodSpec: initContainers in spec.initContainers
May  2 03:38:13.940: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-95b725cb-6c8b-11e9-97f0-0a58ac103caa", GenerateName:"", Namespace:"e2e-tests-init-container-5vd7c", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-5vd7c/pods/pod-init-95b725cb-6c8b-11e9-97f0-0a58ac103caa", UID:"95b93df9-6c8b-11e9-9e44-12f3365d453a", ResourceVersion:"55889", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63692365037, loc:(*time.Location)(0x7b5bbe0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"957274985"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.153\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-4jnz7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0028df240), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4jnz7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0027e11d0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4jnz7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0027e1220), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}, "cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4jnz7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0027e1180), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0025d12f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-135-216.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001d6fb60), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-22556"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0025d13a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0025d13c0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0025d13dc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0025d13e0)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365037, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365037, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365037, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365037, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.135.216", PodIP:"10.131.0.153", StartTime:(*v1.Time)(0xc001b64c00), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005305b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000530620)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"cri-o://aa1da8acfe3be2fa143ec61ed466058c0ff26c9537ed3068e8de8f8b9cf3b487"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b64c40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b64c20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Burstable"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:38:13.940: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-5vd7c" for this suite.
May  2 03:38:38.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:38:38.783: INFO: namespace: e2e-tests-init-container-5vd7c, resource: packagemanifests, items remaining: 1
May  2 03:38:38.799: INFO: namespace: e2e-tests-init-container-5vd7c, resource: bindings, ignored listing per whitelist
May  2 03:38:40.071: INFO: namespace: e2e-tests-init-container-5vd7c no longer exists
May  2 03:38:40.088: INFO: namespace: e2e-tests-init-container-5vd7c, total namespaces: 47, active: 47, terminating: 0
May  2 03:38:40.104: INFO: namespace e2e-tests-init-container-5vd7c deletion completed in 26.120988773s

â€¢ [SLOW TEST:83.208 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:38:40.104: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-d86dm.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-d86dm.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-d86dm.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-d86dm.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-d86dm.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-d86dm.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  2 03:39:07.451: INFO: DNS probes using e2e-tests-dns-d86dm/dns-test-c73e3fb8-6c8b-11e9-97f0-0a58ac103caa succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:39:07.476: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-d86dm" for this suite.
May  2 03:39:13.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:39:15.212: INFO: namespace: e2e-tests-dns-d86dm, resource: bindings, ignored listing per whitelist
May  2 03:39:15.527: INFO: namespace: e2e-tests-dns-d86dm, resource: packagemanifests, items remaining: 1
May  2 03:39:15.591: INFO: namespace: e2e-tests-dns-d86dm no longer exists
May  2 03:39:15.608: INFO: namespace: e2e-tests-dns-d86dm, total namespaces: 47, active: 47, terminating: 0
May  2 03:39:15.624: INFO: namespace e2e-tests-dns-d86dm deletion completed in 8.117767095s

â€¢ [SLOW TEST:35.519 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:39:15.624: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
May  2 03:39:16.557: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:17.890: INFO: stderr: ""
May  2 03:39:17.890: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  2 03:39:17.890: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:18.054: INFO: stderr: ""
May  2 03:39:18.054: INFO: stdout: "update-demo-nautilus-2s957 update-demo-nautilus-lrnvc "
May  2 03:39:18.054: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2s957 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:18.204: INFO: stderr: ""
May  2 03:39:18.204: INFO: stdout: ""
May  2 03:39:18.204: INFO: update-demo-nautilus-2s957 is created but not running
May  2 03:39:23.204: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:23.361: INFO: stderr: ""
May  2 03:39:23.361: INFO: stdout: "update-demo-nautilus-2s957 update-demo-nautilus-lrnvc "
May  2 03:39:23.361: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2s957 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:23.513: INFO: stderr: ""
May  2 03:39:23.513: INFO: stdout: ""
May  2 03:39:23.513: INFO: update-demo-nautilus-2s957 is created but not running
May  2 03:39:28.513: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:28.671: INFO: stderr: ""
May  2 03:39:28.671: INFO: stdout: "update-demo-nautilus-2s957 update-demo-nautilus-lrnvc "
May  2 03:39:28.671: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2s957 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:28.826: INFO: stderr: ""
May  2 03:39:28.826: INFO: stdout: ""
May  2 03:39:28.826: INFO: update-demo-nautilus-2s957 is created but not running
May  2 03:39:33.826: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:33.990: INFO: stderr: ""
May  2 03:39:33.990: INFO: stdout: "update-demo-nautilus-2s957 update-demo-nautilus-lrnvc "
May  2 03:39:33.990: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2s957 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:34.148: INFO: stderr: ""
May  2 03:39:34.148: INFO: stdout: "true"
May  2 03:39:34.148: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2s957 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:34.309: INFO: stderr: ""
May  2 03:39:34.309: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  2 03:39:34.309: INFO: validating pod update-demo-nautilus-2s957
May  2 03:39:34.329: INFO: got data: {
  "image": "nautilus.jpg"
}

May  2 03:39:34.329: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  2 03:39:34.329: INFO: update-demo-nautilus-2s957 is verified up and running
May  2 03:39:34.329: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-lrnvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:34.480: INFO: stderr: ""
May  2 03:39:34.480: INFO: stdout: "true"
May  2 03:39:34.481: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-lrnvc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:39:34.633: INFO: stderr: ""
May  2 03:39:34.633: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  2 03:39:34.633: INFO: validating pod update-demo-nautilus-lrnvc
May  2 03:39:34.652: INFO: got data: {
  "image": "nautilus.jpg"
}

May  2 03:39:34.652: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  2 03:39:34.652: INFO: update-demo-nautilus-lrnvc is verified up and running
STEP: rolling-update to new replication controller
May  2 03:39:34.655: INFO: scanned /tmp/home for discovery docs: <nil>
May  2 03:39:34.655: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:40:05.160: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May  2 03:40:05.160: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  2 03:40:05.160: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:40:05.314: INFO: stderr: ""
May  2 03:40:05.314: INFO: stdout: "update-demo-kitten-lnxv6 update-demo-kitten-wjqhn "
May  2 03:40:05.314: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-lnxv6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:40:05.462: INFO: stderr: ""
May  2 03:40:05.462: INFO: stdout: "true"
May  2 03:40:05.462: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-lnxv6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:40:05.601: INFO: stderr: ""
May  2 03:40:05.601: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May  2 03:40:05.601: INFO: validating pod update-demo-kitten-lnxv6
May  2 03:40:05.621: INFO: got data: {
  "image": "kitten.jpg"
}

May  2 03:40:05.621: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May  2 03:40:05.621: INFO: update-demo-kitten-lnxv6 is verified up and running
May  2 03:40:05.621: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-wjqhn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:40:05.772: INFO: stderr: ""
May  2 03:40:05.772: INFO: stdout: "true"
May  2 03:40:05.772: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-wjqhn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fp2f6'
May  2 03:40:06.237: INFO: stderr: ""
May  2 03:40:06.237: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May  2 03:40:06.237: INFO: validating pod update-demo-kitten-wjqhn
May  2 03:40:06.256: INFO: got data: {
  "image": "kitten.jpg"
}

May  2 03:40:06.256: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May  2 03:40:06.256: INFO: update-demo-kitten-wjqhn is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:40:06.256: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fp2f6" for this suite.
May  2 03:40:30.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:40:31.400: INFO: namespace: e2e-tests-kubectl-fp2f6, resource: bindings, ignored listing per whitelist
May  2 03:40:31.717: INFO: namespace: e2e-tests-kubectl-fp2f6, resource: packagemanifests, items remaining: 1
May  2 03:40:32.371: INFO: namespace: e2e-tests-kubectl-fp2f6 no longer exists
May  2 03:40:32.389: INFO: namespace: e2e-tests-kubectl-fp2f6, total namespaces: 47, active: 47, terminating: 0
May  2 03:40:32.404: INFO: namespace e2e-tests-kubectl-fp2f6 deletion completed in 26.118156107s

â€¢ [SLOW TEST:76.781 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:40:32.405: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
May  2 03:40:33.371: INFO: Waiting up to 5m0s for pod "var-expansion-0a2cf6bd-6c8c-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-var-expansion-zp7zh" to be "success or failure"
May  2 03:40:33.387: INFO: Pod "var-expansion-0a2cf6bd-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.634198ms
May  2 03:40:35.403: INFO: Pod "var-expansion-0a2cf6bd-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031954733s
May  2 03:40:37.420: INFO: Pod "var-expansion-0a2cf6bd-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048676386s
May  2 03:40:39.454: INFO: Pod "var-expansion-0a2cf6bd-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082984947s
May  2 03:40:41.471: INFO: Pod "var-expansion-0a2cf6bd-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100023412s
May  2 03:40:43.487: INFO: Pod "var-expansion-0a2cf6bd-6c8c-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.116078634s
STEP: Saw pod success
May  2 03:40:43.488: INFO: Pod "var-expansion-0a2cf6bd-6c8c-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:40:43.504: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod var-expansion-0a2cf6bd-6c8c-11e9-97f0-0a58ac103caa container dapi-container: <nil>
STEP: delete the pod
May  2 03:40:43.550: INFO: Waiting for pod var-expansion-0a2cf6bd-6c8c-11e9-97f0-0a58ac103caa to disappear
May  2 03:40:43.567: INFO: Pod var-expansion-0a2cf6bd-6c8c-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:40:43.567: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-zp7zh" for this suite.
May  2 03:40:49.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:40:50.456: INFO: namespace: e2e-tests-var-expansion-zp7zh, resource: bindings, ignored listing per whitelist
May  2 03:40:50.683: INFO: namespace: e2e-tests-var-expansion-zp7zh, resource: packagemanifests, items remaining: 1
May  2 03:40:51.694: INFO: namespace: e2e-tests-var-expansion-zp7zh no longer exists
May  2 03:40:51.711: INFO: namespace: e2e-tests-var-expansion-zp7zh, total namespaces: 47, active: 47, terminating: 0
May  2 03:40:51.726: INFO: namespace e2e-tests-var-expansion-zp7zh deletion completed in 8.116526119s

â€¢ [SLOW TEST:19.322 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:40:51.727: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-15b3428d-6c8c-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 03:40:52.723: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-15b5f9e6-6c8c-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-9n4q7" to be "success or failure"
May  2 03:40:52.739: INFO: Pod "pod-projected-secrets-15b5f9e6-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.871433ms
May  2 03:40:54.756: INFO: Pod "pod-projected-secrets-15b5f9e6-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032694464s
May  2 03:40:56.773: INFO: Pod "pod-projected-secrets-15b5f9e6-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049745824s
May  2 03:40:58.789: INFO: Pod "pod-projected-secrets-15b5f9e6-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065707309s
May  2 03:41:00.805: INFO: Pod "pod-projected-secrets-15b5f9e6-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081652778s
May  2 03:41:02.822: INFO: Pod "pod-projected-secrets-15b5f9e6-6c8c-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.099346928s
STEP: Saw pod success
May  2 03:41:02.822: INFO: Pod "pod-projected-secrets-15b5f9e6-6c8c-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:41:02.838: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-secrets-15b5f9e6-6c8c-11e9-97f0-0a58ac103caa container projected-secret-volume-test: <nil>
STEP: delete the pod
May  2 03:41:02.881: INFO: Waiting for pod pod-projected-secrets-15b5f9e6-6c8c-11e9-97f0-0a58ac103caa to disappear
May  2 03:41:02.896: INFO: Pod pod-projected-secrets-15b5f9e6-6c8c-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:41:02.896: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9n4q7" for this suite.
May  2 03:41:08.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:41:09.999: INFO: namespace: e2e-tests-projected-9n4q7, resource: packagemanifests, items remaining: 1
May  2 03:41:10.082: INFO: namespace: e2e-tests-projected-9n4q7, resource: bindings, ignored listing per whitelist
May  2 03:41:11.023: INFO: namespace: e2e-tests-projected-9n4q7 no longer exists
May  2 03:41:11.040: INFO: namespace: e2e-tests-projected-9n4q7, total namespaces: 47, active: 47, terminating: 0
May  2 03:41:11.055: INFO: namespace e2e-tests-projected-9n4q7 deletion completed in 8.116491155s

â€¢ [SLOW TEST:19.329 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:41:11.056: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-xcccx
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-xcccx to expose endpoints map[]
May  2 03:41:12.070: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-xcccx exposes endpoints map[] (35.816072ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-xcccx
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-xcccx to expose endpoints map[pod1:[100]]
May  2 03:41:16.261: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.162287963s elapsed, will retry)
May  2 03:41:21.449: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-xcccx exposes endpoints map[pod1:[100]] (9.349877338s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-xcccx
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-xcccx to expose endpoints map[pod1:[100] pod2:[101]]
May  2 03:41:25.714: INFO: Unexpected endpoints: found map[21442193-6c8c-11e9-9e44-12f3365d453a:[100]], expected map[pod1:[100] pod2:[101]] (4.241347282s elapsed, will retry)
May  2 03:41:29.932: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-xcccx exposes endpoints map[pod1:[100] pod2:[101]] (8.45962032s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-xcccx
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-xcccx to expose endpoints map[pod2:[101]]
May  2 03:41:29.985: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-xcccx exposes endpoints map[pod2:[101]] (34.376842ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-xcccx
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-xcccx to expose endpoints map[]
May  2 03:41:30.020: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-xcccx exposes endpoints map[] (16.883691ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:41:30.053: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-xcccx" for this suite.
May  2 03:41:54.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:41:55.639: INFO: namespace: e2e-tests-services-xcccx, resource: packagemanifests, items remaining: 1
May  2 03:41:55.655: INFO: namespace: e2e-tests-services-xcccx, resource: bindings, ignored listing per whitelist
May  2 03:41:56.184: INFO: namespace: e2e-tests-services-xcccx no longer exists
May  2 03:41:56.201: INFO: namespace: e2e-tests-services-xcccx, total namespaces: 47, active: 47, terminating: 0
May  2 03:41:56.216: INFO: namespace e2e-tests-services-xcccx deletion completed in 26.120000988s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:45.160 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:41:56.216: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:41:57.180: INFO: Creating deployment "test-recreate-deployment"
May  2 03:41:57.198: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May  2 03:41:57.259: INFO: Waiting deployment "test-recreate-deployment" to complete
May  2 03:41:57.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:41:59.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:42:01.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:42:03.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365317, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:42:05.296: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May  2 03:42:05.333: INFO: Updating deployment test-recreate-deployment
May  2 03:42:05.333: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May  2 03:42:05.437: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-jp2wj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jp2wj/deployments/test-recreate-deployment,UID:3c268126-6c8c-11e9-9e44-12f3365d453a,ResourceVersion:57653,Generation:2,CreationTimestamp:2019-05-02 03:41:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-02 03:42:05 +0000 UTC 2019-05-02 03:42:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-02 03:42:05 +0000 UTC 2019-05-02 03:41:57 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May  2 03:42:05.455: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-jp2wj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jp2wj/replicasets/test-recreate-deployment-697fbf54bf,UID:410572fe-6c8c-11e9-8dad-121ea440cb2c,ResourceVersion:57652,Generation:1,CreationTimestamp:2019-05-02 03:42:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 3c268126-6c8c-11e9-9e44-12f3365d453a 0xc00174c5d7 0xc00174c5d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  2 03:42:05.455: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May  2 03:42:05.455: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-jp2wj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jp2wj/replicasets/test-recreate-deployment-5dfdcc846d,UID:3c271683-6c8c-11e9-8dad-121ea440cb2c,ResourceVersion:57642,Generation:2,CreationTimestamp:2019-05-02 03:41:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 3c268126-6c8c-11e9-9e44-12f3365d453a 0xc00174c4f7 0xc00174c4f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  2 03:42:05.471: INFO: Pod "test-recreate-deployment-697fbf54bf-mnq55" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-mnq55,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-jp2wj,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jp2wj/pods/test-recreate-deployment-697fbf54bf-mnq55,UID:41068e6d-6c8c-11e9-8dad-121ea440cb2c,ResourceVersion:57654,Generation:0,CreationTimestamp:2019-05-02 03:42:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 410572fe-6c8c-11e9-8dad-121ea440cb2c 0xc001c3f997 0xc001c3f998}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4ztzs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4ztzs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4ztzs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-6sgr9}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c3fa00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c3fa20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:42:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:42:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:42:05 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:,StartTime:2019-05-02 03:42:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:42:05.471: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-jp2wj" for this suite.
May  2 03:42:11.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:42:13.102: INFO: namespace: e2e-tests-deployment-jp2wj, resource: bindings, ignored listing per whitelist
May  2 03:42:13.207: INFO: namespace: e2e-tests-deployment-jp2wj, resource: packagemanifests, items remaining: 1
May  2 03:42:13.598: INFO: namespace: e2e-tests-deployment-jp2wj no longer exists
May  2 03:42:13.615: INFO: namespace: e2e-tests-deployment-jp2wj, total namespaces: 47, active: 47, terminating: 0
May  2 03:42:13.632: INFO: namespace e2e-tests-deployment-jp2wj deletion completed in 8.118641457s

â€¢ [SLOW TEST:17.416 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:42:13.633: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:42:14.576: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:42:24.813: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-kxkfp" for this suite.
May  2 03:43:10.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:43:11.888: INFO: namespace: e2e-tests-pods-kxkfp, resource: bindings, ignored listing per whitelist
May  2 03:43:12.375: INFO: namespace: e2e-tests-pods-kxkfp, resource: packagemanifests, items remaining: 1
May  2 03:43:12.940: INFO: namespace: e2e-tests-pods-kxkfp no longer exists
May  2 03:43:12.958: INFO: namespace: e2e-tests-pods-kxkfp, total namespaces: 47, active: 47, terminating: 0
May  2 03:43:12.974: INFO: namespace e2e-tests-pods-kxkfp deletion completed in 48.11823422s

â€¢ [SLOW TEST:59.341 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:43:12.974: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 03:43:13.967: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69e59bd6-6c8c-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-9cldb" to be "success or failure"
May  2 03:43:13.982: INFO: Pod "downwardapi-volume-69e59bd6-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.603662ms
May  2 03:43:15.999: INFO: Pod "downwardapi-volume-69e59bd6-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032036305s
May  2 03:43:18.020: INFO: Pod "downwardapi-volume-69e59bd6-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052851361s
May  2 03:43:20.036: INFO: Pod "downwardapi-volume-69e59bd6-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068810604s
May  2 03:43:22.053: INFO: Pod "downwardapi-volume-69e59bd6-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086019263s
May  2 03:43:24.069: INFO: Pod "downwardapi-volume-69e59bd6-6c8c-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.102040138s
STEP: Saw pod success
May  2 03:43:24.069: INFO: Pod "downwardapi-volume-69e59bd6-6c8c-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:43:24.084: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-69e59bd6-6c8c-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 03:43:24.129: INFO: Waiting for pod downwardapi-volume-69e59bd6-6c8c-11e9-97f0-0a58ac103caa to disappear
May  2 03:43:24.144: INFO: Pod downwardapi-volume-69e59bd6-6c8c-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:43:24.144: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-9cldb" for this suite.
May  2 03:43:30.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:43:31.540: INFO: namespace: e2e-tests-downward-api-9cldb, resource: packagemanifests, items remaining: 1
May  2 03:43:31.556: INFO: namespace: e2e-tests-downward-api-9cldb, resource: bindings, ignored listing per whitelist
May  2 03:43:32.274: INFO: namespace: e2e-tests-downward-api-9cldb no longer exists
May  2 03:43:32.291: INFO: namespace: e2e-tests-downward-api-9cldb, total namespaces: 47, active: 47, terminating: 0
May  2 03:43:32.307: INFO: namespace e2e-tests-downward-api-9cldb deletion completed in 8.119826299s

â€¢ [SLOW TEST:19.333 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:43:32.307: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
May  2 03:43:33.283: INFO: Waiting up to 5m0s for pod "client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-containers-6cxt4" to be "success or failure"
May  2 03:43:33.299: INFO: Pod "client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.777093ms
May  2 03:43:35.316: INFO: Pod "client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031993956s
May  2 03:43:37.331: INFO: Pod "client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047674742s
May  2 03:43:39.348: INFO: Pod "client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064283763s
May  2 03:43:41.364: INFO: Pod "client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080246229s
May  2 03:43:43.380: INFO: Pod "client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.096275919s
May  2 03:43:45.396: INFO: Pod "client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.112387321s
STEP: Saw pod success
May  2 03:43:45.396: INFO: Pod "client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:43:45.411: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 03:43:45.454: INFO: Waiting for pod client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa to disappear
May  2 03:43:45.470: INFO: Pod client-containers-75695aa0-6c8c-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:43:45.470: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-6cxt4" for this suite.
May  2 03:43:51.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:43:52.229: INFO: namespace: e2e-tests-containers-6cxt4, resource: packagemanifests, items remaining: 1
May  2 03:43:52.836: INFO: namespace: e2e-tests-containers-6cxt4, resource: bindings, ignored listing per whitelist
May  2 03:43:53.598: INFO: namespace: e2e-tests-containers-6cxt4 no longer exists
May  2 03:43:53.615: INFO: namespace: e2e-tests-containers-6cxt4, total namespaces: 47, active: 47, terminating: 0
May  2 03:43:53.631: INFO: namespace e2e-tests-containers-6cxt4 deletion completed in 8.118709946s

â€¢ [SLOW TEST:21.323 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:43:53.631: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 03:43:54.597: INFO: Waiting up to 5m0s for pod "downwardapi-volume-821da582-6c8c-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-b99tl" to be "success or failure"
May  2 03:43:54.613: INFO: Pod "downwardapi-volume-821da582-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.320631ms
May  2 03:43:56.629: INFO: Pod "downwardapi-volume-821da582-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031595648s
May  2 03:43:58.645: INFO: Pod "downwardapi-volume-821da582-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047472985s
May  2 03:44:00.662: INFO: Pod "downwardapi-volume-821da582-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064425743s
May  2 03:44:02.679: INFO: Pod "downwardapi-volume-821da582-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081218052s
May  2 03:44:04.696: INFO: Pod "downwardapi-volume-821da582-6c8c-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098130416s
STEP: Saw pod success
May  2 03:44:04.696: INFO: Pod "downwardapi-volume-821da582-6c8c-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:44:04.711: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-821da582-6c8c-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 03:44:04.754: INFO: Waiting for pod downwardapi-volume-821da582-6c8c-11e9-97f0-0a58ac103caa to disappear
May  2 03:44:04.769: INFO: Pod downwardapi-volume-821da582-6c8c-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:44:04.770: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-b99tl" for this suite.
May  2 03:44:10.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:44:11.608: INFO: namespace: e2e-tests-downward-api-b99tl, resource: packagemanifests, items remaining: 1
May  2 03:44:12.388: INFO: namespace: e2e-tests-downward-api-b99tl, resource: bindings, ignored listing per whitelist
May  2 03:44:12.895: INFO: namespace: e2e-tests-downward-api-b99tl no longer exists
May  2 03:44:12.913: INFO: namespace: e2e-tests-downward-api-b99tl, total namespaces: 47, active: 47, terminating: 0
May  2 03:44:12.928: INFO: namespace e2e-tests-downward-api-b99tl deletion completed in 8.11575851s

â€¢ [SLOW TEST:19.297 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:44:12.928: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 03:44:13.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d9e689d-6c8c-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-8kfg9" to be "success or failure"
May  2 03:44:13.914: INFO: Pod "downwardapi-volume-8d9e689d-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.108959ms
May  2 03:44:15.930: INFO: Pod "downwardapi-volume-8d9e689d-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032325087s
May  2 03:44:17.946: INFO: Pod "downwardapi-volume-8d9e689d-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048148191s
May  2 03:44:19.964: INFO: Pod "downwardapi-volume-8d9e689d-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065381607s
May  2 03:44:21.980: INFO: Pod "downwardapi-volume-8d9e689d-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08165257s
May  2 03:44:23.996: INFO: Pod "downwardapi-volume-8d9e689d-6c8c-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097687888s
STEP: Saw pod success
May  2 03:44:23.996: INFO: Pod "downwardapi-volume-8d9e689d-6c8c-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:44:24.012: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-8d9e689d-6c8c-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 03:44:24.054: INFO: Waiting for pod downwardapi-volume-8d9e689d-6c8c-11e9-97f0-0a58ac103caa to disappear
May  2 03:44:24.069: INFO: Pod downwardapi-volume-8d9e689d-6c8c-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:44:24.069: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8kfg9" for this suite.
May  2 03:44:30.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:44:31.664: INFO: namespace: e2e-tests-projected-8kfg9, resource: packagemanifests, items remaining: 1
May  2 03:44:31.941: INFO: namespace: e2e-tests-projected-8kfg9, resource: bindings, ignored listing per whitelist
May  2 03:44:32.196: INFO: namespace: e2e-tests-projected-8kfg9 no longer exists
May  2 03:44:32.213: INFO: namespace: e2e-tests-projected-8kfg9, total namespaces: 47, active: 47, terminating: 0
May  2 03:44:32.229: INFO: namespace e2e-tests-projected-8kfg9 deletion completed in 8.117190483s

â€¢ [SLOW TEST:19.301 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:44:32.229: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:44:33.229: INFO: (0) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 19.359263ms)
May  2 03:44:33.246: INFO: (1) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.502079ms)
May  2 03:44:33.262: INFO: (2) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.409501ms)
May  2 03:44:33.279: INFO: (3) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.322054ms)
May  2 03:44:33.295: INFO: (4) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.359994ms)
May  2 03:44:33.312: INFO: (5) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.454669ms)
May  2 03:44:33.328: INFO: (6) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.233291ms)
May  2 03:44:33.344: INFO: (7) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.206924ms)
May  2 03:44:33.360: INFO: (8) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.352557ms)
May  2 03:44:33.377: INFO: (9) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.239752ms)
May  2 03:44:33.394: INFO: (10) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 17.086538ms)
May  2 03:44:33.410: INFO: (11) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.240274ms)
May  2 03:44:33.427: INFO: (12) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.365733ms)
May  2 03:44:33.449: INFO: (13) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 22.356025ms)
May  2 03:44:33.467: INFO: (14) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 18.141782ms)
May  2 03:44:33.485: INFO: (15) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 17.217448ms)
May  2 03:44:33.502: INFO: (16) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.972858ms)
May  2 03:44:33.521: INFO: (17) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 18.964344ms)
May  2 03:44:33.537: INFO: (18) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 16.354992ms)
May  2 03:44:33.555: INFO: (19) /api/v1/nodes/ip-10-0-135-216.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a hr... (200; 17.594171ms)
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:44:33.555: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-44665" for this suite.
May  2 03:44:39.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:44:39.756: INFO: namespace: e2e-tests-proxy-44665, resource: packagemanifests, items remaining: 1
May  2 03:44:40.462: INFO: namespace: e2e-tests-proxy-44665, resource: bindings, ignored listing per whitelist
May  2 03:44:40.976: INFO: namespace: e2e-tests-proxy-44665 no longer exists
May  2 03:44:40.993: INFO: namespace: e2e-tests-proxy-44665, total namespaces: 47, active: 47, terminating: 0
May  2 03:44:41.009: INFO: namespace e2e-tests-proxy-44665 deletion completed in 7.437216172s

â€¢ [SLOW TEST:8.780 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:44:41.009: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
May  2 03:44:41.953: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May  2 03:44:41.954: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:44:42.759: INFO: stderr: ""
May  2 03:44:42.759: INFO: stdout: "service/redis-slave created\n"
May  2 03:44:42.759: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May  2 03:44:42.759: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:44:43.233: INFO: stderr: ""
May  2 03:44:43.233: INFO: stdout: "service/redis-master created\n"
May  2 03:44:43.234: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May  2 03:44:43.234: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:44:44.158: INFO: stderr: ""
May  2 03:44:44.158: INFO: stdout: "service/frontend created\n"
May  2 03:44:44.159: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May  2 03:44:44.159: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:44:45.114: INFO: stderr: ""
May  2 03:44:45.114: INFO: stdout: "deployment.extensions/frontend created\n"
May  2 03:44:45.115: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  2 03:44:45.115: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:44:45.514: INFO: stderr: ""
May  2 03:44:45.514: INFO: stdout: "deployment.extensions/redis-master created\n"
May  2 03:44:45.514: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May  2 03:44:45.514: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:44:46.240: INFO: stderr: ""
May  2 03:44:46.240: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
May  2 03:44:46.240: INFO: Waiting for all frontend pods to be Running.
May  2 03:45:21.298: INFO: Waiting for frontend to serve content.
May  2 03:45:21.329: INFO: Trying to add a new entry to the guestbook.
May  2 03:45:21.364: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May  2 03:45:21.395: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:45:21.595: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  2 03:45:21.595: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May  2 03:45:21.595: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:45:21.783: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  2 03:45:21.783: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May  2 03:45:21.783: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:45:21.962: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  2 03:45:21.962: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May  2 03:45:21.962: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:45:22.125: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  2 03:45:22.125: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May  2 03:45:22.126: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:45:22.293: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  2 03:45:22.293: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May  2 03:45:22.293: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5twb6'
May  2 03:45:22.456: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  2 03:45:22.456: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:45:22.456: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5twb6" for this suite.
May  2 03:46:02.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:46:03.366: INFO: namespace: e2e-tests-kubectl-5twb6, resource: bindings, ignored listing per whitelist
May  2 03:46:03.965: INFO: namespace: e2e-tests-kubectl-5twb6, resource: packagemanifests, items remaining: 1
May  2 03:46:04.582: INFO: namespace: e2e-tests-kubectl-5twb6 no longer exists
May  2 03:46:04.600: INFO: namespace: e2e-tests-kubectl-5twb6, total namespaces: 47, active: 47, terminating: 0
May  2 03:46:04.615: INFO: namespace e2e-tests-kubectl-5twb6 deletion completed in 42.116664804s

â€¢ [SLOW TEST:83.607 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:46:04.616: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 03:46:05.590: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d030d2a1-6c8c-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-9b5ww" to be "success or failure"
May  2 03:46:05.605: INFO: Pod "downwardapi-volume-d030d2a1-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.320615ms
May  2 03:46:07.621: INFO: Pod "downwardapi-volume-d030d2a1-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031708556s
May  2 03:46:09.637: INFO: Pod "downwardapi-volume-d030d2a1-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047869181s
May  2 03:46:11.654: INFO: Pod "downwardapi-volume-d030d2a1-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064177457s
May  2 03:46:13.670: INFO: Pod "downwardapi-volume-d030d2a1-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080525532s
May  2 03:46:15.686: INFO: Pod "downwardapi-volume-d030d2a1-6c8c-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.096550815s
STEP: Saw pod success
May  2 03:46:15.686: INFO: Pod "downwardapi-volume-d030d2a1-6c8c-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:46:15.702: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-d030d2a1-6c8c-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 03:46:15.745: INFO: Waiting for pod downwardapi-volume-d030d2a1-6c8c-11e9-97f0-0a58ac103caa to disappear
May  2 03:46:15.760: INFO: Pod downwardapi-volume-d030d2a1-6c8c-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:46:15.761: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9b5ww" for this suite.
May  2 03:46:21.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:46:22.704: INFO: namespace: e2e-tests-projected-9b5ww, resource: bindings, ignored listing per whitelist
May  2 03:46:23.572: INFO: namespace: e2e-tests-projected-9b5ww, resource: packagemanifests, items remaining: 1
May  2 03:46:23.888: INFO: namespace: e2e-tests-projected-9b5ww no longer exists
May  2 03:46:23.907: INFO: namespace: e2e-tests-projected-9b5ww, total namespaces: 47, active: 47, terminating: 0
May  2 03:46:23.923: INFO: namespace e2e-tests-projected-9b5ww deletion completed in 8.119646919s

â€¢ [SLOW TEST:19.307 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:46:23.923: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
May  2 03:46:24.885: INFO: Waiting up to 5m0s for pod "pod-dbb05eba-6c8c-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-k5d2p" to be "success or failure"
May  2 03:46:24.903: INFO: Pod "pod-dbb05eba-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 17.778136ms
May  2 03:46:26.920: INFO: Pod "pod-dbb05eba-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0350661s
May  2 03:46:28.940: INFO: Pod "pod-dbb05eba-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054947627s
May  2 03:46:30.956: INFO: Pod "pod-dbb05eba-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071366799s
May  2 03:46:32.973: INFO: Pod "pod-dbb05eba-6c8c-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.088072664s
May  2 03:46:34.989: INFO: Pod "pod-dbb05eba-6c8c-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104429547s
STEP: Saw pod success
May  2 03:46:34.989: INFO: Pod "pod-dbb05eba-6c8c-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:46:35.005: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-dbb05eba-6c8c-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 03:46:35.051: INFO: Waiting for pod pod-dbb05eba-6c8c-11e9-97f0-0a58ac103caa to disappear
May  2 03:46:35.066: INFO: Pod pod-dbb05eba-6c8c-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:46:35.066: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-k5d2p" for this suite.
May  2 03:46:41.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:46:42.004: INFO: namespace: e2e-tests-emptydir-k5d2p, resource: packagemanifests, items remaining: 1
May  2 03:46:42.582: INFO: namespace: e2e-tests-emptydir-k5d2p, resource: bindings, ignored listing per whitelist
May  2 03:46:43.194: INFO: namespace: e2e-tests-emptydir-k5d2p no longer exists
May  2 03:46:43.212: INFO: namespace: e2e-tests-emptydir-k5d2p, total namespaces: 47, active: 47, terminating: 0
May  2 03:46:43.228: INFO: namespace e2e-tests-emptydir-k5d2p deletion completed in 8.118870626s

â€¢ [SLOW TEST:19.305 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:46:43.228: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:46:44.189: INFO: Pod name rollover-pod: Found 0 pods out of 1
May  2 03:46:49.205: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  2 03:46:53.237: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May  2 03:46:55.257: INFO: Creating deployment "test-rollover-deployment"
May  2 03:46:55.294: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May  2 03:46:57.329: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May  2 03:46:57.363: INFO: Ensure that both replica sets have 1 created replica
May  2 03:46:57.396: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May  2 03:46:57.432: INFO: Updating deployment test-rollover-deployment
May  2 03:46:57.432: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May  2 03:46:59.466: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May  2 03:46:59.498: INFO: Make sure deployment "test-rollover-deployment" is complete
May  2 03:46:59.532: INFO: all replica sets need to contain the pod-template-hash label
May  2 03:46:59.532: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365617, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:47:01.568: INFO: all replica sets need to contain the pod-template-hash label
May  2 03:47:01.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365617, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:47:03.567: INFO: all replica sets need to contain the pod-template-hash label
May  2 03:47:03.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365617, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:47:05.566: INFO: all replica sets need to contain the pod-template-hash label
May  2 03:47:05.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365617, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:47:07.566: INFO: all replica sets need to contain the pod-template-hash label
May  2 03:47:07.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365625, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:47:09.567: INFO: all replica sets need to contain the pod-template-hash label
May  2 03:47:09.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365625, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:47:11.566: INFO: all replica sets need to contain the pod-template-hash label
May  2 03:47:11.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365625, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:47:13.566: INFO: all replica sets need to contain the pod-template-hash label
May  2 03:47:13.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365625, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:47:15.566: INFO: all replica sets need to contain the pod-template-hash label
May  2 03:47:15.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365625, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692365615, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:47:17.567: INFO: 
May  2 03:47:17.567: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May  2 03:47:17.616: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-z7plt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-z7plt/deployments/test-rollover-deployment,UID:edd17f83-6c8c-11e9-9e44-12f3365d453a,ResourceVersion:60058,Generation:2,CreationTimestamp:2019-05-02 03:46:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-02 03:46:55 +0000 UTC 2019-05-02 03:46:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-02 03:47:15 +0000 UTC 2019-05-02 03:46:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May  2 03:47:17.633: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-z7plt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-z7plt/replicasets/test-rollover-deployment-6b7f9d6597,UID:ef1b1671-6c8c-11e9-8dad-121ea440cb2c,ResourceVersion:60047,Generation:2,CreationTimestamp:2019-05-02 03:46:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment edd17f83-6c8c-11e9-9e44-12f3365d453a 0xc001b54367 0xc001b54368}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May  2 03:47:17.633: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May  2 03:47:17.634: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-z7plt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-z7plt/replicasets/test-rollover-controller,UID:e732c5ab-6c8c-11e9-9e44-12f3365d453a,ResourceVersion:60057,Generation:2,CreationTimestamp:2019-05-02 03:46:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment edd17f83-6c8c-11e9-9e44-12f3365d453a 0xc00114bf07 0xc00114bf08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  2 03:47:17.634: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-z7plt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-z7plt/replicasets/test-rollover-deployment-6586df867b,UID:edd34b02-6c8c-11e9-8dad-121ea440cb2c,ResourceVersion:59944,Generation:2,CreationTimestamp:2019-05-02 03:46:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment edd17f83-6c8c-11e9-9e44-12f3365d453a 0xc001b54227 0xc001b54228}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  2 03:47:17.650: INFO: Pod "test-rollover-deployment-6b7f9d6597-djr7w" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-djr7w,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-z7plt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-z7plt/pods/test-rollover-deployment-6b7f9d6597-djr7w,UID:ef203404-6c8c-11e9-8dad-121ea440cb2c,ResourceVersion:59999,Generation:0,CreationTimestamp:2019-05-02 03:46:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.171"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 ef1b1671-6c8c-11e9-8dad-121ea440cb2c 0xc00129c997 0xc00129c998}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-5clk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5clk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-5clk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pthg2}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00129ca70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00129ca90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:46:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:47:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:47:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:46:57 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:10.131.0.171,StartTime:2019-05-02 03:46:57 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-02 03:47:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://8a52711840bbd4d999a16bd967d25f6f5d0d21dbbb8118e6ad10d7dc13d60ea3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:47:17.650: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-z7plt" for this suite.
May  2 03:47:23.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:47:24.313: INFO: namespace: e2e-tests-deployment-z7plt, resource: packagemanifests, items remaining: 1
May  2 03:47:24.421: INFO: namespace: e2e-tests-deployment-z7plt, resource: bindings, ignored listing per whitelist
May  2 03:47:25.775: INFO: namespace: e2e-tests-deployment-z7plt no longer exists
May  2 03:47:25.792: INFO: namespace: e2e-tests-deployment-z7plt, total namespaces: 47, active: 47, terminating: 0
May  2 03:47:25.807: INFO: namespace e2e-tests-deployment-z7plt deletion completed in 8.114080099s

â€¢ [SLOW TEST:42.579 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:47:25.807: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
May  2 03:47:26.779: INFO: Waiting up to 5m0s for pod "pod-0095b565-6c8d-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-5f27z" to be "success or failure"
May  2 03:47:26.794: INFO: Pod "pod-0095b565-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.396582ms
May  2 03:47:28.810: INFO: Pod "pod-0095b565-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0314757s
May  2 03:47:30.826: INFO: Pod "pod-0095b565-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047661019s
May  2 03:47:32.843: INFO: Pod "pod-0095b565-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063858192s
May  2 03:47:34.859: INFO: Pod "pod-0095b565-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.079902185s
May  2 03:47:36.875: INFO: Pod "pod-0095b565-6c8d-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.096395622s
STEP: Saw pod success
May  2 03:47:36.875: INFO: Pod "pod-0095b565-6c8d-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:47:36.891: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-0095b565-6c8d-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 03:47:36.933: INFO: Waiting for pod pod-0095b565-6c8d-11e9-97f0-0a58ac103caa to disappear
May  2 03:47:36.948: INFO: Pod pod-0095b565-6c8d-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:47:36.948: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5f27z" for this suite.
May  2 03:47:43.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:47:43.641: INFO: namespace: e2e-tests-emptydir-5f27z, resource: bindings, ignored listing per whitelist
May  2 03:47:44.691: INFO: namespace: e2e-tests-emptydir-5f27z, resource: packagemanifests, items remaining: 1
May  2 03:47:45.074: INFO: namespace: e2e-tests-emptydir-5f27z no longer exists
May  2 03:47:45.091: INFO: namespace: e2e-tests-emptydir-5f27z, total namespaces: 47, active: 47, terminating: 0
May  2 03:47:45.107: INFO: namespace e2e-tests-emptydir-5f27z deletion completed in 8.116024786s

â€¢ [SLOW TEST:19.300 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:47:45.107: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May  2 03:47:46.068: INFO: Waiting up to 5m0s for pod "downward-api-0c15315f-6c8d-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-xddlb" to be "success or failure"
May  2 03:47:46.085: INFO: Pod "downward-api-0c15315f-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.323911ms
May  2 03:47:48.101: INFO: Pod "downward-api-0c15315f-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0326061s
May  2 03:47:50.118: INFO: Pod "downward-api-0c15315f-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049725902s
May  2 03:47:52.135: INFO: Pod "downward-api-0c15315f-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066022547s
May  2 03:47:54.151: INFO: Pod "downward-api-0c15315f-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082596712s
May  2 03:47:56.167: INFO: Pod "downward-api-0c15315f-6c8d-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098980776s
STEP: Saw pod success
May  2 03:47:56.168: INFO: Pod "downward-api-0c15315f-6c8d-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:47:56.183: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downward-api-0c15315f-6c8d-11e9-97f0-0a58ac103caa container dapi-container: <nil>
STEP: delete the pod
May  2 03:47:56.226: INFO: Waiting for pod downward-api-0c15315f-6c8d-11e9-97f0-0a58ac103caa to disappear
May  2 03:47:56.243: INFO: Pod downward-api-0c15315f-6c8d-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:47:56.243: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xddlb" for this suite.
May  2 03:48:02.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:48:02.955: INFO: namespace: e2e-tests-downward-api-xddlb, resource: packagemanifests, items remaining: 1
May  2 03:48:03.250: INFO: namespace: e2e-tests-downward-api-xddlb, resource: bindings, ignored listing per whitelist
May  2 03:48:04.371: INFO: namespace: e2e-tests-downward-api-xddlb no longer exists
May  2 03:48:04.389: INFO: namespace: e2e-tests-downward-api-xddlb, total namespaces: 47, active: 47, terminating: 0
May  2 03:48:04.404: INFO: namespace e2e-tests-downward-api-xddlb deletion completed in 8.118710361s

â€¢ [SLOW TEST:19.297 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:48:04.404: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 03:48:05.361: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1794f6ed-6c8d-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-bxpnl" to be "success or failure"
May  2 03:48:05.377: INFO: Pod "downwardapi-volume-1794f6ed-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.948637ms
May  2 03:48:07.393: INFO: Pod "downwardapi-volume-1794f6ed-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032230517s
May  2 03:48:09.410: INFO: Pod "downwardapi-volume-1794f6ed-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048818523s
May  2 03:48:11.427: INFO: Pod "downwardapi-volume-1794f6ed-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065736546s
May  2 03:48:13.443: INFO: Pod "downwardapi-volume-1794f6ed-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082065493s
May  2 03:48:15.460: INFO: Pod "downwardapi-volume-1794f6ed-6c8d-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098510191s
STEP: Saw pod success
May  2 03:48:15.460: INFO: Pod "downwardapi-volume-1794f6ed-6c8d-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:48:15.475: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-1794f6ed-6c8d-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 03:48:15.518: INFO: Waiting for pod downwardapi-volume-1794f6ed-6c8d-11e9-97f0-0a58ac103caa to disappear
May  2 03:48:15.533: INFO: Pod downwardapi-volume-1794f6ed-6c8d-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:48:15.533: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-bxpnl" for this suite.
May  2 03:48:21.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:48:22.343: INFO: namespace: e2e-tests-downward-api-bxpnl, resource: bindings, ignored listing per whitelist
May  2 03:48:23.279: INFO: namespace: e2e-tests-downward-api-bxpnl, resource: packagemanifests, items remaining: 1
May  2 03:48:23.659: INFO: namespace: e2e-tests-downward-api-bxpnl no longer exists
May  2 03:48:23.677: INFO: namespace: e2e-tests-downward-api-bxpnl, total namespaces: 47, active: 47, terminating: 0
May  2 03:48:23.692: INFO: namespace e2e-tests-downward-api-bxpnl deletion completed in 8.115788349s

â€¢ [SLOW TEST:19.288 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:48:23.693: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:48:34.763: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-gmhg9" for this suite.
May  2 03:49:20.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:49:22.219: INFO: namespace: e2e-tests-kubelet-test-gmhg9, resource: bindings, ignored listing per whitelist
May  2 03:49:22.333: INFO: namespace: e2e-tests-kubelet-test-gmhg9, resource: packagemanifests, items remaining: 1
May  2 03:49:22.890: INFO: namespace: e2e-tests-kubelet-test-gmhg9 no longer exists
May  2 03:49:22.906: INFO: namespace: e2e-tests-kubelet-test-gmhg9, total namespaces: 47, active: 47, terminating: 0
May  2 03:49:22.922: INFO: namespace e2e-tests-kubelet-test-gmhg9 deletion completed in 48.115989931s

â€¢ [SLOW TEST:59.229 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:49:22.922: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-466591c4-6c8d-11e9-97f0-0a58ac103caa
STEP: Creating secret with name secret-projected-all-test-volume-466591ae-6c8d-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test Check all projections for projected volume plugin
May  2 03:49:23.941: INFO: Waiting up to 5m0s for pod "projected-volume-4665916c-6c8d-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-ftbxs" to be "success or failure"
May  2 03:49:23.959: INFO: Pod "projected-volume-4665916c-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 17.50234ms
May  2 03:49:25.976: INFO: Pod "projected-volume-4665916c-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034465778s
May  2 03:49:27.992: INFO: Pod "projected-volume-4665916c-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051141302s
May  2 03:49:30.009: INFO: Pod "projected-volume-4665916c-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067440792s
May  2 03:49:32.025: INFO: Pod "projected-volume-4665916c-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083680026s
May  2 03:49:34.041: INFO: Pod "projected-volume-4665916c-6c8d-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.100339892s
STEP: Saw pod success
May  2 03:49:34.041: INFO: Pod "projected-volume-4665916c-6c8d-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:49:34.057: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod projected-volume-4665916c-6c8d-11e9-97f0-0a58ac103caa container projected-all-volume-test: <nil>
STEP: delete the pod
May  2 03:49:34.101: INFO: Waiting for pod projected-volume-4665916c-6c8d-11e9-97f0-0a58ac103caa to disappear
May  2 03:49:34.117: INFO: Pod projected-volume-4665916c-6c8d-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:49:34.117: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ftbxs" for this suite.
May  2 03:49:40.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:49:40.871: INFO: namespace: e2e-tests-projected-ftbxs, resource: packagemanifests, items remaining: 1
May  2 03:49:40.871: INFO: namespace: e2e-tests-projected-ftbxs, resource: bindings, ignored listing per whitelist
May  2 03:49:42.239: INFO: namespace: e2e-tests-projected-ftbxs no longer exists
May  2 03:49:42.256: INFO: namespace: e2e-tests-projected-ftbxs, total namespaces: 47, active: 47, terminating: 0
May  2 03:49:42.272: INFO: namespace e2e-tests-projected-ftbxs deletion completed in 8.113326797s

â€¢ [SLOW TEST:19.350 seconds]
[sig-storage] Projected combined
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:49:42.272: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May  2 03:49:43.214: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
May  2 03:49:43.247: INFO: Waiting for terminating namespaces to be deleted...
May  2 03:49:43.264: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-135-216.ec2.internal before test
May  2 03:49:43.285: INFO: kube-state-metrics-74d989d8d8-ffvw4 from openshift-monitoring started at 2019-05-02 01:50:34 +0000 UTC (3 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May  2 03:49:43.285: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May  2 03:49:43.285: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  2 03:49:43.285: INFO: community-operators-67554d9c5f-f9lqv from openshift-marketplace started at 2019-05-02 01:50:36 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container community-operators ready: true, restart count 0
May  2 03:49:43.285: INFO: sdn-vnzxb from openshift-sdn started at 2019-05-02 01:49:53 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container sdn ready: true, restart count 0
May  2 03:49:43.285: INFO: dns-default-j5c7f from openshift-dns started at 2019-05-02 01:49:53 +0000 UTC (2 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container dns ready: true, restart count 0
May  2 03:49:43.285: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  2 03:49:43.285: INFO: redhat-operators-8658cf87c-9pfhf from openshift-marketplace started at 2019-05-02 01:50:36 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container redhat-operators ready: true, restart count 0
May  2 03:49:43.285: INFO: ovs-ckrk2 from openshift-sdn started at 2019-05-02 01:49:53 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container openvswitch ready: true, restart count 0
May  2 03:49:43.285: INFO: machine-config-daemon-6kqb6 from openshift-machine-config-operator started at 2019-05-02 01:50:32 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container machine-config-daemon ready: true, restart count 0
May  2 03:49:43.285: INFO: node-ca-cvq2r from openshift-image-registry started at 2019-05-02 01:51:28 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container node-ca ready: true, restart count 0
May  2 03:49:43.285: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-05-02 01:52:08 +0000 UTC (3 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container alertmanager ready: true, restart count 0
May  2 03:49:43.285: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  2 03:49:43.285: INFO: 	Container config-reloader ready: true, restart count 0
May  2 03:49:43.285: INFO: prometheus-adapter-88656d548-swtj7 from openshift-monitoring started at 2019-05-02 01:52:10 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  2 03:49:43.285: INFO: multus-8jgrn from openshift-multus started at 2019-05-02 01:49:53 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container kube-multus ready: true, restart count 0
May  2 03:49:43.285: INFO: tuned-t9pcv from openshift-cluster-node-tuning-operator started at 2019-05-02 01:50:14 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container tuned ready: true, restart count 0
May  2 03:49:43.285: INFO: certified-operators-6f5774bb49-zqjw9 from openshift-marketplace started at 2019-05-02 01:50:35 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container certified-operators ready: true, restart count 0
May  2 03:49:43.285: INFO: router-default-8db6b5c76-pmbks from openshift-ingress started at 2019-05-02 01:50:55 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container router ready: true, restart count 0
May  2 03:49:43.285: INFO: node-exporter-jvhfx from openshift-monitoring started at 2019-05-02 01:51:03 +0000 UTC (2 container statuses recorded)
May  2 03:49:43.285: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:49:43.285: INFO: 	Container node-exporter ready: true, restart count 0
May  2 03:49:43.285: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-140-17.ec2.internal before test
May  2 03:49:43.309: INFO: downloads-5b9759bd45-ffhzh from openshift-console started at 2019-05-02 01:50:08 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.309: INFO: 	Container download-server ready: true, restart count 0
May  2 03:49:43.309: INFO: machine-config-daemon-fvkb5 from openshift-machine-config-operator started at 2019-05-02 01:50:37 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.309: INFO: 	Container machine-config-daemon ready: true, restart count 0
May  2 03:49:43.309: INFO: dns-default-l8vqv from openshift-dns started at 2019-05-02 01:49:57 +0000 UTC (2 container statuses recorded)
May  2 03:49:43.309: INFO: 	Container dns ready: true, restart count 0
May  2 03:49:43.309: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  2 03:49:43.309: INFO: multus-ngggs from openshift-multus started at 2019-05-02 01:49:57 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.309: INFO: 	Container kube-multus ready: true, restart count 0
May  2 03:49:43.309: INFO: sdn-ww2h4 from openshift-sdn started at 2019-05-02 01:49:57 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.309: INFO: 	Container sdn ready: true, restart count 0
May  2 03:49:43.309: INFO: telemeter-client-7cccb458c9-j8xss from openshift-monitoring started at 2019-05-02 01:50:38 +0000 UTC (3 container statuses recorded)
May  2 03:49:43.309: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:49:43.309: INFO: 	Container reload ready: true, restart count 0
May  2 03:49:43.309: INFO: 	Container telemeter-client ready: true, restart count 0
May  2 03:49:43.309: INFO: node-ca-9nbxx from openshift-image-registry started at 2019-05-02 01:51:28 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.309: INFO: 	Container node-ca ready: true, restart count 0
May  2 03:49:43.309: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-05-02 01:51:31 +0000 UTC (3 container statuses recorded)
May  2 03:49:43.309: INFO: 	Container alertmanager ready: true, restart count 0
May  2 03:49:43.309: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  2 03:49:43.309: INFO: 	Container config-reloader ready: true, restart count 0
May  2 03:49:43.309: INFO: image-registry-7d85675b77-2xv8x from openshift-image-registry started at 2019-05-02 01:51:06 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.309: INFO: 	Container registry ready: true, restart count 0
May  2 03:49:43.309: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-05-02 01:52:12 +0000 UTC (6 container statuses recorded)
May  2 03:49:43.309: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:49:43.309: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  2 03:49:43.309: INFO: 	Container prometheus ready: true, restart count 1
May  2 03:49:43.309: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May  2 03:49:43.309: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  2 03:49:43.309: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May  2 03:49:43.309: INFO: ovs-9m7kl from openshift-sdn started at 2019-05-02 01:49:57 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.309: INFO: 	Container openvswitch ready: true, restart count 0
May  2 03:49:43.310: INFO: tuned-l87zf from openshift-cluster-node-tuning-operator started at 2019-05-02 01:50:14 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.310: INFO: 	Container tuned ready: true, restart count 0
May  2 03:49:43.310: INFO: router-default-8db6b5c76-sn9q9 from openshift-ingress started at 2019-05-02 01:50:55 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.310: INFO: 	Container router ready: true, restart count 0
May  2 03:49:43.310: INFO: node-exporter-5ms2n from openshift-monitoring started at 2019-05-02 01:51:03 +0000 UTC (2 container statuses recorded)
May  2 03:49:43.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:49:43.310: INFO: 	Container node-exporter ready: true, restart count 0
May  2 03:49:43.310: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-153-32.ec2.internal before test
May  2 03:49:43.333: INFO: dns-default-kqjfp from openshift-dns started at 2019-05-02 01:50:03 +0000 UTC (2 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container dns ready: true, restart count 0
May  2 03:49:43.333: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  2 03:49:43.333: INFO: node-ca-vpzc4 from openshift-image-registry started at 2019-05-02 01:51:28 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container node-ca ready: true, restart count 0
May  2 03:49:43.333: INFO: ovs-zvrlj from openshift-sdn started at 2019-05-02 01:50:03 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container openvswitch ready: true, restart count 0
May  2 03:49:43.333: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-05-02 01:51:47 +0000 UTC (3 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container alertmanager ready: true, restart count 0
May  2 03:49:43.333: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  2 03:49:43.333: INFO: 	Container config-reloader ready: true, restart count 0
May  2 03:49:43.333: INFO: machine-config-daemon-clvrk from openshift-machine-config-operator started at 2019-05-02 01:51:01 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container machine-config-daemon ready: true, restart count 0
May  2 03:49:43.333: INFO: sdn-22xbv from openshift-sdn started at 2019-05-02 01:50:03 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container sdn ready: true, restart count 0
May  2 03:49:43.333: INFO: prometheus-adapter-88656d548-666vp from openshift-monitoring started at 2019-05-02 01:52:10 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  2 03:49:43.333: INFO: multus-vt2rg from openshift-multus started at 2019-05-02 01:50:03 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container kube-multus ready: true, restart count 0
May  2 03:49:43.333: INFO: downloads-5b9759bd45-rw4d7 from openshift-console started at 2019-05-02 01:50:08 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container download-server ready: true, restart count 0
May  2 03:49:43.333: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-05-02 01:52:12 +0000 UTC (6 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:49:43.333: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  2 03:49:43.333: INFO: 	Container prometheus ready: true, restart count 1
May  2 03:49:43.333: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May  2 03:49:43.333: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  2 03:49:43.333: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May  2 03:49:43.333: INFO: tuned-crx5s from openshift-cluster-node-tuning-operator started at 2019-05-02 01:50:14 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container tuned ready: true, restart count 0
May  2 03:49:43.333: INFO: grafana-5ccd6d5857-65m9v from openshift-monitoring started at 2019-05-02 01:51:27 +0000 UTC (2 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container grafana ready: true, restart count 0
May  2 03:49:43.333: INFO: 	Container grafana-proxy ready: true, restart count 0
May  2 03:49:43.333: INFO: node-exporter-c4m2k from openshift-monitoring started at 2019-05-02 01:51:03 +0000 UTC (2 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  2 03:49:43.333: INFO: 	Container node-exporter ready: true, restart count 0
May  2 03:49:43.333: INFO: prometheus-operator-f7c6b5c59-6wjgw from openshift-monitoring started at 2019-05-02 01:51:59 +0000 UTC (1 container statuses recorded)
May  2 03:49:43.333: INFO: 	Container prometheus-operator ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.159ac1246fad0d33], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match node selector, 3 node(s) were unschedulable.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:49:44.468: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-7npk8" for this suite.
May  2 03:49:50.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:49:51.505: INFO: namespace: e2e-tests-sched-pred-7npk8, resource: bindings, ignored listing per whitelist
May  2 03:49:52.265: INFO: namespace: e2e-tests-sched-pred-7npk8, resource: packagemanifests, items remaining: 1
May  2 03:49:52.584: INFO: namespace: e2e-tests-sched-pred-7npk8 no longer exists
May  2 03:49:52.602: INFO: namespace: e2e-tests-sched-pred-7npk8, total namespaces: 47, active: 47, terminating: 0
May  2 03:49:52.617: INFO: namespace e2e-tests-sched-pred-7npk8 deletion completed in 8.114923435s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:10.345 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:49:52.617: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May  2 03:49:53.550: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-sw622'
May  2 03:49:54.591: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  2 03:49:54.591: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
May  2 03:49:56.632: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-sw622'
May  2 03:49:56.816: INFO: stderr: ""
May  2 03:49:56.816: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:49:56.816: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sw622" for this suite.
May  2 03:50:02.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:50:03.934: INFO: namespace: e2e-tests-kubectl-sw622, resource: bindings, ignored listing per whitelist
May  2 03:50:04.067: INFO: namespace: e2e-tests-kubectl-sw622, resource: packagemanifests, items remaining: 1
May  2 03:50:04.944: INFO: namespace: e2e-tests-kubectl-sw622 no longer exists
May  2 03:50:04.962: INFO: namespace: e2e-tests-kubectl-sw622, total namespaces: 47, active: 47, terminating: 0
May  2 03:50:04.978: INFO: namespace e2e-tests-kubectl-sw622 deletion completed in 8.118710396s

â€¢ [SLOW TEST:12.360 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:50:04.978: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-5f75aba7-6c8d-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 03:50:05.976: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5f789077-6c8d-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-nhkd7" to be "success or failure"
May  2 03:50:05.991: INFO: Pod "pod-projected-secrets-5f789077-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.378393ms
May  2 03:50:08.008: INFO: Pod "pod-projected-secrets-5f789077-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031828772s
May  2 03:50:10.024: INFO: Pod "pod-projected-secrets-5f789077-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048158945s
May  2 03:50:12.040: INFO: Pod "pod-projected-secrets-5f789077-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064493847s
May  2 03:50:14.056: INFO: Pod "pod-projected-secrets-5f789077-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080496718s
May  2 03:50:16.073: INFO: Pod "pod-projected-secrets-5f789077-6c8d-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097017891s
STEP: Saw pod success
May  2 03:50:16.073: INFO: Pod "pod-projected-secrets-5f789077-6c8d-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:50:16.088: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-secrets-5f789077-6c8d-11e9-97f0-0a58ac103caa container projected-secret-volume-test: <nil>
STEP: delete the pod
May  2 03:50:16.133: INFO: Waiting for pod pod-projected-secrets-5f789077-6c8d-11e9-97f0-0a58ac103caa to disappear
May  2 03:50:16.149: INFO: Pod pod-projected-secrets-5f789077-6c8d-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:50:16.149: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nhkd7" for this suite.
May  2 03:50:22.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:50:22.917: INFO: namespace: e2e-tests-projected-nhkd7, resource: bindings, ignored listing per whitelist
May  2 03:50:23.118: INFO: namespace: e2e-tests-projected-nhkd7, resource: packagemanifests, items remaining: 1
May  2 03:50:24.278: INFO: namespace: e2e-tests-projected-nhkd7 no longer exists
May  2 03:50:24.295: INFO: namespace: e2e-tests-projected-nhkd7, total namespaces: 47, active: 47, terminating: 0
May  2 03:50:24.310: INFO: namespace e2e-tests-projected-nhkd7 deletion completed in 8.11858115s

â€¢ [SLOW TEST:19.333 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:50:24.310: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
May  2 03:50:25.276: INFO: Waiting up to 5m0s for pod "client-containers-6af9c2a3-6c8d-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-containers-9srbb" to be "success or failure"
May  2 03:50:25.292: INFO: Pod "client-containers-6af9c2a3-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.38047ms
May  2 03:50:27.309: INFO: Pod "client-containers-6af9c2a3-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032799429s
May  2 03:50:29.325: INFO: Pod "client-containers-6af9c2a3-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049156012s
May  2 03:50:31.342: INFO: Pod "client-containers-6af9c2a3-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065495212s
May  2 03:50:33.358: INFO: Pod "client-containers-6af9c2a3-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081758926s
May  2 03:50:35.375: INFO: Pod "client-containers-6af9c2a3-6c8d-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098451136s
STEP: Saw pod success
May  2 03:50:35.375: INFO: Pod "client-containers-6af9c2a3-6c8d-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:50:35.390: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod client-containers-6af9c2a3-6c8d-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 03:50:35.433: INFO: Waiting for pod client-containers-6af9c2a3-6c8d-11e9-97f0-0a58ac103caa to disappear
May  2 03:50:35.449: INFO: Pod client-containers-6af9c2a3-6c8d-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:50:35.449: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-9srbb" for this suite.
May  2 03:50:41.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:50:42.599: INFO: namespace: e2e-tests-containers-9srbb, resource: bindings, ignored listing per whitelist
May  2 03:50:42.666: INFO: namespace: e2e-tests-containers-9srbb, resource: packagemanifests, items remaining: 1
May  2 03:50:43.582: INFO: namespace: e2e-tests-containers-9srbb no longer exists
May  2 03:50:43.600: INFO: namespace: e2e-tests-containers-9srbb, total namespaces: 47, active: 47, terminating: 0
May  2 03:50:43.615: INFO: namespace e2e-tests-containers-9srbb deletion completed in 8.123268089s

â€¢ [SLOW TEST:19.305 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:50:43.616: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
May  2 03:50:44.592: INFO: Waiting up to 5m0s for pod "pod-767d6db8-6c8d-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-l2c79" to be "success or failure"
May  2 03:50:44.608: INFO: Pod "pod-767d6db8-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.641776ms
May  2 03:50:46.624: INFO: Pod "pod-767d6db8-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032157412s
May  2 03:50:48.641: INFO: Pod "pod-767d6db8-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048890553s
May  2 03:50:50.657: INFO: Pod "pod-767d6db8-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065078305s
May  2 03:50:52.673: INFO: Pod "pod-767d6db8-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080917463s
May  2 03:50:54.689: INFO: Pod "pod-767d6db8-6c8d-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097246043s
STEP: Saw pod success
May  2 03:50:54.689: INFO: Pod "pod-767d6db8-6c8d-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:50:54.705: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-767d6db8-6c8d-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 03:50:54.747: INFO: Waiting for pod pod-767d6db8-6c8d-11e9-97f0-0a58ac103caa to disappear
May  2 03:50:54.762: INFO: Pod pod-767d6db8-6c8d-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:50:54.763: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-l2c79" for this suite.
May  2 03:51:00.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:51:01.585: INFO: namespace: e2e-tests-emptydir-l2c79, resource: packagemanifests, items remaining: 1
May  2 03:51:01.954: INFO: namespace: e2e-tests-emptydir-l2c79, resource: bindings, ignored listing per whitelist
May  2 03:51:02.892: INFO: namespace: e2e-tests-emptydir-l2c79 no longer exists
May  2 03:51:02.910: INFO: namespace: e2e-tests-emptydir-l2c79, total namespaces: 47, active: 47, terminating: 0
May  2 03:51:02.925: INFO: namespace e2e-tests-emptydir-l2c79 deletion completed in 8.11976941s

â€¢ [SLOW TEST:19.310 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:51:02.925: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-8vsx6
May  2 03:51:13.966: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-8vsx6
STEP: checking the pod's current state and verifying that restartCount is present
May  2 03:51:13.981: INFO: Initial restart count of pod liveness-exec is 0
May  2 03:52:06.428: INFO: Restart count of pod e2e-tests-container-probe-8vsx6/liveness-exec is now 1 (52.446615878s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:52:06.450: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-8vsx6" for this suite.
May  2 03:52:12.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:52:13.343: INFO: namespace: e2e-tests-container-probe-8vsx6, resource: packagemanifests, items remaining: 1
May  2 03:52:13.408: INFO: namespace: e2e-tests-container-probe-8vsx6, resource: bindings, ignored listing per whitelist
May  2 03:52:14.576: INFO: namespace: e2e-tests-container-probe-8vsx6 no longer exists
May  2 03:52:14.594: INFO: namespace: e2e-tests-container-probe-8vsx6, total namespaces: 47, active: 47, terminating: 0
May  2 03:52:14.609: INFO: namespace e2e-tests-container-probe-8vsx6 deletion completed in 8.116350956s

â€¢ [SLOW TEST:71.684 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:52:14.609: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:52:15.560: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-acba8ca0-6c8d-11e9-97f0-0a58ac103caa
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-acba8ca0-6c8d-11e9-97f0-0a58ac103caa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:53:30.409: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-nffxr" for this suite.
May  2 03:53:54.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:53:55.285: INFO: namespace: e2e-tests-configmap-nffxr, resource: packagemanifests, items remaining: 1
May  2 03:53:55.704: INFO: namespace: e2e-tests-configmap-nffxr, resource: bindings, ignored listing per whitelist
May  2 03:53:56.536: INFO: namespace: e2e-tests-configmap-nffxr no longer exists
May  2 03:53:56.553: INFO: namespace: e2e-tests-configmap-nffxr, total namespaces: 47, active: 47, terminating: 0
May  2 03:53:56.569: INFO: namespace e2e-tests-configmap-nffxr deletion completed in 26.116518632s

â€¢ [SLOW TEST:101.959 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:53:56.569: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
May  2 03:54:07.619: INFO: Pod pod-hostip-e980256a-6c8d-11e9-97f0-0a58ac103caa has hostIP: 10.0.135.216
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:54:07.619: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-wscrm" for this suite.
May  2 03:54:29.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:54:30.643: INFO: namespace: e2e-tests-pods-wscrm, resource: packagemanifests, items remaining: 1
May  2 03:54:30.862: INFO: namespace: e2e-tests-pods-wscrm, resource: bindings, ignored listing per whitelist
May  2 03:54:31.746: INFO: namespace: e2e-tests-pods-wscrm no longer exists
May  2 03:54:31.763: INFO: namespace: e2e-tests-pods-wscrm, total namespaces: 47, active: 47, terminating: 0
May  2 03:54:31.779: INFO: namespace e2e-tests-pods-wscrm deletion completed in 24.117009007s

â€¢ [SLOW TEST:35.210 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:54:31.779: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-n844v/configmap-test-fe7c1a67-6c8d-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 03:54:32.770: INFO: Waiting up to 5m0s for pod "pod-configmaps-fe7eec35-6c8d-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-configmap-n844v" to be "success or failure"
May  2 03:54:32.786: INFO: Pod "pod-configmaps-fe7eec35-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.37941ms
May  2 03:54:34.803: INFO: Pod "pod-configmaps-fe7eec35-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03306836s
May  2 03:54:36.819: INFO: Pod "pod-configmaps-fe7eec35-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049323479s
May  2 03:54:38.836: INFO: Pod "pod-configmaps-fe7eec35-6c8d-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06578949s
May  2 03:54:40.852: INFO: Pod "pod-configmaps-fe7eec35-6c8d-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.082425718s
STEP: Saw pod success
May  2 03:54:40.852: INFO: Pod "pod-configmaps-fe7eec35-6c8d-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:54:40.868: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-configmaps-fe7eec35-6c8d-11e9-97f0-0a58ac103caa container env-test: <nil>
STEP: delete the pod
May  2 03:54:40.911: INFO: Waiting for pod pod-configmaps-fe7eec35-6c8d-11e9-97f0-0a58ac103caa to disappear
May  2 03:54:40.926: INFO: Pod pod-configmaps-fe7eec35-6c8d-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:54:40.927: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-n844v" for this suite.
May  2 03:54:47.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:54:47.852: INFO: namespace: e2e-tests-configmap-n844v, resource: bindings, ignored listing per whitelist
May  2 03:54:48.019: INFO: namespace: e2e-tests-configmap-n844v, resource: packagemanifests, items remaining: 1
May  2 03:54:49.054: INFO: namespace: e2e-tests-configmap-n844v no longer exists
May  2 03:54:49.072: INFO: namespace: e2e-tests-configmap-n844v, total namespaces: 47, active: 47, terminating: 0
May  2 03:54:49.088: INFO: namespace e2e-tests-configmap-n844v deletion completed in 8.117852683s

â€¢ [SLOW TEST:17.309 seconds]
[sig-node] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:54:49.088: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
May  2 03:54:50.061: INFO: Waiting up to 5m0s for pod "pod-08cd23fc-6c8e-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-hwfx6" to be "success or failure"
May  2 03:54:50.078: INFO: Pod "pod-08cd23fc-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.420381ms
May  2 03:54:52.094: INFO: Pod "pod-08cd23fc-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032788129s
May  2 03:54:54.110: INFO: Pod "pod-08cd23fc-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048986649s
May  2 03:54:56.127: INFO: Pod "pod-08cd23fc-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065196373s
May  2 03:54:58.143: INFO: Pod "pod-08cd23fc-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081700448s
May  2 03:55:00.161: INFO: Pod "pod-08cd23fc-6c8e-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.099653519s
STEP: Saw pod success
May  2 03:55:00.161: INFO: Pod "pod-08cd23fc-6c8e-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:55:00.177: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-08cd23fc-6c8e-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 03:55:00.227: INFO: Waiting for pod pod-08cd23fc-6c8e-11e9-97f0-0a58ac103caa to disappear
May  2 03:55:00.243: INFO: Pod pod-08cd23fc-6c8e-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:55:00.243: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hwfx6" for this suite.
May  2 03:55:06.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:55:07.216: INFO: namespace: e2e-tests-emptydir-hwfx6, resource: packagemanifests, items remaining: 1
May  2 03:55:07.542: INFO: namespace: e2e-tests-emptydir-hwfx6, resource: bindings, ignored listing per whitelist
May  2 03:55:08.371: INFO: namespace: e2e-tests-emptydir-hwfx6 no longer exists
May  2 03:55:08.388: INFO: namespace: e2e-tests-emptydir-hwfx6, total namespaces: 47, active: 47, terminating: 0
May  2 03:55:08.405: INFO: namespace e2e-tests-emptydir-hwfx6 deletion completed in 8.118726496s

â€¢ [SLOW TEST:19.317 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:55:08.405: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-1451207e-6c8e-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 03:55:09.403: INFO: Waiting up to 5m0s for pod "pod-secrets-1454329f-6c8e-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-secrets-8d9sb" to be "success or failure"
May  2 03:55:09.419: INFO: Pod "pod-secrets-1454329f-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.143729ms
May  2 03:55:11.435: INFO: Pod "pod-secrets-1454329f-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032276158s
May  2 03:55:13.451: INFO: Pod "pod-secrets-1454329f-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048297804s
May  2 03:55:15.467: INFO: Pod "pod-secrets-1454329f-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064418369s
May  2 03:55:17.484: INFO: Pod "pod-secrets-1454329f-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081326987s
May  2 03:55:19.501: INFO: Pod "pod-secrets-1454329f-6c8e-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097849303s
STEP: Saw pod success
May  2 03:55:19.501: INFO: Pod "pod-secrets-1454329f-6c8e-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 03:55:19.517: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-secrets-1454329f-6c8e-11e9-97f0-0a58ac103caa container secret-volume-test: <nil>
STEP: delete the pod
May  2 03:55:19.561: INFO: Waiting for pod pod-secrets-1454329f-6c8e-11e9-97f0-0a58ac103caa to disappear
May  2 03:55:19.576: INFO: Pod pod-secrets-1454329f-6c8e-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:55:19.576: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8d9sb" for this suite.
May  2 03:55:25.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:55:26.301: INFO: namespace: e2e-tests-secrets-8d9sb, resource: bindings, ignored listing per whitelist
May  2 03:55:27.237: INFO: namespace: e2e-tests-secrets-8d9sb, resource: packagemanifests, items remaining: 1
May  2 03:55:27.702: INFO: namespace: e2e-tests-secrets-8d9sb no longer exists
May  2 03:55:27.718: INFO: namespace: e2e-tests-secrets-8d9sb, total namespaces: 47, active: 47, terminating: 0
May  2 03:55:27.734: INFO: namespace e2e-tests-secrets-8d9sb deletion completed in 8.114694068s

â€¢ [SLOW TEST:19.329 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:55:27.734: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:55:28.677: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May  2 03:55:28.717: INFO: Pod name sample-pod: Found 0 pods out of 1
May  2 03:55:33.733: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  2 03:55:37.766: INFO: Creating deployment "test-rolling-update-deployment"
May  2 03:55:37.786: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May  2 03:55:37.821: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
May  2 03:55:39.856: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May  2 03:55:39.875: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:55:41.892: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:55:43.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:55:45.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692366137, loc:(*time.Location)(0x7b5bbe0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  2 03:55:47.893: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May  2 03:55:47.945: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-l9n7d,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-l9n7d/deployments/test-rolling-update-deployment,UID:254216f5-6c8e-11e9-9e44-12f3365d453a,ResourceVersion:63810,Generation:1,CreationTimestamp:2019-05-02 03:55:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-02 03:55:37 +0000 UTC 2019-05-02 03:55:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-02 03:55:45 +0000 UTC 2019-05-02 03:55:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May  2 03:55:47.962: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-l9n7d,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-l9n7d/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:254441f5-6c8e-11e9-8dad-121ea440cb2c,ResourceVersion:63800,Generation:1,CreationTimestamp:2019-05-02 03:55:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 254216f5-6c8e-11e9-9e44-12f3365d453a 0xc002597dc7 0xc002597dc8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May  2 03:55:47.962: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May  2 03:55:47.962: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-l9n7d,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-l9n7d/replicasets/test-rolling-update-controller,UID:1fd72455-6c8e-11e9-9e44-12f3365d453a,ResourceVersion:63809,Generation:2,CreationTimestamp:2019-05-02 03:55:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 254216f5-6c8e-11e9-9e44-12f3365d453a 0xc002597d07 0xc002597d08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  2 03:55:47.978: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-zc7r5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-zc7r5,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-l9n7d,SelfLink:/api/v1/namespaces/e2e-tests-deployment-l9n7d/pods/test-rolling-update-deployment-68b55d7bc6-zc7r5,UID:25459b63-6c8e-11e9-8dad-121ea440cb2c,ResourceVersion:63799,Generation:0,CreationTimestamp:2019-05-02 03:55:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.188"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 254441f5-6c8e-11e9-8dad-121ea440cb2c 0xc001e0b8c7 0xc001e0b8c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fk4zl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fk4zl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-fk4zl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-216.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-74msf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e0b930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e0b950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:55:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:55:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:55:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-02 03:55:37 +0000 UTC  }],Message:,Reason:,HostIP:10.0.135.216,PodIP:10.131.0.188,StartTime:2019-05-02 03:55:37 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-02 03:55:45 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://5935a63e6832c7f9ac2fbfa6bfa02d638ea990cdd150c5c16df50e6438490fc9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:55:47.978: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-l9n7d" for this suite.
May  2 03:55:54.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:55:54.961: INFO: namespace: e2e-tests-deployment-l9n7d, resource: packagemanifests, items remaining: 1
May  2 03:55:55.940: INFO: namespace: e2e-tests-deployment-l9n7d, resource: bindings, ignored listing per whitelist
May  2 03:55:56.105: INFO: namespace: e2e-tests-deployment-l9n7d no longer exists
May  2 03:55:56.123: INFO: namespace: e2e-tests-deployment-l9n7d, total namespaces: 47, active: 47, terminating: 0
May  2 03:55:56.138: INFO: namespace e2e-tests-deployment-l9n7d deletion completed in 8.117277779s

â€¢ [SLOW TEST:28.405 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:55:56.139: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
May  2 03:55:57.150: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:55:57.815: INFO: stderr: ""
May  2 03:55:57.815: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  2 03:55:57.815: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:55:58.038: INFO: stderr: ""
May  2 03:55:58.038: INFO: stdout: "update-demo-nautilus-5l5vs update-demo-nautilus-fmwnx "
May  2 03:55:58.038: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-5l5vs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:55:58.277: INFO: stderr: ""
May  2 03:55:58.277: INFO: stdout: ""
May  2 03:55:58.277: INFO: update-demo-nautilus-5l5vs is created but not running
May  2 03:56:03.277: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:03.740: INFO: stderr: ""
May  2 03:56:03.740: INFO: stdout: "update-demo-nautilus-5l5vs update-demo-nautilus-fmwnx "
May  2 03:56:03.741: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-5l5vs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:04.145: INFO: stderr: ""
May  2 03:56:04.145: INFO: stdout: ""
May  2 03:56:04.145: INFO: update-demo-nautilus-5l5vs is created but not running
May  2 03:56:09.150: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:09.464: INFO: stderr: ""
May  2 03:56:09.464: INFO: stdout: "update-demo-nautilus-5l5vs update-demo-nautilus-fmwnx "
May  2 03:56:09.464: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-5l5vs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:09.721: INFO: stderr: ""
May  2 03:56:09.721: INFO: stdout: "true"
May  2 03:56:09.721: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-5l5vs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:10.090: INFO: stderr: ""
May  2 03:56:10.090: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  2 03:56:10.090: INFO: validating pod update-demo-nautilus-5l5vs
May  2 03:56:10.111: INFO: got data: {
  "image": "nautilus.jpg"
}

May  2 03:56:10.111: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  2 03:56:10.111: INFO: update-demo-nautilus-5l5vs is verified up and running
May  2 03:56:10.111: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-fmwnx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:10.468: INFO: stderr: ""
May  2 03:56:10.468: INFO: stdout: "true"
May  2 03:56:10.469: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-fmwnx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:10.810: INFO: stderr: ""
May  2 03:56:10.810: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  2 03:56:10.810: INFO: validating pod update-demo-nautilus-fmwnx
May  2 03:56:10.830: INFO: got data: {
  "image": "nautilus.jpg"
}

May  2 03:56:10.830: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  2 03:56:10.830: INFO: update-demo-nautilus-fmwnx is verified up and running
STEP: scaling down the replication controller
May  2 03:56:10.834: INFO: scanned /tmp/home for discovery docs: <nil>
May  2 03:56:10.834: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:11.297: INFO: stderr: ""
May  2 03:56:11.297: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  2 03:56:11.297: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:11.563: INFO: stderr: ""
May  2 03:56:11.563: INFO: stdout: "update-demo-nautilus-5l5vs update-demo-nautilus-fmwnx "
STEP: Replicas for name=update-demo: expected=1 actual=2
May  2 03:56:16.563: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:16.959: INFO: stderr: ""
May  2 03:56:16.960: INFO: stdout: "update-demo-nautilus-fmwnx "
May  2 03:56:16.960: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-fmwnx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:17.286: INFO: stderr: ""
May  2 03:56:17.286: INFO: stdout: "true"
May  2 03:56:17.287: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-fmwnx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:17.666: INFO: stderr: ""
May  2 03:56:17.666: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  2 03:56:17.666: INFO: validating pod update-demo-nautilus-fmwnx
May  2 03:56:17.688: INFO: got data: {
  "image": "nautilus.jpg"
}

May  2 03:56:17.688: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  2 03:56:17.688: INFO: update-demo-nautilus-fmwnx is verified up and running
STEP: scaling up the replication controller
May  2 03:56:17.699: INFO: scanned /tmp/home for discovery docs: <nil>
May  2 03:56:17.700: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:19.276: INFO: stderr: ""
May  2 03:56:19.276: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  2 03:56:19.276: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:19.567: INFO: stderr: ""
May  2 03:56:19.567: INFO: stdout: "update-demo-nautilus-9vjms update-demo-nautilus-fmwnx "
May  2 03:56:19.568: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-9vjms -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:19.993: INFO: stderr: ""
May  2 03:56:19.993: INFO: stdout: ""
May  2 03:56:19.993: INFO: update-demo-nautilus-9vjms is created but not running
May  2 03:56:24.994: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:25.245: INFO: stderr: ""
May  2 03:56:25.245: INFO: stdout: "update-demo-nautilus-9vjms update-demo-nautilus-fmwnx "
May  2 03:56:25.245: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-9vjms -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:25.556: INFO: stderr: ""
May  2 03:56:25.556: INFO: stdout: ""
May  2 03:56:25.556: INFO: update-demo-nautilus-9vjms is created but not running
May  2 03:56:30.557: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:30.739: INFO: stderr: ""
May  2 03:56:30.739: INFO: stdout: "update-demo-nautilus-9vjms update-demo-nautilus-fmwnx "
May  2 03:56:30.739: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-9vjms -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:30.898: INFO: stderr: ""
May  2 03:56:30.898: INFO: stdout: "true"
May  2 03:56:30.898: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-9vjms -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:31.067: INFO: stderr: ""
May  2 03:56:31.067: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  2 03:56:31.067: INFO: validating pod update-demo-nautilus-9vjms
May  2 03:56:31.087: INFO: got data: {
  "image": "nautilus.jpg"
}

May  2 03:56:31.087: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  2 03:56:31.087: INFO: update-demo-nautilus-9vjms is verified up and running
May  2 03:56:31.087: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-fmwnx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:31.252: INFO: stderr: ""
May  2 03:56:31.252: INFO: stdout: "true"
May  2 03:56:31.252: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-fmwnx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:31.402: INFO: stderr: ""
May  2 03:56:31.402: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  2 03:56:31.402: INFO: validating pod update-demo-nautilus-fmwnx
May  2 03:56:31.420: INFO: got data: {
  "image": "nautilus.jpg"
}

May  2 03:56:31.420: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  2 03:56:31.420: INFO: update-demo-nautilus-fmwnx is verified up and running
STEP: using delete to clean up resources
May  2 03:56:31.420: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:31.597: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  2 03:56:31.597: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  2 03:56:31.597: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-4rlxg'
May  2 03:56:31.767: INFO: stderr: "No resources found.\n"
May  2 03:56:31.767: INFO: stdout: ""
May  2 03:56:31.767: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-4rlxg -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  2 03:56:31.931: INFO: stderr: ""
May  2 03:56:31.931: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:56:31.931: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4rlxg" for this suite.
May  2 03:56:56.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:56:56.576: INFO: namespace: e2e-tests-kubectl-4rlxg, resource: bindings, ignored listing per whitelist
May  2 03:56:57.945: INFO: namespace: e2e-tests-kubectl-4rlxg, resource: packagemanifests, items remaining: 1
May  2 03:56:58.060: INFO: namespace: e2e-tests-kubectl-4rlxg no longer exists
May  2 03:56:58.078: INFO: namespace: e2e-tests-kubectl-4rlxg, total namespaces: 47, active: 47, terminating: 0
May  2 03:56:58.094: INFO: namespace e2e-tests-kubectl-4rlxg deletion completed in 26.120174546s

â€¢ [SLOW TEST:61.956 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:56:58.094: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-ctb2f A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-ctb2f;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-ctb2f A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-ctb2f;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-ctb2f.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-ctb2f.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-ctb2f.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-ctb2f.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-ctb2f.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-ctb2f.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-ctb2f.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-ctb2f.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-ctb2f.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 167.160.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.160.167_udp@PTR;check="$$(dig +tcp +noall +answer +search 167.160.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.160.167_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-ctb2f A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-ctb2f;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-ctb2f A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-ctb2f;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-ctb2f.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-ctb2f.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-ctb2f.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-ctb2f.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-ctb2f.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-ctb2f.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-ctb2f.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-ctb2f.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-ctb2f.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 167.160.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.160.167_udp@PTR;check="$$(dig +tcp +noall +answer +search 167.160.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.160.167_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  2 03:57:09.199: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.216: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.233: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-ctb2f from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.249: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-ctb2f from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.266: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-ctb2f.svc from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.282: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-ctb2f.svc from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.299: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.319: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.439: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.456: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.473: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-ctb2f from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.489: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-ctb2f from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.506: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-ctb2f.svc from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.523: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-ctb2f.svc from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.539: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.556: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc from pod e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa: the server could not find the requested resource (get pods dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa)
May  2 03:57:09.659: INFO: Lookups using e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-ctb2f wheezy_tcp@dns-test-service.e2e-tests-dns-ctb2f wheezy_udp@dns-test-service.e2e-tests-dns-ctb2f.svc wheezy_tcp@dns-test-service.e2e-tests-dns-ctb2f.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-ctb2f jessie_tcp@dns-test-service.e2e-tests-dns-ctb2f jessie_udp@dns-test-service.e2e-tests-dns-ctb2f.svc jessie_tcp@dns-test-service.e2e-tests-dns-ctb2f.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ctb2f.svc]

May  2 03:57:15.118: INFO: DNS probes using e2e-tests-dns-ctb2f/dns-test-55bbb530-6c8e-11e9-97f0-0a58ac103caa succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:57:15.235: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-ctb2f" for this suite.
May  2 03:57:21.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:57:22.393: INFO: namespace: e2e-tests-dns-ctb2f, resource: bindings, ignored listing per whitelist
May  2 03:57:22.826: INFO: namespace: e2e-tests-dns-ctb2f, resource: packagemanifests, items remaining: 1
May  2 03:57:23.362: INFO: namespace: e2e-tests-dns-ctb2f no longer exists
May  2 03:57:23.379: INFO: namespace: e2e-tests-dns-ctb2f, total namespaces: 47, active: 47, terminating: 0
May  2 03:57:23.394: INFO: namespace e2e-tests-dns-ctb2f deletion completed in 8.128843409s

â€¢ [SLOW TEST:25.300 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:57:23.394: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:57:24.444: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"64d12a33-6c8e-11e9-9e44-12f3365d453a", Controller:(*bool)(0xc0026064d6), BlockOwnerDeletion:(*bool)(0xc0026064d7)}}
May  2 03:57:24.463: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"64c9771b-6c8e-11e9-9e44-12f3365d453a", Controller:(*bool)(0xc00247ad06), BlockOwnerDeletion:(*bool)(0xc00247ad07)}}
May  2 03:57:24.482: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"64cd387d-6c8e-11e9-9e44-12f3365d453a", Controller:(*bool)(0xc002606802), BlockOwnerDeletion:(*bool)(0xc002606803)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:57:29.516: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-x9ht5" for this suite.
May  2 03:57:35.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:57:37.286: INFO: namespace: e2e-tests-gc-x9ht5, resource: packagemanifests, items remaining: 1
May  2 03:57:37.366: INFO: namespace: e2e-tests-gc-x9ht5, resource: bindings, ignored listing per whitelist
May  2 03:57:37.652: INFO: namespace: e2e-tests-gc-x9ht5 no longer exists
May  2 03:57:37.669: INFO: namespace: e2e-tests-gc-x9ht5, total namespaces: 47, active: 47, terminating: 0
May  2 03:57:37.685: INFO: namespace e2e-tests-gc-x9ht5 deletion completed in 8.125755835s

â€¢ [SLOW TEST:14.290 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:57:37.685: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May  2 03:57:56.783: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  2 03:57:56.799: INFO: Pod pod-with-prestop-http-hook still exists
May  2 03:57:58.800: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  2 03:57:58.816: INFO: Pod pod-with-prestop-http-hook still exists
May  2 03:58:00.800: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  2 03:58:00.816: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:58:00.838: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-b7pzv" for this suite.
May  2 03:58:24.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:58:25.901: INFO: namespace: e2e-tests-container-lifecycle-hook-b7pzv, resource: packagemanifests, items remaining: 1
May  2 03:58:26.193: INFO: namespace: e2e-tests-container-lifecycle-hook-b7pzv, resource: bindings, ignored listing per whitelist
May  2 03:58:26.964: INFO: namespace: e2e-tests-container-lifecycle-hook-b7pzv no longer exists
May  2 03:58:26.982: INFO: namespace: e2e-tests-container-lifecycle-hook-b7pzv, total namespaces: 47, active: 47, terminating: 0
May  2 03:58:26.997: INFO: namespace e2e-tests-container-lifecycle-hook-b7pzv deletion completed in 26.117437993s

â€¢ [SLOW TEST:49.313 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:58:26.998: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 03:58:28.089: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May  2 03:58:28.126: INFO: Number of nodes with available pods: 0
May  2 03:58:28.126: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May  2 03:58:28.196: INFO: Number of nodes with available pods: 0
May  2 03:58:28.196: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:29.212: INFO: Number of nodes with available pods: 0
May  2 03:58:29.212: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:30.219: INFO: Number of nodes with available pods: 0
May  2 03:58:30.219: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:31.213: INFO: Number of nodes with available pods: 0
May  2 03:58:31.213: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:32.213: INFO: Number of nodes with available pods: 0
May  2 03:58:32.213: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:33.212: INFO: Number of nodes with available pods: 0
May  2 03:58:33.212: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:34.212: INFO: Number of nodes with available pods: 0
May  2 03:58:34.212: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:35.213: INFO: Number of nodes with available pods: 0
May  2 03:58:35.213: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:36.213: INFO: Number of nodes with available pods: 0
May  2 03:58:36.213: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:37.213: INFO: Number of nodes with available pods: 1
May  2 03:58:37.213: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May  2 03:58:37.281: INFO: Number of nodes with available pods: 0
May  2 03:58:37.281: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May  2 03:58:37.316: INFO: Number of nodes with available pods: 0
May  2 03:58:37.316: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:38.333: INFO: Number of nodes with available pods: 0
May  2 03:58:38.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:39.333: INFO: Number of nodes with available pods: 0
May  2 03:58:39.334: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:40.333: INFO: Number of nodes with available pods: 0
May  2 03:58:40.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:41.333: INFO: Number of nodes with available pods: 0
May  2 03:58:41.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:42.333: INFO: Number of nodes with available pods: 0
May  2 03:58:42.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:43.333: INFO: Number of nodes with available pods: 0
May  2 03:58:43.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:44.333: INFO: Number of nodes with available pods: 0
May  2 03:58:44.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:45.333: INFO: Number of nodes with available pods: 0
May  2 03:58:45.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:46.333: INFO: Number of nodes with available pods: 0
May  2 03:58:46.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:47.333: INFO: Number of nodes with available pods: 0
May  2 03:58:47.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:48.333: INFO: Number of nodes with available pods: 0
May  2 03:58:48.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:49.333: INFO: Number of nodes with available pods: 0
May  2 03:58:49.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:50.333: INFO: Number of nodes with available pods: 0
May  2 03:58:50.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:51.334: INFO: Number of nodes with available pods: 0
May  2 03:58:51.334: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:52.333: INFO: Number of nodes with available pods: 0
May  2 03:58:52.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:53.333: INFO: Number of nodes with available pods: 0
May  2 03:58:53.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:54.333: INFO: Number of nodes with available pods: 0
May  2 03:58:54.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:55.333: INFO: Number of nodes with available pods: 0
May  2 03:58:55.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:56.333: INFO: Number of nodes with available pods: 0
May  2 03:58:56.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:57.333: INFO: Number of nodes with available pods: 0
May  2 03:58:57.333: INFO: Node ip-10-0-135-216.ec2.internal is running more than one daemon pod
May  2 03:58:58.333: INFO: Number of nodes with available pods: 1
May  2 03:58:58.333: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-f7mh5, will wait for the garbage collector to delete the pods
May  2 03:58:58.450: INFO: Deleting DaemonSet.extensions daemon-set took: 19.213106ms
May  2 03:58:58.551: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.289499ms
May  2 03:59:08.667: INFO: Number of nodes with available pods: 0
May  2 03:59:08.667: INFO: Number of running nodes: 0, number of available pods: 0
May  2 03:59:08.683: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-f7mh5/daemonsets","resourceVersion":"65294"},"items":null}

May  2 03:59:08.698: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-f7mh5/pods","resourceVersion":"65294"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:59:08.793: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-f7mh5" for this suite.
May  2 03:59:14.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:59:15.616: INFO: namespace: e2e-tests-daemonsets-f7mh5, resource: bindings, ignored listing per whitelist
May  2 03:59:16.183: INFO: namespace: e2e-tests-daemonsets-f7mh5, resource: packagemanifests, items remaining: 1
May  2 03:59:16.904: INFO: namespace: e2e-tests-daemonsets-f7mh5 no longer exists
May  2 03:59:16.921: INFO: namespace: e2e-tests-daemonsets-f7mh5, total namespaces: 47, active: 47, terminating: 0
May  2 03:59:16.936: INFO: namespace e2e-tests-daemonsets-f7mh5 deletion completed in 8.114000953s

â€¢ [SLOW TEST:49.939 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:59:16.936: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
May  2 03:59:17.883: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-kzrjp'
May  2 03:59:18.345: INFO: stderr: ""
May  2 03:59:18.345: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May  2 03:59:19.363: INFO: Selector matched 1 pods for map[app:redis]
May  2 03:59:19.363: INFO: Found 0 / 1
May  2 03:59:20.362: INFO: Selector matched 1 pods for map[app:redis]
May  2 03:59:20.362: INFO: Found 0 / 1
May  2 03:59:21.362: INFO: Selector matched 1 pods for map[app:redis]
May  2 03:59:21.362: INFO: Found 0 / 1
May  2 03:59:22.362: INFO: Selector matched 1 pods for map[app:redis]
May  2 03:59:22.362: INFO: Found 0 / 1
May  2 03:59:23.362: INFO: Selector matched 1 pods for map[app:redis]
May  2 03:59:23.362: INFO: Found 0 / 1
May  2 03:59:24.362: INFO: Selector matched 1 pods for map[app:redis]
May  2 03:59:24.362: INFO: Found 0 / 1
May  2 03:59:25.364: INFO: Selector matched 1 pods for map[app:redis]
May  2 03:59:25.364: INFO: Found 0 / 1
May  2 03:59:26.362: INFO: Selector matched 1 pods for map[app:redis]
May  2 03:59:26.362: INFO: Found 0 / 1
May  2 03:59:27.362: INFO: Selector matched 1 pods for map[app:redis]
May  2 03:59:27.363: INFO: Found 1 / 1
May  2 03:59:27.363: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May  2 03:59:27.378: INFO: Selector matched 1 pods for map[app:redis]
May  2 03:59:27.378: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  2 03:59:27.378: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig patch pod redis-master-2l7jg --namespace=e2e-tests-kubectl-kzrjp -p {"metadata":{"annotations":{"x":"y"}}}'
May  2 03:59:27.548: INFO: stderr: ""
May  2 03:59:27.548: INFO: stdout: "pod/redis-master-2l7jg patched\n"
STEP: checking annotations
May  2 03:59:27.565: INFO: Selector matched 1 pods for map[app:redis]
May  2 03:59:27.565: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:59:27.565: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-kzrjp" for this suite.
May  2 03:59:51.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 03:59:53.054: INFO: namespace: e2e-tests-kubectl-kzrjp, resource: bindings, ignored listing per whitelist
May  2 03:59:53.633: INFO: namespace: e2e-tests-kubectl-kzrjp, resource: packagemanifests, items remaining: 1
May  2 03:59:53.697: INFO: namespace: e2e-tests-kubectl-kzrjp no longer exists
May  2 03:59:53.713: INFO: namespace: e2e-tests-kubectl-kzrjp, total namespaces: 47, active: 47, terminating: 0
May  2 03:59:53.728: INFO: namespace e2e-tests-kubectl-kzrjp deletion completed in 26.119906432s

â€¢ [SLOW TEST:36.792 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 03:59:53.729: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May  2 03:59:54.796: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-wvdjl,SelfLink:/api/v1/namespaces/e2e-tests-watch-wvdjl/configmaps/e2e-watch-test-resource-version,UID:be61c7ec-6c8e-11e9-9e44-12f3365d453a,ResourceVersion:65675,Generation:0,CreationTimestamp:2019-05-02 03:59:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  2 03:59:54.796: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-wvdjl,SelfLink:/api/v1/namespaces/e2e-tests-watch-wvdjl/configmaps/e2e-watch-test-resource-version,UID:be61c7ec-6c8e-11e9-9e44-12f3365d453a,ResourceVersion:65676,Generation:0,CreationTimestamp:2019-05-02 03:59:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 03:59:54.796: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-wvdjl" for this suite.
May  2 04:00:00.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:00:01.485: INFO: namespace: e2e-tests-watch-wvdjl, resource: bindings, ignored listing per whitelist
May  2 04:00:02.320: INFO: namespace: e2e-tests-watch-wvdjl, resource: packagemanifests, items remaining: 1
May  2 04:00:02.899: INFO: namespace: e2e-tests-watch-wvdjl no longer exists
May  2 04:00:02.921: INFO: namespace: e2e-tests-watch-wvdjl, total namespaces: 47, active: 47, terminating: 0
May  2 04:00:02.937: INFO: namespace e2e-tests-watch-wvdjl deletion completed in 8.121742437s

â€¢ [SLOW TEST:9.208 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:00:02.937: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May  2 04:00:03.965: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-82kct,SelfLink:/api/v1/namespaces/e2e-tests-watch-82kct/configmaps/e2e-watch-test-watch-closed,UID:c3e18898-6c8e-11e9-9e44-12f3365d453a,ResourceVersion:65770,Generation:0,CreationTimestamp:2019-05-02 04:00:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  2 04:00:03.966: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-82kct,SelfLink:/api/v1/namespaces/e2e-tests-watch-82kct/configmaps/e2e-watch-test-watch-closed,UID:c3e18898-6c8e-11e9-9e44-12f3365d453a,ResourceVersion:65773,Generation:0,CreationTimestamp:2019-05-02 04:00:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May  2 04:00:04.041: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-82kct,SelfLink:/api/v1/namespaces/e2e-tests-watch-82kct/configmaps/e2e-watch-test-watch-closed,UID:c3e18898-6c8e-11e9-9e44-12f3365d453a,ResourceVersion:65775,Generation:0,CreationTimestamp:2019-05-02 04:00:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  2 04:00:04.041: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-82kct,SelfLink:/api/v1/namespaces/e2e-tests-watch-82kct/configmaps/e2e-watch-test-watch-closed,UID:c3e18898-6c8e-11e9-9e44-12f3365d453a,ResourceVersion:65776,Generation:0,CreationTimestamp:2019-05-02 04:00:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:00:04.041: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-82kct" for this suite.
May  2 04:00:10.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:00:10.767: INFO: namespace: e2e-tests-watch-82kct, resource: packagemanifests, items remaining: 1
May  2 04:00:11.532: INFO: namespace: e2e-tests-watch-82kct, resource: bindings, ignored listing per whitelist
May  2 04:00:12.150: INFO: namespace: e2e-tests-watch-82kct no longer exists
May  2 04:00:12.168: INFO: namespace: e2e-tests-watch-82kct, total namespaces: 47, active: 47, terminating: 0
May  2 04:00:12.183: INFO: namespace e2e-tests-watch-82kct deletion completed in 8.122820264s

â€¢ [SLOW TEST:9.246 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:00:12.184: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-c9611937-6c8e-11e9-97f0-0a58ac103caa
May  2 04:00:13.162: INFO: Pod name my-hostname-basic-c9611937-6c8e-11e9-97f0-0a58ac103caa: Found 1 pods out of 1
May  2 04:00:13.162: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c9611937-6c8e-11e9-97f0-0a58ac103caa" are running
May  2 04:00:23.194: INFO: Pod "my-hostname-basic-c9611937-6c8e-11e9-97f0-0a58ac103caa-94lx5" is running (conditions: [])
May  2 04:00:23.194: INFO: Trying to dial the pod
May  2 04:00:28.243: INFO: Controller my-hostname-basic-c9611937-6c8e-11e9-97f0-0a58ac103caa: Got expected result from replica 1 [my-hostname-basic-c9611937-6c8e-11e9-97f0-0a58ac103caa-94lx5]: "my-hostname-basic-c9611937-6c8e-11e9-97f0-0a58ac103caa-94lx5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:00:28.244: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-2l4ct" for this suite.
May  2 04:00:34.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:00:35.150: INFO: namespace: e2e-tests-replication-controller-2l4ct, resource: bindings, ignored listing per whitelist
May  2 04:00:35.256: INFO: namespace: e2e-tests-replication-controller-2l4ct, resource: packagemanifests, items remaining: 1
May  2 04:00:36.371: INFO: namespace: e2e-tests-replication-controller-2l4ct no longer exists
May  2 04:00:36.389: INFO: namespace: e2e-tests-replication-controller-2l4ct, total namespaces: 47, active: 47, terminating: 0
May  2 04:00:36.405: INFO: namespace e2e-tests-replication-controller-2l4ct deletion completed in 8.118161624s

â€¢ [SLOW TEST:24.221 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:00:36.405: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-d7d24ef1-6c8e-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 04:00:37.400: INFO: Waiting up to 5m0s for pod "pod-secrets-d7d54276-6c8e-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-secrets-mgchj" to be "success or failure"
May  2 04:00:37.416: INFO: Pod "pod-secrets-d7d54276-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.635234ms
May  2 04:00:39.432: INFO: Pod "pod-secrets-d7d54276-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032185108s
May  2 04:00:41.450: INFO: Pod "pod-secrets-d7d54276-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049236446s
May  2 04:00:43.466: INFO: Pod "pod-secrets-d7d54276-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065695853s
May  2 04:00:45.483: INFO: Pod "pod-secrets-d7d54276-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082626787s
May  2 04:00:47.499: INFO: Pod "pod-secrets-d7d54276-6c8e-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098841024s
STEP: Saw pod success
May  2 04:00:47.499: INFO: Pod "pod-secrets-d7d54276-6c8e-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:00:47.516: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-secrets-d7d54276-6c8e-11e9-97f0-0a58ac103caa container secret-volume-test: <nil>
STEP: delete the pod
May  2 04:00:47.560: INFO: Waiting for pod pod-secrets-d7d54276-6c8e-11e9-97f0-0a58ac103caa to disappear
May  2 04:00:47.576: INFO: Pod pod-secrets-d7d54276-6c8e-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:00:47.576: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mgchj" for this suite.
May  2 04:00:53.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:00:54.251: INFO: namespace: e2e-tests-secrets-mgchj, resource: bindings, ignored listing per whitelist
May  2 04:00:55.539: INFO: namespace: e2e-tests-secrets-mgchj, resource: packagemanifests, items remaining: 1
May  2 04:00:55.704: INFO: namespace: e2e-tests-secrets-mgchj no longer exists
May  2 04:00:55.722: INFO: namespace: e2e-tests-secrets-mgchj, total namespaces: 47, active: 47, terminating: 0
May  2 04:00:55.738: INFO: namespace e2e-tests-secrets-mgchj deletion completed in 8.118803303s

â€¢ [SLOW TEST:19.333 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:00:55.738: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
May  2 04:00:56.716: INFO: Waiting up to 5m0s for pod "pod-e35885fd-6c8e-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-4xdvr" to be "success or failure"
May  2 04:00:56.732: INFO: Pod "pod-e35885fd-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.761229ms
May  2 04:00:58.748: INFO: Pod "pod-e35885fd-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031835078s
May  2 04:01:00.766: INFO: Pod "pod-e35885fd-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049179735s
May  2 04:01:02.782: INFO: Pod "pod-e35885fd-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065357653s
May  2 04:01:04.799: INFO: Pod "pod-e35885fd-6c8e-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082307582s
May  2 04:01:06.815: INFO: Pod "pod-e35885fd-6c8e-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098565889s
STEP: Saw pod success
May  2 04:01:06.815: INFO: Pod "pod-e35885fd-6c8e-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:01:06.830: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-e35885fd-6c8e-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 04:01:06.873: INFO: Waiting for pod pod-e35885fd-6c8e-11e9-97f0-0a58ac103caa to disappear
May  2 04:01:06.888: INFO: Pod pod-e35885fd-6c8e-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:01:06.888: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-4xdvr" for this suite.
May  2 04:01:12.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:01:13.837: INFO: namespace: e2e-tests-emptydir-4xdvr, resource: packagemanifests, items remaining: 1
May  2 04:01:14.433: INFO: namespace: e2e-tests-emptydir-4xdvr, resource: bindings, ignored listing per whitelist
May  2 04:01:15.015: INFO: namespace: e2e-tests-emptydir-4xdvr no longer exists
May  2 04:01:15.032: INFO: namespace: e2e-tests-emptydir-4xdvr, total namespaces: 47, active: 47, terminating: 0
May  2 04:01:15.047: INFO: namespace e2e-tests-emptydir-4xdvr deletion completed in 8.116582722s

â€¢ [SLOW TEST:19.309 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:01:15.047: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:01:26.086: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-9npbs" for this suite.
May  2 04:02:10.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:02:10.747: INFO: namespace: e2e-tests-kubelet-test-9npbs, resource: bindings, ignored listing per whitelist
May  2 04:02:11.616: INFO: namespace: e2e-tests-kubelet-test-9npbs, resource: packagemanifests, items remaining: 1
May  2 04:02:12.213: INFO: namespace: e2e-tests-kubelet-test-9npbs no longer exists
May  2 04:02:12.230: INFO: namespace: e2e-tests-kubelet-test-9npbs, total namespaces: 47, active: 47, terminating: 0
May  2 04:02:12.245: INFO: namespace e2e-tests-kubelet-test-9npbs deletion completed in 46.116585382s

â€¢ [SLOW TEST:57.198 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:02:12.245: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-vpzz4
I0502 04:02:13.224431   12606 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-vpzz4, replica count: 1
I0502 04:02:14.274973   12606 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 04:02:15.275336   12606 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 04:02:16.275609   12606 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 04:02:17.275915   12606 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 04:02:18.276168   12606 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 04:02:19.276449   12606 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 04:02:20.276687   12606 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 04:02:21.276945   12606 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0502 04:02:22.277198   12606 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  2 04:02:22.404: INFO: Created: latency-svc-bwcmq
May  2 04:02:22.412: INFO: Got endpoints: latency-svc-bwcmq [32.532023ms]
May  2 04:02:22.438: INFO: Created: latency-svc-z666z
May  2 04:02:22.446: INFO: Got endpoints: latency-svc-z666z [33.392994ms]
May  2 04:02:22.452: INFO: Created: latency-svc-rck2b
May  2 04:02:22.458: INFO: Got endpoints: latency-svc-rck2b [45.240269ms]
May  2 04:02:22.462: INFO: Created: latency-svc-jlw72
May  2 04:02:22.466: INFO: Got endpoints: latency-svc-jlw72 [53.755174ms]
May  2 04:02:22.469: INFO: Created: latency-svc-94bss
May  2 04:02:22.476: INFO: Got endpoints: latency-svc-94bss [63.418677ms]
May  2 04:02:22.479: INFO: Created: latency-svc-6bsxg
May  2 04:02:22.487: INFO: Got endpoints: latency-svc-6bsxg [74.259405ms]
May  2 04:02:22.490: INFO: Created: latency-svc-v7gv6
May  2 04:02:22.495: INFO: Got endpoints: latency-svc-v7gv6 [82.524667ms]
May  2 04:02:22.498: INFO: Created: latency-svc-lkr9l
May  2 04:02:22.504: INFO: Got endpoints: latency-svc-lkr9l [91.217313ms]
May  2 04:02:22.508: INFO: Created: latency-svc-v85ct
May  2 04:02:22.513: INFO: Got endpoints: latency-svc-v85ct [100.562704ms]
May  2 04:02:22.518: INFO: Created: latency-svc-d2hjb
May  2 04:02:22.523: INFO: Got endpoints: latency-svc-d2hjb [110.329128ms]
May  2 04:02:22.527: INFO: Created: latency-svc-2h56f
May  2 04:02:22.536: INFO: Got endpoints: latency-svc-2h56f [123.919081ms]
May  2 04:02:22.537: INFO: Created: latency-svc-q2cvt
May  2 04:02:22.542: INFO: Got endpoints: latency-svc-q2cvt [129.872612ms]
May  2 04:02:22.545: INFO: Created: latency-svc-n9kmv
May  2 04:02:22.552: INFO: Got endpoints: latency-svc-n9kmv [139.23205ms]
May  2 04:02:22.553: INFO: Created: latency-svc-24hcq
May  2 04:02:22.559: INFO: Got endpoints: latency-svc-24hcq [146.487346ms]
May  2 04:02:22.564: INFO: Created: latency-svc-nstnn
May  2 04:02:22.570: INFO: Got endpoints: latency-svc-nstnn [33.74421ms]
May  2 04:02:22.576: INFO: Created: latency-svc-qh7q5
May  2 04:02:22.585: INFO: Got endpoints: latency-svc-qh7q5 [172.126763ms]
May  2 04:02:22.587: INFO: Created: latency-svc-5jsfz
May  2 04:02:22.595: INFO: Got endpoints: latency-svc-5jsfz [182.622794ms]
May  2 04:02:22.598: INFO: Created: latency-svc-q8pgd
May  2 04:02:22.604: INFO: Got endpoints: latency-svc-q8pgd [157.855377ms]
May  2 04:02:22.606: INFO: Created: latency-svc-twb5z
May  2 04:02:22.615: INFO: Got endpoints: latency-svc-twb5z [157.063939ms]
May  2 04:02:22.620: INFO: Created: latency-svc-gc5hg
May  2 04:02:22.626: INFO: Got endpoints: latency-svc-gc5hg [160.326398ms]
May  2 04:02:22.629: INFO: Created: latency-svc-hrdp7
May  2 04:02:22.635: INFO: Got endpoints: latency-svc-hrdp7 [159.476286ms]
May  2 04:02:22.639: INFO: Created: latency-svc-p4s74
May  2 04:02:22.643: INFO: Got endpoints: latency-svc-p4s74 [156.477756ms]
May  2 04:02:22.647: INFO: Created: latency-svc-dm99j
May  2 04:02:22.653: INFO: Got endpoints: latency-svc-dm99j [157.558907ms]
May  2 04:02:22.657: INFO: Created: latency-svc-cxpsc
May  2 04:02:22.662: INFO: Got endpoints: latency-svc-cxpsc [158.436497ms]
May  2 04:02:22.667: INFO: Created: latency-svc-pp5pw
May  2 04:02:22.673: INFO: Got endpoints: latency-svc-pp5pw [159.973584ms]
May  2 04:02:22.676: INFO: Created: latency-svc-hswrk
May  2 04:02:22.682: INFO: Got endpoints: latency-svc-hswrk [158.962479ms]
May  2 04:02:22.686: INFO: Created: latency-svc-mbs7f
May  2 04:02:22.696: INFO: Got endpoints: latency-svc-mbs7f [153.601459ms]
May  2 04:02:22.699: INFO: Created: latency-svc-4mmxj
May  2 04:02:22.704: INFO: Got endpoints: latency-svc-4mmxj [152.631317ms]
May  2 04:02:22.707: INFO: Created: latency-svc-vnm5b
May  2 04:02:22.715: INFO: Got endpoints: latency-svc-vnm5b [155.815991ms]
May  2 04:02:22.721: INFO: Created: latency-svc-6ll8x
May  2 04:02:22.724: INFO: Got endpoints: latency-svc-6ll8x [154.233509ms]
May  2 04:02:22.727: INFO: Created: latency-svc-dtdwc
May  2 04:02:22.734: INFO: Got endpoints: latency-svc-dtdwc [148.965887ms]
May  2 04:02:22.736: INFO: Created: latency-svc-wjdh6
May  2 04:02:22.741: INFO: Got endpoints: latency-svc-wjdh6 [145.953976ms]
May  2 04:02:22.745: INFO: Created: latency-svc-9zhnw
May  2 04:02:22.751: INFO: Got endpoints: latency-svc-9zhnw [147.742452ms]
May  2 04:02:22.753: INFO: Created: latency-svc-msp7r
May  2 04:02:22.759: INFO: Got endpoints: latency-svc-msp7r [144.419312ms]
May  2 04:02:22.768: INFO: Created: latency-svc-pq6jm
May  2 04:02:22.774: INFO: Got endpoints: latency-svc-pq6jm [147.422099ms]
May  2 04:02:22.777: INFO: Created: latency-svc-wm22c
May  2 04:02:22.786: INFO: Got endpoints: latency-svc-wm22c [150.503602ms]
May  2 04:02:22.786: INFO: Created: latency-svc-v8d7z
May  2 04:02:22.793: INFO: Got endpoints: latency-svc-v8d7z [149.751772ms]
May  2 04:02:22.796: INFO: Created: latency-svc-sfngl
May  2 04:02:22.801: INFO: Got endpoints: latency-svc-sfngl [148.303404ms]
May  2 04:02:22.804: INFO: Created: latency-svc-vwz67
May  2 04:02:22.813: INFO: Got endpoints: latency-svc-vwz67 [150.495286ms]
May  2 04:02:22.816: INFO: Created: latency-svc-c5lch
May  2 04:02:22.821: INFO: Got endpoints: latency-svc-c5lch [148.020153ms]
May  2 04:02:22.832: INFO: Created: latency-svc-ph7hf
May  2 04:02:22.840: INFO: Got endpoints: latency-svc-ph7hf [157.729555ms]
May  2 04:02:22.843: INFO: Created: latency-svc-72z7q
May  2 04:02:22.848: INFO: Got endpoints: latency-svc-72z7q [152.074211ms]
May  2 04:02:22.852: INFO: Created: latency-svc-pgw7p
May  2 04:02:22.857: INFO: Got endpoints: latency-svc-pgw7p [152.591688ms]
May  2 04:02:22.867: INFO: Created: latency-svc-6jdcq
May  2 04:02:22.872: INFO: Got endpoints: latency-svc-6jdcq [156.853448ms]
May  2 04:02:22.877: INFO: Created: latency-svc-z7scw
May  2 04:02:22.881: INFO: Got endpoints: latency-svc-z7scw [156.075552ms]
May  2 04:02:22.888: INFO: Created: latency-svc-mrrdp
May  2 04:02:22.892: INFO: Got endpoints: latency-svc-mrrdp [158.35683ms]
May  2 04:02:22.896: INFO: Created: latency-svc-xfzv8
May  2 04:02:22.902: INFO: Got endpoints: latency-svc-xfzv8 [160.983477ms]
May  2 04:02:22.905: INFO: Created: latency-svc-xcgkt
May  2 04:02:22.910: INFO: Got endpoints: latency-svc-xcgkt [158.578457ms]
May  2 04:02:22.914: INFO: Created: latency-svc-2w2bq
May  2 04:02:22.920: INFO: Got endpoints: latency-svc-2w2bq [160.6392ms]
May  2 04:02:22.922: INFO: Created: latency-svc-9h775
May  2 04:02:22.928: INFO: Got endpoints: latency-svc-9h775 [154.611254ms]
May  2 04:02:22.931: INFO: Created: latency-svc-n5nzr
May  2 04:02:22.937: INFO: Got endpoints: latency-svc-n5nzr [150.760084ms]
May  2 04:02:22.939: INFO: Created: latency-svc-tlf6r
May  2 04:02:22.944: INFO: Got endpoints: latency-svc-tlf6r [151.26485ms]
May  2 04:02:22.953: INFO: Created: latency-svc-hxl64
May  2 04:02:22.958: INFO: Got endpoints: latency-svc-hxl64 [157.330515ms]
May  2 04:02:22.963: INFO: Created: latency-svc-f9pws
May  2 04:02:22.969: INFO: Got endpoints: latency-svc-f9pws [156.705033ms]
May  2 04:02:22.974: INFO: Created: latency-svc-mrshs
May  2 04:02:22.979: INFO: Got endpoints: latency-svc-mrshs [157.226068ms]
May  2 04:02:22.985: INFO: Created: latency-svc-q54c7
May  2 04:02:22.997: INFO: Created: latency-svc-j4nkb
May  2 04:02:22.998: INFO: Got endpoints: latency-svc-q54c7 [157.896176ms]
May  2 04:02:22.999: INFO: Got endpoints: latency-svc-j4nkb [151.215156ms]
May  2 04:02:23.006: INFO: Created: latency-svc-twm2l
May  2 04:02:23.013: INFO: Got endpoints: latency-svc-twm2l [155.555504ms]
May  2 04:02:23.015: INFO: Created: latency-svc-lspgb
May  2 04:02:23.021: INFO: Got endpoints: latency-svc-lspgb [149.293688ms]
May  2 04:02:23.024: INFO: Created: latency-svc-mk5nf
May  2 04:02:23.031: INFO: Got endpoints: latency-svc-mk5nf [149.939213ms]
May  2 04:02:23.033: INFO: Created: latency-svc-gtfz5
May  2 04:02:23.040: INFO: Got endpoints: latency-svc-gtfz5 [147.660436ms]
May  2 04:02:23.044: INFO: Created: latency-svc-vpj9g
May  2 04:02:23.053: INFO: Created: latency-svc-qnbb4
May  2 04:02:23.053: INFO: Got endpoints: latency-svc-vpj9g [150.960281ms]
May  2 04:02:23.059: INFO: Got endpoints: latency-svc-qnbb4 [148.564508ms]
May  2 04:02:23.066: INFO: Created: latency-svc-w5pk5
May  2 04:02:23.072: INFO: Got endpoints: latency-svc-w5pk5 [151.731581ms]
May  2 04:02:23.075: INFO: Created: latency-svc-mtm69
May  2 04:02:23.086: INFO: Got endpoints: latency-svc-mtm69 [157.95643ms]
May  2 04:02:23.088: INFO: Created: latency-svc-sb8kz
May  2 04:02:23.095: INFO: Got endpoints: latency-svc-sb8kz [158.684323ms]
May  2 04:02:23.099: INFO: Created: latency-svc-sjhdn
May  2 04:02:23.106: INFO: Got endpoints: latency-svc-sjhdn [161.070034ms]
May  2 04:02:23.108: INFO: Created: latency-svc-c8lfr
May  2 04:02:23.116: INFO: Got endpoints: latency-svc-c8lfr [157.380336ms]
May  2 04:02:23.119: INFO: Created: latency-svc-b4xwg
May  2 04:02:23.125: INFO: Got endpoints: latency-svc-b4xwg [155.29951ms]
May  2 04:02:23.130: INFO: Created: latency-svc-j2gnj
May  2 04:02:23.135: INFO: Got endpoints: latency-svc-j2gnj [156.790239ms]
May  2 04:02:23.138: INFO: Created: latency-svc-mtg5j
May  2 04:02:23.148: INFO: Got endpoints: latency-svc-mtg5j [150.1164ms]
May  2 04:02:23.152: INFO: Created: latency-svc-xlj96
May  2 04:02:23.158: INFO: Got endpoints: latency-svc-xlj96 [158.280518ms]
May  2 04:02:23.161: INFO: Created: latency-svc-rj5s4
May  2 04:02:23.167: INFO: Got endpoints: latency-svc-rj5s4 [154.101751ms]
May  2 04:02:23.169: INFO: Created: latency-svc-rjgsj
May  2 04:02:23.177: INFO: Got endpoints: latency-svc-rjgsj [155.389903ms]
May  2 04:02:23.180: INFO: Created: latency-svc-9xgvd
May  2 04:02:23.185: INFO: Got endpoints: latency-svc-9xgvd [153.996116ms]
May  2 04:02:23.187: INFO: Created: latency-svc-kffsx
May  2 04:02:23.193: INFO: Got endpoints: latency-svc-kffsx [153.481103ms]
May  2 04:02:23.195: INFO: Created: latency-svc-8dbws
May  2 04:02:23.201: INFO: Got endpoints: latency-svc-8dbws [147.733251ms]
May  2 04:02:23.205: INFO: Created: latency-svc-gncrt
May  2 04:02:23.210: INFO: Got endpoints: latency-svc-gncrt [151.527045ms]
May  2 04:02:23.215: INFO: Created: latency-svc-pvlwc
May  2 04:02:23.221: INFO: Got endpoints: latency-svc-pvlwc [149.257326ms]
May  2 04:02:23.225: INFO: Created: latency-svc-nmft9
May  2 04:02:23.231: INFO: Got endpoints: latency-svc-nmft9 [144.067215ms]
May  2 04:02:23.235: INFO: Created: latency-svc-m7827
May  2 04:02:23.240: INFO: Got endpoints: latency-svc-m7827 [144.358921ms]
May  2 04:02:23.242: INFO: Created: latency-svc-6dsss
May  2 04:02:23.252: INFO: Got endpoints: latency-svc-6dsss [146.137495ms]
May  2 04:02:23.255: INFO: Created: latency-svc-85rbq
May  2 04:02:23.263: INFO: Created: latency-svc-szdb2
May  2 04:02:23.263: INFO: Got endpoints: latency-svc-85rbq [147.622903ms]
May  2 04:02:23.268: INFO: Got endpoints: latency-svc-szdb2 [143.360717ms]
May  2 04:02:23.270: INFO: Created: latency-svc-vnrms
May  2 04:02:23.275: INFO: Got endpoints: latency-svc-vnrms [139.823797ms]
May  2 04:02:23.281: INFO: Created: latency-svc-vktg9
May  2 04:02:23.286: INFO: Got endpoints: latency-svc-vktg9 [138.60044ms]
May  2 04:02:23.290: INFO: Created: latency-svc-t8l8n
May  2 04:02:23.303: INFO: Created: latency-svc-gtskv
May  2 04:02:23.304: INFO: Got endpoints: latency-svc-t8l8n [145.854655ms]
May  2 04:02:23.310: INFO: Got endpoints: latency-svc-gtskv [143.046223ms]
May  2 04:02:23.314: INFO: Created: latency-svc-hj7df
May  2 04:02:23.321: INFO: Got endpoints: latency-svc-hj7df [144.333179ms]
May  2 04:02:23.323: INFO: Created: latency-svc-7dckl
May  2 04:02:23.329: INFO: Got endpoints: latency-svc-7dckl [144.167512ms]
May  2 04:02:23.333: INFO: Created: latency-svc-fcrvt
May  2 04:02:23.341: INFO: Got endpoints: latency-svc-fcrvt [147.67575ms]
May  2 04:02:23.344: INFO: Created: latency-svc-g8h82
May  2 04:02:23.349: INFO: Got endpoints: latency-svc-g8h82 [148.399814ms]
May  2 04:02:23.352: INFO: Created: latency-svc-4mknr
May  2 04:02:23.359: INFO: Got endpoints: latency-svc-4mknr [148.23465ms]
May  2 04:02:23.362: INFO: Created: latency-svc-wjmt5
May  2 04:02:23.369: INFO: Got endpoints: latency-svc-wjmt5 [147.599426ms]
May  2 04:02:23.372: INFO: Created: latency-svc-47w7s
May  2 04:02:23.379: INFO: Got endpoints: latency-svc-47w7s [148.007215ms]
May  2 04:02:23.383: INFO: Created: latency-svc-phtmw
May  2 04:02:23.390: INFO: Got endpoints: latency-svc-phtmw [149.704912ms]
May  2 04:02:23.392: INFO: Created: latency-svc-kczqx
May  2 04:02:23.399: INFO: Got endpoints: latency-svc-kczqx [147.612731ms]
May  2 04:02:23.402: INFO: Created: latency-svc-8drqx
May  2 04:02:23.410: INFO: Got endpoints: latency-svc-8drqx [146.119311ms]
May  2 04:02:23.412: INFO: Created: latency-svc-42vz8
May  2 04:02:23.419: INFO: Got endpoints: latency-svc-42vz8 [150.432764ms]
May  2 04:02:23.420: INFO: Created: latency-svc-qbnm7
May  2 04:02:23.427: INFO: Got endpoints: latency-svc-qbnm7 [151.41288ms]
May  2 04:02:23.430: INFO: Created: latency-svc-fjhv2
May  2 04:02:23.436: INFO: Got endpoints: latency-svc-fjhv2 [149.639134ms]
May  2 04:02:23.441: INFO: Created: latency-svc-fzhh5
May  2 04:02:23.445: INFO: Got endpoints: latency-svc-fzhh5 [141.401292ms]
May  2 04:02:23.449: INFO: Created: latency-svc-nd56b
May  2 04:02:23.457: INFO: Got endpoints: latency-svc-nd56b [146.901897ms]
May  2 04:02:23.462: INFO: Created: latency-svc-c9xtc
May  2 04:02:23.467: INFO: Got endpoints: latency-svc-c9xtc [146.025185ms]
May  2 04:02:23.473: INFO: Created: latency-svc-wv572
May  2 04:02:23.477: INFO: Got endpoints: latency-svc-wv572 [148.102325ms]
May  2 04:02:23.480: INFO: Created: latency-svc-c6jl2
May  2 04:02:23.485: INFO: Got endpoints: latency-svc-c6jl2 [144.282928ms]
May  2 04:02:23.489: INFO: Created: latency-svc-68bgl
May  2 04:02:23.495: INFO: Got endpoints: latency-svc-68bgl [145.452673ms]
May  2 04:02:23.498: INFO: Created: latency-svc-z4kcf
May  2 04:02:23.504: INFO: Got endpoints: latency-svc-z4kcf [145.539479ms]
May  2 04:02:23.511: INFO: Created: latency-svc-cpkkb
May  2 04:02:23.517: INFO: Got endpoints: latency-svc-cpkkb [148.208952ms]
May  2 04:02:23.529: INFO: Created: latency-svc-p5cdn
May  2 04:02:23.535: INFO: Got endpoints: latency-svc-p5cdn [156.32381ms]
May  2 04:02:23.547: INFO: Created: latency-svc-qv72m
May  2 04:02:23.550: INFO: Got endpoints: latency-svc-qv72m [160.433994ms]
May  2 04:02:23.555: INFO: Created: latency-svc-ldpxv
May  2 04:02:23.561: INFO: Got endpoints: latency-svc-ldpxv [161.215931ms]
May  2 04:02:23.565: INFO: Created: latency-svc-w4c6w
May  2 04:02:23.573: INFO: Got endpoints: latency-svc-w4c6w [163.031225ms]
May  2 04:02:23.577: INFO: Created: latency-svc-gctcd
May  2 04:02:23.588: INFO: Got endpoints: latency-svc-gctcd [169.318336ms]
May  2 04:02:23.591: INFO: Created: latency-svc-jvrgk
May  2 04:02:23.598: INFO: Got endpoints: latency-svc-jvrgk [171.572695ms]
May  2 04:02:23.601: INFO: Created: latency-svc-m6czl
May  2 04:02:23.607: INFO: Got endpoints: latency-svc-m6czl [171.373527ms]
May  2 04:02:23.611: INFO: Created: latency-svc-27ldx
May  2 04:02:23.620: INFO: Got endpoints: latency-svc-27ldx [174.447179ms]
May  2 04:02:23.624: INFO: Created: latency-svc-gvxgw
May  2 04:02:23.631: INFO: Got endpoints: latency-svc-gvxgw [174.244158ms]
May  2 04:02:23.637: INFO: Created: latency-svc-rb8p5
May  2 04:02:23.643: INFO: Got endpoints: latency-svc-rb8p5 [176.258451ms]
May  2 04:02:23.647: INFO: Created: latency-svc-k6bzt
May  2 04:02:23.653: INFO: Got endpoints: latency-svc-k6bzt [175.926267ms]
May  2 04:02:23.658: INFO: Created: latency-svc-6j2kd
May  2 04:02:23.665: INFO: Got endpoints: latency-svc-6j2kd [179.341666ms]
May  2 04:02:23.670: INFO: Created: latency-svc-rr64h
May  2 04:02:23.678: INFO: Got endpoints: latency-svc-rr64h [183.510271ms]
May  2 04:02:23.679: INFO: Created: latency-svc-4wzgt
May  2 04:02:23.686: INFO: Got endpoints: latency-svc-4wzgt [181.465959ms]
May  2 04:02:23.690: INFO: Created: latency-svc-59ctl
May  2 04:02:23.696: INFO: Got endpoints: latency-svc-59ctl [178.616667ms]
May  2 04:02:23.700: INFO: Created: latency-svc-rr857
May  2 04:02:23.708: INFO: Got endpoints: latency-svc-rr857 [173.343497ms]
May  2 04:02:23.713: INFO: Created: latency-svc-wvcvj
May  2 04:02:23.723: INFO: Got endpoints: latency-svc-wvcvj [172.53786ms]
May  2 04:02:23.728: INFO: Created: latency-svc-j8mrj
May  2 04:02:23.736: INFO: Got endpoints: latency-svc-j8mrj [175.140682ms]
May  2 04:02:23.739: INFO: Created: latency-svc-6892k
May  2 04:02:23.745: INFO: Got endpoints: latency-svc-6892k [172.186243ms]
May  2 04:02:23.757: INFO: Created: latency-svc-hjhqc
May  2 04:02:23.764: INFO: Got endpoints: latency-svc-hjhqc [175.966797ms]
May  2 04:02:23.768: INFO: Created: latency-svc-q9kc4
May  2 04:02:23.779: INFO: Got endpoints: latency-svc-q9kc4 [181.017205ms]
May  2 04:02:23.783: INFO: Created: latency-svc-gl98x
May  2 04:02:23.788: INFO: Got endpoints: latency-svc-gl98x [180.387129ms]
May  2 04:02:23.794: INFO: Created: latency-svc-rdk7s
May  2 04:02:23.800: INFO: Got endpoints: latency-svc-rdk7s [180.651063ms]
May  2 04:02:23.805: INFO: Created: latency-svc-86hrb
May  2 04:02:23.810: INFO: Got endpoints: latency-svc-86hrb [178.74657ms]
May  2 04:02:23.814: INFO: Created: latency-svc-cl2tz
May  2 04:02:23.819: INFO: Got endpoints: latency-svc-cl2tz [175.863033ms]
May  2 04:02:23.828: INFO: Created: latency-svc-8zt6d
May  2 04:02:23.835: INFO: Got endpoints: latency-svc-8zt6d [181.945686ms]
May  2 04:02:23.841: INFO: Created: latency-svc-nklm6
May  2 04:02:23.846: INFO: Got endpoints: latency-svc-nklm6 [181.578901ms]
May  2 04:02:23.852: INFO: Created: latency-svc-pb5gs
May  2 04:02:23.859: INFO: Got endpoints: latency-svc-pb5gs [180.015227ms]
May  2 04:02:23.863: INFO: Created: latency-svc-k6dsd
May  2 04:02:23.871: INFO: Got endpoints: latency-svc-k6dsd [185.186878ms]
May  2 04:02:23.877: INFO: Created: latency-svc-7vmvm
May  2 04:02:23.885: INFO: Got endpoints: latency-svc-7vmvm [189.549547ms]
May  2 04:02:23.889: INFO: Created: latency-svc-sf994
May  2 04:02:23.902: INFO: Got endpoints: latency-svc-sf994 [193.566747ms]
May  2 04:02:23.907: INFO: Created: latency-svc-4mfr4
May  2 04:02:23.913: INFO: Got endpoints: latency-svc-4mfr4 [190.394615ms]
May  2 04:02:23.917: INFO: Created: latency-svc-8wvtl
May  2 04:02:23.926: INFO: Got endpoints: latency-svc-8wvtl [189.784905ms]
May  2 04:02:23.928: INFO: Created: latency-svc-s7gr9
May  2 04:02:23.940: INFO: Got endpoints: latency-svc-s7gr9 [195.470934ms]
May  2 04:02:23.944: INFO: Created: latency-svc-w4849
May  2 04:02:23.951: INFO: Got endpoints: latency-svc-w4849 [186.495772ms]
May  2 04:02:23.953: INFO: Created: latency-svc-zp6w2
May  2 04:02:23.962: INFO: Got endpoints: latency-svc-zp6w2 [182.647715ms]
May  2 04:02:23.966: INFO: Created: latency-svc-t9lst
May  2 04:02:23.972: INFO: Got endpoints: latency-svc-t9lst [183.858675ms]
May  2 04:02:23.975: INFO: Created: latency-svc-txksm
May  2 04:02:23.982: INFO: Got endpoints: latency-svc-txksm [181.625676ms]
May  2 04:02:23.984: INFO: Created: latency-svc-5nm6d
May  2 04:02:23.994: INFO: Got endpoints: latency-svc-5nm6d [183.655802ms]
May  2 04:02:23.995: INFO: Created: latency-svc-dnp8g
May  2 04:02:24.004: INFO: Got endpoints: latency-svc-dnp8g [185.016841ms]
May  2 04:02:24.006: INFO: Created: latency-svc-trdw4
May  2 04:02:24.012: INFO: Got endpoints: latency-svc-trdw4 [176.9543ms]
May  2 04:02:24.015: INFO: Created: latency-svc-k7882
May  2 04:02:24.021: INFO: Got endpoints: latency-svc-k7882 [174.503791ms]
May  2 04:02:24.025: INFO: Created: latency-svc-x2zjx
May  2 04:02:24.030: INFO: Got endpoints: latency-svc-x2zjx [171.672896ms]
May  2 04:02:24.032: INFO: Created: latency-svc-pjfjl
May  2 04:02:24.038: INFO: Got endpoints: latency-svc-pjfjl [167.248485ms]
May  2 04:02:24.040: INFO: Created: latency-svc-zgcz2
May  2 04:02:24.046: INFO: Got endpoints: latency-svc-zgcz2 [160.500343ms]
May  2 04:02:24.049: INFO: Created: latency-svc-jb727
May  2 04:02:24.054: INFO: Got endpoints: latency-svc-jb727 [151.761654ms]
May  2 04:02:24.057: INFO: Created: latency-svc-gm8l5
May  2 04:02:24.063: INFO: Got endpoints: latency-svc-gm8l5 [149.769005ms]
May  2 04:02:24.068: INFO: Created: latency-svc-w8h27
May  2 04:02:24.084: INFO: Got endpoints: latency-svc-w8h27 [158.728737ms]
May  2 04:02:24.088: INFO: Created: latency-svc-hnmfv
May  2 04:02:24.093: INFO: Got endpoints: latency-svc-hnmfv [152.421035ms]
May  2 04:02:24.097: INFO: Created: latency-svc-8z4w5
May  2 04:02:24.102: INFO: Got endpoints: latency-svc-8z4w5 [151.591968ms]
May  2 04:02:24.105: INFO: Created: latency-svc-fvfdj
May  2 04:02:24.111: INFO: Got endpoints: latency-svc-fvfdj [149.248489ms]
May  2 04:02:24.114: INFO: Created: latency-svc-5r8dl
May  2 04:02:24.121: INFO: Got endpoints: latency-svc-5r8dl [149.358138ms]
May  2 04:02:24.125: INFO: Created: latency-svc-vbn7r
May  2 04:02:24.131: INFO: Got endpoints: latency-svc-vbn7r [148.919156ms]
May  2 04:02:24.135: INFO: Created: latency-svc-cw7zd
May  2 04:02:24.142: INFO: Got endpoints: latency-svc-cw7zd [148.598209ms]
May  2 04:02:24.147: INFO: Created: latency-svc-7gkwr
May  2 04:02:24.155: INFO: Got endpoints: latency-svc-7gkwr [150.713479ms]
May  2 04:02:24.158: INFO: Created: latency-svc-xt98f
May  2 04:02:24.164: INFO: Got endpoints: latency-svc-xt98f [151.873333ms]
May  2 04:02:24.167: INFO: Created: latency-svc-8fj5c
May  2 04:02:24.172: INFO: Got endpoints: latency-svc-8fj5c [151.187858ms]
May  2 04:02:24.174: INFO: Created: latency-svc-96w22
May  2 04:02:24.180: INFO: Got endpoints: latency-svc-96w22 [149.584868ms]
May  2 04:02:24.183: INFO: Created: latency-svc-4mpmc
May  2 04:02:24.196: INFO: Got endpoints: latency-svc-4mpmc [157.54274ms]
May  2 04:02:24.199: INFO: Created: latency-svc-kkv9h
May  2 04:02:24.206: INFO: Got endpoints: latency-svc-kkv9h [159.975157ms]
May  2 04:02:24.210: INFO: Created: latency-svc-qs2cz
May  2 04:02:24.216: INFO: Got endpoints: latency-svc-qs2cz [162.619337ms]
May  2 04:02:24.219: INFO: Created: latency-svc-r8l6z
May  2 04:02:24.224: INFO: Got endpoints: latency-svc-r8l6z [161.533284ms]
May  2 04:02:24.228: INFO: Created: latency-svc-n7nrh
May  2 04:02:24.234: INFO: Got endpoints: latency-svc-n7nrh [149.579871ms]
May  2 04:02:24.238: INFO: Created: latency-svc-n8qvx
May  2 04:02:24.244: INFO: Got endpoints: latency-svc-n8qvx [150.762847ms]
May  2 04:02:24.246: INFO: Created: latency-svc-n4h8k
May  2 04:02:24.255: INFO: Got endpoints: latency-svc-n4h8k [152.935204ms]
May  2 04:02:24.259: INFO: Created: latency-svc-d6lpb
May  2 04:02:24.265: INFO: Got endpoints: latency-svc-d6lpb [153.24537ms]
May  2 04:02:24.270: INFO: Created: latency-svc-vfrkw
May  2 04:02:24.275: INFO: Got endpoints: latency-svc-vfrkw [154.026575ms]
May  2 04:02:24.278: INFO: Created: latency-svc-22ncb
May  2 04:02:24.284: INFO: Got endpoints: latency-svc-22ncb [153.534526ms]
May  2 04:02:24.291: INFO: Created: latency-svc-pc9gw
May  2 04:02:24.300: INFO: Created: latency-svc-bksd5
May  2 04:02:24.302: INFO: Got endpoints: latency-svc-pc9gw [159.42016ms]
May  2 04:02:24.307: INFO: Got endpoints: latency-svc-bksd5 [152.32566ms]
May  2 04:02:24.313: INFO: Created: latency-svc-msx92
May  2 04:02:24.318: INFO: Got endpoints: latency-svc-msx92 [154.471563ms]
May  2 04:02:24.322: INFO: Created: latency-svc-j2qhj
May  2 04:02:24.328: INFO: Got endpoints: latency-svc-j2qhj [155.895664ms]
May  2 04:02:24.331: INFO: Created: latency-svc-rcjk4
May  2 04:02:24.339: INFO: Got endpoints: latency-svc-rcjk4 [159.338062ms]
May  2 04:02:24.341: INFO: Created: latency-svc-699pv
May  2 04:02:24.348: INFO: Got endpoints: latency-svc-699pv [152.028193ms]
May  2 04:02:24.352: INFO: Created: latency-svc-x9p9k
May  2 04:02:24.358: INFO: Got endpoints: latency-svc-x9p9k [152.123493ms]
May  2 04:02:24.361: INFO: Created: latency-svc-4nsth
May  2 04:02:24.366: INFO: Got endpoints: latency-svc-4nsth [150.011937ms]
May  2 04:02:24.370: INFO: Created: latency-svc-gp2xg
May  2 04:02:24.377: INFO: Got endpoints: latency-svc-gp2xg [152.400616ms]
May  2 04:02:24.384: INFO: Created: latency-svc-v6w4c
May  2 04:02:24.384: INFO: Got endpoints: latency-svc-v6w4c [150.052015ms]
May  2 04:02:24.391: INFO: Created: latency-svc-ml4sx
May  2 04:02:24.402: INFO: Got endpoints: latency-svc-ml4sx [158.490523ms]
May  2 04:02:24.406: INFO: Created: latency-svc-24vzm
May  2 04:02:24.412: INFO: Got endpoints: latency-svc-24vzm [156.77891ms]
May  2 04:02:24.414: INFO: Created: latency-svc-lfh96
May  2 04:02:24.437: INFO: Got endpoints: latency-svc-lfh96 [172.749707ms]
May  2 04:02:24.441: INFO: Created: latency-svc-hr7jr
May  2 04:02:24.446: INFO: Got endpoints: latency-svc-hr7jr [170.907114ms]
May  2 04:02:24.456: INFO: Created: latency-svc-49snw
May  2 04:02:24.467: INFO: Got endpoints: latency-svc-49snw [182.161719ms]
May  2 04:02:24.470: INFO: Created: latency-svc-9wgxb
May  2 04:02:24.481: INFO: Got endpoints: latency-svc-9wgxb [178.765691ms]
May  2 04:02:24.491: INFO: Created: latency-svc-csbhb
May  2 04:02:24.510: INFO: Got endpoints: latency-svc-csbhb [202.673826ms]
May  2 04:02:24.519: INFO: Created: latency-svc-pkqdq
May  2 04:02:24.524: INFO: Got endpoints: latency-svc-pkqdq [205.131988ms]
May  2 04:02:24.528: INFO: Created: latency-svc-bdpxx
May  2 04:02:24.536: INFO: Got endpoints: latency-svc-bdpxx [207.87081ms]
May  2 04:02:24.540: INFO: Created: latency-svc-cj5gr
May  2 04:02:24.546: INFO: Got endpoints: latency-svc-cj5gr [206.454856ms]
May  2 04:02:24.552: INFO: Created: latency-svc-lp5qv
May  2 04:02:24.557: INFO: Got endpoints: latency-svc-lp5qv [209.544199ms]
May  2 04:02:24.560: INFO: Created: latency-svc-8llww
May  2 04:02:24.566: INFO: Got endpoints: latency-svc-8llww [208.249951ms]
May  2 04:02:24.570: INFO: Created: latency-svc-btkkw
May  2 04:02:24.576: INFO: Got endpoints: latency-svc-btkkw [209.953122ms]
May  2 04:02:24.580: INFO: Created: latency-svc-ql5qf
May  2 04:02:24.583: INFO: Got endpoints: latency-svc-ql5qf [206.551454ms]
May  2 04:02:24.584: INFO: Latencies: [33.392994ms 33.74421ms 45.240269ms 53.755174ms 63.418677ms 74.259405ms 82.524667ms 91.217313ms 100.562704ms 110.329128ms 123.919081ms 129.872612ms 138.60044ms 139.23205ms 139.823797ms 141.401292ms 143.046223ms 143.360717ms 144.067215ms 144.167512ms 144.282928ms 144.333179ms 144.358921ms 144.419312ms 145.452673ms 145.539479ms 145.854655ms 145.953976ms 146.025185ms 146.119311ms 146.137495ms 146.487346ms 146.901897ms 147.422099ms 147.599426ms 147.612731ms 147.622903ms 147.660436ms 147.67575ms 147.733251ms 147.742452ms 148.007215ms 148.020153ms 148.102325ms 148.208952ms 148.23465ms 148.303404ms 148.399814ms 148.564508ms 148.598209ms 148.919156ms 148.965887ms 149.248489ms 149.257326ms 149.293688ms 149.358138ms 149.579871ms 149.584868ms 149.639134ms 149.704912ms 149.751772ms 149.769005ms 149.939213ms 150.011937ms 150.052015ms 150.1164ms 150.432764ms 150.495286ms 150.503602ms 150.713479ms 150.760084ms 150.762847ms 150.960281ms 151.187858ms 151.215156ms 151.26485ms 151.41288ms 151.527045ms 151.591968ms 151.731581ms 151.761654ms 151.873333ms 152.028193ms 152.074211ms 152.123493ms 152.32566ms 152.400616ms 152.421035ms 152.591688ms 152.631317ms 152.935204ms 153.24537ms 153.481103ms 153.534526ms 153.601459ms 153.996116ms 154.026575ms 154.101751ms 154.233509ms 154.471563ms 154.611254ms 155.29951ms 155.389903ms 155.555504ms 155.815991ms 155.895664ms 156.075552ms 156.32381ms 156.477756ms 156.705033ms 156.77891ms 156.790239ms 156.853448ms 157.063939ms 157.226068ms 157.330515ms 157.380336ms 157.54274ms 157.558907ms 157.729555ms 157.855377ms 157.896176ms 157.95643ms 158.280518ms 158.35683ms 158.436497ms 158.490523ms 158.578457ms 158.684323ms 158.728737ms 158.962479ms 159.338062ms 159.42016ms 159.476286ms 159.973584ms 159.975157ms 160.326398ms 160.433994ms 160.500343ms 160.6392ms 160.983477ms 161.070034ms 161.215931ms 161.533284ms 162.619337ms 163.031225ms 167.248485ms 169.318336ms 170.907114ms 171.373527ms 171.572695ms 171.672896ms 172.126763ms 172.186243ms 172.53786ms 172.749707ms 173.343497ms 174.244158ms 174.447179ms 174.503791ms 175.140682ms 175.863033ms 175.926267ms 175.966797ms 176.258451ms 176.9543ms 178.616667ms 178.74657ms 178.765691ms 179.341666ms 180.015227ms 180.387129ms 180.651063ms 181.017205ms 181.465959ms 181.578901ms 181.625676ms 181.945686ms 182.161719ms 182.622794ms 182.647715ms 183.510271ms 183.655802ms 183.858675ms 185.016841ms 185.186878ms 186.495772ms 189.549547ms 189.784905ms 190.394615ms 193.566747ms 195.470934ms 202.673826ms 205.131988ms 206.454856ms 206.551454ms 207.87081ms 208.249951ms 209.544199ms 209.953122ms]
May  2 04:02:24.584: INFO: 50 %ile: 154.611254ms
May  2 04:02:24.584: INFO: 90 %ile: 182.647715ms
May  2 04:02:24.584: INFO: 99 %ile: 209.544199ms
May  2 04:02:24.584: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:02:24.584: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-vpzz4" for this suite.
May  2 04:02:40.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:02:41.677: INFO: namespace: e2e-tests-svc-latency-vpzz4, resource: bindings, ignored listing per whitelist
May  2 04:02:42.266: INFO: namespace: e2e-tests-svc-latency-vpzz4, resource: packagemanifests, items remaining: 1
May  2 04:02:42.701: INFO: namespace: e2e-tests-svc-latency-vpzz4 no longer exists
May  2 04:02:42.720: INFO: namespace: e2e-tests-svc-latency-vpzz4, total namespaces: 47, active: 47, terminating: 0
May  2 04:02:42.736: INFO: namespace e2e-tests-svc-latency-vpzz4 deletion completed in 18.122737225s

â€¢ [SLOW TEST:30.491 seconds]
[sig-network] Service endpoints latency
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:02:42.737: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 04:02:43.720: INFO: Waiting up to 5m0s for pod "downwardapi-volume-231fed6c-6c8f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-pgnqs" to be "success or failure"
May  2 04:02:43.735: INFO: Pod "downwardapi-volume-231fed6c-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.455742ms
May  2 04:02:45.757: INFO: Pod "downwardapi-volume-231fed6c-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036731465s
May  2 04:02:47.777: INFO: Pod "downwardapi-volume-231fed6c-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05698369s
May  2 04:02:49.798: INFO: Pod "downwardapi-volume-231fed6c-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077903054s
May  2 04:02:51.821: INFO: Pod "downwardapi-volume-231fed6c-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100664158s
May  2 04:02:53.838: INFO: Pod "downwardapi-volume-231fed6c-6c8f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.117861868s
STEP: Saw pod success
May  2 04:02:53.838: INFO: Pod "downwardapi-volume-231fed6c-6c8f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:02:53.855: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-231fed6c-6c8f-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 04:02:53.906: INFO: Waiting for pod downwardapi-volume-231fed6c-6c8f-11e9-97f0-0a58ac103caa to disappear
May  2 04:02:53.922: INFO: Pod downwardapi-volume-231fed6c-6c8f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:02:53.922: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pgnqs" for this suite.
May  2 04:03:00.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:03:01.129: INFO: namespace: e2e-tests-projected-pgnqs, resource: packagemanifests, items remaining: 1
May  2 04:03:01.369: INFO: namespace: e2e-tests-projected-pgnqs, resource: bindings, ignored listing per whitelist
May  2 04:03:02.076: INFO: namespace: e2e-tests-projected-pgnqs no longer exists
May  2 04:03:02.099: INFO: namespace: e2e-tests-projected-pgnqs, total namespaces: 47, active: 47, terminating: 0
May  2 04:03:02.116: INFO: namespace e2e-tests-projected-pgnqs deletion completed in 8.141655137s

â€¢ [SLOW TEST:19.379 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:03:02.116: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-2eb121b5-6c8f-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 04:03:03.144: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2eb3e71e-6c8f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-dbcqs" to be "success or failure"
May  2 04:03:03.160: INFO: Pod "pod-projected-configmaps-2eb3e71e-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.790413ms
May  2 04:03:05.176: INFO: Pod "pod-projected-configmaps-2eb3e71e-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0320024s
May  2 04:03:07.192: INFO: Pod "pod-projected-configmaps-2eb3e71e-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048395206s
May  2 04:03:09.209: INFO: Pod "pod-projected-configmaps-2eb3e71e-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064704858s
May  2 04:03:11.225: INFO: Pod "pod-projected-configmaps-2eb3e71e-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081074112s
May  2 04:03:13.242: INFO: Pod "pod-projected-configmaps-2eb3e71e-6c8f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097904408s
STEP: Saw pod success
May  2 04:03:13.242: INFO: Pod "pod-projected-configmaps-2eb3e71e-6c8f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:03:13.258: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-configmaps-2eb3e71e-6c8f-11e9-97f0-0a58ac103caa container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  2 04:03:13.305: INFO: Waiting for pod pod-projected-configmaps-2eb3e71e-6c8f-11e9-97f0-0a58ac103caa to disappear
May  2 04:03:13.321: INFO: Pod pod-projected-configmaps-2eb3e71e-6c8f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:03:13.321: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dbcqs" for this suite.
May  2 04:03:19.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:03:20.184: INFO: namespace: e2e-tests-projected-dbcqs, resource: bindings, ignored listing per whitelist
May  2 04:03:21.135: INFO: namespace: e2e-tests-projected-dbcqs, resource: packagemanifests, items remaining: 1
May  2 04:03:21.448: INFO: namespace: e2e-tests-projected-dbcqs no longer exists
May  2 04:03:21.465: INFO: namespace: e2e-tests-projected-dbcqs, total namespaces: 47, active: 47, terminating: 0
May  2 04:03:21.480: INFO: namespace e2e-tests-projected-dbcqs deletion completed in 8.117311959s

â€¢ [SLOW TEST:19.365 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:03:21.480: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 04:03:22.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a354ba4-6c8f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-mhqv8" to be "success or failure"
May  2 04:03:22.464: INFO: Pod "downwardapi-volume-3a354ba4-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.92995ms
May  2 04:03:24.481: INFO: Pod "downwardapi-volume-3a354ba4-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032403962s
May  2 04:03:26.497: INFO: Pod "downwardapi-volume-3a354ba4-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048826467s
May  2 04:03:28.513: INFO: Pod "downwardapi-volume-3a354ba4-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064827249s
May  2 04:03:30.530: INFO: Pod "downwardapi-volume-3a354ba4-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080992382s
May  2 04:03:32.546: INFO: Pod "downwardapi-volume-3a354ba4-6c8f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097099893s
STEP: Saw pod success
May  2 04:03:32.546: INFO: Pod "downwardapi-volume-3a354ba4-6c8f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:03:32.561: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-3a354ba4-6c8f-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 04:03:32.604: INFO: Waiting for pod downwardapi-volume-3a354ba4-6c8f-11e9-97f0-0a58ac103caa to disappear
May  2 04:03:32.619: INFO: Pod downwardapi-volume-3a354ba4-6c8f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:03:32.619: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mhqv8" for this suite.
May  2 04:03:38.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:03:39.284: INFO: namespace: e2e-tests-downward-api-mhqv8, resource: packagemanifests, items remaining: 1
May  2 04:03:40.531: INFO: namespace: e2e-tests-downward-api-mhqv8, resource: bindings, ignored listing per whitelist
May  2 04:03:40.748: INFO: namespace: e2e-tests-downward-api-mhqv8 no longer exists
May  2 04:03:40.765: INFO: namespace: e2e-tests-downward-api-mhqv8, total namespaces: 47, active: 47, terminating: 0
May  2 04:03:40.781: INFO: namespace e2e-tests-downward-api-mhqv8 deletion completed in 8.118692868s

â€¢ [SLOW TEST:19.300 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:03:40.781: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
May  2 04:03:41.732: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-x4fs8'
May  2 04:03:43.185: INFO: stderr: ""
May  2 04:03:43.185: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
May  2 04:03:44.202: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:03:44.202: INFO: Found 0 / 1
May  2 04:03:45.202: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:03:45.202: INFO: Found 0 / 1
May  2 04:03:46.201: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:03:46.202: INFO: Found 0 / 1
May  2 04:03:47.202: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:03:47.202: INFO: Found 0 / 1
May  2 04:03:48.202: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:03:48.202: INFO: Found 0 / 1
May  2 04:03:49.202: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:03:49.202: INFO: Found 0 / 1
May  2 04:03:50.203: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:03:50.203: INFO: Found 0 / 1
May  2 04:03:51.202: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:03:51.202: INFO: Found 0 / 1
May  2 04:03:52.202: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:03:52.202: INFO: Found 1 / 1
May  2 04:03:52.202: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  2 04:03:52.218: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:03:52.218: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May  2 04:03:52.218: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs redis-master-ssngt redis-master --namespace=e2e-tests-kubectl-x4fs8'
May  2 04:03:52.389: INFO: stderr: ""
May  2 04:03:52.389: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 02 May 04:03:51.394 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 02 May 04:03:51.394 # Server started, Redis version 3.2.12\n1:M 02 May 04:03:51.394 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 02 May 04:03:51.394 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May  2 04:03:52.389: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-ssngt redis-master --namespace=e2e-tests-kubectl-x4fs8 --tail=1'
May  2 04:03:52.598: INFO: stderr: ""
May  2 04:03:52.598: INFO: stdout: "1:M 02 May 04:03:51.394 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May  2 04:03:52.598: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-ssngt redis-master --namespace=e2e-tests-kubectl-x4fs8 --limit-bytes=1'
May  2 04:03:52.776: INFO: stderr: ""
May  2 04:03:52.776: INFO: stdout: " "
STEP: exposing timestamps
May  2 04:03:52.776: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-ssngt redis-master --namespace=e2e-tests-kubectl-x4fs8 --tail=1 --timestamps'
May  2 04:03:52.958: INFO: stderr: ""
May  2 04:03:52.958: INFO: stdout: "2019-05-02T04:03:51.394756261Z 1:M 02 May 04:03:51.394 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May  2 04:03:55.458: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-ssngt redis-master --namespace=e2e-tests-kubectl-x4fs8 --since=1s'
May  2 04:03:55.632: INFO: stderr: ""
May  2 04:03:55.632: INFO: stdout: ""
May  2 04:03:55.632: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-ssngt redis-master --namespace=e2e-tests-kubectl-x4fs8 --since=24h'
May  2 04:03:55.854: INFO: stderr: ""
May  2 04:03:55.854: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 02 May 04:03:51.394 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 02 May 04:03:51.394 # Server started, Redis version 3.2.12\n1:M 02 May 04:03:51.394 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 02 May 04:03:51.394 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
May  2 04:03:55.855: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-x4fs8'
May  2 04:03:56.023: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  2 04:03:56.024: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May  2 04:03:56.024: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-x4fs8'
May  2 04:03:56.190: INFO: stderr: "No resources found.\n"
May  2 04:03:56.190: INFO: stdout: ""
May  2 04:03:56.190: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=nginx --namespace=e2e-tests-kubectl-x4fs8 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  2 04:03:56.343: INFO: stderr: ""
May  2 04:03:56.343: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:03:56.343: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x4fs8" for this suite.
May  2 04:04:18.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:04:19.454: INFO: namespace: e2e-tests-kubectl-x4fs8, resource: bindings, ignored listing per whitelist
May  2 04:04:19.633: INFO: namespace: e2e-tests-kubectl-x4fs8, resource: packagemanifests, items remaining: 1
May  2 04:04:20.472: INFO: namespace: e2e-tests-kubectl-x4fs8 no longer exists
May  2 04:04:20.489: INFO: namespace: e2e-tests-kubectl-x4fs8, total namespaces: 47, active: 47, terminating: 0
May  2 04:04:20.505: INFO: namespace e2e-tests-kubectl-x4fs8 deletion completed in 24.118758425s

â€¢ [SLOW TEST:39.724 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:04:20.505: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May  2 04:04:21.484: INFO: Waiting up to 5m0s for pod "downward-api-5d659d38-6c8f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-sr47p" to be "success or failure"
May  2 04:04:21.499: INFO: Pod "downward-api-5d659d38-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.426848ms
May  2 04:04:23.516: INFO: Pod "downward-api-5d659d38-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032041513s
May  2 04:04:25.532: INFO: Pod "downward-api-5d659d38-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048188678s
May  2 04:04:27.548: INFO: Pod "downward-api-5d659d38-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06469029s
May  2 04:04:29.565: INFO: Pod "downward-api-5d659d38-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080940889s
May  2 04:04:31.581: INFO: Pod "downward-api-5d659d38-6c8f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.096887465s
STEP: Saw pod success
May  2 04:04:31.581: INFO: Pod "downward-api-5d659d38-6c8f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:04:31.596: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downward-api-5d659d38-6c8f-11e9-97f0-0a58ac103caa container dapi-container: <nil>
STEP: delete the pod
May  2 04:04:31.640: INFO: Waiting for pod downward-api-5d659d38-6c8f-11e9-97f0-0a58ac103caa to disappear
May  2 04:04:31.656: INFO: Pod downward-api-5d659d38-6c8f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:04:31.656: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sr47p" for this suite.
May  2 04:04:37.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:04:39.462: INFO: namespace: e2e-tests-downward-api-sr47p, resource: bindings, ignored listing per whitelist
May  2 04:04:39.620: INFO: namespace: e2e-tests-downward-api-sr47p, resource: packagemanifests, items remaining: 1
May  2 04:04:39.783: INFO: namespace: e2e-tests-downward-api-sr47p no longer exists
May  2 04:04:39.801: INFO: namespace: e2e-tests-downward-api-sr47p, total namespaces: 47, active: 47, terminating: 0
May  2 04:04:39.817: INFO: namespace e2e-tests-downward-api-sr47p deletion completed in 8.118304886s

â€¢ [SLOW TEST:19.312 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:04:39.817: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-vh4br
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-vh4br to expose endpoints map[]
May  2 04:04:40.812: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-vh4br exposes endpoints map[] (16.034887ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-vh4br
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-vh4br to expose endpoints map[pod1:[80]]
May  2 04:04:45.004: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.163761759s elapsed, will retry)
May  2 04:04:50.173: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-vh4br exposes endpoints map[pod1:[80]] (9.332977634s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-vh4br
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-vh4br to expose endpoints map[pod1:[80] pod2:[80]]
May  2 04:04:54.437: INFO: Unexpected endpoints: found map[68f14b93-6c8f-11e9-9e44-12f3365d453a:[80]], expected map[pod1:[80] pod2:[80]] (4.240928908s elapsed, will retry)
May  2 04:04:58.632: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-vh4br exposes endpoints map[pod1:[80] pod2:[80]] (8.435236934s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-vh4br
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-vh4br to expose endpoints map[pod2:[80]]
May  2 04:04:58.683: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-vh4br exposes endpoints map[pod2:[80]] (31.941316ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-vh4br
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-vh4br to expose endpoints map[]
May  2 04:04:58.719: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-vh4br exposes endpoints map[] (16.106889ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:04:58.751: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-vh4br" for this suite.
May  2 04:05:22.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:05:24.325: INFO: namespace: e2e-tests-services-vh4br, resource: bindings, ignored listing per whitelist
May  2 04:05:24.377: INFO: namespace: e2e-tests-services-vh4br, resource: packagemanifests, items remaining: 1
May  2 04:05:24.878: INFO: namespace: e2e-tests-services-vh4br no longer exists
May  2 04:05:24.895: INFO: namespace: e2e-tests-services-vh4br, total namespaces: 47, active: 47, terminating: 0
May  2 04:05:24.910: INFO: namespace e2e-tests-services-vh4br deletion completed in 26.115734291s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:45.093 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:05:24.910: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
May  2 04:05:25.874: INFO: Waiting up to 5m0s for pod "pod-83c6e86f-6c8f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-8n9q6" to be "success or failure"
May  2 04:05:25.890: INFO: Pod "pod-83c6e86f-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.190398ms
May  2 04:05:27.906: INFO: Pod "pod-83c6e86f-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032242543s
May  2 04:05:29.923: INFO: Pod "pod-83c6e86f-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049001497s
May  2 04:05:31.939: INFO: Pod "pod-83c6e86f-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065193539s
May  2 04:05:33.956: INFO: Pod "pod-83c6e86f-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081750133s
May  2 04:05:35.972: INFO: Pod "pod-83c6e86f-6c8f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098040349s
STEP: Saw pod success
May  2 04:05:35.972: INFO: Pod "pod-83c6e86f-6c8f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:05:35.987: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-83c6e86f-6c8f-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 04:05:36.029: INFO: Waiting for pod pod-83c6e86f-6c8f-11e9-97f0-0a58ac103caa to disappear
May  2 04:05:36.045: INFO: Pod pod-83c6e86f-6c8f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:05:36.045: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8n9q6" for this suite.
May  2 04:05:42.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:05:43.299: INFO: namespace: e2e-tests-emptydir-8n9q6, resource: packagemanifests, items remaining: 1
May  2 04:05:43.726: INFO: namespace: e2e-tests-emptydir-8n9q6, resource: bindings, ignored listing per whitelist
May  2 04:05:44.172: INFO: namespace: e2e-tests-emptydir-8n9q6 no longer exists
May  2 04:05:44.189: INFO: namespace: e2e-tests-emptydir-8n9q6, total namespaces: 47, active: 47, terminating: 0
May  2 04:05:44.204: INFO: namespace e2e-tests-emptydir-8n9q6 deletion completed in 8.116463134s

â€¢ [SLOW TEST:19.294 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:05:44.205: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:05:57.269: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-p22dj" for this suite.
May  2 04:06:03.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:06:03.916: INFO: namespace: e2e-tests-kubelet-test-p22dj, resource: bindings, ignored listing per whitelist
May  2 04:06:04.816: INFO: namespace: e2e-tests-kubelet-test-p22dj, resource: packagemanifests, items remaining: 1
May  2 04:06:05.398: INFO: namespace: e2e-tests-kubelet-test-p22dj no longer exists
May  2 04:06:05.416: INFO: namespace: e2e-tests-kubelet-test-p22dj, total namespaces: 47, active: 47, terminating: 0
May  2 04:06:05.431: INFO: namespace e2e-tests-kubelet-test-p22dj deletion completed in 8.117260919s

â€¢ [SLOW TEST:21.226 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:06:05.431: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
May  2 04:06:06.400: INFO: Waiting up to 5m0s for pod "client-containers-9bee62ed-6c8f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-containers-gv9kt" to be "success or failure"
May  2 04:06:06.415: INFO: Pod "client-containers-9bee62ed-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.768854ms
May  2 04:06:08.432: INFO: Pod "client-containers-9bee62ed-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032853035s
May  2 04:06:10.449: INFO: Pod "client-containers-9bee62ed-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049021655s
May  2 04:06:12.465: INFO: Pod "client-containers-9bee62ed-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06555359s
May  2 04:06:14.482: INFO: Pod "client-containers-9bee62ed-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082080814s
May  2 04:06:16.498: INFO: Pod "client-containers-9bee62ed-6c8f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098539397s
STEP: Saw pod success
May  2 04:06:16.498: INFO: Pod "client-containers-9bee62ed-6c8f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:06:16.514: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod client-containers-9bee62ed-6c8f-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 04:06:16.557: INFO: Waiting for pod client-containers-9bee62ed-6c8f-11e9-97f0-0a58ac103caa to disappear
May  2 04:06:16.572: INFO: Pod client-containers-9bee62ed-6c8f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:06:16.572: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-gv9kt" for this suite.
May  2 04:06:22.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:06:23.283: INFO: namespace: e2e-tests-containers-gv9kt, resource: packagemanifests, items remaining: 1
May  2 04:06:23.829: INFO: namespace: e2e-tests-containers-gv9kt, resource: bindings, ignored listing per whitelist
May  2 04:06:24.700: INFO: namespace: e2e-tests-containers-gv9kt no longer exists
May  2 04:06:24.717: INFO: namespace: e2e-tests-containers-gv9kt, total namespaces: 47, active: 47, terminating: 0
May  2 04:06:24.733: INFO: namespace e2e-tests-containers-gv9kt deletion completed in 8.117246821s

â€¢ [SLOW TEST:19.301 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:06:24.733: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
May  2 04:06:25.711: INFO: Waiting up to 5m0s for pod "pod-a76f32a9-6c8f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-brdpf" to be "success or failure"
May  2 04:06:25.726: INFO: Pod "pod-a76f32a9-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.736021ms
May  2 04:06:27.743: INFO: Pod "pod-a76f32a9-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031973081s
May  2 04:06:29.759: INFO: Pod "pod-a76f32a9-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048193598s
May  2 04:06:31.775: INFO: Pod "pod-a76f32a9-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064385604s
May  2 04:06:33.791: INFO: Pod "pod-a76f32a9-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080182873s
May  2 04:06:35.807: INFO: Pod "pod-a76f32a9-6c8f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.096393134s
STEP: Saw pod success
May  2 04:06:35.807: INFO: Pod "pod-a76f32a9-6c8f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:06:35.822: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-a76f32a9-6c8f-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 04:06:35.866: INFO: Waiting for pod pod-a76f32a9-6c8f-11e9-97f0-0a58ac103caa to disappear
May  2 04:06:35.881: INFO: Pod pod-a76f32a9-6c8f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:06:35.881: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-brdpf" for this suite.
May  2 04:06:41.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:06:43.181: INFO: namespace: e2e-tests-emptydir-brdpf, resource: packagemanifests, items remaining: 1
May  2 04:06:43.668: INFO: namespace: e2e-tests-emptydir-brdpf, resource: bindings, ignored listing per whitelist
May  2 04:06:44.017: INFO: namespace: e2e-tests-emptydir-brdpf no longer exists
May  2 04:06:44.034: INFO: namespace: e2e-tests-emptydir-brdpf, total namespaces: 47, active: 47, terminating: 0
May  2 04:06:44.052: INFO: namespace e2e-tests-emptydir-brdpf deletion completed in 8.127854803s

â€¢ [SLOW TEST:19.319 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:06:44.052: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 04:06:45.128: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3040550-6c8f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-dvwn8" to be "success or failure"
May  2 04:06:45.145: INFO: Pod "downwardapi-volume-b3040550-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.388849ms
May  2 04:06:47.169: INFO: Pod "downwardapi-volume-b3040550-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040397111s
May  2 04:06:49.188: INFO: Pod "downwardapi-volume-b3040550-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059576416s
May  2 04:06:51.206: INFO: Pod "downwardapi-volume-b3040550-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077373695s
May  2 04:06:53.225: INFO: Pod "downwardapi-volume-b3040550-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096464649s
May  2 04:06:55.242: INFO: Pod "downwardapi-volume-b3040550-6c8f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.113413868s
STEP: Saw pod success
May  2 04:06:55.242: INFO: Pod "downwardapi-volume-b3040550-6c8f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:06:55.262: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-b3040550-6c8f-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 04:06:55.313: INFO: Waiting for pod downwardapi-volume-b3040550-6c8f-11e9-97f0-0a58ac103caa to disappear
May  2 04:06:55.331: INFO: Pod downwardapi-volume-b3040550-6c8f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:06:55.331: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dvwn8" for this suite.
May  2 04:07:01.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:07:02.426: INFO: namespace: e2e-tests-projected-dvwn8, resource: bindings, ignored listing per whitelist
May  2 04:07:02.634: INFO: namespace: e2e-tests-projected-dvwn8, resource: packagemanifests, items remaining: 1
May  2 04:07:03.465: INFO: namespace: e2e-tests-projected-dvwn8 no longer exists
May  2 04:07:03.482: INFO: namespace: e2e-tests-projected-dvwn8, total namespaces: 47, active: 47, terminating: 0
May  2 04:07:03.498: INFO: namespace e2e-tests-projected-dvwn8 deletion completed in 8.123991206s

â€¢ [SLOW TEST:19.446 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:07:03.498: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May  2 04:07:14.590: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:07:14.644: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-vtvgv" for this suite.
May  2 04:07:38.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:07:39.729: INFO: namespace: e2e-tests-replicaset-vtvgv, resource: bindings, ignored listing per whitelist
May  2 04:07:40.158: INFO: namespace: e2e-tests-replicaset-vtvgv, resource: packagemanifests, items remaining: 1
May  2 04:07:40.772: INFO: namespace: e2e-tests-replicaset-vtvgv no longer exists
May  2 04:07:40.789: INFO: namespace: e2e-tests-replicaset-vtvgv, total namespaces: 47, active: 47, terminating: 0
May  2 04:07:40.805: INFO: namespace e2e-tests-replicaset-vtvgv deletion completed in 26.117089651s

â€¢ [SLOW TEST:37.307 seconds]
[sig-apps] ReplicaSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:07:40.805: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:07:41.776: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-xh25t" for this suite.
May  2 04:08:05.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:08:07.110: INFO: namespace: e2e-tests-pods-xh25t, resource: bindings, ignored listing per whitelist
May  2 04:08:07.287: INFO: namespace: e2e-tests-pods-xh25t, resource: packagemanifests, items remaining: 1
May  2 04:08:07.888: INFO: namespace: e2e-tests-pods-xh25t no longer exists
May  2 04:08:07.906: INFO: namespace: e2e-tests-pods-xh25t, total namespaces: 47, active: 47, terminating: 0
May  2 04:08:07.921: INFO: namespace e2e-tests-pods-xh25t deletion completed in 26.128586216s

â€¢ [SLOW TEST:27.117 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:08:07.922: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-e4f00b41-6c8f-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 04:08:08.906: INFO: Waiting up to 5m0s for pod "pod-secrets-e4f3196e-6c8f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-secrets-gxcss" to be "success or failure"
May  2 04:08:08.921: INFO: Pod "pod-secrets-e4f3196e-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.280142ms
May  2 04:08:10.937: INFO: Pod "pod-secrets-e4f3196e-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031206983s
May  2 04:08:12.953: INFO: Pod "pod-secrets-e4f3196e-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047407056s
May  2 04:08:14.970: INFO: Pod "pod-secrets-e4f3196e-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063688062s
May  2 04:08:16.986: INFO: Pod "pod-secrets-e4f3196e-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080101236s
May  2 04:08:19.005: INFO: Pod "pod-secrets-e4f3196e-6c8f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.09887189s
STEP: Saw pod success
May  2 04:08:19.005: INFO: Pod "pod-secrets-e4f3196e-6c8f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:08:19.021: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-secrets-e4f3196e-6c8f-11e9-97f0-0a58ac103caa container secret-env-test: <nil>
STEP: delete the pod
May  2 04:08:19.065: INFO: Waiting for pod pod-secrets-e4f3196e-6c8f-11e9-97f0-0a58ac103caa to disappear
May  2 04:08:19.080: INFO: Pod pod-secrets-e4f3196e-6c8f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:08:19.080: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-gxcss" for this suite.
May  2 04:08:25.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:08:25.759: INFO: namespace: e2e-tests-secrets-gxcss, resource: packagemanifests, items remaining: 1
May  2 04:08:27.090: INFO: namespace: e2e-tests-secrets-gxcss, resource: bindings, ignored listing per whitelist
May  2 04:08:27.206: INFO: namespace: e2e-tests-secrets-gxcss no longer exists
May  2 04:08:27.223: INFO: namespace: e2e-tests-secrets-gxcss, total namespaces: 47, active: 47, terminating: 0
May  2 04:08:27.238: INFO: namespace e2e-tests-secrets-gxcss deletion completed in 8.115944112s

â€¢ [SLOW TEST:19.316 seconds]
[sig-api-machinery] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:08:27.238: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
May  2 04:08:28.201: INFO: Waiting up to 5m0s for pod "client-containers-f0739183-6c8f-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-containers-qclcg" to be "success or failure"
May  2 04:08:28.216: INFO: Pod "client-containers-f0739183-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.201865ms
May  2 04:08:30.233: INFO: Pod "client-containers-f0739183-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03208126s
May  2 04:08:32.249: INFO: Pod "client-containers-f0739183-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048108026s
May  2 04:08:34.266: INFO: Pod "client-containers-f0739183-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064723578s
May  2 04:08:36.282: INFO: Pod "client-containers-f0739183-6c8f-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08092086s
May  2 04:08:38.298: INFO: Pod "client-containers-f0739183-6c8f-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097149512s
STEP: Saw pod success
May  2 04:08:38.298: INFO: Pod "client-containers-f0739183-6c8f-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:08:38.314: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod client-containers-f0739183-6c8f-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 04:08:38.357: INFO: Waiting for pod client-containers-f0739183-6c8f-11e9-97f0-0a58ac103caa to disappear
May  2 04:08:38.372: INFO: Pod client-containers-f0739183-6c8f-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:08:38.372: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-qclcg" for this suite.
May  2 04:08:44.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:08:45.600: INFO: namespace: e2e-tests-containers-qclcg, resource: packagemanifests, items remaining: 1
May  2 04:08:45.698: INFO: namespace: e2e-tests-containers-qclcg, resource: bindings, ignored listing per whitelist
May  2 04:08:46.501: INFO: namespace: e2e-tests-containers-qclcg no longer exists
May  2 04:08:46.518: INFO: namespace: e2e-tests-containers-qclcg, total namespaces: 47, active: 47, terminating: 0
May  2 04:08:46.533: INFO: namespace e2e-tests-containers-qclcg deletion completed in 8.1181894s

â€¢ [SLOW TEST:19.295 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:08:46.534: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-4xkht
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  2 04:08:47.477: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
May  2 04:09:23.855: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.217:8080/dial?request=hostName&protocol=http&host=10.131.0.216&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-4xkht PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 04:09:23.855: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 04:09:24.076: INFO: Waiting for endpoints: map[]
May  2 04:09:24.092: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.217:8080/dial?request=hostName&protocol=http&host=10.129.2.50&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-4xkht PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 04:09:24.092: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 04:09:24.260: INFO: Waiting for endpoints: map[]
May  2 04:09:24.277: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.217:8080/dial?request=hostName&protocol=http&host=10.128.2.44&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-4xkht PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  2 04:09:24.277: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
May  2 04:09:24.489: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:09:24.489: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-4xkht" for this suite.
May  2 04:09:48.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:09:49.446: INFO: namespace: e2e-tests-pod-network-test-4xkht, resource: packagemanifests, items remaining: 1
May  2 04:09:50.046: INFO: namespace: e2e-tests-pod-network-test-4xkht, resource: bindings, ignored listing per whitelist
May  2 04:09:50.615: INFO: namespace: e2e-tests-pod-network-test-4xkht no longer exists
May  2 04:09:50.632: INFO: namespace: e2e-tests-pod-network-test-4xkht, total namespaces: 47, active: 47, terminating: 0
May  2 04:09:50.647: INFO: namespace e2e-tests-pod-network-test-4xkht deletion completed in 26.115263889s

â€¢ [SLOW TEST:64.114 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:09:50.648: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May  2 04:09:51.598: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-hmdcx'
May  2 04:09:51.747: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  2 04:09:51.747: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
May  2 04:09:51.763: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-hmdcx'
May  2 04:09:51.927: INFO: stderr: ""
May  2 04:09:51.927: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:09:51.927: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hmdcx" for this suite.
May  2 04:10:16.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:10:17.518: INFO: namespace: e2e-tests-kubectl-hmdcx, resource: packagemanifests, items remaining: 1
May  2 04:10:17.581: INFO: namespace: e2e-tests-kubectl-hmdcx, resource: bindings, ignored listing per whitelist
May  2 04:10:18.037: INFO: namespace: e2e-tests-kubectl-hmdcx no longer exists
May  2 04:10:18.058: INFO: namespace: e2e-tests-kubectl-hmdcx, total namespaces: 47, active: 47, terminating: 0
May  2 04:10:18.075: INFO: namespace e2e-tests-kubectl-hmdcx deletion completed in 26.131128932s

â€¢ [SLOW TEST:27.428 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:10:18.076: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0502 04:10:49.212422   12606 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  2 04:10:49.212: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:10:49.212: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-nmbf7" for this suite.
May  2 04:10:55.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:10:56.536: INFO: namespace: e2e-tests-gc-nmbf7, resource: packagemanifests, items remaining: 1
May  2 04:10:56.784: INFO: namespace: e2e-tests-gc-nmbf7, resource: bindings, ignored listing per whitelist
May  2 04:10:57.317: INFO: namespace: e2e-tests-gc-nmbf7 no longer exists
May  2 04:10:57.335: INFO: namespace: e2e-tests-gc-nmbf7, total namespaces: 47, active: 47, terminating: 0
May  2 04:10:57.352: INFO: namespace e2e-tests-gc-nmbf7 deletion completed in 8.12239506s

â€¢ [SLOW TEST:39.276 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:10:57.352: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
May  2 04:10:58.378: INFO: Waiting up to 5m0s for pod "pod-49f6699f-6c90-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-g5l4l" to be "success or failure"
May  2 04:10:58.393: INFO: Pod "pod-49f6699f-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.522799ms
May  2 04:11:00.412: INFO: Pod "pod-49f6699f-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033918418s
May  2 04:11:02.430: INFO: Pod "pod-49f6699f-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051916551s
May  2 04:11:04.447: INFO: Pod "pod-49f6699f-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068930041s
May  2 04:11:06.463: INFO: Pod "pod-49f6699f-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085292107s
May  2 04:11:08.481: INFO: Pod "pod-49f6699f-6c90-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.102940034s
STEP: Saw pod success
May  2 04:11:08.481: INFO: Pod "pod-49f6699f-6c90-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:11:08.497: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-49f6699f-6c90-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 04:11:08.543: INFO: Waiting for pod pod-49f6699f-6c90-11e9-97f0-0a58ac103caa to disappear
May  2 04:11:08.560: INFO: Pod pod-49f6699f-6c90-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:11:08.560: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-g5l4l" for this suite.
May  2 04:11:14.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:11:15.463: INFO: namespace: e2e-tests-emptydir-g5l4l, resource: bindings, ignored listing per whitelist
May  2 04:11:16.298: INFO: namespace: e2e-tests-emptydir-g5l4l, resource: packagemanifests, items remaining: 1
May  2 04:11:16.696: INFO: namespace: e2e-tests-emptydir-g5l4l no longer exists
May  2 04:11:16.713: INFO: namespace: e2e-tests-emptydir-g5l4l, total namespaces: 47, active: 47, terminating: 0
May  2 04:11:16.730: INFO: namespace e2e-tests-emptydir-g5l4l deletion completed in 8.124697446s

â€¢ [SLOW TEST:19.378 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:11:16.730: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0502 04:11:23.858507   12606 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  2 04:11:23.858: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:11:23.858: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-j57d9" for this suite.
May  2 04:11:29.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:11:30.767: INFO: namespace: e2e-tests-gc-j57d9, resource: bindings, ignored listing per whitelist
May  2 04:11:30.800: INFO: namespace: e2e-tests-gc-j57d9, resource: packagemanifests, items remaining: 1
May  2 04:11:31.969: INFO: namespace: e2e-tests-gc-j57d9 no longer exists
May  2 04:11:31.988: INFO: namespace: e2e-tests-gc-j57d9, total namespaces: 47, active: 47, terminating: 0
May  2 04:11:32.005: INFO: namespace e2e-tests-gc-j57d9 deletion completed in 8.128355006s

â€¢ [SLOW TEST:15.275 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:11:32.005: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May  2 04:11:33.024: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e9cb929-6c90-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-jgg78" to be "success or failure"
May  2 04:11:33.039: INFO: Pod "downwardapi-volume-5e9cb929-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.312746ms
May  2 04:11:35.057: INFO: Pod "downwardapi-volume-5e9cb929-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033106736s
May  2 04:11:37.073: INFO: Pod "downwardapi-volume-5e9cb929-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049519241s
May  2 04:11:39.090: INFO: Pod "downwardapi-volume-5e9cb929-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066053125s
May  2 04:11:41.107: INFO: Pod "downwardapi-volume-5e9cb929-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082876249s
May  2 04:11:43.123: INFO: Pod "downwardapi-volume-5e9cb929-6c90-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.099207205s
STEP: Saw pod success
May  2 04:11:43.123: INFO: Pod "downwardapi-volume-5e9cb929-6c90-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:11:43.138: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downwardapi-volume-5e9cb929-6c90-11e9-97f0-0a58ac103caa container client-container: <nil>
STEP: delete the pod
May  2 04:11:43.182: INFO: Waiting for pod downwardapi-volume-5e9cb929-6c90-11e9-97f0-0a58ac103caa to disappear
May  2 04:11:43.197: INFO: Pod downwardapi-volume-5e9cb929-6c90-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:11:43.197: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jgg78" for this suite.
May  2 04:11:49.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:11:50.535: INFO: namespace: e2e-tests-downward-api-jgg78, resource: bindings, ignored listing per whitelist
May  2 04:11:50.958: INFO: namespace: e2e-tests-downward-api-jgg78, resource: packagemanifests, items remaining: 1
May  2 04:11:51.328: INFO: namespace: e2e-tests-downward-api-jgg78 no longer exists
May  2 04:11:51.345: INFO: namespace: e2e-tests-downward-api-jgg78, total namespaces: 47, active: 47, terminating: 0
May  2 04:11:51.361: INFO: namespace e2e-tests-downward-api-jgg78 deletion completed in 8.120796778s

â€¢ [SLOW TEST:19.357 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:11:51.363: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May  2 04:11:52.352: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-gsms2'
May  2 04:11:52.576: INFO: stderr: ""
May  2 04:11:52.576: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May  2 04:12:02.627: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-gsms2 -o json'
May  2 04:12:02.781: INFO: stderr: ""
May  2 04:12:02.781: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"openshift-sdn\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.0.221\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2019-05-02T04:11:52Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-gsms2\",\n        \"resourceVersion\": \"72810\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-gsms2/pods/e2e-test-nginx-pod\",\n        \"uid\": \"6a4518ef-6c90-11e9-8dad-121ea440cb2c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    },\n                    \"procMount\": \"Default\"\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-848t8\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-7phh9\"\n            }\n        ],\n        \"nodeName\": \"ip-10-0-135-216.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c48,c27\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-848t8\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-848t8\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-02T04:11:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-02T04:12:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-02T04:12:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-02T04:11:52Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://2738a1af4c4345bcddf9f47d26471059f43f67dd23e8428db66f4e1beb378548\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-02T04:12:00Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.135.216\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.131.0.221\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-02T04:11:52Z\"\n    }\n}\n"
STEP: replace the image in the pod
May  2 04:12:02.781: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig replace -f - --namespace=e2e-tests-kubectl-gsms2'
May  2 04:12:03.099: INFO: stderr: ""
May  2 04:12:03.099: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
May  2 04:12:03.115: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-gsms2'
May  2 04:12:05.356: INFO: stderr: ""
May  2 04:12:05.356: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:12:05.356: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gsms2" for this suite.
May  2 04:12:11.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:12:12.419: INFO: namespace: e2e-tests-kubectl-gsms2, resource: packagemanifests, items remaining: 1
May  2 04:12:13.323: INFO: namespace: e2e-tests-kubectl-gsms2, resource: bindings, ignored listing per whitelist
May  2 04:12:13.486: INFO: namespace: e2e-tests-kubectl-gsms2 no longer exists
May  2 04:12:13.503: INFO: namespace: e2e-tests-kubectl-gsms2, total namespaces: 47, active: 47, terminating: 0
May  2 04:12:13.519: INFO: namespace e2e-tests-kubectl-gsms2 deletion completed in 8.119339418s

â€¢ [SLOW TEST:22.156 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:12:13.519: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-w7b7
STEP: Creating a pod to test atomic-volume-subpath
May  2 04:12:14.519: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-w7b7" in namespace "e2e-tests-subpath-q9psq" to be "success or failure"
May  2 04:12:14.535: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.784213ms
May  2 04:12:16.551: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032259252s
May  2 04:12:18.568: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048492505s
May  2 04:12:20.584: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064985148s
May  2 04:12:22.601: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08170753s
May  2 04:12:24.621: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Running", Reason="", readiness=false. Elapsed: 10.101729979s
May  2 04:12:26.637: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Running", Reason="", readiness=false. Elapsed: 12.117732901s
May  2 04:12:28.653: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Running", Reason="", readiness=false. Elapsed: 14.133951881s
May  2 04:12:30.669: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Running", Reason="", readiness=false. Elapsed: 16.149843272s
May  2 04:12:32.685: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Running", Reason="", readiness=false. Elapsed: 18.166156776s
May  2 04:12:34.702: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Running", Reason="", readiness=false. Elapsed: 20.182465253s
May  2 04:12:36.718: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Running", Reason="", readiness=false. Elapsed: 22.198569584s
May  2 04:12:38.735: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Running", Reason="", readiness=false. Elapsed: 24.216266975s
May  2 04:12:40.752: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Running", Reason="", readiness=false. Elapsed: 26.232482704s
May  2 04:12:42.768: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Running", Reason="", readiness=false. Elapsed: 28.248444095s
May  2 04:12:44.784: INFO: Pod "pod-subpath-test-secret-w7b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.264691662s
STEP: Saw pod success
May  2 04:12:44.784: INFO: Pod "pod-subpath-test-secret-w7b7" satisfied condition "success or failure"
May  2 04:12:44.800: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-subpath-test-secret-w7b7 container test-container-subpath-secret-w7b7: <nil>
STEP: delete the pod
May  2 04:12:44.843: INFO: Waiting for pod pod-subpath-test-secret-w7b7 to disappear
May  2 04:12:44.859: INFO: Pod pod-subpath-test-secret-w7b7 no longer exists
STEP: Deleting pod pod-subpath-test-secret-w7b7
May  2 04:12:44.859: INFO: Deleting pod "pod-subpath-test-secret-w7b7" in namespace "e2e-tests-subpath-q9psq"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:12:44.875: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-q9psq" for this suite.
May  2 04:12:50.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:12:52.098: INFO: namespace: e2e-tests-subpath-q9psq, resource: packagemanifests, items remaining: 1
May  2 04:12:52.130: INFO: namespace: e2e-tests-subpath-q9psq, resource: bindings, ignored listing per whitelist
May  2 04:12:53.003: INFO: namespace: e2e-tests-subpath-q9psq no longer exists
May  2 04:12:53.022: INFO: namespace: e2e-tests-subpath-q9psq, total namespaces: 47, active: 47, terminating: 0
May  2 04:12:53.037: INFO: namespace e2e-tests-subpath-q9psq deletion completed in 8.119982473s

â€¢ [SLOW TEST:39.518 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:12:53.037: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
May  2 04:12:54.005: INFO: Waiting up to 5m0s for pod "pod-8ee22b7e-6c90-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-6k8dl" to be "success or failure"
May  2 04:12:54.020: INFO: Pod "pod-8ee22b7e-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.238783ms
May  2 04:12:56.036: INFO: Pod "pod-8ee22b7e-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031039135s
May  2 04:12:58.053: INFO: Pod "pod-8ee22b7e-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047420177s
May  2 04:13:00.069: INFO: Pod "pod-8ee22b7e-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063702227s
May  2 04:13:02.086: INFO: Pod "pod-8ee22b7e-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080484332s
May  2 04:13:04.103: INFO: Pod "pod-8ee22b7e-6c90-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097447435s
STEP: Saw pod success
May  2 04:13:04.103: INFO: Pod "pod-8ee22b7e-6c90-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:13:04.119: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-8ee22b7e-6c90-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 04:13:04.163: INFO: Waiting for pod pod-8ee22b7e-6c90-11e9-97f0-0a58ac103caa to disappear
May  2 04:13:04.179: INFO: Pod pod-8ee22b7e-6c90-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:13:04.179: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6k8dl" for this suite.
May  2 04:13:10.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:13:10.840: INFO: namespace: e2e-tests-emptydir-6k8dl, resource: bindings, ignored listing per whitelist
May  2 04:13:10.923: INFO: namespace: e2e-tests-emptydir-6k8dl, resource: packagemanifests, items remaining: 1
May  2 04:13:12.305: INFO: namespace: e2e-tests-emptydir-6k8dl no longer exists
May  2 04:13:12.329: INFO: namespace: e2e-tests-emptydir-6k8dl, total namespaces: 47, active: 47, terminating: 0
May  2 04:13:12.344: INFO: namespace e2e-tests-emptydir-6k8dl deletion completed in 8.121793077s

â€¢ [SLOW TEST:19.307 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:13:12.345: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-9a634f50-6c90-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume secrets
May  2 04:13:13.328: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9a6611a3-6c90-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-projected-z5h8x" to be "success or failure"
May  2 04:13:13.344: INFO: Pod "pod-projected-secrets-9a6611a3-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.826326ms
May  2 04:13:15.360: INFO: Pod "pod-projected-secrets-9a6611a3-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03193958s
May  2 04:13:17.376: INFO: Pod "pod-projected-secrets-9a6611a3-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04785287s
May  2 04:13:19.392: INFO: Pod "pod-projected-secrets-9a6611a3-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063923816s
May  2 04:13:21.409: INFO: Pod "pod-projected-secrets-9a6611a3-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080928819s
May  2 04:13:23.426: INFO: Pod "pod-projected-secrets-9a6611a3-6c90-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098055528s
STEP: Saw pod success
May  2 04:13:23.426: INFO: Pod "pod-projected-secrets-9a6611a3-6c90-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:13:23.441: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-projected-secrets-9a6611a3-6c90-11e9-97f0-0a58ac103caa container projected-secret-volume-test: <nil>
STEP: delete the pod
May  2 04:13:23.486: INFO: Waiting for pod pod-projected-secrets-9a6611a3-6c90-11e9-97f0-0a58ac103caa to disappear
May  2 04:13:23.501: INFO: Pod pod-projected-secrets-9a6611a3-6c90-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:13:23.501: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-z5h8x" for this suite.
May  2 04:13:29.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:13:30.407: INFO: namespace: e2e-tests-projected-z5h8x, resource: packagemanifests, items remaining: 1
May  2 04:13:30.629: INFO: namespace: e2e-tests-projected-z5h8x, resource: bindings, ignored listing per whitelist
May  2 04:13:31.632: INFO: namespace: e2e-tests-projected-z5h8x no longer exists
May  2 04:13:31.651: INFO: namespace: e2e-tests-projected-z5h8x, total namespaces: 47, active: 47, terminating: 0
May  2 04:13:31.666: INFO: namespace e2e-tests-projected-z5h8x deletion completed in 8.122449013s

â€¢ [SLOW TEST:19.322 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:13:31.667: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
May  2 04:13:32.603: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:32.951: INFO: stderr: ""
May  2 04:13:32.951: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  2 04:13:32.951: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:33.096: INFO: stderr: ""
May  2 04:13:33.096: INFO: stdout: "update-demo-nautilus-9g2nj update-demo-nautilus-pt8qh "
May  2 04:13:33.096: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-9g2nj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:33.233: INFO: stderr: ""
May  2 04:13:33.233: INFO: stdout: ""
May  2 04:13:33.233: INFO: update-demo-nautilus-9g2nj is created but not running
May  2 04:13:38.234: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:38.377: INFO: stderr: ""
May  2 04:13:38.377: INFO: stdout: "update-demo-nautilus-9g2nj update-demo-nautilus-pt8qh "
May  2 04:13:38.377: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-9g2nj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:38.515: INFO: stderr: ""
May  2 04:13:38.515: INFO: stdout: ""
May  2 04:13:38.515: INFO: update-demo-nautilus-9g2nj is created but not running
May  2 04:13:43.516: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:43.757: INFO: stderr: ""
May  2 04:13:43.757: INFO: stdout: "update-demo-nautilus-9g2nj update-demo-nautilus-pt8qh "
May  2 04:13:43.757: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-9g2nj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:43.897: INFO: stderr: ""
May  2 04:13:43.897: INFO: stdout: "true"
May  2 04:13:43.897: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-9g2nj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:44.041: INFO: stderr: ""
May  2 04:13:44.041: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  2 04:13:44.041: INFO: validating pod update-demo-nautilus-9g2nj
May  2 04:13:44.060: INFO: got data: {
  "image": "nautilus.jpg"
}

May  2 04:13:44.060: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  2 04:13:44.060: INFO: update-demo-nautilus-9g2nj is verified up and running
May  2 04:13:44.060: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-pt8qh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:44.198: INFO: stderr: ""
May  2 04:13:44.198: INFO: stdout: "true"
May  2 04:13:44.198: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-pt8qh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:44.335: INFO: stderr: ""
May  2 04:13:44.335: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  2 04:13:44.335: INFO: validating pod update-demo-nautilus-pt8qh
May  2 04:13:44.354: INFO: got data: {
  "image": "nautilus.jpg"
}

May  2 04:13:44.354: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  2 04:13:44.354: INFO: update-demo-nautilus-pt8qh is verified up and running
STEP: using delete to clean up resources
May  2 04:13:44.354: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:44.506: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  2 04:13:44.506: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  2 04:13:44.506: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-jrjn9'
May  2 04:13:44.663: INFO: stderr: "No resources found.\n"
May  2 04:13:44.663: INFO: stdout: ""
May  2 04:13:44.663: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-jrjn9 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  2 04:13:44.809: INFO: stderr: ""
May  2 04:13:44.809: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:13:44.809: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jrjn9" for this suite.
May  2 04:13:50.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:13:51.770: INFO: namespace: e2e-tests-kubectl-jrjn9, resource: packagemanifests, items remaining: 1
May  2 04:13:52.918: INFO: namespace: e2e-tests-kubectl-jrjn9, resource: bindings, ignored listing per whitelist
May  2 04:13:52.934: INFO: namespace: e2e-tests-kubectl-jrjn9 no longer exists
May  2 04:13:52.951: INFO: namespace: e2e-tests-kubectl-jrjn9, total namespaces: 47, active: 47, terminating: 0
May  2 04:13:52.966: INFO: namespace e2e-tests-kubectl-jrjn9 deletion completed in 8.11409486s

â€¢ [SLOW TEST:21.299 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:13:52.966: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
May  2 04:13:53.905: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig api-versions'
May  2 04:13:54.042: INFO: stderr: ""
May  2 04:13:54.042: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nhealthchecking.openshift.io/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.openshift.io/v1\nnetworking.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:13:54.042: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7qhss" for this suite.
May  2 04:14:00.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:14:01.075: INFO: namespace: e2e-tests-kubectl-7qhss, resource: bindings, ignored listing per whitelist
May  2 04:14:01.290: INFO: namespace: e2e-tests-kubectl-7qhss, resource: packagemanifests, items remaining: 1
May  2 04:14:02.142: INFO: namespace: e2e-tests-kubectl-7qhss no longer exists
May  2 04:14:02.159: INFO: namespace: e2e-tests-kubectl-7qhss, total namespaces: 47, active: 47, terminating: 0
May  2 04:14:02.175: INFO: namespace e2e-tests-kubectl-7qhss deletion completed in 8.11575465s

â€¢ [SLOW TEST:9.208 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:14:02.175: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-b8167068-6c90-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 04:14:03.157: INFO: Waiting up to 5m0s for pod "pod-configmaps-b8194891-6c90-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-configmap-cdpdn" to be "success or failure"
May  2 04:14:03.172: INFO: Pod "pod-configmaps-b8194891-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.737587ms
May  2 04:14:05.189: INFO: Pod "pod-configmaps-b8194891-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032231095s
May  2 04:14:07.206: INFO: Pod "pod-configmaps-b8194891-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049033872s
May  2 04:14:09.222: INFO: Pod "pod-configmaps-b8194891-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065362273s
May  2 04:14:11.239: INFO: Pod "pod-configmaps-b8194891-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082022914s
May  2 04:14:13.255: INFO: Pod "pod-configmaps-b8194891-6c90-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098877698s
STEP: Saw pod success
May  2 04:14:13.256: INFO: Pod "pod-configmaps-b8194891-6c90-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:14:13.271: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-configmaps-b8194891-6c90-11e9-97f0-0a58ac103caa container configmap-volume-test: <nil>
STEP: delete the pod
May  2 04:14:13.316: INFO: Waiting for pod pod-configmaps-b8194891-6c90-11e9-97f0-0a58ac103caa to disappear
May  2 04:14:13.331: INFO: Pod pod-configmaps-b8194891-6c90-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:14:13.332: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cdpdn" for this suite.
May  2 04:14:19.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:14:20.171: INFO: namespace: e2e-tests-configmap-cdpdn, resource: packagemanifests, items remaining: 1
May  2 04:14:20.754: INFO: namespace: e2e-tests-configmap-cdpdn, resource: bindings, ignored listing per whitelist
May  2 04:14:21.456: INFO: namespace: e2e-tests-configmap-cdpdn no longer exists
May  2 04:14:21.473: INFO: namespace: e2e-tests-configmap-cdpdn, total namespaces: 47, active: 47, terminating: 0
May  2 04:14:21.488: INFO: namespace e2e-tests-configmap-cdpdn deletion completed in 8.114649619s

â€¢ [SLOW TEST:19.314 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:14:21.488: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May  2 04:14:22.468: INFO: Waiting up to 5m0s for pod "downward-api-c39c2a0d-6c90-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-downward-api-zgkt7" to be "success or failure"
May  2 04:14:22.484: INFO: Pod "downward-api-c39c2a0d-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.563841ms
May  2 04:14:24.500: INFO: Pod "downward-api-c39c2a0d-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031954191s
May  2 04:14:26.518: INFO: Pod "downward-api-c39c2a0d-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049936966s
May  2 04:14:28.534: INFO: Pod "downward-api-c39c2a0d-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06631838s
May  2 04:14:30.551: INFO: Pod "downward-api-c39c2a0d-6c90-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082639264s
May  2 04:14:32.567: INFO: Pod "downward-api-c39c2a0d-6c90-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098826113s
STEP: Saw pod success
May  2 04:14:32.567: INFO: Pod "downward-api-c39c2a0d-6c90-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:14:32.583: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod downward-api-c39c2a0d-6c90-11e9-97f0-0a58ac103caa container dapi-container: <nil>
STEP: delete the pod
May  2 04:14:32.626: INFO: Waiting for pod downward-api-c39c2a0d-6c90-11e9-97f0-0a58ac103caa to disappear
May  2 04:14:32.641: INFO: Pod downward-api-c39c2a0d-6c90-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:14:32.641: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zgkt7" for this suite.
May  2 04:14:38.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:14:39.500: INFO: namespace: e2e-tests-downward-api-zgkt7, resource: packagemanifests, items remaining: 1
May  2 04:14:40.503: INFO: namespace: e2e-tests-downward-api-zgkt7, resource: bindings, ignored listing per whitelist
May  2 04:14:40.769: INFO: namespace: e2e-tests-downward-api-zgkt7 no longer exists
May  2 04:14:40.786: INFO: namespace: e2e-tests-downward-api-zgkt7, total namespaces: 47, active: 47, terminating: 0
May  2 04:14:40.801: INFO: namespace e2e-tests-downward-api-zgkt7 deletion completed in 8.117123487s

â€¢ [SLOW TEST:19.312 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:14:40.801: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May  2 04:14:59.896: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  2 04:14:59.911: INFO: Pod pod-with-prestop-exec-hook still exists
May  2 04:15:01.911: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  2 04:15:01.928: INFO: Pod pod-with-prestop-exec-hook still exists
May  2 04:15:03.911: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  2 04:15:03.928: INFO: Pod pod-with-prestop-exec-hook still exists
May  2 04:15:05.911: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  2 04:15:05.928: INFO: Pod pod-with-prestop-exec-hook still exists
May  2 04:15:07.911: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  2 04:15:07.928: INFO: Pod pod-with-prestop-exec-hook still exists
May  2 04:15:09.911: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  2 04:15:09.928: INFO: Pod pod-with-prestop-exec-hook still exists
May  2 04:15:11.911: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  2 04:15:11.928: INFO: Pod pod-with-prestop-exec-hook still exists
May  2 04:15:13.911: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  2 04:15:13.928: INFO: Pod pod-with-prestop-exec-hook still exists
May  2 04:15:15.911: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  2 04:15:15.928: INFO: Pod pod-with-prestop-exec-hook still exists
May  2 04:15:17.911: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  2 04:15:17.928: INFO: Pod pod-with-prestop-exec-hook still exists
May  2 04:15:19.911: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  2 04:15:19.928: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:15:20.004: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-r48vw" for this suite.
May  2 04:15:44.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:15:45.062: INFO: namespace: e2e-tests-container-lifecycle-hook-r48vw, resource: bindings, ignored listing per whitelist
May  2 04:15:45.498: INFO: namespace: e2e-tests-container-lifecycle-hook-r48vw, resource: packagemanifests, items remaining: 1
May  2 04:15:46.131: INFO: namespace: e2e-tests-container-lifecycle-hook-r48vw no longer exists
May  2 04:15:46.148: INFO: namespace: e2e-tests-container-lifecycle-hook-r48vw, total namespaces: 47, active: 47, terminating: 0
May  2 04:15:46.163: INFO: namespace e2e-tests-container-lifecycle-hook-r48vw deletion completed in 26.116166875s

â€¢ [SLOW TEST:65.362 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:15:46.163: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0502 04:15:57.224871   12606 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  2 04:15:57.224: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:15:57.224: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-fwzwz" for this suite.
May  2 04:16:03.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:16:04.033: INFO: namespace: e2e-tests-gc-fwzwz, resource: packagemanifests, items remaining: 1
May  2 04:16:04.416: INFO: namespace: e2e-tests-gc-fwzwz, resource: bindings, ignored listing per whitelist
May  2 04:16:05.328: INFO: namespace: e2e-tests-gc-fwzwz no longer exists
May  2 04:16:05.345: INFO: namespace: e2e-tests-gc-fwzwz, total namespaces: 47, active: 47, terminating: 0
May  2 04:16:05.361: INFO: namespace e2e-tests-gc-fwzwz deletion completed in 8.119790393s

â€¢ [SLOW TEST:19.198 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:16:05.361: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May  2 04:16:06.396: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-wgmhb'
May  2 04:16:06.587: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  2 04:16:06.587: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May  2 04:16:06.619: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-r85kg]
May  2 04:16:06.619: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-r85kg" in namespace "e2e-tests-kubectl-wgmhb" to be "running and ready"
May  2 04:16:06.634: INFO: Pod "e2e-test-nginx-rc-r85kg": Phase="Pending", Reason="", readiness=false. Elapsed: 15.241238ms
May  2 04:16:08.650: INFO: Pod "e2e-test-nginx-rc-r85kg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031426797s
May  2 04:16:10.666: INFO: Pod "e2e-test-nginx-rc-r85kg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047353941s
May  2 04:16:12.683: INFO: Pod "e2e-test-nginx-rc-r85kg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063641599s
May  2 04:16:14.700: INFO: Pod "e2e-test-nginx-rc-r85kg": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081127925s
May  2 04:16:16.718: INFO: Pod "e2e-test-nginx-rc-r85kg": Phase="Running", Reason="", readiness=true. Elapsed: 10.099136953s
May  2 04:16:16.718: INFO: Pod "e2e-test-nginx-rc-r85kg" satisfied condition "running and ready"
May  2 04:16:16.718: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-r85kg]
May  2 04:16:16.719: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-wgmhb'
May  2 04:16:16.914: INFO: stderr: ""
May  2 04:16:16.914: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
May  2 04:16:16.914: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-wgmhb'
May  2 04:16:17.093: INFO: stderr: ""
May  2 04:16:17.093: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:16:17.094: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wgmhb" for this suite.
May  2 04:16:37.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:16:38.388: INFO: namespace: e2e-tests-kubectl-wgmhb, resource: bindings, ignored listing per whitelist
May  2 04:16:38.508: INFO: namespace: e2e-tests-kubectl-wgmhb, resource: packagemanifests, items remaining: 1
May  2 04:16:39.222: INFO: namespace: e2e-tests-kubectl-wgmhb no longer exists
May  2 04:16:39.239: INFO: namespace: e2e-tests-kubectl-wgmhb, total namespaces: 47, active: 47, terminating: 0
May  2 04:16:39.255: INFO: namespace e2e-tests-kubectl-wgmhb deletion completed in 22.117919972s

â€¢ [SLOW TEST:33.894 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:16:39.255: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:16:46.452: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-p45x2" for this suite.
May  2 04:16:52.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:16:53.320: INFO: namespace: e2e-tests-namespaces-p45x2, resource: bindings, ignored listing per whitelist
May  2 04:16:53.456: INFO: namespace: e2e-tests-namespaces-p45x2, resource: packagemanifests, items remaining: 1
May  2 04:16:54.579: INFO: namespace: e2e-tests-namespaces-p45x2 no longer exists
May  2 04:16:54.597: INFO: namespace: e2e-tests-namespaces-p45x2, total namespaces: 48, active: 48, terminating: 0
May  2 04:16:54.612: INFO: namespace e2e-tests-namespaces-p45x2 deletion completed in 8.116385427s
STEP: Destroying namespace "e2e-tests-nsdeletetest-fdx5s" for this suite.
May  2 04:16:54.629: INFO: Namespace e2e-tests-nsdeletetest-fdx5s was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-dv4gz" for this suite.
May  2 04:17:00.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:17:01.947: INFO: namespace: e2e-tests-nsdeletetest-dv4gz, resource: bindings, ignored listing per whitelist
May  2 04:17:02.140: INFO: namespace: e2e-tests-nsdeletetest-dv4gz, resource: packagemanifests, items remaining: 1
May  2 04:17:02.713: INFO: namespace: e2e-tests-nsdeletetest-dv4gz no longer exists
May  2 04:17:02.730: INFO: namespace: e2e-tests-nsdeletetest-dv4gz, total namespaces: 47, active: 47, terminating: 0
May  2 04:17:02.745: INFO: namespace e2e-tests-nsdeletetest-dv4gz deletion completed in 8.116549362s

â€¢ [SLOW TEST:23.491 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:17:02.745: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-23bd5fc0-6c91-11e9-97f0-0a58ac103caa
STEP: Creating a pod to test consume configMaps
May  2 04:17:03.776: INFO: Waiting up to 5m0s for pod "pod-configmaps-23c03ea3-6c91-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-configmap-f75vc" to be "success or failure"
May  2 04:17:03.792: INFO: Pod "pod-configmaps-23c03ea3-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.747812ms
May  2 04:17:05.808: INFO: Pod "pod-configmaps-23c03ea3-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032186212s
May  2 04:17:07.825: INFO: Pod "pod-configmaps-23c03ea3-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04870657s
May  2 04:17:09.842: INFO: Pod "pod-configmaps-23c03ea3-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06522311s
May  2 04:17:11.858: INFO: Pod "pod-configmaps-23c03ea3-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081734179s
May  2 04:17:13.874: INFO: Pod "pod-configmaps-23c03ea3-6c91-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098124549s
STEP: Saw pod success
May  2 04:17:13.874: INFO: Pod "pod-configmaps-23c03ea3-6c91-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:17:13.890: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-configmaps-23c03ea3-6c91-11e9-97f0-0a58ac103caa container configmap-volume-test: <nil>
STEP: delete the pod
May  2 04:17:13.934: INFO: Waiting for pod pod-configmaps-23c03ea3-6c91-11e9-97f0-0a58ac103caa to disappear
May  2 04:17:13.949: INFO: Pod pod-configmaps-23c03ea3-6c91-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:17:13.949: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-f75vc" for this suite.
May  2 04:17:20.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:17:20.789: INFO: namespace: e2e-tests-configmap-f75vc, resource: packagemanifests, items remaining: 1
May  2 04:17:21.551: INFO: namespace: e2e-tests-configmap-f75vc, resource: bindings, ignored listing per whitelist
May  2 04:17:22.076: INFO: namespace: e2e-tests-configmap-f75vc no longer exists
May  2 04:17:22.093: INFO: namespace: e2e-tests-configmap-f75vc, total namespaces: 47, active: 47, terminating: 0
May  2 04:17:22.109: INFO: namespace e2e-tests-configmap-f75vc deletion completed in 8.117031611s

â€¢ [SLOW TEST:19.364 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:17:22.109: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May  2 04:17:23.048: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-zdjj7'
May  2 04:17:23.227: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  2 04:17:23.227: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
May  2 04:17:23.264: INFO: scanned /tmp/home for discovery docs: <nil>
May  2 04:17:23.264: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-zdjj7'
May  2 04:17:40.653: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May  2 04:17:40.653: INFO: stdout: "Created e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443\nScaling up e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May  2 04:17:40.653: INFO: stdout: "Created e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443\nScaling up e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May  2 04:17:40.654: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-zdjj7'
May  2 04:17:40.826: INFO: stderr: ""
May  2 04:17:40.826: INFO: stdout: "e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443-mxkbb "
May  2 04:17:40.826: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443-mxkbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zdjj7'
May  2 04:17:40.982: INFO: stderr: ""
May  2 04:17:40.982: INFO: stdout: "true"
May  2 04:17:40.982: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443-mxkbb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zdjj7'
May  2 04:17:41.138: INFO: stderr: ""
May  2 04:17:41.138: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
May  2 04:17:41.138: INFO: e2e-test-nginx-rc-80e3d175815fe2868bf13a7388810443-mxkbb is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
May  2 04:17:41.138: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-zdjj7'
May  2 04:17:41.306: INFO: stderr: ""
May  2 04:17:41.306: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:17:41.306: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zdjj7" for this suite.
May  2 04:17:47.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:17:48.951: INFO: namespace: e2e-tests-kubectl-zdjj7, resource: bindings, ignored listing per whitelist
May  2 04:17:49.023: INFO: namespace: e2e-tests-kubectl-zdjj7, resource: packagemanifests, items remaining: 1
May  2 04:17:49.435: INFO: namespace: e2e-tests-kubectl-zdjj7 no longer exists
May  2 04:17:49.452: INFO: namespace: e2e-tests-kubectl-zdjj7, total namespaces: 47, active: 47, terminating: 0
May  2 04:17:49.467: INFO: namespace e2e-tests-kubectl-zdjj7 deletion completed in 8.118573145s

â€¢ [SLOW TEST:27.358 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:17:49.468: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
May  2 04:17:50.427: INFO: namespace e2e-tests-kubectl-5dqb9
May  2 04:17:50.427: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-5dqb9'
May  2 04:17:50.862: INFO: stderr: ""
May  2 04:17:50.862: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May  2 04:17:51.878: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:17:51.878: INFO: Found 0 / 1
May  2 04:17:52.878: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:17:52.878: INFO: Found 0 / 1
May  2 04:17:53.878: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:17:53.878: INFO: Found 0 / 1
May  2 04:17:54.878: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:17:54.878: INFO: Found 0 / 1
May  2 04:17:55.878: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:17:55.878: INFO: Found 0 / 1
May  2 04:17:56.878: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:17:56.878: INFO: Found 0 / 1
May  2 04:17:57.878: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:17:57.879: INFO: Found 0 / 1
May  2 04:17:58.878: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:17:58.878: INFO: Found 0 / 1
May  2 04:17:59.879: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:17:59.879: INFO: Found 0 / 1
May  2 04:18:00.878: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:18:00.878: INFO: Found 1 / 1
May  2 04:18:00.878: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  2 04:18:00.894: INFO: Selector matched 1 pods for map[app:redis]
May  2 04:18:00.894: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  2 04:18:00.894: INFO: wait on redis-master startup in e2e-tests-kubectl-5dqb9 
May  2 04:18:00.894: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs redis-master-pk9gs redis-master --namespace=e2e-tests-kubectl-5dqb9'
May  2 04:18:01.069: INFO: stderr: ""
May  2 04:18:01.069: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 02 May 04:17:59.354 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 02 May 04:17:59.354 # Server started, Redis version 3.2.12\n1:M 02 May 04:17:59.355 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 02 May 04:17:59.355 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May  2 04:18:01.069: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-5dqb9'
May  2 04:18:01.263: INFO: stderr: ""
May  2 04:18:01.263: INFO: stdout: "service/rm2 exposed\n"
May  2 04:18:01.279: INFO: Service rm2 in namespace e2e-tests-kubectl-5dqb9 found.
STEP: exposing service
May  2 04:18:03.312: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-5dqb9'
May  2 04:18:03.492: INFO: stderr: ""
May  2 04:18:03.492: INFO: stdout: "service/rm3 exposed\n"
May  2 04:18:03.510: INFO: Service rm3 in namespace e2e-tests-kubectl-5dqb9 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:18:05.542: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5dqb9" for this suite.
May  2 04:18:29.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:18:30.431: INFO: namespace: e2e-tests-kubectl-5dqb9, resource: packagemanifests, items remaining: 1
May  2 04:18:31.280: INFO: namespace: e2e-tests-kubectl-5dqb9, resource: bindings, ignored listing per whitelist
May  2 04:18:31.670: INFO: namespace: e2e-tests-kubectl-5dqb9 no longer exists
May  2 04:18:31.687: INFO: namespace: e2e-tests-kubectl-5dqb9, total namespaces: 47, active: 47, terminating: 0
May  2 04:18:31.703: INFO: namespace e2e-tests-kubectl-5dqb9 deletion completed in 26.118457206s

â€¢ [SLOW TEST:42.236 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:18:31.703: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 04:18:32.658: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig version --client'
May  2 04:18:32.721: INFO: stderr: ""
May  2 04:18:32.721: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13+\", GitVersion:\"v1.13.6-beta.0.64+300fedeffa6d25\", GitCommit:\"300fedeffa6d25789599e9cdf5b27cdf2f32df24\", GitTreeState:\"clean\", BuildDate:\"2019-05-02T02:04:22Z\", GoVersion:\"go1.11.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May  2 04:18:32.736: INFO: Not supported for server versions before "1.13.6-beta.0.64+300fedeffa6d25"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:18:32.737: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8lx2b" for this suite.
May  2 04:18:38.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:18:39.375: INFO: namespace: e2e-tests-kubectl-8lx2b, resource: bindings, ignored listing per whitelist
May  2 04:18:40.779: INFO: namespace: e2e-tests-kubectl-8lx2b, resource: packagemanifests, items remaining: 1
May  2 04:18:40.843: INFO: namespace: e2e-tests-kubectl-8lx2b no longer exists
May  2 04:18:40.860: INFO: namespace: e2e-tests-kubectl-8lx2b, total namespaces: 47, active: 47, terminating: 0
May  2 04:18:40.881: INFO: namespace e2e-tests-kubectl-8lx2b deletion completed in 8.126672751s

S [SKIPPING] [9.177 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance] [It]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

    May  2 04:18:32.736: Not supported for server versions before "1.13.6-beta.0.64+300fedeffa6d25"

    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:18:40.881: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 04:18:41.911: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig version'
May  2 04:18:42.231: INFO: stderr: ""
May  2 04:18:42.231: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13+\", GitVersion:\"v1.13.6-beta.0.64+300fedeffa6d25\", GitCommit:\"300fedeffa6d25789599e9cdf5b27cdf2f32df24\", GitTreeState:\"clean\", BuildDate:\"2019-05-02T02:04:22Z\", GoVersion:\"go1.11.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13+\", GitVersion:\"v1.13.4+b1e7641\", GitCommit:\"b1e7641\", GitTreeState:\"clean\", BuildDate:\"2019-05-02T01:08:35Z\", GoVersion:\"go1.11.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:18:42.232: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nnfzs" for this suite.
May  2 04:18:48.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:18:49.170: INFO: namespace: e2e-tests-kubectl-nnfzs, resource: packagemanifests, items remaining: 1
May  2 04:18:49.875: INFO: namespace: e2e-tests-kubectl-nnfzs, resource: bindings, ignored listing per whitelist
May  2 04:18:50.370: INFO: namespace: e2e-tests-kubectl-nnfzs no longer exists
May  2 04:18:50.387: INFO: namespace: e2e-tests-kubectl-nnfzs, total namespaces: 47, active: 47, terminating: 0
May  2 04:18:50.409: INFO: namespace e2e-tests-kubectl-nnfzs deletion completed in 8.155411803s

â€¢ [SLOW TEST:9.528 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:18:50.409: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-jhtr
STEP: Creating a pod to test atomic-volume-subpath
May  2 04:18:51.575: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jhtr" in namespace "e2e-tests-subpath-78f2c" to be "success or failure"
May  2 04:18:51.591: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Pending", Reason="", readiness=false. Elapsed: 15.532357ms
May  2 04:18:53.607: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032169992s
May  2 04:18:55.624: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048658706s
May  2 04:18:57.640: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064611074s
May  2 04:18:59.656: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080456601s
May  2 04:19:01.672: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Running", Reason="", readiness=false. Elapsed: 10.097037907s
May  2 04:19:03.692: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Running", Reason="", readiness=false. Elapsed: 12.116212933s
May  2 04:19:05.708: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Running", Reason="", readiness=false. Elapsed: 14.132785025s
May  2 04:19:07.725: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Running", Reason="", readiness=false. Elapsed: 16.149607006s
May  2 04:19:09.741: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Running", Reason="", readiness=false. Elapsed: 18.165978823s
May  2 04:19:11.758: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Running", Reason="", readiness=false. Elapsed: 20.182194814s
May  2 04:19:13.774: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Running", Reason="", readiness=false. Elapsed: 22.198702123s
May  2 04:19:15.791: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Running", Reason="", readiness=false. Elapsed: 24.215504211s
May  2 04:19:17.810: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Running", Reason="", readiness=false. Elapsed: 26.234910921s
May  2 04:19:19.827: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Running", Reason="", readiness=false. Elapsed: 28.251564575s
May  2 04:19:21.843: INFO: Pod "pod-subpath-test-configmap-jhtr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.267833589s
STEP: Saw pod success
May  2 04:19:21.843: INFO: Pod "pod-subpath-test-configmap-jhtr" satisfied condition "success or failure"
May  2 04:19:21.859: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-subpath-test-configmap-jhtr container test-container-subpath-configmap-jhtr: <nil>
STEP: delete the pod
May  2 04:19:21.909: INFO: Waiting for pod pod-subpath-test-configmap-jhtr to disappear
May  2 04:19:21.925: INFO: Pod pod-subpath-test-configmap-jhtr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jhtr
May  2 04:19:21.925: INFO: Deleting pod "pod-subpath-test-configmap-jhtr" in namespace "e2e-tests-subpath-78f2c"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:19:21.941: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-78f2c" for this suite.
May  2 04:19:28.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:19:28.992: INFO: namespace: e2e-tests-subpath-78f2c, resource: packagemanifests, items remaining: 1
May  2 04:19:29.709: INFO: namespace: e2e-tests-subpath-78f2c, resource: bindings, ignored listing per whitelist
May  2 04:19:30.070: INFO: namespace: e2e-tests-subpath-78f2c no longer exists
May  2 04:19:30.087: INFO: namespace: e2e-tests-subpath-78f2c, total namespaces: 47, active: 47, terminating: 0
May  2 04:19:30.103: INFO: namespace e2e-tests-subpath-78f2c deletion completed in 8.119034202s

â€¢ [SLOW TEST:39.694 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:19:30.103: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May  2 04:19:41.195: INFO: Waiting up to 5m0s for pod "client-envvars-8196d2c5-6c91-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-pods-87s7p" to be "success or failure"
May  2 04:19:41.212: INFO: Pod "client-envvars-8196d2c5-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.697516ms
May  2 04:19:43.229: INFO: Pod "client-envvars-8196d2c5-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033133958s
May  2 04:19:45.246: INFO: Pod "client-envvars-8196d2c5-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050355978s
May  2 04:19:47.266: INFO: Pod "client-envvars-8196d2c5-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06951588s
May  2 04:19:49.282: INFO: Pod "client-envvars-8196d2c5-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085561783s
May  2 04:19:51.299: INFO: Pod "client-envvars-8196d2c5-6c91-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.102545969s
STEP: Saw pod success
May  2 04:19:51.299: INFO: Pod "client-envvars-8196d2c5-6c91-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:19:51.315: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod client-envvars-8196d2c5-6c91-11e9-97f0-0a58ac103caa container env3cont: <nil>
STEP: delete the pod
May  2 04:19:51.358: INFO: Waiting for pod client-envvars-8196d2c5-6c91-11e9-97f0-0a58ac103caa to disappear
May  2 04:19:51.375: INFO: Pod client-envvars-8196d2c5-6c91-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:19:51.375: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-87s7p" for this suite.
May  2 04:20:31.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:20:32.191: INFO: namespace: e2e-tests-pods-87s7p, resource: bindings, ignored listing per whitelist
May  2 04:20:33.027: INFO: namespace: e2e-tests-pods-87s7p, resource: packagemanifests, items remaining: 1
May  2 04:20:33.503: INFO: namespace: e2e-tests-pods-87s7p no longer exists
May  2 04:20:33.521: INFO: namespace: e2e-tests-pods-87s7p, total namespaces: 47, active: 47, terminating: 0
May  2 04:20:33.537: INFO: namespace e2e-tests-pods-87s7p deletion completed in 42.117601566s

â€¢ [SLOW TEST:63.434 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May  2 04:20:33.537: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
May  2 04:20:34.491: INFO: Waiting up to 5m0s for pod "pod-a15a7088-6c91-11e9-97f0-0a58ac103caa" in namespace "e2e-tests-emptydir-p25tt" to be "success or failure"
May  2 04:20:34.506: INFO: Pod "pod-a15a7088-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.957011ms
May  2 04:20:36.522: INFO: Pod "pod-a15a7088-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030873374s
May  2 04:20:38.537: INFO: Pod "pod-a15a7088-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046360886s
May  2 04:20:40.553: INFO: Pod "pod-a15a7088-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.062459434s
May  2 04:20:42.569: INFO: Pod "pod-a15a7088-6c91-11e9-97f0-0a58ac103caa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.078209534s
May  2 04:20:44.586: INFO: Pod "pod-a15a7088-6c91-11e9-97f0-0a58ac103caa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.094746023s
STEP: Saw pod success
May  2 04:20:44.586: INFO: Pod "pod-a15a7088-6c91-11e9-97f0-0a58ac103caa" satisfied condition "success or failure"
May  2 04:20:44.601: INFO: Trying to get logs from node ip-10-0-135-216.ec2.internal pod pod-a15a7088-6c91-11e9-97f0-0a58ac103caa container test-container: <nil>
STEP: delete the pod
May  2 04:20:44.643: INFO: Waiting for pod pod-a15a7088-6c91-11e9-97f0-0a58ac103caa to disappear
May  2 04:20:44.658: INFO: Pod pod-a15a7088-6c91-11e9-97f0-0a58ac103caa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May  2 04:20:44.658: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-p25tt" for this suite.
May  2 04:20:50.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  2 04:20:51.681: INFO: namespace: e2e-tests-emptydir-p25tt, resource: bindings, ignored listing per whitelist
May  2 04:20:52.183: INFO: namespace: e2e-tests-emptydir-p25tt, resource: packagemanifests, items remaining: 1
May  2 04:20:52.784: INFO: namespace: e2e-tests-emptydir-p25tt no longer exists
May  2 04:20:52.808: INFO: namespace: e2e-tests-emptydir-p25tt, total namespaces: 47, active: 47, terminating: 0
May  2 04:20:52.824: INFO: namespace e2e-tests-emptydir-p25tt deletion completed in 8.124551549s

â€¢ [SLOW TEST:19.287 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSMay  2 04:20:52.824: INFO: Running AfterSuite actions on all nodes
May  2 04:20:52.824: INFO: Running AfterSuite actions on node 1
May  2 04:20:52.824: INFO: Dumping logs locally to: /tmp/artifacts
May  2 04:20:52.825: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

Ran 199 of 2162 Specs in 7949.370 seconds
SUCCESS! -- 199 Passed | 0 Failed | 0 Pending | 1963 Skipped PASS
