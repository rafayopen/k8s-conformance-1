I0223 07:11:22.902698      16 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-203633079
I0223 07:11:22.902844      16 e2e.go:224] Starting e2e run "396235de-373a-11e9-a63e-52687f37ce9e" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1550905882 - Will randomize all specs
Will run 201 of 1946 specs

Feb 23 07:11:23.336: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:11:23.340: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 23 07:11:26.352: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 23 07:11:26.382: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 23 07:11:26.382: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Feb 23 07:11:26.382: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 23 07:11:26.389: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 23 07:11:26.389: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Feb 23 07:11:26.389: INFO: e2e test version: v1.13.0
Feb 23 07:11:26.390: INFO: kube-apiserver version: v1.13.2
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:11:26.391: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
Feb 23 07:11:26.461: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 23 07:11:26.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-s5k7l'
Feb 23 07:11:26.696: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 07:11:26.696: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 23 07:11:26.709: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-2l698]
Feb 23 07:11:26.709: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-2l698" in namespace "e2e-tests-kubectl-s5k7l" to be "running and ready"
Feb 23 07:11:26.713: INFO: Pod "e2e-test-nginx-rc-2l698": Phase="Pending", Reason="", readiness=false. Elapsed: 3.845465ms
Feb 23 07:11:28.717: INFO: Pod "e2e-test-nginx-rc-2l698": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007661574s
Feb 23 07:11:30.720: INFO: Pod "e2e-test-nginx-rc-2l698": Phase="Running", Reason="", readiness=true. Elapsed: 4.011232089s
Feb 23 07:11:30.720: INFO: Pod "e2e-test-nginx-rc-2l698" satisfied condition "running and ready"
Feb 23 07:11:30.720: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-2l698]
Feb 23 07:11:30.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-s5k7l'
Feb 23 07:11:30.816: INFO: stderr: ""
Feb 23 07:11:30.816: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Feb 23 07:11:30.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-s5k7l'
Feb 23 07:11:30.896: INFO: stderr: ""
Feb 23 07:11:30.896: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:11:30.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-s5k7l" for this suite.
Feb 23 07:11:52.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:11:52.965: INFO: namespace: e2e-tests-kubectl-s5k7l, resource: bindings, ignored listing per whitelist
Feb 23 07:11:53.022: INFO: namespace e2e-tests-kubectl-s5k7l deletion completed in 22.121149441s

• [SLOW TEST:26.631 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:11:53.022: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4bccc81a-373a-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 07:11:53.096: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4bcd6051-373a-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-h4mm5" to be "success or failure"
Feb 23 07:11:53.099: INFO: Pod "pod-projected-configmaps-4bcd6051-373a-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.774387ms
Feb 23 07:11:55.104: INFO: Pod "pod-projected-configmaps-4bcd6051-373a-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008478948s
Feb 23 07:11:57.108: INFO: Pod "pod-projected-configmaps-4bcd6051-373a-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012220737s
STEP: Saw pod success
Feb 23 07:11:57.108: INFO: Pod "pod-projected-configmaps-4bcd6051-373a-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:11:57.111: INFO: Trying to get logs from node kube-node-01 pod pod-projected-configmaps-4bcd6051-373a-11e9-a63e-52687f37ce9e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 07:11:57.142: INFO: Waiting for pod pod-projected-configmaps-4bcd6051-373a-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:11:57.146: INFO: Pod pod-projected-configmaps-4bcd6051-373a-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:11:57.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h4mm5" for this suite.
Feb 23 07:12:03.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:12:03.266: INFO: namespace: e2e-tests-projected-h4mm5, resource: bindings, ignored listing per whitelist
Feb 23 07:12:03.286: INFO: namespace e2e-tests-projected-h4mm5 deletion completed in 6.136613683s

• [SLOW TEST:10.264 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:12:03.286: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-x4cfg
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 23 07:12:03.366: INFO: Found 0 stateful pods, waiting for 3
Feb 23 07:12:13.374: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 07:12:13.374: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 07:12:13.374: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 23 07:12:13.402: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 23 07:12:23.438: INFO: Updating stateful set ss2
Feb 23 07:12:23.445: INFO: Waiting for Pod e2e-tests-statefulset-x4cfg/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 23 07:12:33.490: INFO: Found 2 stateful pods, waiting for 3
Feb 23 07:12:43.494: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 07:12:43.494: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 07:12:43.494: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 23 07:12:43.525: INFO: Updating stateful set ss2
Feb 23 07:12:43.532: INFO: Waiting for Pod e2e-tests-statefulset-x4cfg/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 23 07:12:53.557: INFO: Updating stateful set ss2
Feb 23 07:12:53.564: INFO: Waiting for StatefulSet e2e-tests-statefulset-x4cfg/ss2 to complete update
Feb 23 07:12:53.564: INFO: Waiting for Pod e2e-tests-statefulset-x4cfg/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 23 07:13:03.572: INFO: Deleting all statefulset in ns e2e-tests-statefulset-x4cfg
Feb 23 07:13:03.575: INFO: Scaling statefulset ss2 to 0
Feb 23 07:13:33.592: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 07:13:33.596: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:13:33.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-x4cfg" for this suite.
Feb 23 07:13:39.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:13:39.753: INFO: namespace: e2e-tests-statefulset-x4cfg, resource: bindings, ignored listing per whitelist
Feb 23 07:13:39.753: INFO: namespace e2e-tests-statefulset-x4cfg deletion completed in 6.136945999s

• [SLOW TEST:96.467 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:13:39.753: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:13:39.834: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 23 07:13:39.841: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-2kwjr/daemonsets","resourceVersion":"2722"},"items":null}

Feb 23 07:13:39.844: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-2kwjr/pods","resourceVersion":"2722"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:13:39.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-2kwjr" for this suite.
Feb 23 07:13:45.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:13:45.878: INFO: namespace: e2e-tests-daemonsets-2kwjr, resource: bindings, ignored listing per whitelist
Feb 23 07:13:45.976: INFO: namespace e2e-tests-daemonsets-2kwjr deletion completed in 6.119505777s

S [SKIPPING] [6.223 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 23 07:13:39.834: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:13:45.976: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-sp9r6/configmap-test-8f211016-373a-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 07:13:46.055: INFO: Waiting up to 5m0s for pod "pod-configmaps-8f21a4f1-373a-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-configmap-sp9r6" to be "success or failure"
Feb 23 07:13:46.061: INFO: Pod "pod-configmaps-8f21a4f1-373a-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.318921ms
Feb 23 07:13:48.065: INFO: Pod "pod-configmaps-8f21a4f1-373a-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009413424s
STEP: Saw pod success
Feb 23 07:13:48.065: INFO: Pod "pod-configmaps-8f21a4f1-373a-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:13:48.068: INFO: Trying to get logs from node kube-node-02 pod pod-configmaps-8f21a4f1-373a-11e9-a63e-52687f37ce9e container env-test: <nil>
STEP: delete the pod
Feb 23 07:13:48.088: INFO: Waiting for pod pod-configmaps-8f21a4f1-373a-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:13:48.090: INFO: Pod pod-configmaps-8f21a4f1-373a-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:13:48.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sp9r6" for this suite.
Feb 23 07:13:54.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:13:54.128: INFO: namespace: e2e-tests-configmap-sp9r6, resource: bindings, ignored listing per whitelist
Feb 23 07:13:54.226: INFO: namespace e2e-tests-configmap-sp9r6 deletion completed in 6.132088312s

• [SLOW TEST:8.250 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:13:54.227: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 23 07:13:56.825: INFO: Successfully updated pod "pod-update-940b5c7b-373a-11e9-a63e-52687f37ce9e"
STEP: verifying the updated pod is in kubernetes
Feb 23 07:13:56.830: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:13:56.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-s5gs6" for this suite.
Feb 23 07:14:18.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:14:18.921: INFO: namespace: e2e-tests-pods-s5gs6, resource: bindings, ignored listing per whitelist
Feb 23 07:14:18.973: INFO: namespace e2e-tests-pods-s5gs6 deletion completed in 22.139232157s

• [SLOW TEST:24.746 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:14:18.973: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 23 07:14:19.048: INFO: Waiting up to 5m0s for pod "pod-a2cc010b-373a-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-g2fwj" to be "success or failure"
Feb 23 07:14:19.053: INFO: Pod "pod-a2cc010b-373a-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.719791ms
Feb 23 07:14:21.058: INFO: Pod "pod-a2cc010b-373a-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00929126s
STEP: Saw pod success
Feb 23 07:14:21.058: INFO: Pod "pod-a2cc010b-373a-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:14:21.061: INFO: Trying to get logs from node kube-node-02 pod pod-a2cc010b-373a-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 07:14:21.087: INFO: Waiting for pod pod-a2cc010b-373a-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:14:21.090: INFO: Pod pod-a2cc010b-373a-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:14:21.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-g2fwj" for this suite.
Feb 23 07:14:27.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:14:27.216: INFO: namespace: e2e-tests-emptydir-g2fwj, resource: bindings, ignored listing per whitelist
Feb 23 07:14:27.216: INFO: namespace e2e-tests-emptydir-g2fwj deletion completed in 6.120645623s

• [SLOW TEST:8.243 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:14:27.216: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 23 07:14:29.831: INFO: Successfully updated pod "annotationupdatea7b50352-373a-11e9-a63e-52687f37ce9e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:14:33.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kqx7m" for this suite.
Feb 23 07:14:55.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:14:55.981: INFO: namespace: e2e-tests-downward-api-kqx7m, resource: bindings, ignored listing per whitelist
Feb 23 07:14:55.986: INFO: namespace e2e-tests-downward-api-kqx7m deletion completed in 22.121437084s

• [SLOW TEST:28.769 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:14:55.986: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 23 07:14:56.053: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:14:58.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-v2ctz" for this suite.
Feb 23 07:15:04.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:15:04.138: INFO: namespace: e2e-tests-init-container-v2ctz, resource: bindings, ignored listing per whitelist
Feb 23 07:15:04.138: INFO: namespace e2e-tests-init-container-v2ctz deletion completed in 6.129327361s

• [SLOW TEST:8.152 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:15:04.138: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:15:04.204: INFO: Creating deployment "test-recreate-deployment"
Feb 23 07:15:04.209: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 23 07:15:04.215: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Feb 23 07:15:06.222: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 23 07:15:06.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686502904, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686502904, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686502904, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686502904, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 07:15:08.228: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 23 07:15:08.237: INFO: Updating deployment test-recreate-deployment
Feb 23 07:15:08.237: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 23 07:15:08.311: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-qr7g9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qr7g9/deployments/test-recreate-deployment,UID:bdb85725-373a-11e9-8d12-02951b420040,ResourceVersion:3066,Generation:2,CreationTimestamp:2019-02-23 07:15:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-23 07:15:08 +0000 UTC 2019-02-23 07:15:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-23 07:15:08 +0000 UTC 2019-02-23 07:15:04 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 23 07:15:08.315: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-qr7g9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qr7g9/replicasets/test-recreate-deployment-697fbf54bf,UID:c02529f0-373a-11e9-8d12-02951b420040,ResourceVersion:3062,Generation:1,CreationTimestamp:2019-02-23 07:15:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment bdb85725-373a-11e9-8d12-02951b420040 0xc001cf9f07 0xc001cf9f08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 23 07:15:08.315: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 23 07:15:08.315: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-qr7g9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qr7g9/replicasets/test-recreate-deployment-5dfdcc846d,UID:bdba0bc8-373a-11e9-8d12-02951b420040,ResourceVersion:3054,Generation:2,CreationTimestamp:2019-02-23 07:15:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment bdb85725-373a-11e9-8d12-02951b420040 0xc001cf9e57 0xc001cf9e58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 23 07:15:08.319: INFO: Pod "test-recreate-deployment-697fbf54bf-54qnq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-54qnq,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-qr7g9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qr7g9/pods/test-recreate-deployment-697fbf54bf-54qnq,UID:c025e17b-373a-11e9-8d12-02951b420040,ResourceVersion:3065,Generation:0,CreationTimestamp:2019-02-23 07:15:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf c02529f0-373a-11e9-8d12-02951b420040 0xc000f22d37 0xc000f22d38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vm858 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vm858,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vm858 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f22db0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f22dd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:15:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:15:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:15:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:15:08 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.103,PodIP:,StartTime:2019-02-23 07:15:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:15:08.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-qr7g9" for this suite.
Feb 23 07:15:14.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:15:14.428: INFO: namespace: e2e-tests-deployment-qr7g9, resource: bindings, ignored listing per whitelist
Feb 23 07:15:14.456: INFO: namespace e2e-tests-deployment-qr7g9 deletion completed in 6.133179497s

• [SLOW TEST:10.318 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:15:14.456: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-c3de6fb5-373a-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 07:15:14.539: INFO: Waiting up to 5m0s for pod "pod-configmaps-c3df3656-373a-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-configmap-5cfw2" to be "success or failure"
Feb 23 07:15:14.543: INFO: Pod "pod-configmaps-c3df3656-373a-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907265ms
Feb 23 07:15:16.547: INFO: Pod "pod-configmaps-c3df3656-373a-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008052791s
STEP: Saw pod success
Feb 23 07:15:16.547: INFO: Pod "pod-configmaps-c3df3656-373a-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:15:16.550: INFO: Trying to get logs from node kube-node-01 pod pod-configmaps-c3df3656-373a-11e9-a63e-52687f37ce9e container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 07:15:16.572: INFO: Waiting for pod pod-configmaps-c3df3656-373a-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:15:16.575: INFO: Pod pod-configmaps-c3df3656-373a-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:15:16.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5cfw2" for this suite.
Feb 23 07:15:22.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:15:22.612: INFO: namespace: e2e-tests-configmap-5cfw2, resource: bindings, ignored listing per whitelist
Feb 23 07:15:22.707: INFO: namespace e2e-tests-configmap-5cfw2 deletion completed in 6.127947406s

• [SLOW TEST:8.251 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:15:22.707: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 23 07:15:22.778: INFO: Waiting up to 5m0s for pod "downward-api-c8c85236-373a-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-r5vch" to be "success or failure"
Feb 23 07:15:22.784: INFO: Pod "downward-api-c8c85236-373a-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.163633ms
Feb 23 07:15:24.788: INFO: Pod "downward-api-c8c85236-373a-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009186226s
STEP: Saw pod success
Feb 23 07:15:24.788: INFO: Pod "downward-api-c8c85236-373a-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:15:24.791: INFO: Trying to get logs from node kube-node-02 pod downward-api-c8c85236-373a-11e9-a63e-52687f37ce9e container dapi-container: <nil>
STEP: delete the pod
Feb 23 07:15:24.810: INFO: Waiting for pod downward-api-c8c85236-373a-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:15:24.814: INFO: Pod downward-api-c8c85236-373a-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:15:24.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-r5vch" for this suite.
Feb 23 07:15:30.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:15:30.925: INFO: namespace: e2e-tests-downward-api-r5vch, resource: bindings, ignored listing per whitelist
Feb 23 07:15:30.936: INFO: namespace e2e-tests-downward-api-r5vch deletion completed in 6.118450345s

• [SLOW TEST:8.229 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:15:30.936: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 23 07:15:31.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-vbt9r'
Feb 23 07:15:31.084: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 07:15:31.084: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Feb 23 07:15:33.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-vbt9r'
Feb 23 07:15:33.214: INFO: stderr: ""
Feb 23 07:15:33.214: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:15:33.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vbt9r" for this suite.
Feb 23 07:15:39.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:15:39.262: INFO: namespace: e2e-tests-kubectl-vbt9r, resource: bindings, ignored listing per whitelist
Feb 23 07:15:39.338: INFO: namespace e2e-tests-kubectl-vbt9r deletion completed in 6.119102154s

• [SLOW TEST:8.402 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:15:39.338: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:15:39.402: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2b0a820-373a-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-n74mk" to be "success or failure"
Feb 23 07:15:39.405: INFO: Pod "downwardapi-volume-d2b0a820-373a-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.984588ms
Feb 23 07:15:41.408: INFO: Pod "downwardapi-volume-d2b0a820-373a-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006641208s
STEP: Saw pod success
Feb 23 07:15:41.408: INFO: Pod "downwardapi-volume-d2b0a820-373a-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:15:41.412: INFO: Trying to get logs from node kube-node-02 pod downwardapi-volume-d2b0a820-373a-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:15:41.432: INFO: Waiting for pod downwardapi-volume-d2b0a820-373a-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:15:41.435: INFO: Pod downwardapi-volume-d2b0a820-373a-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:15:41.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n74mk" for this suite.
Feb 23 07:15:47.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:15:47.499: INFO: namespace: e2e-tests-projected-n74mk, resource: bindings, ignored listing per whitelist
Feb 23 07:15:47.562: INFO: namespace e2e-tests-projected-n74mk deletion completed in 6.123435374s

• [SLOW TEST:8.224 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:15:47.563: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:15:49.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-qjmwt" for this suite.
Feb 23 07:16:35.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:16:35.694: INFO: namespace: e2e-tests-kubelet-test-qjmwt, resource: bindings, ignored listing per whitelist
Feb 23 07:16:35.791: INFO: namespace e2e-tests-kubelet-test-qjmwt deletion completed in 46.130531306s

• [SLOW TEST:48.228 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:16:35.791: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 23 07:16:35.862: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 07:16:35.872: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 07:16:35.875: INFO: 
Logging pods the kubelet thinks is on node kube-node-01 before test
Feb 23 07:16:35.883: INFO: kube-proxy-622xh from kube-system started at 2019-02-23 07:03:02 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.883: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 07:16:35.883: INFO: rook-agent-bm2h9 from rook-system started at 2019-02-23 07:03:42 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.883: INFO: 	Container rook-agent ready: true, restart count 0
Feb 23 07:16:35.883: INFO: rook-ceph-mon2-jdx9z from rook started at 2019-02-23 07:04:44 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.883: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Feb 23 07:16:35.883: INFO: rook-ceph-mgr0-65cb98fd4f-swbqs from rook started at 2019-02-23 07:04:58 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.883: INFO: 	Container rook-ceph-mgr0 ready: true, restart count 0
Feb 23 07:16:35.883: INFO: rook-ceph-osd-fxwdx from rook started at 2019-02-23 07:05:00 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.883: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Feb 23 07:16:35.883: INFO: sonobuoy-systemd-logs-daemon-set-b1c5aeee47bb4835-tp9jc from heptio-sonobuoy started at 2019-02-23 07:11:00 +0000 UTC (2 container statuses recorded)
Feb 23 07:16:35.883: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb 23 07:16:35.883: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 07:16:35.883: INFO: weave-net-vxfzf from kube-system started at 2019-02-23 07:03:02 +0000 UTC (2 container statuses recorded)
Feb 23 07:16:35.883: INFO: 	Container weave ready: true, restart count 1
Feb 23 07:16:35.883: INFO: 	Container weave-npc ready: true, restart count 0
Feb 23 07:16:35.883: INFO: rook-api-77bc4484c6-dfhs4 from rook started at 2019-02-23 07:04:58 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.883: INFO: 	Container rook-api ready: true, restart count 0
Feb 23 07:16:35.883: INFO: rook-ceph-mon0-ljg4v from rook started at 2019-02-23 07:03:45 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.883: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Feb 23 07:16:35.883: INFO: 
Logging pods the kubelet thinks is on node kube-node-02 before test
Feb 23 07:16:35.893: INFO: sonobuoy-systemd-logs-daemon-set-b1c5aeee47bb4835-42s5h from heptio-sonobuoy started at 2019-02-23 07:11:00 +0000 UTC (2 container statuses recorded)
Feb 23 07:16:35.893: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb 23 07:16:35.893: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 07:16:35.893: INFO: tiller-deploy-dbb85cb99-9gspb from kube-system started at 2019-02-23 07:02:59 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.893: INFO: 	Container tiller ready: true, restart count 0
Feb 23 07:16:35.893: INFO: kube-proxy-lc5r2 from kube-system started at 2019-02-23 07:02:59 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.893: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 07:16:35.893: INFO: weave-net-psnqt from kube-system started at 2019-02-23 07:02:59 +0000 UTC (2 container statuses recorded)
Feb 23 07:16:35.893: INFO: 	Container weave ready: true, restart count 0
Feb 23 07:16:35.893: INFO: 	Container weave-npc ready: true, restart count 0
Feb 23 07:16:35.893: INFO: rook-ceph-osd-mfkgj from rook started at 2019-02-23 07:05:00 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.893: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Feb 23 07:16:35.893: INFO: rook-operator-98c6d84c5-8dvfs from rook-system started at 2019-02-23 07:02:59 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.893: INFO: 	Container rook-operator ready: true, restart count 0
Feb 23 07:16:35.893: INFO: rook-agent-8lrhx from rook-system started at 2019-02-23 07:03:29 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.893: INFO: 	Container rook-agent ready: true, restart count 0
Feb 23 07:16:35.893: INFO: rook-ceph-mon1-cpwgw from rook started at 2019-02-23 07:04:38 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.893: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Feb 23 07:16:35.893: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-23 07:10:57 +0000 UTC (1 container statuses recorded)
Feb 23 07:16:35.893: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f5943c40-373a-11e9-a63e-52687f37ce9e 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-f5943c40-373a-11e9-a63e-52687f37ce9e off the node kube-node-02
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f5943c40-373a-11e9-a63e-52687f37ce9e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:16:39.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-jjst8" for this suite.
Feb 23 07:16:53.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:16:54.048: INFO: namespace: e2e-tests-sched-pred-jjst8, resource: bindings, ignored listing per whitelist
Feb 23 07:16:54.099: INFO: namespace e2e-tests-sched-pred-jjst8 deletion completed in 14.119570972s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:18.308 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:16:54.100: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-wd4j5
Feb 23 07:16:56.177: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-wd4j5
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 07:16:56.180: INFO: Initial restart count of pod liveness-http is 0
Feb 23 07:17:14.232: INFO: Restart count of pod e2e-tests-container-probe-wd4j5/liveness-http is now 1 (18.051828532s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:17:14.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-wd4j5" for this suite.
Feb 23 07:17:20.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:17:20.391: INFO: namespace: e2e-tests-container-probe-wd4j5, resource: bindings, ignored listing per whitelist
Feb 23 07:17:20.401: INFO: namespace e2e-tests-container-probe-wd4j5 deletion completed in 6.14815168s

• [SLOW TEST:26.302 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:17:20.401: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 23 07:17:20.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 cluster-info'
Feb 23 07:17:20.537: INFO: stderr: ""
Feb 23 07:17:20.537: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:17:20.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9pxnv" for this suite.
Feb 23 07:17:26.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:17:26.593: INFO: namespace: e2e-tests-kubectl-9pxnv, resource: bindings, ignored listing per whitelist
Feb 23 07:17:26.661: INFO: namespace e2e-tests-kubectl-9pxnv deletion completed in 6.118604707s

• [SLOW TEST:6.259 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:17:26.661: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:17:26.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12a94a64-373b-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-dk7zj" to be "success or failure"
Feb 23 07:17:26.730: INFO: Pod "downwardapi-volume-12a94a64-373b-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.545865ms
Feb 23 07:17:28.733: INFO: Pod "downwardapi-volume-12a94a64-373b-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007161384s
STEP: Saw pod success
Feb 23 07:17:28.733: INFO: Pod "downwardapi-volume-12a94a64-373b-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:17:28.736: INFO: Trying to get logs from node kube-node-02 pod downwardapi-volume-12a94a64-373b-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:17:28.755: INFO: Waiting for pod downwardapi-volume-12a94a64-373b-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:17:28.758: INFO: Pod downwardapi-volume-12a94a64-373b-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:17:28.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dk7zj" for this suite.
Feb 23 07:17:34.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:17:34.817: INFO: namespace: e2e-tests-projected-dk7zj, resource: bindings, ignored listing per whitelist
Feb 23 07:17:34.879: INFO: namespace e2e-tests-projected-dk7zj deletion completed in 6.11669666s

• [SLOW TEST:8.218 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:17:34.879: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-xddxg
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 23 07:17:34.958: INFO: Found 0 stateful pods, waiting for 3
Feb 23 07:17:44.963: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 07:17:44.963: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 07:17:44.963: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 07:17:44.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-xddxg ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 23 07:17:45.118: INFO: stderr: ""
Feb 23 07:17:45.118: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 23 07:17:45.118: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 23 07:17:55.151: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 23 07:18:05.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-xddxg ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 07:18:05.296: INFO: stderr: ""
Feb 23 07:18:05.296: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 23 07:18:05.296: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

STEP: Rolling back to a previous revision
Feb 23 07:18:25.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-xddxg ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 23 07:18:25.446: INFO: stderr: ""
Feb 23 07:18:25.446: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 23 07:18:25.446: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 23 07:18:35.482: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 23 07:18:45.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-xddxg ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 07:18:45.647: INFO: stderr: ""
Feb 23 07:18:45.647: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 23 07:18:45.647: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 23 07:18:55.668: INFO: Waiting for StatefulSet e2e-tests-statefulset-xddxg/ss2 to complete update
Feb 23 07:18:55.668: INFO: Waiting for Pod e2e-tests-statefulset-xddxg/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 23 07:19:05.676: INFO: Deleting all statefulset in ns e2e-tests-statefulset-xddxg
Feb 23 07:19:05.681: INFO: Scaling statefulset ss2 to 0
Feb 23 07:19:25.699: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 07:19:25.702: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:19:25.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-xddxg" for this suite.
Feb 23 07:19:31.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:19:31.826: INFO: namespace: e2e-tests-statefulset-xddxg, resource: bindings, ignored listing per whitelist
Feb 23 07:19:31.838: INFO: namespace e2e-tests-statefulset-xddxg deletion completed in 6.11682204s

• [SLOW TEST:116.959 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:19:31.838: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 23 07:19:31.912: INFO: Waiting up to 5m0s for pod "pod-5d468be6-373b-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-59jbb" to be "success or failure"
Feb 23 07:19:31.917: INFO: Pod "pod-5d468be6-373b-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.30655ms
Feb 23 07:19:33.921: INFO: Pod "pod-5d468be6-373b-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009243285s
STEP: Saw pod success
Feb 23 07:19:33.921: INFO: Pod "pod-5d468be6-373b-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:19:33.924: INFO: Trying to get logs from node kube-node-02 pod pod-5d468be6-373b-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 07:19:33.945: INFO: Waiting for pod pod-5d468be6-373b-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:19:33.948: INFO: Pod pod-5d468be6-373b-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:19:33.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-59jbb" for this suite.
Feb 23 07:19:39.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:19:40.066: INFO: namespace: e2e-tests-emptydir-59jbb, resource: bindings, ignored listing per whitelist
Feb 23 07:19:40.066: INFO: namespace e2e-tests-emptydir-59jbb deletion completed in 6.114137676s

• [SLOW TEST:8.228 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:19:40.066: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 23 07:19:40.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-hzmzb'
Feb 23 07:19:40.380: INFO: stderr: ""
Feb 23 07:19:40.380: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 23 07:19:41.385: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 07:19:41.385: INFO: Found 0 / 1
Feb 23 07:19:42.384: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 07:19:42.384: INFO: Found 1 / 1
Feb 23 07:19:42.384: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 23 07:19:42.387: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 07:19:42.387: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 23 07:19:42.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 patch pod redis-master-g74gj --namespace=e2e-tests-kubectl-hzmzb -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 23 07:19:42.459: INFO: stderr: ""
Feb 23 07:19:42.459: INFO: stdout: "pod/redis-master-g74gj patched\n"
STEP: checking annotations
Feb 23 07:19:42.462: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 07:19:42.462: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:19:42.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hzmzb" for this suite.
Feb 23 07:20:04.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:20:04.535: INFO: namespace: e2e-tests-kubectl-hzmzb, resource: bindings, ignored listing per whitelist
Feb 23 07:20:04.604: INFO: namespace e2e-tests-kubectl-hzmzb deletion completed in 22.138341482s

• [SLOW TEST:24.538 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:20:04.604: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 23 07:20:04.706: INFO: Waiting up to 5m0s for pod "pod-70d2d0cb-373b-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-nld9s" to be "success or failure"
Feb 23 07:20:04.710: INFO: Pod "pod-70d2d0cb-373b-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.788421ms
Feb 23 07:20:06.713: INFO: Pod "pod-70d2d0cb-373b-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007690286s
STEP: Saw pod success
Feb 23 07:20:06.713: INFO: Pod "pod-70d2d0cb-373b-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:20:06.716: INFO: Trying to get logs from node kube-node-02 pod pod-70d2d0cb-373b-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 07:20:06.736: INFO: Waiting for pod pod-70d2d0cb-373b-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:20:06.739: INFO: Pod pod-70d2d0cb-373b-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:20:06.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nld9s" for this suite.
Feb 23 07:20:12.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:20:12.817: INFO: namespace: e2e-tests-emptydir-nld9s, resource: bindings, ignored listing per whitelist
Feb 23 07:20:12.883: INFO: namespace e2e-tests-emptydir-nld9s deletion completed in 6.135869514s

• [SLOW TEST:8.279 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:20:12.884: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-75bd767c-373b-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 07:20:12.961: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-75be2695-373b-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-87rv7" to be "success or failure"
Feb 23 07:20:12.964: INFO: Pod "pod-projected-secrets-75be2695-373b-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033144ms
Feb 23 07:20:14.972: INFO: Pod "pod-projected-secrets-75be2695-373b-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010848309s
STEP: Saw pod success
Feb 23 07:20:14.972: INFO: Pod "pod-projected-secrets-75be2695-373b-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:20:14.975: INFO: Trying to get logs from node kube-node-01 pod pod-projected-secrets-75be2695-373b-11e9-a63e-52687f37ce9e container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 07:20:15.004: INFO: Waiting for pod pod-projected-secrets-75be2695-373b-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:20:15.008: INFO: Pod pod-projected-secrets-75be2695-373b-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:20:15.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-87rv7" for this suite.
Feb 23 07:20:21.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:20:21.059: INFO: namespace: e2e-tests-projected-87rv7, resource: bindings, ignored listing per whitelist
Feb 23 07:20:21.138: INFO: namespace e2e-tests-projected-87rv7 deletion completed in 6.126389867s

• [SLOW TEST:8.255 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:20:21.139: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-f5m7x
I0223 07:20:21.213311      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-f5m7x, replica count: 1
I0223 07:20:22.263765      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0223 07:20:23.264003      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 07:20:23.383: INFO: Created: latency-svc-vcgsb
Feb 23 07:20:23.393: INFO: Got endpoints: latency-svc-vcgsb [29.457558ms]
Feb 23 07:20:23.412: INFO: Created: latency-svc-fp66h
Feb 23 07:20:23.420: INFO: Got endpoints: latency-svc-fp66h [26.900625ms]
Feb 23 07:20:23.430: INFO: Created: latency-svc-wn5pz
Feb 23 07:20:23.438: INFO: Got endpoints: latency-svc-wn5pz [44.684481ms]
Feb 23 07:20:23.444: INFO: Created: latency-svc-drjwt
Feb 23 07:20:23.452: INFO: Got endpoints: latency-svc-drjwt [58.040927ms]
Feb 23 07:20:23.459: INFO: Created: latency-svc-dff5c
Feb 23 07:20:23.462: INFO: Got endpoints: latency-svc-dff5c [68.354422ms]
Feb 23 07:20:23.474: INFO: Created: latency-svc-znzfb
Feb 23 07:20:23.481: INFO: Got endpoints: latency-svc-znzfb [87.15264ms]
Feb 23 07:20:23.500: INFO: Created: latency-svc-27pzs
Feb 23 07:20:23.532: INFO: Got endpoints: latency-svc-27pzs [138.239145ms]
Feb 23 07:20:23.538: INFO: Created: latency-svc-rv622
Feb 23 07:20:23.547: INFO: Got endpoints: latency-svc-rv622 [153.471251ms]
Feb 23 07:20:23.555: INFO: Created: latency-svc-qd4st
Feb 23 07:20:23.562: INFO: Got endpoints: latency-svc-qd4st [168.670662ms]
Feb 23 07:20:23.576: INFO: Created: latency-svc-wk5tj
Feb 23 07:20:23.590: INFO: Got endpoints: latency-svc-wk5tj [196.627779ms]
Feb 23 07:20:23.594: INFO: Created: latency-svc-wgr55
Feb 23 07:20:23.607: INFO: Got endpoints: latency-svc-wgr55 [213.161709ms]
Feb 23 07:20:23.612: INFO: Created: latency-svc-h2jfp
Feb 23 07:20:23.622: INFO: Got endpoints: latency-svc-h2jfp [227.957753ms]
Feb 23 07:20:23.627: INFO: Created: latency-svc-7xbh5
Feb 23 07:20:23.638: INFO: Got endpoints: latency-svc-7xbh5 [243.56761ms]
Feb 23 07:20:23.642: INFO: Created: latency-svc-vqnpk
Feb 23 07:20:23.649: INFO: Got endpoints: latency-svc-vqnpk [255.411514ms]
Feb 23 07:20:23.654: INFO: Created: latency-svc-q8ctj
Feb 23 07:20:23.662: INFO: Got endpoints: latency-svc-q8ctj [267.534842ms]
Feb 23 07:20:23.669: INFO: Created: latency-svc-rqqrm
Feb 23 07:20:23.680: INFO: Got endpoints: latency-svc-rqqrm [285.620588ms]
Feb 23 07:20:23.685: INFO: Created: latency-svc-dvkrq
Feb 23 07:20:23.693: INFO: Got endpoints: latency-svc-dvkrq [272.5597ms]
Feb 23 07:20:23.700: INFO: Created: latency-svc-27jp6
Feb 23 07:20:23.713: INFO: Got endpoints: latency-svc-27jp6 [274.956564ms]
Feb 23 07:20:23.717: INFO: Created: latency-svc-7h9lb
Feb 23 07:20:23.728: INFO: Got endpoints: latency-svc-7h9lb [34.639215ms]
Feb 23 07:20:23.738: INFO: Created: latency-svc-5nz2t
Feb 23 07:20:23.746: INFO: Got endpoints: latency-svc-5nz2t [294.505188ms]
Feb 23 07:20:23.752: INFO: Created: latency-svc-txt68
Feb 23 07:20:23.760: INFO: Got endpoints: latency-svc-txt68 [297.780841ms]
Feb 23 07:20:23.767: INFO: Created: latency-svc-x6fqp
Feb 23 07:20:23.776: INFO: Got endpoints: latency-svc-x6fqp [294.565773ms]
Feb 23 07:20:23.791: INFO: Created: latency-svc-2fz6t
Feb 23 07:20:23.795: INFO: Created: latency-svc-r8mts
Feb 23 07:20:23.797: INFO: Got endpoints: latency-svc-2fz6t [264.595182ms]
Feb 23 07:20:23.805: INFO: Got endpoints: latency-svc-r8mts [257.525546ms]
Feb 23 07:20:23.809: INFO: Created: latency-svc-rpn7w
Feb 23 07:20:23.814: INFO: Got endpoints: latency-svc-rpn7w [251.172816ms]
Feb 23 07:20:23.825: INFO: Created: latency-svc-t7h9c
Feb 23 07:20:23.834: INFO: Got endpoints: latency-svc-t7h9c [243.076232ms]
Feb 23 07:20:23.841: INFO: Created: latency-svc-4x8fs
Feb 23 07:20:23.848: INFO: Got endpoints: latency-svc-4x8fs [240.984886ms]
Feb 23 07:20:23.854: INFO: Created: latency-svc-s7c24
Feb 23 07:20:23.860: INFO: Got endpoints: latency-svc-s7c24 [236.932693ms]
Feb 23 07:20:23.867: INFO: Created: latency-svc-sdw4b
Feb 23 07:20:23.876: INFO: Got endpoints: latency-svc-sdw4b [238.038143ms]
Feb 23 07:20:23.883: INFO: Created: latency-svc-dpmm5
Feb 23 07:20:23.896: INFO: Got endpoints: latency-svc-dpmm5 [246.855638ms]
Feb 23 07:20:23.902: INFO: Created: latency-svc-6lxpk
Feb 23 07:20:23.915: INFO: Got endpoints: latency-svc-6lxpk [253.085023ms]
Feb 23 07:20:23.917: INFO: Created: latency-svc-5jfhg
Feb 23 07:20:23.925: INFO: Got endpoints: latency-svc-5jfhg [244.848844ms]
Feb 23 07:20:23.931: INFO: Created: latency-svc-qsnhs
Feb 23 07:20:23.941: INFO: Got endpoints: latency-svc-qsnhs [227.345032ms]
Feb 23 07:20:23.944: INFO: Created: latency-svc-cj5h7
Feb 23 07:20:23.950: INFO: Got endpoints: latency-svc-cj5h7 [222.356995ms]
Feb 23 07:20:23.959: INFO: Created: latency-svc-tfxqb
Feb 23 07:20:23.965: INFO: Got endpoints: latency-svc-tfxqb [218.310743ms]
Feb 23 07:20:23.973: INFO: Created: latency-svc-9sj4l
Feb 23 07:20:23.980: INFO: Got endpoints: latency-svc-9sj4l [220.311384ms]
Feb 23 07:20:23.985: INFO: Created: latency-svc-m2996
Feb 23 07:20:24.001: INFO: Got endpoints: latency-svc-m2996 [225.580553ms]
Feb 23 07:20:24.012: INFO: Created: latency-svc-6q4dn
Feb 23 07:20:24.020: INFO: Got endpoints: latency-svc-6q4dn [223.076623ms]
Feb 23 07:20:24.025: INFO: Created: latency-svc-559sf
Feb 23 07:20:24.033: INFO: Got endpoints: latency-svc-559sf [228.358974ms]
Feb 23 07:20:24.041: INFO: Created: latency-svc-g2sdn
Feb 23 07:20:24.050: INFO: Got endpoints: latency-svc-g2sdn [236.64815ms]
Feb 23 07:20:24.066: INFO: Created: latency-svc-ns52m
Feb 23 07:20:24.072: INFO: Got endpoints: latency-svc-ns52m [238.870052ms]
Feb 23 07:20:24.095: INFO: Created: latency-svc-4pprr
Feb 23 07:20:24.101: INFO: Got endpoints: latency-svc-4pprr [253.382017ms]
Feb 23 07:20:24.113: INFO: Created: latency-svc-6s2vj
Feb 23 07:20:24.117: INFO: Got endpoints: latency-svc-6s2vj [257.250308ms]
Feb 23 07:20:24.129: INFO: Created: latency-svc-n5kbl
Feb 23 07:20:24.137: INFO: Got endpoints: latency-svc-n5kbl [261.569794ms]
Feb 23 07:20:24.145: INFO: Created: latency-svc-wpbpw
Feb 23 07:20:24.149: INFO: Got endpoints: latency-svc-wpbpw [252.556902ms]
Feb 23 07:20:24.174: INFO: Created: latency-svc-j8q9h
Feb 23 07:20:24.193: INFO: Got endpoints: latency-svc-j8q9h [278.6838ms]
Feb 23 07:20:24.199: INFO: Created: latency-svc-pwt85
Feb 23 07:20:24.236: INFO: Created: latency-svc-97t7p
Feb 23 07:20:24.242: INFO: Got endpoints: latency-svc-pwt85 [317.014432ms]
Feb 23 07:20:24.248: INFO: Created: latency-svc-xml5d
Feb 23 07:20:24.267: INFO: Created: latency-svc-qwbmz
Feb 23 07:20:24.282: INFO: Created: latency-svc-9brch
Feb 23 07:20:24.290: INFO: Got endpoints: latency-svc-97t7p [349.207752ms]
Feb 23 07:20:24.298: INFO: Created: latency-svc-n9h5v
Feb 23 07:20:24.310: INFO: Created: latency-svc-7rx2r
Feb 23 07:20:24.333: INFO: Created: latency-svc-tkvvm
Feb 23 07:20:24.342: INFO: Got endpoints: latency-svc-xml5d [391.749932ms]
Feb 23 07:20:24.349: INFO: Created: latency-svc-z8zv4
Feb 23 07:20:24.363: INFO: Created: latency-svc-9qwln
Feb 23 07:20:24.376: INFO: Created: latency-svc-kdpw6
Feb 23 07:20:24.389: INFO: Created: latency-svc-77j9p
Feb 23 07:20:24.389: INFO: Got endpoints: latency-svc-qwbmz [424.506056ms]
Feb 23 07:20:24.406: INFO: Created: latency-svc-6rspw
Feb 23 07:20:24.420: INFO: Created: latency-svc-595ss
Feb 23 07:20:24.440: INFO: Got endpoints: latency-svc-9brch [459.16929ms]
Feb 23 07:20:24.443: INFO: Created: latency-svc-tjbk8
Feb 23 07:20:24.456: INFO: Created: latency-svc-vc97f
Feb 23 07:20:24.468: INFO: Created: latency-svc-fkxtk
Feb 23 07:20:24.484: INFO: Created: latency-svc-wdzz7
Feb 23 07:20:24.497: INFO: Created: latency-svc-8nxd4
Feb 23 07:20:24.498: INFO: Got endpoints: latency-svc-n9h5v [496.361938ms]
Feb 23 07:20:24.514: INFO: Created: latency-svc-hng7k
Feb 23 07:20:24.526: INFO: Created: latency-svc-5dks7
Feb 23 07:20:24.543: INFO: Got endpoints: latency-svc-7rx2r [523.375066ms]
Feb 23 07:20:24.560: INFO: Created: latency-svc-vrfm7
Feb 23 07:20:24.588: INFO: Got endpoints: latency-svc-tkvvm [555.256468ms]
Feb 23 07:20:24.608: INFO: Created: latency-svc-sthkk
Feb 23 07:20:24.638: INFO: Got endpoints: latency-svc-z8zv4 [587.234676ms]
Feb 23 07:20:24.663: INFO: Created: latency-svc-kdw8g
Feb 23 07:20:24.689: INFO: Got endpoints: latency-svc-9qwln [616.419359ms]
Feb 23 07:20:24.711: INFO: Created: latency-svc-x2vnk
Feb 23 07:20:24.738: INFO: Got endpoints: latency-svc-kdpw6 [636.522034ms]
Feb 23 07:20:24.761: INFO: Created: latency-svc-qbhjm
Feb 23 07:20:24.790: INFO: Got endpoints: latency-svc-77j9p [673.113829ms]
Feb 23 07:20:24.816: INFO: Created: latency-svc-2rmmh
Feb 23 07:20:24.843: INFO: Got endpoints: latency-svc-6rspw [705.225083ms]
Feb 23 07:20:24.864: INFO: Created: latency-svc-f4jmx
Feb 23 07:20:24.889: INFO: Got endpoints: latency-svc-595ss [738.612901ms]
Feb 23 07:20:24.915: INFO: Created: latency-svc-xfd9k
Feb 23 07:20:24.939: INFO: Got endpoints: latency-svc-tjbk8 [745.197167ms]
Feb 23 07:20:24.962: INFO: Created: latency-svc-rdf4v
Feb 23 07:20:24.989: INFO: Got endpoints: latency-svc-vc97f [746.518225ms]
Feb 23 07:20:25.015: INFO: Created: latency-svc-kvtvd
Feb 23 07:20:25.038: INFO: Got endpoints: latency-svc-fkxtk [748.570676ms]
Feb 23 07:20:25.088: INFO: Created: latency-svc-f6q87
Feb 23 07:20:25.089: INFO: Got endpoints: latency-svc-wdzz7 [746.890162ms]
Feb 23 07:20:25.110: INFO: Created: latency-svc-69r6g
Feb 23 07:20:25.138: INFO: Got endpoints: latency-svc-8nxd4 [748.965214ms]
Feb 23 07:20:25.166: INFO: Created: latency-svc-hrkqp
Feb 23 07:20:25.189: INFO: Got endpoints: latency-svc-hng7k [748.127715ms]
Feb 23 07:20:25.209: INFO: Created: latency-svc-q6w4l
Feb 23 07:20:25.240: INFO: Got endpoints: latency-svc-5dks7 [742.133122ms]
Feb 23 07:20:25.262: INFO: Created: latency-svc-5dzkr
Feb 23 07:20:25.289: INFO: Got endpoints: latency-svc-vrfm7 [745.349748ms]
Feb 23 07:20:25.312: INFO: Created: latency-svc-p5vrs
Feb 23 07:20:25.338: INFO: Got endpoints: latency-svc-sthkk [749.391811ms]
Feb 23 07:20:25.361: INFO: Created: latency-svc-dvlsw
Feb 23 07:20:25.390: INFO: Got endpoints: latency-svc-kdw8g [751.74404ms]
Feb 23 07:20:25.411: INFO: Created: latency-svc-2l57z
Feb 23 07:20:25.438: INFO: Got endpoints: latency-svc-x2vnk [749.237033ms]
Feb 23 07:20:25.462: INFO: Created: latency-svc-c4q4l
Feb 23 07:20:25.490: INFO: Got endpoints: latency-svc-qbhjm [752.329569ms]
Feb 23 07:20:25.518: INFO: Created: latency-svc-9bsqj
Feb 23 07:20:25.538: INFO: Got endpoints: latency-svc-2rmmh [747.935216ms]
Feb 23 07:20:25.563: INFO: Created: latency-svc-wdw24
Feb 23 07:20:25.589: INFO: Got endpoints: latency-svc-f4jmx [746.506122ms]
Feb 23 07:20:25.615: INFO: Created: latency-svc-rk9vg
Feb 23 07:20:25.638: INFO: Got endpoints: latency-svc-xfd9k [748.540421ms]
Feb 23 07:20:25.655: INFO: Created: latency-svc-vs46q
Feb 23 07:20:25.689: INFO: Got endpoints: latency-svc-rdf4v [750.678604ms]
Feb 23 07:20:25.712: INFO: Created: latency-svc-xklpk
Feb 23 07:20:25.739: INFO: Got endpoints: latency-svc-kvtvd [750.140266ms]
Feb 23 07:20:25.755: INFO: Created: latency-svc-jkbkt
Feb 23 07:20:25.788: INFO: Got endpoints: latency-svc-f6q87 [749.626279ms]
Feb 23 07:20:25.804: INFO: Created: latency-svc-jstss
Feb 23 07:20:25.839: INFO: Got endpoints: latency-svc-69r6g [749.314318ms]
Feb 23 07:20:25.857: INFO: Created: latency-svc-b5mmt
Feb 23 07:20:25.889: INFO: Got endpoints: latency-svc-hrkqp [750.530284ms]
Feb 23 07:20:25.908: INFO: Created: latency-svc-dlxqm
Feb 23 07:20:25.938: INFO: Got endpoints: latency-svc-q6w4l [749.413878ms]
Feb 23 07:20:25.955: INFO: Created: latency-svc-rt6pf
Feb 23 07:20:25.989: INFO: Got endpoints: latency-svc-5dzkr [749.45045ms]
Feb 23 07:20:26.010: INFO: Created: latency-svc-x8x98
Feb 23 07:20:26.038: INFO: Got endpoints: latency-svc-p5vrs [749.608967ms]
Feb 23 07:20:26.057: INFO: Created: latency-svc-bp5z2
Feb 23 07:20:26.091: INFO: Got endpoints: latency-svc-dvlsw [752.461565ms]
Feb 23 07:20:26.109: INFO: Created: latency-svc-85w9z
Feb 23 07:20:26.138: INFO: Got endpoints: latency-svc-2l57z [748.562591ms]
Feb 23 07:20:26.156: INFO: Created: latency-svc-hcqgb
Feb 23 07:20:26.191: INFO: Got endpoints: latency-svc-c4q4l [752.785083ms]
Feb 23 07:20:26.212: INFO: Created: latency-svc-4sntw
Feb 23 07:20:26.239: INFO: Got endpoints: latency-svc-9bsqj [747.982281ms]
Feb 23 07:20:26.257: INFO: Created: latency-svc-j9r2n
Feb 23 07:20:26.288: INFO: Got endpoints: latency-svc-wdw24 [749.8987ms]
Feb 23 07:20:26.314: INFO: Created: latency-svc-gb2lm
Feb 23 07:20:26.340: INFO: Got endpoints: latency-svc-rk9vg [750.557014ms]
Feb 23 07:20:26.359: INFO: Created: latency-svc-2mwhr
Feb 23 07:20:26.389: INFO: Got endpoints: latency-svc-vs46q [751.263876ms]
Feb 23 07:20:26.408: INFO: Created: latency-svc-ltqvr
Feb 23 07:20:26.438: INFO: Got endpoints: latency-svc-xklpk [748.842291ms]
Feb 23 07:20:26.461: INFO: Created: latency-svc-hn4bl
Feb 23 07:20:26.489: INFO: Got endpoints: latency-svc-jkbkt [749.877856ms]
Feb 23 07:20:26.507: INFO: Created: latency-svc-ndhxc
Feb 23 07:20:26.539: INFO: Got endpoints: latency-svc-jstss [750.142369ms]
Feb 23 07:20:26.560: INFO: Created: latency-svc-fp29q
Feb 23 07:20:26.588: INFO: Got endpoints: latency-svc-b5mmt [749.458813ms]
Feb 23 07:20:26.610: INFO: Created: latency-svc-9ctfs
Feb 23 07:20:26.639: INFO: Got endpoints: latency-svc-dlxqm [749.290546ms]
Feb 23 07:20:26.659: INFO: Created: latency-svc-kcm4j
Feb 23 07:20:26.689: INFO: Got endpoints: latency-svc-rt6pf [750.484739ms]
Feb 23 07:20:26.707: INFO: Created: latency-svc-ntffh
Feb 23 07:20:26.738: INFO: Got endpoints: latency-svc-x8x98 [749.085576ms]
Feb 23 07:20:26.756: INFO: Created: latency-svc-ws7ws
Feb 23 07:20:26.788: INFO: Got endpoints: latency-svc-bp5z2 [749.543143ms]
Feb 23 07:20:26.808: INFO: Created: latency-svc-xdkl9
Feb 23 07:20:26.838: INFO: Got endpoints: latency-svc-85w9z [747.877453ms]
Feb 23 07:20:26.857: INFO: Created: latency-svc-d6nsq
Feb 23 07:20:26.890: INFO: Got endpoints: latency-svc-hcqgb [751.155592ms]
Feb 23 07:20:26.912: INFO: Created: latency-svc-pszmt
Feb 23 07:20:26.938: INFO: Got endpoints: latency-svc-4sntw [747.00829ms]
Feb 23 07:20:26.955: INFO: Created: latency-svc-4xhl2
Feb 23 07:20:26.991: INFO: Got endpoints: latency-svc-j9r2n [751.919199ms]
Feb 23 07:20:27.009: INFO: Created: latency-svc-lt4t8
Feb 23 07:20:27.039: INFO: Got endpoints: latency-svc-gb2lm [750.601436ms]
Feb 23 07:20:27.085: INFO: Created: latency-svc-z4n57
Feb 23 07:20:27.089: INFO: Got endpoints: latency-svc-2mwhr [748.787437ms]
Feb 23 07:20:27.112: INFO: Created: latency-svc-qfd6g
Feb 23 07:20:27.140: INFO: Got endpoints: latency-svc-ltqvr [750.455759ms]
Feb 23 07:20:27.160: INFO: Created: latency-svc-lqzkf
Feb 23 07:20:27.189: INFO: Got endpoints: latency-svc-hn4bl [750.685497ms]
Feb 23 07:20:27.212: INFO: Created: latency-svc-jsfk9
Feb 23 07:20:27.238: INFO: Got endpoints: latency-svc-ndhxc [749.460396ms]
Feb 23 07:20:27.263: INFO: Created: latency-svc-xqptc
Feb 23 07:20:27.289: INFO: Got endpoints: latency-svc-fp29q [749.905809ms]
Feb 23 07:20:27.312: INFO: Created: latency-svc-npphj
Feb 23 07:20:27.338: INFO: Got endpoints: latency-svc-9ctfs [750.039785ms]
Feb 23 07:20:27.359: INFO: Created: latency-svc-hfp8d
Feb 23 07:20:27.389: INFO: Got endpoints: latency-svc-kcm4j [750.237935ms]
Feb 23 07:20:27.411: INFO: Created: latency-svc-b6cf8
Feb 23 07:20:27.438: INFO: Got endpoints: latency-svc-ntffh [749.404878ms]
Feb 23 07:20:27.460: INFO: Created: latency-svc-jt7zg
Feb 23 07:20:27.488: INFO: Got endpoints: latency-svc-ws7ws [749.581945ms]
Feb 23 07:20:27.508: INFO: Created: latency-svc-hjtf8
Feb 23 07:20:27.539: INFO: Got endpoints: latency-svc-xdkl9 [750.550482ms]
Feb 23 07:20:27.560: INFO: Created: latency-svc-62l7s
Feb 23 07:20:27.589: INFO: Got endpoints: latency-svc-d6nsq [750.527636ms]
Feb 23 07:20:27.612: INFO: Created: latency-svc-6kxjz
Feb 23 07:20:27.641: INFO: Got endpoints: latency-svc-pszmt [751.618525ms]
Feb 23 07:20:27.663: INFO: Created: latency-svc-tdsnt
Feb 23 07:20:27.689: INFO: Got endpoints: latency-svc-4xhl2 [750.387798ms]
Feb 23 07:20:27.711: INFO: Created: latency-svc-n4js9
Feb 23 07:20:27.738: INFO: Got endpoints: latency-svc-lt4t8 [747.831358ms]
Feb 23 07:20:27.761: INFO: Created: latency-svc-mm7bj
Feb 23 07:20:27.795: INFO: Got endpoints: latency-svc-z4n57 [755.240418ms]
Feb 23 07:20:27.831: INFO: Created: latency-svc-t9h4b
Feb 23 07:20:27.841: INFO: Got endpoints: latency-svc-qfd6g [752.542049ms]
Feb 23 07:20:27.863: INFO: Created: latency-svc-nznh5
Feb 23 07:20:27.889: INFO: Got endpoints: latency-svc-lqzkf [749.170496ms]
Feb 23 07:20:27.921: INFO: Created: latency-svc-p675t
Feb 23 07:20:27.939: INFO: Got endpoints: latency-svc-jsfk9 [749.293106ms]
Feb 23 07:20:27.962: INFO: Created: latency-svc-4lhd8
Feb 23 07:20:27.988: INFO: Got endpoints: latency-svc-xqptc [749.484995ms]
Feb 23 07:20:28.013: INFO: Created: latency-svc-ppxcx
Feb 23 07:20:28.039: INFO: Got endpoints: latency-svc-npphj [749.90747ms]
Feb 23 07:20:28.061: INFO: Created: latency-svc-8gnkc
Feb 23 07:20:28.089: INFO: Got endpoints: latency-svc-hfp8d [750.918041ms]
Feb 23 07:20:28.112: INFO: Created: latency-svc-hffgm
Feb 23 07:20:28.138: INFO: Got endpoints: latency-svc-b6cf8 [749.059044ms]
Feb 23 07:20:28.163: INFO: Created: latency-svc-59gg5
Feb 23 07:20:28.188: INFO: Got endpoints: latency-svc-jt7zg [750.188708ms]
Feb 23 07:20:28.210: INFO: Created: latency-svc-nxj4p
Feb 23 07:20:28.238: INFO: Got endpoints: latency-svc-hjtf8 [749.841449ms]
Feb 23 07:20:28.263: INFO: Created: latency-svc-mhrqh
Feb 23 07:20:28.288: INFO: Got endpoints: latency-svc-62l7s [749.479675ms]
Feb 23 07:20:28.309: INFO: Created: latency-svc-sqmlf
Feb 23 07:20:28.342: INFO: Got endpoints: latency-svc-6kxjz [753.152888ms]
Feb 23 07:20:28.372: INFO: Created: latency-svc-t8lbm
Feb 23 07:20:28.395: INFO: Got endpoints: latency-svc-tdsnt [753.706217ms]
Feb 23 07:20:28.414: INFO: Created: latency-svc-jrwxq
Feb 23 07:20:28.439: INFO: Got endpoints: latency-svc-n4js9 [750.04642ms]
Feb 23 07:20:28.458: INFO: Created: latency-svc-qslwp
Feb 23 07:20:28.488: INFO: Got endpoints: latency-svc-mm7bj [749.570954ms]
Feb 23 07:20:28.511: INFO: Created: latency-svc-dcbwf
Feb 23 07:20:28.538: INFO: Got endpoints: latency-svc-t9h4b [743.321156ms]
Feb 23 07:20:28.558: INFO: Created: latency-svc-z65cz
Feb 23 07:20:28.590: INFO: Got endpoints: latency-svc-nznh5 [748.858077ms]
Feb 23 07:20:28.611: INFO: Created: latency-svc-9n7hs
Feb 23 07:20:28.638: INFO: Got endpoints: latency-svc-p675t [749.243469ms]
Feb 23 07:20:28.662: INFO: Created: latency-svc-t5vzc
Feb 23 07:20:28.689: INFO: Got endpoints: latency-svc-4lhd8 [750.885393ms]
Feb 23 07:20:28.714: INFO: Created: latency-svc-5cd5c
Feb 23 07:20:28.738: INFO: Got endpoints: latency-svc-ppxcx [750.205425ms]
Feb 23 07:20:28.759: INFO: Created: latency-svc-p6pb4
Feb 23 07:20:28.789: INFO: Got endpoints: latency-svc-8gnkc [750.911848ms]
Feb 23 07:20:28.813: INFO: Created: latency-svc-r5lll
Feb 23 07:20:28.839: INFO: Got endpoints: latency-svc-hffgm [749.612878ms]
Feb 23 07:20:28.886: INFO: Created: latency-svc-7mfp5
Feb 23 07:20:28.891: INFO: Got endpoints: latency-svc-59gg5 [752.731851ms]
Feb 23 07:20:28.916: INFO: Created: latency-svc-zn55m
Feb 23 07:20:28.938: INFO: Got endpoints: latency-svc-nxj4p [749.323211ms]
Feb 23 07:20:28.958: INFO: Created: latency-svc-qhxmg
Feb 23 07:20:28.989: INFO: Got endpoints: latency-svc-mhrqh [751.028629ms]
Feb 23 07:20:29.008: INFO: Created: latency-svc-dnrgg
Feb 23 07:20:29.041: INFO: Got endpoints: latency-svc-sqmlf [753.000646ms]
Feb 23 07:20:29.060: INFO: Created: latency-svc-56r94
Feb 23 07:20:29.088: INFO: Got endpoints: latency-svc-t8lbm [745.91781ms]
Feb 23 07:20:29.108: INFO: Created: latency-svc-gr98t
Feb 23 07:20:29.138: INFO: Got endpoints: latency-svc-jrwxq [743.203783ms]
Feb 23 07:20:29.161: INFO: Created: latency-svc-ztjl8
Feb 23 07:20:29.192: INFO: Got endpoints: latency-svc-qslwp [753.263559ms]
Feb 23 07:20:29.212: INFO: Created: latency-svc-5vk4g
Feb 23 07:20:29.238: INFO: Got endpoints: latency-svc-dcbwf [750.349198ms]
Feb 23 07:20:29.257: INFO: Created: latency-svc-9h49n
Feb 23 07:20:29.291: INFO: Got endpoints: latency-svc-z65cz [752.734914ms]
Feb 23 07:20:29.309: INFO: Created: latency-svc-ssprc
Feb 23 07:20:29.339: INFO: Got endpoints: latency-svc-9n7hs [748.619547ms]
Feb 23 07:20:29.356: INFO: Created: latency-svc-6k6ml
Feb 23 07:20:29.388: INFO: Got endpoints: latency-svc-t5vzc [749.797652ms]
Feb 23 07:20:29.413: INFO: Created: latency-svc-wn26n
Feb 23 07:20:29.438: INFO: Got endpoints: latency-svc-5cd5c [748.691891ms]
Feb 23 07:20:29.464: INFO: Created: latency-svc-xbk4l
Feb 23 07:20:29.489: INFO: Got endpoints: latency-svc-p6pb4 [750.691449ms]
Feb 23 07:20:29.507: INFO: Created: latency-svc-gp57x
Feb 23 07:20:29.538: INFO: Got endpoints: latency-svc-r5lll [748.636013ms]
Feb 23 07:20:29.556: INFO: Created: latency-svc-nzsfv
Feb 23 07:20:29.590: INFO: Got endpoints: latency-svc-7mfp5 [751.626453ms]
Feb 23 07:20:29.611: INFO: Created: latency-svc-tfstg
Feb 23 07:20:29.639: INFO: Got endpoints: latency-svc-zn55m [747.627119ms]
Feb 23 07:20:29.659: INFO: Created: latency-svc-76m57
Feb 23 07:20:29.689: INFO: Got endpoints: latency-svc-qhxmg [750.936495ms]
Feb 23 07:20:29.713: INFO: Created: latency-svc-7nx46
Feb 23 07:20:29.738: INFO: Got endpoints: latency-svc-dnrgg [748.965544ms]
Feb 23 07:20:29.757: INFO: Created: latency-svc-rtngk
Feb 23 07:20:29.790: INFO: Got endpoints: latency-svc-56r94 [748.984718ms]
Feb 23 07:20:29.810: INFO: Created: latency-svc-prsqb
Feb 23 07:20:29.839: INFO: Got endpoints: latency-svc-gr98t [750.323998ms]
Feb 23 07:20:29.856: INFO: Created: latency-svc-6dckf
Feb 23 07:20:29.889: INFO: Got endpoints: latency-svc-ztjl8 [750.154603ms]
Feb 23 07:20:29.906: INFO: Created: latency-svc-phn4r
Feb 23 07:20:29.938: INFO: Got endpoints: latency-svc-5vk4g [745.644004ms]
Feb 23 07:20:29.958: INFO: Created: latency-svc-2h6hp
Feb 23 07:20:29.999: INFO: Got endpoints: latency-svc-9h49n [760.010588ms]
Feb 23 07:20:30.020: INFO: Created: latency-svc-gt7qv
Feb 23 07:20:30.038: INFO: Got endpoints: latency-svc-ssprc [747.288929ms]
Feb 23 07:20:30.061: INFO: Created: latency-svc-8wxb9
Feb 23 07:20:30.088: INFO: Got endpoints: latency-svc-6k6ml [749.311739ms]
Feb 23 07:20:30.109: INFO: Created: latency-svc-7twfk
Feb 23 07:20:30.139: INFO: Got endpoints: latency-svc-wn26n [750.653986ms]
Feb 23 07:20:30.158: INFO: Created: latency-svc-drbz4
Feb 23 07:20:30.189: INFO: Got endpoints: latency-svc-xbk4l [750.460212ms]
Feb 23 07:20:30.208: INFO: Created: latency-svc-zp69x
Feb 23 07:20:30.239: INFO: Got endpoints: latency-svc-gp57x [749.710383ms]
Feb 23 07:20:30.261: INFO: Created: latency-svc-8b78t
Feb 23 07:20:30.290: INFO: Got endpoints: latency-svc-nzsfv [751.311021ms]
Feb 23 07:20:30.314: INFO: Created: latency-svc-9ktr8
Feb 23 07:20:30.347: INFO: Got endpoints: latency-svc-tfstg [756.23317ms]
Feb 23 07:20:30.369: INFO: Created: latency-svc-wppqw
Feb 23 07:20:30.388: INFO: Got endpoints: latency-svc-76m57 [749.243173ms]
Feb 23 07:20:30.409: INFO: Created: latency-svc-gm2sj
Feb 23 07:20:30.439: INFO: Got endpoints: latency-svc-7nx46 [750.177428ms]
Feb 23 07:20:30.467: INFO: Created: latency-svc-8kxt5
Feb 23 07:20:30.488: INFO: Got endpoints: latency-svc-rtngk [749.881726ms]
Feb 23 07:20:30.510: INFO: Created: latency-svc-clhz2
Feb 23 07:20:30.538: INFO: Got endpoints: latency-svc-prsqb [747.909828ms]
Feb 23 07:20:30.579: INFO: Created: latency-svc-j8qf4
Feb 23 07:20:30.590: INFO: Got endpoints: latency-svc-6dckf [751.340597ms]
Feb 23 07:20:30.621: INFO: Created: latency-svc-h7q2f
Feb 23 07:20:30.642: INFO: Got endpoints: latency-svc-phn4r [753.400664ms]
Feb 23 07:20:30.667: INFO: Created: latency-svc-z5hs5
Feb 23 07:20:30.688: INFO: Got endpoints: latency-svc-2h6hp [749.734057ms]
Feb 23 07:20:30.711: INFO: Created: latency-svc-gfldj
Feb 23 07:20:30.743: INFO: Got endpoints: latency-svc-gt7qv [744.818093ms]
Feb 23 07:20:30.767: INFO: Created: latency-svc-5g779
Feb 23 07:20:30.789: INFO: Got endpoints: latency-svc-8wxb9 [750.169677ms]
Feb 23 07:20:30.814: INFO: Created: latency-svc-fbqkp
Feb 23 07:20:30.838: INFO: Got endpoints: latency-svc-7twfk [750.068141ms]
Feb 23 07:20:30.863: INFO: Created: latency-svc-hlnkd
Feb 23 07:20:30.894: INFO: Got endpoints: latency-svc-drbz4 [755.678295ms]
Feb 23 07:20:30.919: INFO: Created: latency-svc-7kr4v
Feb 23 07:20:30.938: INFO: Got endpoints: latency-svc-zp69x [748.958984ms]
Feb 23 07:20:30.985: INFO: Created: latency-svc-xcqgc
Feb 23 07:20:30.989: INFO: Got endpoints: latency-svc-8b78t [750.383862ms]
Feb 23 07:20:31.011: INFO: Created: latency-svc-kcjgl
Feb 23 07:20:31.038: INFO: Got endpoints: latency-svc-9ktr8 [748.219905ms]
Feb 23 07:20:31.058: INFO: Created: latency-svc-9qqrg
Feb 23 07:20:31.089: INFO: Got endpoints: latency-svc-wppqw [741.724179ms]
Feb 23 07:20:31.113: INFO: Created: latency-svc-rkrnv
Feb 23 07:20:31.148: INFO: Got endpoints: latency-svc-gm2sj [759.861919ms]
Feb 23 07:20:31.164: INFO: Created: latency-svc-w29fq
Feb 23 07:20:31.188: INFO: Got endpoints: latency-svc-8kxt5 [749.164493ms]
Feb 23 07:20:31.209: INFO: Created: latency-svc-5jrmc
Feb 23 07:20:31.239: INFO: Got endpoints: latency-svc-clhz2 [750.686469ms]
Feb 23 07:20:31.289: INFO: Got endpoints: latency-svc-j8qf4 [750.879293ms]
Feb 23 07:20:31.339: INFO: Got endpoints: latency-svc-h7q2f [748.939772ms]
Feb 23 07:20:31.389: INFO: Got endpoints: latency-svc-z5hs5 [746.535355ms]
Feb 23 07:20:31.439: INFO: Got endpoints: latency-svc-gfldj [750.806669ms]
Feb 23 07:20:31.490: INFO: Got endpoints: latency-svc-5g779 [746.032927ms]
Feb 23 07:20:31.543: INFO: Got endpoints: latency-svc-fbqkp [753.766381ms]
Feb 23 07:20:31.590: INFO: Got endpoints: latency-svc-hlnkd [752.164781ms]
Feb 23 07:20:31.639: INFO: Got endpoints: latency-svc-7kr4v [744.146555ms]
Feb 23 07:20:31.688: INFO: Got endpoints: latency-svc-xcqgc [750.352065ms]
Feb 23 07:20:31.738: INFO: Got endpoints: latency-svc-kcjgl [748.58789ms]
Feb 23 07:20:31.789: INFO: Got endpoints: latency-svc-9qqrg [750.568927ms]
Feb 23 07:20:31.838: INFO: Got endpoints: latency-svc-rkrnv [749.305353ms]
Feb 23 07:20:31.889: INFO: Got endpoints: latency-svc-w29fq [740.922086ms]
Feb 23 07:20:31.939: INFO: Got endpoints: latency-svc-5jrmc [750.791388ms]
Feb 23 07:20:31.939: INFO: Latencies: [26.900625ms 34.639215ms 44.684481ms 58.040927ms 68.354422ms 87.15264ms 138.239145ms 153.471251ms 168.670662ms 196.627779ms 213.161709ms 218.310743ms 220.311384ms 222.356995ms 223.076623ms 225.580553ms 227.345032ms 227.957753ms 228.358974ms 236.64815ms 236.932693ms 238.038143ms 238.870052ms 240.984886ms 243.076232ms 243.56761ms 244.848844ms 246.855638ms 251.172816ms 252.556902ms 253.085023ms 253.382017ms 255.411514ms 257.250308ms 257.525546ms 261.569794ms 264.595182ms 267.534842ms 272.5597ms 274.956564ms 278.6838ms 285.620588ms 294.505188ms 294.565773ms 297.780841ms 317.014432ms 349.207752ms 391.749932ms 424.506056ms 459.16929ms 496.361938ms 523.375066ms 555.256468ms 587.234676ms 616.419359ms 636.522034ms 673.113829ms 705.225083ms 738.612901ms 740.922086ms 741.724179ms 742.133122ms 743.203783ms 743.321156ms 744.146555ms 744.818093ms 745.197167ms 745.349748ms 745.644004ms 745.91781ms 746.032927ms 746.506122ms 746.518225ms 746.535355ms 746.890162ms 747.00829ms 747.288929ms 747.627119ms 747.831358ms 747.877453ms 747.909828ms 747.935216ms 747.982281ms 748.127715ms 748.219905ms 748.540421ms 748.562591ms 748.570676ms 748.58789ms 748.619547ms 748.636013ms 748.691891ms 748.787437ms 748.842291ms 748.858077ms 748.939772ms 748.958984ms 748.965214ms 748.965544ms 748.984718ms 749.059044ms 749.085576ms 749.164493ms 749.170496ms 749.237033ms 749.243173ms 749.243469ms 749.290546ms 749.293106ms 749.305353ms 749.311739ms 749.314318ms 749.323211ms 749.391811ms 749.404878ms 749.413878ms 749.45045ms 749.458813ms 749.460396ms 749.479675ms 749.484995ms 749.543143ms 749.570954ms 749.581945ms 749.608967ms 749.612878ms 749.626279ms 749.710383ms 749.734057ms 749.797652ms 749.841449ms 749.877856ms 749.881726ms 749.8987ms 749.905809ms 749.90747ms 750.039785ms 750.04642ms 750.068141ms 750.140266ms 750.142369ms 750.154603ms 750.169677ms 750.177428ms 750.188708ms 750.205425ms 750.237935ms 750.323998ms 750.349198ms 750.352065ms 750.383862ms 750.387798ms 750.455759ms 750.460212ms 750.484739ms 750.527636ms 750.530284ms 750.550482ms 750.557014ms 750.568927ms 750.601436ms 750.653986ms 750.678604ms 750.685497ms 750.686469ms 750.691449ms 750.791388ms 750.806669ms 750.879293ms 750.885393ms 750.911848ms 750.918041ms 750.936495ms 751.028629ms 751.155592ms 751.263876ms 751.311021ms 751.340597ms 751.618525ms 751.626453ms 751.74404ms 751.919199ms 752.164781ms 752.329569ms 752.461565ms 752.542049ms 752.731851ms 752.734914ms 752.785083ms 753.000646ms 753.152888ms 753.263559ms 753.400664ms 753.706217ms 753.766381ms 755.240418ms 755.678295ms 756.23317ms 759.861919ms 760.010588ms]
Feb 23 07:20:31.939: INFO: 50 %ile: 749.059044ms
Feb 23 07:20:31.939: INFO: 90 %ile: 751.74404ms
Feb 23 07:20:31.939: INFO: 99 %ile: 759.861919ms
Feb 23 07:20:31.939: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:20:31.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-f5m7x" for this suite.
Feb 23 07:20:43.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:20:44.008: INFO: namespace: e2e-tests-svc-latency-f5m7x, resource: bindings, ignored listing per whitelist
Feb 23 07:20:44.068: INFO: namespace e2e-tests-svc-latency-f5m7x deletion completed in 12.124311726s

• [SLOW TEST:22.930 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:20:44.068: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 23 07:20:44.162: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:44.163: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:44.163: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:44.166: INFO: Number of nodes with available pods: 0
Feb 23 07:20:44.166: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 07:20:45.171: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:45.171: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:45.171: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:45.174: INFO: Number of nodes with available pods: 0
Feb 23 07:20:45.174: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 07:20:46.172: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:46.172: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:46.172: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:46.175: INFO: Number of nodes with available pods: 0
Feb 23 07:20:46.175: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 07:20:47.171: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:47.171: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:47.171: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:47.175: INFO: Number of nodes with available pods: 2
Feb 23 07:20:47.175: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 23 07:20:47.194: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:47.194: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:47.194: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:47.198: INFO: Number of nodes with available pods: 1
Feb 23 07:20:47.198: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 07:20:48.203: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:48.203: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:48.203: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:48.207: INFO: Number of nodes with available pods: 1
Feb 23 07:20:48.207: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 07:20:49.203: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:49.203: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:49.203: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:20:49.207: INFO: Number of nodes with available pods: 2
Feb 23 07:20:49.207: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-vx28k, will wait for the garbage collector to delete the pods
Feb 23 07:20:49.273: INFO: Deleting DaemonSet.extensions daemon-set took: 8.626629ms
Feb 23 07:20:49.374: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.243837ms
Feb 23 07:21:29.677: INFO: Number of nodes with available pods: 0
Feb 23 07:21:29.677: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 07:21:29.680: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-vx28k/daemonsets","resourceVersion":"5818"},"items":null}

Feb 23 07:21:29.682: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-vx28k/pods","resourceVersion":"5818"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:21:29.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-vx28k" for this suite.
Feb 23 07:21:35.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:21:35.740: INFO: namespace: e2e-tests-daemonsets-vx28k, resource: bindings, ignored listing per whitelist
Feb 23 07:21:35.812: INFO: namespace e2e-tests-daemonsets-vx28k deletion completed in 6.117196122s

• [SLOW TEST:51.744 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:21:35.812: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-a72bc47b-373b-11e9-a63e-52687f37ce9e
STEP: Creating secret with name secret-projected-all-test-volume-a72bc464-373b-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 23 07:21:35.892: INFO: Waiting up to 5m0s for pod "projected-volume-a72bc42f-373b-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-cctvt" to be "success or failure"
Feb 23 07:21:35.896: INFO: Pod "projected-volume-a72bc42f-373b-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.565624ms
Feb 23 07:21:37.900: INFO: Pod "projected-volume-a72bc42f-373b-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007581663s
STEP: Saw pod success
Feb 23 07:21:37.900: INFO: Pod "projected-volume-a72bc42f-373b-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:21:37.903: INFO: Trying to get logs from node kube-node-01 pod projected-volume-a72bc42f-373b-11e9-a63e-52687f37ce9e container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 23 07:21:37.923: INFO: Waiting for pod projected-volume-a72bc42f-373b-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:21:37.926: INFO: Pod projected-volume-a72bc42f-373b-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:21:37.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cctvt" for this suite.
Feb 23 07:21:43.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:21:43.995: INFO: namespace: e2e-tests-projected-cctvt, resource: bindings, ignored listing per whitelist
Feb 23 07:21:44.045: INFO: namespace e2e-tests-projected-cctvt deletion completed in 6.115471607s

• [SLOW TEST:8.233 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:21:44.045: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:21:44.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac12f735-373b-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-krrz2" to be "success or failure"
Feb 23 07:21:44.113: INFO: Pod "downwardapi-volume-ac12f735-373b-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.91944ms
Feb 23 07:21:46.117: INFO: Pod "downwardapi-volume-ac12f735-373b-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007594743s
STEP: Saw pod success
Feb 23 07:21:46.117: INFO: Pod "downwardapi-volume-ac12f735-373b-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:21:46.120: INFO: Trying to get logs from node kube-node-02 pod downwardapi-volume-ac12f735-373b-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:21:46.141: INFO: Waiting for pod downwardapi-volume-ac12f735-373b-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:21:46.144: INFO: Pod downwardapi-volume-ac12f735-373b-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:21:46.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-krrz2" for this suite.
Feb 23 07:21:52.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:21:52.252: INFO: namespace: e2e-tests-downward-api-krrz2, resource: bindings, ignored listing per whitelist
Feb 23 07:21:52.267: INFO: namespace e2e-tests-downward-api-krrz2 deletion completed in 6.118795205s

• [SLOW TEST:8.221 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:21:52.267: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 23 07:21:52.329: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:21:55.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-t5n5g" for this suite.
Feb 23 07:22:17.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:22:17.802: INFO: namespace: e2e-tests-init-container-t5n5g, resource: bindings, ignored listing per whitelist
Feb 23 07:22:17.854: INFO: namespace e2e-tests-init-container-t5n5g deletion completed in 22.119399938s

• [SLOW TEST:25.587 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:22:17.854: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-sqhkd
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 07:22:17.920: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 07:22:41.988: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.0.7:8080/dial?request=hostName&protocol=udp&host=10.40.0.6&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-sqhkd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:22:41.988: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:22:42.071: INFO: Waiting for endpoints: map[]
Feb 23 07:22:42.075: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.0.7:8080/dial?request=hostName&protocol=udp&host=10.42.0.6&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-sqhkd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:22:42.075: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:22:42.144: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:22:42.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-sqhkd" for this suite.
Feb 23 07:23:04.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:23:04.204: INFO: namespace: e2e-tests-pod-network-test-sqhkd, resource: bindings, ignored listing per whitelist
Feb 23 07:23:04.268: INFO: namespace e2e-tests-pod-network-test-sqhkd deletion completed in 22.119686251s

• [SLOW TEST:46.414 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:23:04.268: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-dbe53a37-373b-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 07:23:04.345: INFO: Waiting up to 5m0s for pod "pod-secrets-dbe5dd8a-373b-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-secrets-hc9vk" to be "success or failure"
Feb 23 07:23:04.354: INFO: Pod "pod-secrets-dbe5dd8a-373b-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.945307ms
Feb 23 07:23:06.358: INFO: Pod "pod-secrets-dbe5dd8a-373b-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012861471s
STEP: Saw pod success
Feb 23 07:23:06.358: INFO: Pod "pod-secrets-dbe5dd8a-373b-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:23:06.361: INFO: Trying to get logs from node kube-node-02 pod pod-secrets-dbe5dd8a-373b-11e9-a63e-52687f37ce9e container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 07:23:06.383: INFO: Waiting for pod pod-secrets-dbe5dd8a-373b-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:23:06.386: INFO: Pod pod-secrets-dbe5dd8a-373b-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:23:06.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hc9vk" for this suite.
Feb 23 07:23:12.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:23:12.465: INFO: namespace: e2e-tests-secrets-hc9vk, resource: bindings, ignored listing per whitelist
Feb 23 07:23:12.508: INFO: namespace e2e-tests-secrets-hc9vk deletion completed in 6.117688227s

• [SLOW TEST:8.240 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:23:12.508: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:23:18.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-wddrm" for this suite.
Feb 23 07:23:24.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:23:24.861: INFO: namespace: e2e-tests-namespaces-wddrm, resource: bindings, ignored listing per whitelist
Feb 23 07:23:24.864: INFO: namespace e2e-tests-namespaces-wddrm deletion completed in 6.150958609s
STEP: Destroying namespace "e2e-tests-nsdeletetest-cbj46" for this suite.
Feb 23 07:23:24.866: INFO: Namespace e2e-tests-nsdeletetest-cbj46 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-rr5dw" for this suite.
Feb 23 07:23:30.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:23:30.949: INFO: namespace: e2e-tests-nsdeletetest-rr5dw, resource: bindings, ignored listing per whitelist
Feb 23 07:23:30.981: INFO: namespace e2e-tests-nsdeletetest-rr5dw deletion completed in 6.11429583s

• [SLOW TEST:18.472 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:23:30.981: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:23:31.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebd1439e-373b-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-575r5" to be "success or failure"
Feb 23 07:23:31.060: INFO: Pod "downwardapi-volume-ebd1439e-373b-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.863056ms
Feb 23 07:23:33.064: INFO: Pod "downwardapi-volume-ebd1439e-373b-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008821007s
STEP: Saw pod success
Feb 23 07:23:33.064: INFO: Pod "downwardapi-volume-ebd1439e-373b-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:23:33.066: INFO: Trying to get logs from node kube-node-01 pod downwardapi-volume-ebd1439e-373b-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:23:33.088: INFO: Waiting for pod downwardapi-volume-ebd1439e-373b-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:23:33.091: INFO: Pod downwardapi-volume-ebd1439e-373b-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:23:33.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-575r5" for this suite.
Feb 23 07:23:39.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:23:39.118: INFO: namespace: e2e-tests-downward-api-575r5, resource: bindings, ignored listing per whitelist
Feb 23 07:23:39.216: INFO: namespace e2e-tests-downward-api-575r5 deletion completed in 6.121552356s

• [SLOW TEST:8.236 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:23:39.217: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 23 07:23:43.318: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 23 07:23:43.330: INFO: Pod pod-with-prestop-http-hook still exists
Feb 23 07:23:45.330: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 23 07:23:45.333: INFO: Pod pod-with-prestop-http-hook still exists
Feb 23 07:23:47.330: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 23 07:23:47.334: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:23:47.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-8gr8r" for this suite.
Feb 23 07:24:09.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:24:09.442: INFO: namespace: e2e-tests-container-lifecycle-hook-8gr8r, resource: bindings, ignored listing per whitelist
Feb 23 07:24:09.482: INFO: namespace e2e-tests-container-lifecycle-hook-8gr8r deletion completed in 22.137563736s

• [SLOW TEST:30.266 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:24:09.482: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 23 07:24:12.087: INFO: Successfully updated pod "labelsupdate02c405be-373c-11e9-a63e-52687f37ce9e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:24:16.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wf5g6" for this suite.
Feb 23 07:24:38.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:24:38.198: INFO: namespace: e2e-tests-projected-wf5g6, resource: bindings, ignored listing per whitelist
Feb 23 07:24:38.258: INFO: namespace e2e-tests-projected-wf5g6 deletion completed in 22.139048413s

• [SLOW TEST:28.775 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:24:38.258: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-13ea8118-373c-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 07:24:38.332: INFO: Waiting up to 5m0s for pod "pod-configmaps-13eb1fbb-373c-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-configmap-9qb4g" to be "success or failure"
Feb 23 07:24:38.337: INFO: Pod "pod-configmaps-13eb1fbb-373c-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.983384ms
Feb 23 07:24:40.341: INFO: Pod "pod-configmaps-13eb1fbb-373c-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008957427s
STEP: Saw pod success
Feb 23 07:24:40.341: INFO: Pod "pod-configmaps-13eb1fbb-373c-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:24:40.346: INFO: Trying to get logs from node kube-node-01 pod pod-configmaps-13eb1fbb-373c-11e9-a63e-52687f37ce9e container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 07:24:40.366: INFO: Waiting for pod pod-configmaps-13eb1fbb-373c-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:24:40.369: INFO: Pod pod-configmaps-13eb1fbb-373c-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:24:40.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9qb4g" for this suite.
Feb 23 07:24:46.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:24:46.495: INFO: namespace: e2e-tests-configmap-9qb4g, resource: bindings, ignored listing per whitelist
Feb 23 07:24:46.535: INFO: namespace e2e-tests-configmap-9qb4g deletion completed in 6.162076063s

• [SLOW TEST:8.278 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:24:46.535: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-18da9094-373c-11e9-a63e-52687f37ce9e
STEP: Creating secret with name s-test-opt-upd-18da90e6-373c-11e9-a63e-52687f37ce9e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-18da9094-373c-11e9-a63e-52687f37ce9e
STEP: Updating secret s-test-opt-upd-18da90e6-373c-11e9-a63e-52687f37ce9e
STEP: Creating secret with name s-test-opt-create-18da9103-373c-11e9-a63e-52687f37ce9e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:24:50.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-brt9j" for this suite.
Feb 23 07:25:12.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:25:12.819: INFO: namespace: e2e-tests-projected-brt9j, resource: bindings, ignored listing per whitelist
Feb 23 07:25:12.819: INFO: namespace e2e-tests-projected-brt9j deletion completed in 22.116771411s

• [SLOW TEST:26.283 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:25:12.819: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 23 07:25:13.406: INFO: created pod pod-service-account-defaultsa
Feb 23 07:25:13.406: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 23 07:25:13.412: INFO: created pod pod-service-account-mountsa
Feb 23 07:25:13.412: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 23 07:25:13.419: INFO: created pod pod-service-account-nomountsa
Feb 23 07:25:13.419: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 23 07:25:13.429: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 23 07:25:13.431: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 23 07:25:13.439: INFO: created pod pod-service-account-mountsa-mountspec
Feb 23 07:25:13.439: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 23 07:25:13.446: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 23 07:25:13.447: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 23 07:25:13.455: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 23 07:25:13.455: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 23 07:25:13.466: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 23 07:25:13.466: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 23 07:25:13.473: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 23 07:25:13.473: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:25:13.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-5d7jf" for this suite.
Feb 23 07:25:19.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:25:19.612: INFO: namespace: e2e-tests-svcaccounts-5d7jf, resource: bindings, ignored listing per whitelist
Feb 23 07:25:19.617: INFO: namespace e2e-tests-svcaccounts-5d7jf deletion completed in 6.138364444s

• [SLOW TEST:6.798 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:25:19.617: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:25:43.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-kjdl6" for this suite.
Feb 23 07:25:49.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:25:49.938: INFO: namespace: e2e-tests-container-runtime-kjdl6, resource: bindings, ignored listing per whitelist
Feb 23 07:25:50.015: INFO: namespace e2e-tests-container-runtime-kjdl6 deletion completed in 6.119870525s

• [SLOW TEST:30.398 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:25:50.015: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 23 07:25:50.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-26f9r'
Feb 23 07:25:50.322: INFO: stderr: ""
Feb 23 07:25:50.322: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Feb 23 07:25:50.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-26f9r'
Feb 23 07:26:01.823: INFO: stderr: ""
Feb 23 07:26:01.823: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:26:01.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-26f9r" for this suite.
Feb 23 07:26:07.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:26:07.914: INFO: namespace: e2e-tests-kubectl-26f9r, resource: bindings, ignored listing per whitelist
Feb 23 07:26:08.002: INFO: namespace e2e-tests-kubectl-26f9r deletion completed in 6.16822077s

• [SLOW TEST:17.986 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:26:08.002: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:26:08.071: INFO: Waiting up to 5m0s for pod "downwardapi-volume-496856f9-373c-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-zllw8" to be "success or failure"
Feb 23 07:26:08.076: INFO: Pod "downwardapi-volume-496856f9-373c-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.4466ms
Feb 23 07:26:10.082: INFO: Pod "downwardapi-volume-496856f9-373c-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010893646s
STEP: Saw pod success
Feb 23 07:26:10.082: INFO: Pod "downwardapi-volume-496856f9-373c-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:26:10.085: INFO: Trying to get logs from node kube-node-02 pod downwardapi-volume-496856f9-373c-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:26:10.104: INFO: Waiting for pod downwardapi-volume-496856f9-373c-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:26:10.107: INFO: Pod downwardapi-volume-496856f9-373c-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:26:10.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zllw8" for this suite.
Feb 23 07:26:16.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:26:16.188: INFO: namespace: e2e-tests-downward-api-zllw8, resource: bindings, ignored listing per whitelist
Feb 23 07:26:16.237: INFO: namespace e2e-tests-downward-api-zllw8 deletion completed in 6.125637093s

• [SLOW TEST:8.235 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:26:16.237: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 23 07:26:16.304: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-f6dn7,SelfLink:/api/v1/namespaces/e2e-tests-watch-f6dn7/configmaps/e2e-watch-test-watch-closed,UID:4e529160-373c-11e9-8d12-02951b420040,ResourceVersion:6957,Generation:0,CreationTimestamp:2019-02-23 07:26:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 07:26:16.304: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-f6dn7,SelfLink:/api/v1/namespaces/e2e-tests-watch-f6dn7/configmaps/e2e-watch-test-watch-closed,UID:4e529160-373c-11e9-8d12-02951b420040,ResourceVersion:6958,Generation:0,CreationTimestamp:2019-02-23 07:26:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 23 07:26:16.317: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-f6dn7,SelfLink:/api/v1/namespaces/e2e-tests-watch-f6dn7/configmaps/e2e-watch-test-watch-closed,UID:4e529160-373c-11e9-8d12-02951b420040,ResourceVersion:6959,Generation:0,CreationTimestamp:2019-02-23 07:26:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 07:26:16.317: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-f6dn7,SelfLink:/api/v1/namespaces/e2e-tests-watch-f6dn7/configmaps/e2e-watch-test-watch-closed,UID:4e529160-373c-11e9-8d12-02951b420040,ResourceVersion:6960,Generation:0,CreationTimestamp:2019-02-23 07:26:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:26:16.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-f6dn7" for this suite.
Feb 23 07:26:22.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:26:22.490: INFO: namespace: e2e-tests-watch-f6dn7, resource: bindings, ignored listing per whitelist
Feb 23 07:26:22.494: INFO: namespace e2e-tests-watch-f6dn7 deletion completed in 6.167964242s

• [SLOW TEST:6.257 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:26:22.494: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-520b398c-373c-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 07:26:22.567: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-520bd1dd-373c-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-fhhgk" to be "success or failure"
Feb 23 07:26:22.571: INFO: Pod "pod-projected-secrets-520bd1dd-373c-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.349581ms
Feb 23 07:26:24.575: INFO: Pod "pod-projected-secrets-520bd1dd-373c-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008309267s
STEP: Saw pod success
Feb 23 07:26:24.575: INFO: Pod "pod-projected-secrets-520bd1dd-373c-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:26:24.578: INFO: Trying to get logs from node kube-node-01 pod pod-projected-secrets-520bd1dd-373c-11e9-a63e-52687f37ce9e container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 07:26:24.599: INFO: Waiting for pod pod-projected-secrets-520bd1dd-373c-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:26:24.602: INFO: Pod pod-projected-secrets-520bd1dd-373c-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:26:24.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fhhgk" for this suite.
Feb 23 07:26:30.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:26:30.747: INFO: namespace: e2e-tests-projected-fhhgk, resource: bindings, ignored listing per whitelist
Feb 23 07:26:30.749: INFO: namespace e2e-tests-projected-fhhgk deletion completed in 6.143731927s

• [SLOW TEST:8.255 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:26:30.749: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:26:30.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56f6fa3c-373c-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-z4lsn" to be "success or failure"
Feb 23 07:26:30.821: INFO: Pod "downwardapi-volume-56f6fa3c-373c-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015652ms
Feb 23 07:26:32.825: INFO: Pod "downwardapi-volume-56f6fa3c-373c-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008036429s
STEP: Saw pod success
Feb 23 07:26:32.825: INFO: Pod "downwardapi-volume-56f6fa3c-373c-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:26:32.828: INFO: Trying to get logs from node kube-node-02 pod downwardapi-volume-56f6fa3c-373c-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:26:32.848: INFO: Waiting for pod downwardapi-volume-56f6fa3c-373c-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:26:32.851: INFO: Pod downwardapi-volume-56f6fa3c-373c-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:26:32.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-z4lsn" for this suite.
Feb 23 07:26:38.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:26:38.926: INFO: namespace: e2e-tests-downward-api-z4lsn, resource: bindings, ignored listing per whitelist
Feb 23 07:26:38.986: INFO: namespace e2e-tests-downward-api-z4lsn deletion completed in 6.130881315s

• [SLOW TEST:8.237 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:26:38.986: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 23 07:26:39.058: INFO: Waiting up to 5m0s for pod "downward-api-5be0357d-373c-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-g8klf" to be "success or failure"
Feb 23 07:26:39.062: INFO: Pod "downward-api-5be0357d-373c-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.133396ms
Feb 23 07:26:41.066: INFO: Pod "downward-api-5be0357d-373c-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008419522s
STEP: Saw pod success
Feb 23 07:26:41.066: INFO: Pod "downward-api-5be0357d-373c-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:26:41.069: INFO: Trying to get logs from node kube-node-01 pod downward-api-5be0357d-373c-11e9-a63e-52687f37ce9e container dapi-container: <nil>
STEP: delete the pod
Feb 23 07:26:41.093: INFO: Waiting for pod downward-api-5be0357d-373c-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:26:41.097: INFO: Pod downward-api-5be0357d-373c-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:26:41.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-g8klf" for this suite.
Feb 23 07:26:47.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:26:47.189: INFO: namespace: e2e-tests-downward-api-g8klf, resource: bindings, ignored listing per whitelist
Feb 23 07:26:47.218: INFO: namespace e2e-tests-downward-api-g8klf deletion completed in 6.117225488s

• [SLOW TEST:8.232 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:26:47.218: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-nbf8q
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-nbf8q
STEP: Deleting pre-stop pod
Feb 23 07:26:58.390: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:26:58.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-nbf8q" for this suite.
Feb 23 07:27:36.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:27:36.448: INFO: namespace: e2e-tests-prestop-nbf8q, resource: bindings, ignored listing per whitelist
Feb 23 07:27:36.519: INFO: namespace e2e-tests-prestop-nbf8q deletion completed in 38.113981379s

• [SLOW TEST:49.301 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:27:36.519: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-7e2ae116-373c-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 07:27:36.594: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7e2b7964-373c-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-6dtt5" to be "success or failure"
Feb 23 07:27:36.599: INFO: Pod "pod-projected-configmaps-7e2b7964-373c-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.408342ms
Feb 23 07:27:38.602: INFO: Pod "pod-projected-configmaps-7e2b7964-373c-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007963191s
STEP: Saw pod success
Feb 23 07:27:38.602: INFO: Pod "pod-projected-configmaps-7e2b7964-373c-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:27:38.605: INFO: Trying to get logs from node kube-node-02 pod pod-projected-configmaps-7e2b7964-373c-11e9-a63e-52687f37ce9e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 07:27:38.630: INFO: Waiting for pod pod-projected-configmaps-7e2b7964-373c-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:27:38.633: INFO: Pod pod-projected-configmaps-7e2b7964-373c-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:27:38.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6dtt5" for this suite.
Feb 23 07:27:44.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:27:44.719: INFO: namespace: e2e-tests-projected-6dtt5, resource: bindings, ignored listing per whitelist
Feb 23 07:27:44.746: INFO: namespace e2e-tests-projected-6dtt5 deletion completed in 6.109495805s

• [SLOW TEST:8.227 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:27:44.746: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:27:44.812: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8311464b-373c-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-btvjc" to be "success or failure"
Feb 23 07:27:44.816: INFO: Pod "downwardapi-volume-8311464b-373c-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.113998ms
Feb 23 07:27:46.820: INFO: Pod "downwardapi-volume-8311464b-373c-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008051676s
STEP: Saw pod success
Feb 23 07:27:46.820: INFO: Pod "downwardapi-volume-8311464b-373c-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:27:46.823: INFO: Trying to get logs from node kube-node-01 pod downwardapi-volume-8311464b-373c-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:27:46.846: INFO: Waiting for pod downwardapi-volume-8311464b-373c-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:27:46.849: INFO: Pod downwardapi-volume-8311464b-373c-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:27:46.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-btvjc" for this suite.
Feb 23 07:27:52.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:27:52.889: INFO: namespace: e2e-tests-projected-btvjc, resource: bindings, ignored listing per whitelist
Feb 23 07:27:53.000: INFO: namespace e2e-tests-projected-btvjc deletion completed in 6.146922096s

• [SLOW TEST:8.254 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:27:53.000: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 23 07:27:55.604: INFO: Successfully updated pod "annotationupdate87fde6f7-373c-11e9-a63e-52687f37ce9e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:27:59.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bc8fw" for this suite.
Feb 23 07:28:21.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:28:21.743: INFO: namespace: e2e-tests-projected-bc8fw, resource: bindings, ignored listing per whitelist
Feb 23 07:28:21.790: INFO: namespace e2e-tests-projected-bc8fw deletion completed in 22.157264281s

• [SLOW TEST:28.790 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:28:21.790: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:28:21.864: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 23 07:28:26.869: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 23 07:28:26.869: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 23 07:28:28.873: INFO: Creating deployment "test-rollover-deployment"
Feb 23 07:28:28.882: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 23 07:28:30.889: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 23 07:28:30.895: INFO: Ensure that both replica sets have 1 created replica
Feb 23 07:28:30.901: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 23 07:28:30.908: INFO: Updating deployment test-rollover-deployment
Feb 23 07:28:30.908: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 23 07:28:32.917: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 23 07:28:32.923: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 23 07:28:32.929: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 07:28:32.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503710, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 07:28:34.936: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 07:28:34.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503713, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 07:28:36.936: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 07:28:36.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503713, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 07:28:38.936: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 07:28:38.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503713, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 07:28:40.936: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 07:28:40.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503713, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 07:28:42.936: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 07:28:42.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503713, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686503708, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 07:28:44.937: INFO: 
Feb 23 07:28:44.937: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 23 07:28:44.945: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-pfvcw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pfvcw/deployments/test-rollover-deployment,UID:9d58f0bb-373c-11e9-8d12-02951b420040,ResourceVersion:7497,Generation:2,CreationTimestamp:2019-02-23 07:28:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-23 07:28:28 +0000 UTC 2019-02-23 07:28:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-23 07:28:43 +0000 UTC 2019-02-23 07:28:28 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 23 07:28:44.952: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-pfvcw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pfvcw/replicasets/test-rollover-deployment-6b7f9d6597,UID:9e8f7be1-373c-11e9-8d12-02951b420040,ResourceVersion:7488,Generation:2,CreationTimestamp:2019-02-23 07:28:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9d58f0bb-373c-11e9-8d12-02951b420040 0xc002ca1607 0xc002ca1608}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 23 07:28:44.952: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 23 07:28:44.952: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-pfvcw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pfvcw/replicasets/test-rollover-controller,UID:992a33e1-373c-11e9-8d12-02951b420040,ResourceVersion:7496,Generation:2,CreationTimestamp:2019-02-23 07:28:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9d58f0bb-373c-11e9-8d12-02951b420040 0xc002ca147f 0xc002ca1490}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 23 07:28:44.952: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-pfvcw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pfvcw/replicasets/test-rollover-deployment-6586df867b,UID:9d5bb3e8-373c-11e9-8d12-02951b420040,ResourceVersion:7454,Generation:2,CreationTimestamp:2019-02-23 07:28:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9d58f0bb-373c-11e9-8d12-02951b420040 0xc002ca1547 0xc002ca1548}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 23 07:28:44.955: INFO: Pod "test-rollover-deployment-6b7f9d6597-25db4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-25db4,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-pfvcw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-pfvcw/pods/test-rollover-deployment-6b7f9d6597-25db4,UID:9e93ebe2-373c-11e9-8d12-02951b420040,ResourceVersion:7467,Generation:0,CreationTimestamp:2019-02-23 07:28:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 9e8f7be1-373c-11e9-8d12-02951b420040 0xc001d996f7 0xc001d996f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwl8q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwl8q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-kwl8q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d99780} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d997a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:28:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:28:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:28:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:28:30 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.103,PodIP:10.42.0.6,StartTime:2019-02-23 07:28:30 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-23 07:28:32 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://295b757761c77e764d8a1c39cd05295fecc0ae45f98440ab564ddeb6eb2b01f6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:28:44.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-pfvcw" for this suite.
Feb 23 07:28:50.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:28:51.009: INFO: namespace: e2e-tests-deployment-pfvcw, resource: bindings, ignored listing per whitelist
Feb 23 07:28:51.083: INFO: namespace e2e-tests-deployment-pfvcw deletion completed in 6.123841114s

• [SLOW TEST:29.293 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:28:51.083: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-tzfvm
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-tzfvm
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-tzfvm
Feb 23 07:28:51.170: INFO: Found 0 stateful pods, waiting for 1
Feb 23 07:29:01.174: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 23 07:29:01.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-tzfvm ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 23 07:29:01.343: INFO: stderr: ""
Feb 23 07:29:01.343: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 23 07:29:01.343: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 23 07:29:01.348: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 23 07:29:11.353: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 07:29:11.353: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 07:29:11.369: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999532s
Feb 23 07:29:12.373: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994425652s
Feb 23 07:29:13.380: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990108388s
Feb 23 07:29:14.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983827676s
Feb 23 07:29:15.388: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.980220878s
Feb 23 07:29:16.392: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.975981259s
Feb 23 07:29:17.396: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971724724s
Feb 23 07:29:18.400: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.96771195s
Feb 23 07:29:19.405: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963166604s
Feb 23 07:29:20.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 958.958035ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-tzfvm
Feb 23 07:29:21.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-tzfvm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 07:29:21.548: INFO: stderr: ""
Feb 23 07:29:21.548: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 23 07:29:21.548: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 23 07:29:21.552: INFO: Found 1 stateful pods, waiting for 3
Feb 23 07:29:31.556: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 07:29:31.556: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 07:29:31.556: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 23 07:29:31.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-tzfvm ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 23 07:29:31.721: INFO: stderr: ""
Feb 23 07:29:31.721: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 23 07:29:31.721: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 23 07:29:31.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-tzfvm ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 23 07:29:31.889: INFO: stderr: ""
Feb 23 07:29:31.889: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 23 07:29:31.889: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 23 07:29:31.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-tzfvm ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 23 07:29:32.063: INFO: stderr: ""
Feb 23 07:29:32.063: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 23 07:29:32.063: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 23 07:29:32.063: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 07:29:32.067: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 23 07:29:42.074: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 07:29:42.074: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 07:29:42.074: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 07:29:42.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999571s
Feb 23 07:29:43.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996628897s
Feb 23 07:29:44.093: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992183631s
Feb 23 07:29:45.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98795358s
Feb 23 07:29:46.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983667445s
Feb 23 07:29:47.106: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97935821s
Feb 23 07:29:48.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974968492s
Feb 23 07:29:49.115: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970627483s
Feb 23 07:29:50.119: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966210025s
Feb 23 07:29:51.123: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.626178ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-tzfvm
Feb 23 07:29:52.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-tzfvm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 07:29:52.260: INFO: stderr: ""
Feb 23 07:29:52.261: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 23 07:29:52.261: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 23 07:29:52.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-tzfvm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 07:29:52.397: INFO: stderr: ""
Feb 23 07:29:52.398: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 23 07:29:52.398: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 23 07:29:52.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-tzfvm ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 07:29:52.536: INFO: stderr: ""
Feb 23 07:29:52.536: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 23 07:29:52.536: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 23 07:29:52.536: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 23 07:30:02.563: INFO: Deleting all statefulset in ns e2e-tests-statefulset-tzfvm
Feb 23 07:30:02.574: INFO: Scaling statefulset ss to 0
Feb 23 07:30:02.597: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 07:30:02.617: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:30:02.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-tzfvm" for this suite.
Feb 23 07:30:08.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:30:08.788: INFO: namespace: e2e-tests-statefulset-tzfvm, resource: bindings, ignored listing per whitelist
Feb 23 07:30:08.802: INFO: namespace e2e-tests-statefulset-tzfvm deletion completed in 6.132640205s

• [SLOW TEST:77.719 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:30:08.803: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 23 07:30:15.902: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:30:16.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-x8mq6" for this suite.
Feb 23 07:30:38.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:30:39.015: INFO: namespace: e2e-tests-replicaset-x8mq6, resource: bindings, ignored listing per whitelist
Feb 23 07:30:39.044: INFO: namespace e2e-tests-replicaset-x8mq6 deletion completed in 22.11730206s

• [SLOW TEST:30.241 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:30:39.044: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:30:39.121: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 23 07:30:44.125: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 23 07:30:44.125: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 23 07:30:44.142: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-cm8q8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-cm8q8/deployments/test-cleanup-deployment,UID:edf79ae9-373c-11e9-8d12-02951b420040,ResourceVersion:8017,Generation:1,CreationTimestamp:2019-02-23 07:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Feb 23 07:30:44.145: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:30:44.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-cm8q8" for this suite.
Feb 23 07:30:50.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:30:50.213: INFO: namespace: e2e-tests-deployment-cm8q8, resource: bindings, ignored listing per whitelist
Feb 23 07:30:50.285: INFO: namespace e2e-tests-deployment-cm8q8 deletion completed in 6.13077223s

• [SLOW TEST:11.241 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:30:50.286: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 23 07:30:54.389: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6nztb PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:30:54.389: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:30:54.459: INFO: Exec stderr: ""
Feb 23 07:30:54.459: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6nztb PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:30:54.459: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:30:54.523: INFO: Exec stderr: ""
Feb 23 07:30:54.523: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6nztb PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:30:54.523: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:30:54.592: INFO: Exec stderr: ""
Feb 23 07:30:54.592: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6nztb PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:30:54.592: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:30:54.660: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 23 07:30:54.660: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6nztb PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:30:54.660: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:30:54.727: INFO: Exec stderr: ""
Feb 23 07:30:54.727: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6nztb PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:30:54.727: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:30:54.795: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 23 07:30:54.795: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6nztb PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:30:54.795: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:30:54.868: INFO: Exec stderr: ""
Feb 23 07:30:54.868: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6nztb PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:30:54.868: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:30:54.936: INFO: Exec stderr: ""
Feb 23 07:30:54.936: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6nztb PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:30:54.936: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:30:55.003: INFO: Exec stderr: ""
Feb 23 07:30:55.003: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6nztb PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 07:30:55.003: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 07:30:55.071: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:30:55.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-6nztb" for this suite.
Feb 23 07:31:33.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:31:33.163: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-6nztb, resource: bindings, ignored listing per whitelist
Feb 23 07:31:33.207: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-6nztb deletion completed in 38.130514266s

• [SLOW TEST:42.922 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:31:33.207: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-67f9
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 07:31:33.291: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-67f9" in namespace "e2e-tests-subpath-hmrgh" to be "success or failure"
Feb 23 07:31:33.297: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.744356ms
Feb 23 07:31:35.300: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009302909s
Feb 23 07:31:37.304: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Running", Reason="", readiness=false. Elapsed: 4.013289311s
Feb 23 07:31:39.312: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Running", Reason="", readiness=false. Elapsed: 6.020562123s
Feb 23 07:31:41.315: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Running", Reason="", readiness=false. Elapsed: 8.024537878s
Feb 23 07:31:43.320: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Running", Reason="", readiness=false. Elapsed: 10.028715854s
Feb 23 07:31:45.324: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Running", Reason="", readiness=false. Elapsed: 12.032900846s
Feb 23 07:31:47.327: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Running", Reason="", readiness=false. Elapsed: 14.03633085s
Feb 23 07:31:49.331: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Running", Reason="", readiness=false. Elapsed: 16.039997381s
Feb 23 07:31:51.335: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Running", Reason="", readiness=false. Elapsed: 18.043707254s
Feb 23 07:31:53.339: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Running", Reason="", readiness=false. Elapsed: 20.047549519s
Feb 23 07:31:55.343: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Running", Reason="", readiness=false. Elapsed: 22.05158884s
Feb 23 07:31:57.347: INFO: Pod "pod-subpath-test-secret-67f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.055556002s
STEP: Saw pod success
Feb 23 07:31:57.347: INFO: Pod "pod-subpath-test-secret-67f9" satisfied condition "success or failure"
Feb 23 07:31:57.349: INFO: Trying to get logs from node kube-node-02 pod pod-subpath-test-secret-67f9 container test-container-subpath-secret-67f9: <nil>
STEP: delete the pod
Feb 23 07:31:57.378: INFO: Waiting for pod pod-subpath-test-secret-67f9 to disappear
Feb 23 07:31:57.381: INFO: Pod pod-subpath-test-secret-67f9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-67f9
Feb 23 07:31:57.381: INFO: Deleting pod "pod-subpath-test-secret-67f9" in namespace "e2e-tests-subpath-hmrgh"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:31:57.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-hmrgh" for this suite.
Feb 23 07:32:03.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:32:03.514: INFO: namespace: e2e-tests-subpath-hmrgh, resource: bindings, ignored listing per whitelist
Feb 23 07:32:03.518: INFO: namespace e2e-tests-subpath-hmrgh deletion completed in 6.130965684s

• [SLOW TEST:30.311 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:32:03.518: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-1d52e8b3-373d-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 07:32:03.612: INFO: Waiting up to 5m0s for pod "pod-secrets-1d537d2b-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-secrets-4tp8d" to be "success or failure"
Feb 23 07:32:03.615: INFO: Pod "pod-secrets-1d537d2b-373d-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.012387ms
Feb 23 07:32:05.619: INFO: Pod "pod-secrets-1d537d2b-373d-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007596885s
STEP: Saw pod success
Feb 23 07:32:05.619: INFO: Pod "pod-secrets-1d537d2b-373d-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:32:05.622: INFO: Trying to get logs from node kube-node-01 pod pod-secrets-1d537d2b-373d-11e9-a63e-52687f37ce9e container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 07:32:05.648: INFO: Waiting for pod pod-secrets-1d537d2b-373d-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:32:05.651: INFO: Pod pod-secrets-1d537d2b-373d-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:32:05.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4tp8d" for this suite.
Feb 23 07:32:11.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:32:11.717: INFO: namespace: e2e-tests-secrets-4tp8d, resource: bindings, ignored listing per whitelist
Feb 23 07:32:11.794: INFO: namespace e2e-tests-secrets-4tp8d deletion completed in 6.138167s

• [SLOW TEST:8.275 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:32:11.794: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-mlfh
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 07:32:11.888: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mlfh" in namespace "e2e-tests-subpath-krzf6" to be "success or failure"
Feb 23 07:32:11.893: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.772319ms
Feb 23 07:32:13.897: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008785782s
Feb 23 07:32:15.902: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Running", Reason="", readiness=false. Elapsed: 4.014592761s
Feb 23 07:32:17.906: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Running", Reason="", readiness=false. Elapsed: 6.018686182s
Feb 23 07:32:19.910: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Running", Reason="", readiness=false. Elapsed: 8.02269574s
Feb 23 07:32:21.914: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Running", Reason="", readiness=false. Elapsed: 10.026701746s
Feb 23 07:32:23.918: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Running", Reason="", readiness=false. Elapsed: 12.030554833s
Feb 23 07:32:25.922: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Running", Reason="", readiness=false. Elapsed: 14.034008974s
Feb 23 07:32:27.926: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Running", Reason="", readiness=false. Elapsed: 16.038149574s
Feb 23 07:32:29.930: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Running", Reason="", readiness=false. Elapsed: 18.041856606s
Feb 23 07:32:31.933: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Running", Reason="", readiness=false. Elapsed: 20.045422297s
Feb 23 07:32:33.937: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Running", Reason="", readiness=false. Elapsed: 22.049000247s
Feb 23 07:32:35.940: INFO: Pod "pod-subpath-test-projected-mlfh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.052397158s
STEP: Saw pod success
Feb 23 07:32:35.940: INFO: Pod "pod-subpath-test-projected-mlfh" satisfied condition "success or failure"
Feb 23 07:32:35.943: INFO: Trying to get logs from node kube-node-02 pod pod-subpath-test-projected-mlfh container test-container-subpath-projected-mlfh: <nil>
STEP: delete the pod
Feb 23 07:32:35.964: INFO: Waiting for pod pod-subpath-test-projected-mlfh to disappear
Feb 23 07:32:35.967: INFO: Pod pod-subpath-test-projected-mlfh no longer exists
STEP: Deleting pod pod-subpath-test-projected-mlfh
Feb 23 07:32:35.967: INFO: Deleting pod "pod-subpath-test-projected-mlfh" in namespace "e2e-tests-subpath-krzf6"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:32:35.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-krzf6" for this suite.
Feb 23 07:32:41.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:32:42.110: INFO: namespace: e2e-tests-subpath-krzf6, resource: bindings, ignored listing per whitelist
Feb 23 07:32:42.130: INFO: namespace e2e-tests-subpath-krzf6 deletion completed in 6.152842584s

• [SLOW TEST:30.336 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:32:42.130: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Feb 23 07:32:42.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-sr2ct'
Feb 23 07:32:42.346: INFO: stderr: ""
Feb 23 07:32:42.346: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 23 07:32:43.352: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 07:32:43.352: INFO: Found 1 / 1
Feb 23 07:32:43.352: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 23 07:32:43.355: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 07:32:43.355: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 23 07:32:43.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 logs redis-master-85c2d redis-master --namespace=e2e-tests-kubectl-sr2ct'
Feb 23 07:32:43.433: INFO: stderr: ""
Feb 23 07:32:43.433: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Feb 07:32:43.042 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Feb 07:32:43.042 # Server started, Redis version 3.2.12\n1:M 23 Feb 07:32:43.042 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Feb 07:32:43.042 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 23 07:32:43.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 log redis-master-85c2d redis-master --namespace=e2e-tests-kubectl-sr2ct --tail=1'
Feb 23 07:32:43.516: INFO: stderr: ""
Feb 23 07:32:43.516: INFO: stdout: "1:M 23 Feb 07:32:43.042 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 23 07:32:43.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 log redis-master-85c2d redis-master --namespace=e2e-tests-kubectl-sr2ct --limit-bytes=1'
Feb 23 07:32:43.605: INFO: stderr: ""
Feb 23 07:32:43.605: INFO: stdout: " "
STEP: exposing timestamps
Feb 23 07:32:43.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 log redis-master-85c2d redis-master --namespace=e2e-tests-kubectl-sr2ct --tail=1 --timestamps'
Feb 23 07:32:43.690: INFO: stderr: ""
Feb 23 07:32:43.690: INFO: stdout: "2019-02-23T07:32:43.045840603Z 1:M 23 Feb 07:32:43.042 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 23 07:32:46.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 log redis-master-85c2d redis-master --namespace=e2e-tests-kubectl-sr2ct --since=1s'
Feb 23 07:32:46.270: INFO: stderr: ""
Feb 23 07:32:46.270: INFO: stdout: ""
Feb 23 07:32:46.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 log redis-master-85c2d redis-master --namespace=e2e-tests-kubectl-sr2ct --since=24h'
Feb 23 07:32:46.352: INFO: stderr: ""
Feb 23 07:32:46.352: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Feb 07:32:43.042 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Feb 07:32:43.042 # Server started, Redis version 3.2.12\n1:M 23 Feb 07:32:43.042 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Feb 07:32:43.042 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Feb 23 07:32:46.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-sr2ct'
Feb 23 07:32:46.429: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 07:32:46.429: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 23 07:32:46.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-sr2ct'
Feb 23 07:32:46.512: INFO: stderr: "No resources found.\n"
Feb 23 07:32:46.512: INFO: stdout: ""
Feb 23 07:32:46.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -l name=nginx --namespace=e2e-tests-kubectl-sr2ct -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 07:32:46.591: INFO: stderr: ""
Feb 23 07:32:46.591: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:32:46.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sr2ct" for this suite.
Feb 23 07:32:52.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:32:52.665: INFO: namespace: e2e-tests-kubectl-sr2ct, resource: bindings, ignored listing per whitelist
Feb 23 07:32:52.717: INFO: namespace e2e-tests-kubectl-sr2ct deletion completed in 6.12102227s

• [SLOW TEST:10.586 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:32:52.717: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 23 07:32:55.313: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3aa32c6f-373d-11e9-a63e-52687f37ce9e"
Feb 23 07:32:55.313: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3aa32c6f-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-pods-bqz7n" to be "terminated due to deadline exceeded"
Feb 23 07:32:55.322: INFO: Pod "pod-update-activedeadlineseconds-3aa32c6f-373d-11e9-a63e-52687f37ce9e": Phase="Running", Reason="", readiness=true. Elapsed: 9.254643ms
Feb 23 07:32:57.326: INFO: Pod "pod-update-activedeadlineseconds-3aa32c6f-373d-11e9-a63e-52687f37ce9e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013064132s
Feb 23 07:32:59.330: INFO: Pod "pod-update-activedeadlineseconds-3aa32c6f-373d-11e9-a63e-52687f37ce9e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.016521949s
Feb 23 07:32:59.330: INFO: Pod "pod-update-activedeadlineseconds-3aa32c6f-373d-11e9-a63e-52687f37ce9e" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:32:59.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bqz7n" for this suite.
Feb 23 07:33:05.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:33:05.356: INFO: namespace: e2e-tests-pods-bqz7n, resource: bindings, ignored listing per whitelist
Feb 23 07:33:05.457: INFO: namespace e2e-tests-pods-bqz7n deletion completed in 6.123051607s

• [SLOW TEST:12.740 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:33:05.457: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 23 07:33:05.533: INFO: Waiting up to 5m0s for pod "pod-423b656e-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-hpm8p" to be "success or failure"
Feb 23 07:33:05.538: INFO: Pod "pod-423b656e-373d-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.515347ms
Feb 23 07:33:07.542: INFO: Pod "pod-423b656e-373d-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008498622s
STEP: Saw pod success
Feb 23 07:33:07.542: INFO: Pod "pod-423b656e-373d-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:33:07.545: INFO: Trying to get logs from node kube-node-01 pod pod-423b656e-373d-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 07:33:07.566: INFO: Waiting for pod pod-423b656e-373d-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:33:07.569: INFO: Pod pod-423b656e-373d-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:33:07.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hpm8p" for this suite.
Feb 23 07:33:13.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:33:13.604: INFO: namespace: e2e-tests-emptydir-hpm8p, resource: bindings, ignored listing per whitelist
Feb 23 07:33:13.699: INFO: namespace e2e-tests-emptydir-hpm8p deletion completed in 6.125548448s

• [SLOW TEST:8.242 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:33:13.699: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-vv258/configmap-test-472471af-373d-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 07:33:13.772: INFO: Waiting up to 5m0s for pod "pod-configmaps-47252965-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-configmap-vv258" to be "success or failure"
Feb 23 07:33:13.776: INFO: Pod "pod-configmaps-47252965-373d-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.920634ms
Feb 23 07:33:15.780: INFO: Pod "pod-configmaps-47252965-373d-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007935456s
STEP: Saw pod success
Feb 23 07:33:15.780: INFO: Pod "pod-configmaps-47252965-373d-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:33:15.783: INFO: Trying to get logs from node kube-node-02 pod pod-configmaps-47252965-373d-11e9-a63e-52687f37ce9e container env-test: <nil>
STEP: delete the pod
Feb 23 07:33:15.805: INFO: Waiting for pod pod-configmaps-47252965-373d-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:33:15.810: INFO: Pod pod-configmaps-47252965-373d-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:33:15.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vv258" for this suite.
Feb 23 07:33:21.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:33:21.862: INFO: namespace: e2e-tests-configmap-vv258, resource: bindings, ignored listing per whitelist
Feb 23 07:33:21.933: INFO: namespace e2e-tests-configmap-vv258 deletion completed in 6.119368778s

• [SLOW TEST:8.234 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:33:21.933: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:33:22.004: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:33:23.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-96prd" for this suite.
Feb 23 07:33:29.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:33:29.144: INFO: namespace: e2e-tests-custom-resource-definition-96prd, resource: bindings, ignored listing per whitelist
Feb 23 07:33:29.227: INFO: namespace e2e-tests-custom-resource-definition-96prd deletion completed in 6.169098837s

• [SLOW TEST:7.294 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:33:29.227: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-5066d8a3-373d-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 07:33:29.353: INFO: Waiting up to 5m0s for pod "pod-secrets-506da64a-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-secrets-hcc9q" to be "success or failure"
Feb 23 07:33:29.358: INFO: Pod "pod-secrets-506da64a-373d-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.564411ms
Feb 23 07:33:31.362: INFO: Pod "pod-secrets-506da64a-373d-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009493977s
STEP: Saw pod success
Feb 23 07:33:31.362: INFO: Pod "pod-secrets-506da64a-373d-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:33:31.365: INFO: Trying to get logs from node kube-node-01 pod pod-secrets-506da64a-373d-11e9-a63e-52687f37ce9e container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 07:33:31.390: INFO: Waiting for pod pod-secrets-506da64a-373d-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:33:31.393: INFO: Pod pod-secrets-506da64a-373d-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:33:31.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hcc9q" for this suite.
Feb 23 07:33:37.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:33:37.519: INFO: namespace: e2e-tests-secrets-hcc9q, resource: bindings, ignored listing per whitelist
Feb 23 07:33:37.553: INFO: namespace e2e-tests-secrets-hcc9q deletion completed in 6.148263103s
STEP: Destroying namespace "e2e-tests-secret-namespace-zjqsc" for this suite.
Feb 23 07:33:43.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:33:43.655: INFO: namespace: e2e-tests-secret-namespace-zjqsc, resource: bindings, ignored listing per whitelist
Feb 23 07:33:43.720: INFO: namespace e2e-tests-secret-namespace-zjqsc deletion completed in 6.166503178s

• [SLOW TEST:14.493 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:33:43.720: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:34:07.830: INFO: Container started at 2019-02-23 07:33:45 +0000 UTC, pod became ready at 2019-02-23 07:34:07 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:34:07.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-h767c" for this suite.
Feb 23 07:34:29.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:34:29.993: INFO: namespace: e2e-tests-container-probe-h767c, resource: bindings, ignored listing per whitelist
Feb 23 07:34:30.101: INFO: namespace e2e-tests-container-probe-h767c deletion completed in 22.266912158s

• [SLOW TEST:46.382 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:34:30.102: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-74b1cba7-373d-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 07:34:30.196: INFO: Waiting up to 5m0s for pod "pod-configmaps-74b28208-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-configmap-mf9tb" to be "success or failure"
Feb 23 07:34:30.201: INFO: Pod "pod-configmaps-74b28208-373d-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.75855ms
Feb 23 07:34:32.206: INFO: Pod "pod-configmaps-74b28208-373d-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010135424s
STEP: Saw pod success
Feb 23 07:34:32.206: INFO: Pod "pod-configmaps-74b28208-373d-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:34:32.221: INFO: Trying to get logs from node kube-node-01 pod pod-configmaps-74b28208-373d-11e9-a63e-52687f37ce9e container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 07:34:32.297: INFO: Waiting for pod pod-configmaps-74b28208-373d-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:34:32.306: INFO: Pod pod-configmaps-74b28208-373d-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:34:32.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mf9tb" for this suite.
Feb 23 07:34:38.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:34:38.437: INFO: namespace: e2e-tests-configmap-mf9tb, resource: bindings, ignored listing per whitelist
Feb 23 07:34:38.480: INFO: namespace e2e-tests-configmap-mf9tb deletion completed in 6.170603331s

• [SLOW TEST:8.379 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:34:38.480: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 23 07:34:38.543: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-203633079 proxy --unix-socket=/tmp/kubectl-proxy-unix883728810/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:34:38.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wh2tw" for this suite.
Feb 23 07:34:44.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:34:44.716: INFO: namespace: e2e-tests-kubectl-wh2tw, resource: bindings, ignored listing per whitelist
Feb 23 07:34:44.727: INFO: namespace e2e-tests-kubectl-wh2tw deletion completed in 6.127686749s

• [SLOW TEST:6.246 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:34:44.727: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:34:44.800: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d66dae5-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-cch7p" to be "success or failure"
Feb 23 07:34:44.805: INFO: Pod "downwardapi-volume-7d66dae5-373d-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.708405ms
Feb 23 07:34:46.809: INFO: Pod "downwardapi-volume-7d66dae5-373d-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008599268s
STEP: Saw pod success
Feb 23 07:34:46.809: INFO: Pod "downwardapi-volume-7d66dae5-373d-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:34:46.811: INFO: Trying to get logs from node kube-node-02 pod downwardapi-volume-7d66dae5-373d-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:34:46.832: INFO: Waiting for pod downwardapi-volume-7d66dae5-373d-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:34:46.835: INFO: Pod downwardapi-volume-7d66dae5-373d-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:34:46.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cch7p" for this suite.
Feb 23 07:34:52.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:34:52.920: INFO: namespace: e2e-tests-projected-cch7p, resource: bindings, ignored listing per whitelist
Feb 23 07:34:52.969: INFO: namespace e2e-tests-projected-cch7p deletion completed in 6.128996925s

• [SLOW TEST:8.242 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:34:52.970: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-824ff713-373d-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 07:34:53.043: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8250984c-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-lg9c4" to be "success or failure"
Feb 23 07:34:53.047: INFO: Pod "pod-projected-secrets-8250984c-373d-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.737009ms
Feb 23 07:34:55.051: INFO: Pod "pod-projected-secrets-8250984c-373d-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007748544s
STEP: Saw pod success
Feb 23 07:34:55.051: INFO: Pod "pod-projected-secrets-8250984c-373d-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:34:55.054: INFO: Trying to get logs from node kube-node-01 pod pod-projected-secrets-8250984c-373d-11e9-a63e-52687f37ce9e container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 07:34:55.072: INFO: Waiting for pod pod-projected-secrets-8250984c-373d-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:34:55.075: INFO: Pod pod-projected-secrets-8250984c-373d-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:34:55.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lg9c4" for this suite.
Feb 23 07:35:01.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:35:01.176: INFO: namespace: e2e-tests-projected-lg9c4, resource: bindings, ignored listing per whitelist
Feb 23 07:35:01.213: INFO: namespace e2e-tests-projected-lg9c4 deletion completed in 6.133857595s

• [SLOW TEST:8.243 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:35:01.213: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-z2xhh A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-z2xhh;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-z2xhh A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-z2xhh;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-z2xhh.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-z2xhh.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-z2xhh.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-z2xhh.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-z2xhh.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 172.163.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.163.172_udp@PTR;check="$$(dig +tcp +noall +answer +search 172.163.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.163.172_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-z2xhh A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-z2xhh;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-z2xhh A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-z2xhh;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-z2xhh.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-z2xhh.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-z2xhh.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-z2xhh.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-z2xhh.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 172.163.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.163.172_udp@PTR;check="$$(dig +tcp +noall +answer +search 172.163.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.163.172_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 07:35:11.349: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.353: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.357: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-z2xhh from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.361: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-z2xhh from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.367: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.371: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.375: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.378: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.382: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.386: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.392: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.400: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.404: INFO: Unable to read 10.100.163.172_udp@PTR from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.408: INFO: Unable to read 10.100.163.172_tcp@PTR from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.412: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.415: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.419: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-z2xhh from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.423: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-z2xhh from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.428: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.431: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.435: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.439: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.447: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.451: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.455: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.460: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.463: INFO: Unable to read 10.100.163.172_udp@PTR from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.468: INFO: Unable to read 10.100.163.172_tcp@PTR from pod e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e)
Feb 23 07:35:11.468: INFO: Lookups using e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-z2xhh wheezy_tcp@dns-test-service.e2e-tests-dns-z2xhh wheezy_udp@dns-test-service.e2e-tests-dns-z2xhh.svc wheezy_tcp@dns-test-service.e2e-tests-dns-z2xhh.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.100.163.172_udp@PTR 10.100.163.172_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-z2xhh jessie_tcp@dns-test-service.e2e-tests-dns-z2xhh jessie_udp@dns-test-service.e2e-tests-dns-z2xhh.svc jessie_tcp@dns-test-service.e2e-tests-dns-z2xhh.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-z2xhh.svc jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-z2xhh.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.100.163.172_udp@PTR 10.100.163.172_tcp@PTR]

Feb 23 07:35:16.587: INFO: DNS probes using e2e-tests-dns-z2xhh/dns-test-873f6ea4-373d-11e9-a63e-52687f37ce9e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:35:16.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-z2xhh" for this suite.
Feb 23 07:35:22.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:35:22.714: INFO: namespace: e2e-tests-dns-z2xhh, resource: bindings, ignored listing per whitelist
Feb 23 07:35:22.785: INFO: namespace e2e-tests-dns-z2xhh deletion completed in 6.125148689s

• [SLOW TEST:21.572 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:35:22.785: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:35:22.857: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 23 07:35:22.867: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 23 07:35:27.872: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 23 07:35:27.872: INFO: Creating deployment "test-rolling-update-deployment"
Feb 23 07:35:27.877: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 23 07:35:27.883: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 23 07:35:29.891: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 23 07:35:29.893: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 23 07:35:29.902: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-7f999,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7f999/deployments/test-rolling-update-deployment,UID:9717e4df-373d-11e9-8d12-02951b420040,ResourceVersion:9110,Generation:1,CreationTimestamp:2019-02-23 07:35:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-23 07:35:27 +0000 UTC 2019-02-23 07:35:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-23 07:35:29 +0000 UTC 2019-02-23 07:35:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 23 07:35:29.905: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-7f999,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7f999/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:971a91ec-373d-11e9-8d12-02951b420040,ResourceVersion:9101,Generation:1,CreationTimestamp:2019-02-23 07:35:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 9717e4df-373d-11e9-8d12-02951b420040 0xc002dc5b47 0xc002dc5b48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 23 07:35:29.905: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 23 07:35:29.905: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-7f999,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7f999/replicasets/test-rolling-update-controller,UID:941ac021-373d-11e9-8d12-02951b420040,ResourceVersion:9109,Generation:2,CreationTimestamp:2019-02-23 07:35:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 9717e4df-373d-11e9-8d12-02951b420040 0xc002dc5717 0xc002dc5718}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 23 07:35:29.909: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-rtcp6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-rtcp6,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-7f999,SelfLink:/api/v1/namespaces/e2e-tests-deployment-7f999/pods/test-rolling-update-deployment-68b55d7bc6-rtcp6,UID:971b8e06-373d-11e9-8d12-02951b420040,ResourceVersion:9100,Generation:0,CreationTimestamp:2019-02-23 07:35:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 971a91ec-373d-11e9-8d12-02951b420040 0xc00200e1c7 0xc00200e1c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-lq7nw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lq7nw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-lq7nw true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00200e240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00200e260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:35:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:35:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:35:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:35:27 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.103,PodIP:10.42.0.6,StartTime:2019-02-23 07:35:27 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-23 07:35:28 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7b048f67016015793faae73fa57fff59acc3727a530fd106c9d16d92d4f74c0f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:35:29.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-7f999" for this suite.
Feb 23 07:35:35.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:35:35.971: INFO: namespace: e2e-tests-deployment-7f999, resource: bindings, ignored listing per whitelist
Feb 23 07:35:36.068: INFO: namespace e2e-tests-deployment-7f999 deletion completed in 6.155540666s

• [SLOW TEST:13.283 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:35:36.068: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 23 07:35:36.150: INFO: Waiting up to 5m0s for pod "var-expansion-9c022760-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-var-expansion-7fxkg" to be "success or failure"
Feb 23 07:35:36.155: INFO: Pod "var-expansion-9c022760-373d-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.148458ms
Feb 23 07:35:38.159: INFO: Pod "var-expansion-9c022760-373d-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009162423s
STEP: Saw pod success
Feb 23 07:35:38.159: INFO: Pod "var-expansion-9c022760-373d-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:35:38.162: INFO: Trying to get logs from node kube-node-01 pod var-expansion-9c022760-373d-11e9-a63e-52687f37ce9e container dapi-container: <nil>
STEP: delete the pod
Feb 23 07:35:38.183: INFO: Waiting for pod var-expansion-9c022760-373d-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:35:38.186: INFO: Pod var-expansion-9c022760-373d-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:35:38.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-7fxkg" for this suite.
Feb 23 07:35:44.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:35:44.311: INFO: namespace: e2e-tests-var-expansion-7fxkg, resource: bindings, ignored listing per whitelist
Feb 23 07:35:44.336: INFO: namespace e2e-tests-var-expansion-7fxkg deletion completed in 6.14626167s

• [SLOW TEST:8.268 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:35:44.336: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-a0ee20bb-373d-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 07:35:44.412: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a0eec6c9-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-b5z6t" to be "success or failure"
Feb 23 07:35:44.418: INFO: Pod "pod-projected-secrets-a0eec6c9-373d-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.758691ms
Feb 23 07:35:46.422: INFO: Pod "pod-projected-secrets-a0eec6c9-373d-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009801673s
STEP: Saw pod success
Feb 23 07:35:46.422: INFO: Pod "pod-projected-secrets-a0eec6c9-373d-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:35:46.426: INFO: Trying to get logs from node kube-node-02 pod pod-projected-secrets-a0eec6c9-373d-11e9-a63e-52687f37ce9e container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 07:35:46.447: INFO: Waiting for pod pod-projected-secrets-a0eec6c9-373d-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:35:46.453: INFO: Pod pod-projected-secrets-a0eec6c9-373d-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:35:46.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-b5z6t" for this suite.
Feb 23 07:35:52.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:35:52.518: INFO: namespace: e2e-tests-projected-b5z6t, resource: bindings, ignored listing per whitelist
Feb 23 07:35:52.625: INFO: namespace e2e-tests-projected-b5z6t deletion completed in 6.167373084s

• [SLOW TEST:8.289 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:35:52.626: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 23 07:35:52.709: INFO: Waiting up to 5m0s for pod "client-containers-a5e0b9f6-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-containers-jpn87" to be "success or failure"
Feb 23 07:35:52.714: INFO: Pod "client-containers-a5e0b9f6-373d-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.216592ms
Feb 23 07:35:54.718: INFO: Pod "client-containers-a5e0b9f6-373d-11e9-a63e-52687f37ce9e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008338874s
Feb 23 07:35:56.722: INFO: Pod "client-containers-a5e0b9f6-373d-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012345751s
STEP: Saw pod success
Feb 23 07:35:56.722: INFO: Pod "client-containers-a5e0b9f6-373d-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:35:56.725: INFO: Trying to get logs from node kube-node-01 pod client-containers-a5e0b9f6-373d-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 07:35:56.744: INFO: Waiting for pod client-containers-a5e0b9f6-373d-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:35:56.748: INFO: Pod client-containers-a5e0b9f6-373d-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:35:56.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-jpn87" for this suite.
Feb 23 07:36:02.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:36:02.909: INFO: namespace: e2e-tests-containers-jpn87, resource: bindings, ignored listing per whitelist
Feb 23 07:36:02.912: INFO: namespace e2e-tests-containers-jpn87 deletion completed in 6.160742601s

• [SLOW TEST:10.287 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:36:02.913: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:36:03.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac02b39e-373d-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-z2vg7" to be "success or failure"
Feb 23 07:36:03.009: INFO: Pod "downwardapi-volume-ac02b39e-373d-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.943289ms
Feb 23 07:36:05.013: INFO: Pod "downwardapi-volume-ac02b39e-373d-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010761762s
STEP: Saw pod success
Feb 23 07:36:05.013: INFO: Pod "downwardapi-volume-ac02b39e-373d-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:36:05.016: INFO: Trying to get logs from node kube-node-02 pod downwardapi-volume-ac02b39e-373d-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:36:05.038: INFO: Waiting for pod downwardapi-volume-ac02b39e-373d-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:36:05.041: INFO: Pod downwardapi-volume-ac02b39e-373d-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:36:05.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-z2vg7" for this suite.
Feb 23 07:36:11.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:36:11.093: INFO: namespace: e2e-tests-downward-api-z2vg7, resource: bindings, ignored listing per whitelist
Feb 23 07:36:11.222: INFO: namespace e2e-tests-downward-api-z2vg7 deletion completed in 6.17716606s

• [SLOW TEST:8.309 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:36:11.222: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Feb 23 07:36:12.344: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:36:12.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-nf2ql" for this suite.
Feb 23 07:36:18.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:36:18.395: INFO: namespace: e2e-tests-gc-nf2ql, resource: bindings, ignored listing per whitelist
Feb 23 07:36:18.479: INFO: namespace e2e-tests-gc-nf2ql deletion completed in 6.13126765s

• [SLOW TEST:7.257 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:36:18.479: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-6m4t9
Feb 23 07:36:20.563: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-6m4t9
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 07:36:20.566: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:40:21.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-6m4t9" for this suite.
Feb 23 07:40:27.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:40:27.205: INFO: namespace: e2e-tests-container-probe-6m4t9, resource: bindings, ignored listing per whitelist
Feb 23 07:40:27.298: INFO: namespace e2e-tests-container-probe-6m4t9 deletion completed in 6.127672514s

• [SLOW TEST:248.819 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:40:27.298: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 23 07:40:27.367: INFO: Waiting up to 5m0s for pod "pod-49968cb7-373e-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-lvnjx" to be "success or failure"
Feb 23 07:40:27.373: INFO: Pod "pod-49968cb7-373e-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.831075ms
Feb 23 07:40:29.377: INFO: Pod "pod-49968cb7-373e-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009778913s
STEP: Saw pod success
Feb 23 07:40:29.377: INFO: Pod "pod-49968cb7-373e-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:40:29.381: INFO: Trying to get logs from node kube-node-02 pod pod-49968cb7-373e-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 07:40:29.403: INFO: Waiting for pod pod-49968cb7-373e-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:40:29.406: INFO: Pod pod-49968cb7-373e-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:40:29.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lvnjx" for this suite.
Feb 23 07:40:35.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:40:35.473: INFO: namespace: e2e-tests-emptydir-lvnjx, resource: bindings, ignored listing per whitelist
Feb 23 07:40:35.560: INFO: namespace e2e-tests-emptydir-lvnjx deletion completed in 6.150333828s

• [SLOW TEST:8.262 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:40:35.560: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 23 07:40:35.713: INFO: Waiting up to 5m0s for pod "downward-api-4e8ffba2-373e-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-f5vpb" to be "success or failure"
Feb 23 07:40:35.719: INFO: Pod "downward-api-4e8ffba2-373e-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.835997ms
Feb 23 07:40:37.723: INFO: Pod "downward-api-4e8ffba2-373e-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010107202s
STEP: Saw pod success
Feb 23 07:40:37.723: INFO: Pod "downward-api-4e8ffba2-373e-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:40:37.726: INFO: Trying to get logs from node kube-node-01 pod downward-api-4e8ffba2-373e-11e9-a63e-52687f37ce9e container dapi-container: <nil>
STEP: delete the pod
Feb 23 07:40:37.752: INFO: Waiting for pod downward-api-4e8ffba2-373e-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:40:37.755: INFO: Pod downward-api-4e8ffba2-373e-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:40:37.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-f5vpb" for this suite.
Feb 23 07:40:43.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:40:43.800: INFO: namespace: e2e-tests-downward-api-f5vpb, resource: bindings, ignored listing per whitelist
Feb 23 07:40:43.878: INFO: namespace e2e-tests-downward-api-f5vpb deletion completed in 6.119405904s

• [SLOW TEST:8.318 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:40:43.878: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:40:43.955: INFO: (0) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.51863ms)
Feb 23 07:40:43.958: INFO: (1) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.480877ms)
Feb 23 07:40:43.962: INFO: (2) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.397926ms)
Feb 23 07:40:43.965: INFO: (3) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.658967ms)
Feb 23 07:40:43.969: INFO: (4) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.420457ms)
Feb 23 07:40:43.972: INFO: (5) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.573096ms)
Feb 23 07:40:43.976: INFO: (6) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.583675ms)
Feb 23 07:40:43.980: INFO: (7) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.901053ms)
Feb 23 07:40:43.984: INFO: (8) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.644603ms)
Feb 23 07:40:43.987: INFO: (9) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.286666ms)
Feb 23 07:40:43.990: INFO: (10) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.415569ms)
Feb 23 07:40:43.994: INFO: (11) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.463258ms)
Feb 23 07:40:43.997: INFO: (12) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.47329ms)
Feb 23 07:40:44.001: INFO: (13) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.478995ms)
Feb 23 07:40:44.004: INFO: (14) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.24289ms)
Feb 23 07:40:44.007: INFO: (15) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.271362ms)
Feb 23 07:40:44.011: INFO: (16) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.442441ms)
Feb 23 07:40:44.014: INFO: (17) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.440017ms)
Feb 23 07:40:44.018: INFO: (18) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.469458ms)
Feb 23 07:40:44.021: INFO: (19) /api/v1/nodes/kube-node-01/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.546698ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:40:44.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-zdqth" for this suite.
Feb 23 07:40:50.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:40:50.092: INFO: namespace: e2e-tests-proxy-zdqth, resource: bindings, ignored listing per whitelist
Feb 23 07:40:50.219: INFO: namespace e2e-tests-proxy-zdqth deletion completed in 6.193376233s

• [SLOW TEST:6.341 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:40:50.219: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Feb 23 07:41:20.867: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:41:20.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-v6xgd" for this suite.
Feb 23 07:41:26.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:41:26.943: INFO: namespace: e2e-tests-gc-v6xgd, resource: bindings, ignored listing per whitelist
Feb 23 07:41:27.050: INFO: namespace e2e-tests-gc-v6xgd deletion completed in 6.178396807s

• [SLOW TEST:36.832 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:41:27.051: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 23 07:41:27.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 api-versions'
Feb 23 07:41:27.521: INFO: stderr: ""
Feb 23 07:41:27.521: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrook.io/v1alpha1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:41:27.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-426nm" for this suite.
Feb 23 07:41:33.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:41:33.594: INFO: namespace: e2e-tests-kubectl-426nm, resource: bindings, ignored listing per whitelist
Feb 23 07:41:33.661: INFO: namespace e2e-tests-kubectl-426nm deletion completed in 6.132902152s

• [SLOW TEST:6.611 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:41:33.662: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 23 07:41:33.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 --namespace=e2e-tests-kubectl-s8wvx run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 23 07:41:34.956: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 23 07:41:34.956: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:41:36.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-s8wvx" for this suite.
Feb 23 07:41:42.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:41:43.188: INFO: namespace: e2e-tests-kubectl-s8wvx, resource: bindings, ignored listing per whitelist
Feb 23 07:41:43.211: INFO: namespace e2e-tests-kubectl-s8wvx deletion completed in 6.240188433s

• [SLOW TEST:9.550 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:41:43.211: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 23 07:41:45.328: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-76dab171-373e-11e9-a63e-52687f37ce9e,GenerateName:,Namespace:e2e-tests-events-v7x66,SelfLink:/api/v1/namespaces/e2e-tests-events-v7x66/pods/send-events-76dab171-373e-11e9-a63e-52687f37ce9e,UID:76df5def-373e-11e9-8d12-02951b420040,ResourceVersion:10158,Generation:0,CreationTimestamp:2019-02-23 07:41:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 305462486,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-5ffsd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5ffsd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-5ffsd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0015856c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0015856e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:41:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:41:44 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:41:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:41:43 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.102,PodIP:10.40.0.6,StartTime:2019-02-23 07:41:43 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-23 07:41:44 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://beecc04f6d957912ed896be585ae0175cb1440b63c84020a9fa2489bce8b3b46}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 23 07:41:47.332: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 23 07:41:49.336: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:41:49.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-v7x66" for this suite.
Feb 23 07:42:35.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:42:35.426: INFO: namespace: e2e-tests-events-v7x66, resource: bindings, ignored listing per whitelist
Feb 23 07:42:35.495: INFO: namespace e2e-tests-events-v7x66 deletion completed in 46.147753843s

• [SLOW TEST:52.283 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:42:35.495: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-l85c4/secret-test-96063d60-373e-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 07:42:35.612: INFO: Waiting up to 5m0s for pod "pod-configmaps-9606e128-373e-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-secrets-l85c4" to be "success or failure"
Feb 23 07:42:35.617: INFO: Pod "pod-configmaps-9606e128-373e-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228716ms
Feb 23 07:42:37.621: INFO: Pod "pod-configmaps-9606e128-373e-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00913996s
STEP: Saw pod success
Feb 23 07:42:37.622: INFO: Pod "pod-configmaps-9606e128-373e-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:42:37.624: INFO: Trying to get logs from node kube-node-02 pod pod-configmaps-9606e128-373e-11e9-a63e-52687f37ce9e container env-test: <nil>
STEP: delete the pod
Feb 23 07:42:37.643: INFO: Waiting for pod pod-configmaps-9606e128-373e-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:42:37.646: INFO: Pod pod-configmaps-9606e128-373e-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:42:37.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-l85c4" for this suite.
Feb 23 07:42:43.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:42:43.734: INFO: namespace: e2e-tests-secrets-l85c4, resource: bindings, ignored listing per whitelist
Feb 23 07:42:43.811: INFO: namespace e2e-tests-secrets-l85c4 deletion completed in 6.161404686s

• [SLOW TEST:8.316 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:42:43.811: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:42:43.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 version'
Feb 23 07:42:43.946: INFO: stderr: ""
Feb 23 07:42:43.946: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.2\", GitCommit:\"cff46ab41ff0bb44d8584413b598ad8360ec1def\", GitTreeState:\"clean\", BuildDate:\"2019-01-10T23:28:14Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:42:43.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-chfqd" for this suite.
Feb 23 07:42:49.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:42:50.074: INFO: namespace: e2e-tests-kubectl-chfqd, resource: bindings, ignored listing per whitelist
Feb 23 07:42:50.135: INFO: namespace e2e-tests-kubectl-chfqd deletion completed in 6.184437765s

• [SLOW TEST:6.324 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:42:50.135: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 23 07:42:50.398: INFO: Pod name wrapped-volume-race-9ed5f241-373e-11e9-a63e-52687f37ce9e: Found 0 pods out of 5
Feb 23 07:42:55.406: INFO: Pod name wrapped-volume-race-9ed5f241-373e-11e9-a63e-52687f37ce9e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9ed5f241-373e-11e9-a63e-52687f37ce9e in namespace e2e-tests-emptydir-wrapper-5wfzb, will wait for the garbage collector to delete the pods
Feb 23 07:43:05.497: INFO: Deleting ReplicationController wrapped-volume-race-9ed5f241-373e-11e9-a63e-52687f37ce9e took: 16.347627ms
Feb 23 07:43:05.597: INFO: Terminating ReplicationController wrapped-volume-race-9ed5f241-373e-11e9-a63e-52687f37ce9e pods took: 100.254401ms
STEP: Creating RC which spawns configmap-volume pods
Feb 23 07:43:41.817: INFO: Pod name wrapped-volume-race-bd7b221f-373e-11e9-a63e-52687f37ce9e: Found 0 pods out of 5
Feb 23 07:43:46.823: INFO: Pod name wrapped-volume-race-bd7b221f-373e-11e9-a63e-52687f37ce9e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bd7b221f-373e-11e9-a63e-52687f37ce9e in namespace e2e-tests-emptydir-wrapper-5wfzb, will wait for the garbage collector to delete the pods
Feb 23 07:43:56.928: INFO: Deleting ReplicationController wrapped-volume-race-bd7b221f-373e-11e9-a63e-52687f37ce9e took: 23.168465ms
Feb 23 07:43:57.029: INFO: Terminating ReplicationController wrapped-volume-race-bd7b221f-373e-11e9-a63e-52687f37ce9e pods took: 100.258115ms
STEP: Creating RC which spawns configmap-volume pods
Feb 23 07:44:32.151: INFO: Pod name wrapped-volume-race-db7b2add-373e-11e9-a63e-52687f37ce9e: Found 0 pods out of 5
Feb 23 07:44:37.159: INFO: Pod name wrapped-volume-race-db7b2add-373e-11e9-a63e-52687f37ce9e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-db7b2add-373e-11e9-a63e-52687f37ce9e in namespace e2e-tests-emptydir-wrapper-5wfzb, will wait for the garbage collector to delete the pods
Feb 23 07:44:47.269: INFO: Deleting ReplicationController wrapped-volume-race-db7b2add-373e-11e9-a63e-52687f37ce9e took: 23.803237ms
Feb 23 07:44:47.369: INFO: Terminating ReplicationController wrapped-volume-race-db7b2add-373e-11e9-a63e-52687f37ce9e pods took: 100.242974ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:45:32.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-5wfzb" for this suite.
Feb 23 07:45:40.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:45:40.701: INFO: namespace: e2e-tests-emptydir-wrapper-5wfzb, resource: bindings, ignored listing per whitelist
Feb 23 07:45:40.786: INFO: namespace e2e-tests-emptydir-wrapper-5wfzb deletion completed in 8.116227868s

• [SLOW TEST:170.651 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:45:40.787: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 23 07:45:40.859: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 23 07:45:45.863: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:45:46.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-4575r" for this suite.
Feb 23 07:45:52.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:45:53.037: INFO: namespace: e2e-tests-replication-controller-4575r, resource: bindings, ignored listing per whitelist
Feb 23 07:45:53.050: INFO: namespace e2e-tests-replication-controller-4575r deletion completed in 6.161772778s

• [SLOW TEST:12.263 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:45:53.050: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 23 07:45:53.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-rgqbx'
Feb 23 07:45:53.212: INFO: stderr: ""
Feb 23 07:45:53.212: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 23 07:45:58.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-rgqbx -o json'
Feb 23 07:45:58.335: INFO: stderr: ""
Feb 23 07:45:58.335: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-02-23T07:45:53Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-rgqbx\",\n        \"resourceVersion\": \"11565\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-rgqbx/pods/e2e-test-nginx-pod\",\n        \"uid\": \"0bccf262-373f-11e9-81c6-02ace7db5250\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5dv9v\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kube-node-01\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5dv9v\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5dv9v\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-23T07:45:53Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-23T07:45:53Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-23T07:45:53Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-23T07:45:53Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://f08857bdd2d6ab7bde4ce892050c3d10d1f7aed686179e17728ad7990a05774e\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-23T07:45:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.17.10.102\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.40.0.6\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-23T07:45:53Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 23 07:45:58.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 replace -f - --namespace=e2e-tests-kubectl-rgqbx'
Feb 23 07:45:58.487: INFO: stderr: ""
Feb 23 07:45:58.487: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Feb 23 07:45:58.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-rgqbx'
Feb 23 07:46:00.013: INFO: stderr: ""
Feb 23 07:46:00.013: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:46:00.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rgqbx" for this suite.
Feb 23 07:46:06.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:46:06.040: INFO: namespace: e2e-tests-kubectl-rgqbx, resource: bindings, ignored listing per whitelist
Feb 23 07:46:06.176: INFO: namespace e2e-tests-kubectl-rgqbx deletion completed in 6.158580064s

• [SLOW TEST:13.126 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:46:06.177: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:46:06.258: INFO: Creating deployment "nginx-deployment"
Feb 23 07:46:06.263: INFO: Waiting for observed generation 1
Feb 23 07:46:08.271: INFO: Waiting for all required pods to come up
Feb 23 07:46:08.277: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 23 07:46:10.285: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 23 07:46:10.295: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 23 07:46:10.306: INFO: Updating deployment nginx-deployment
Feb 23 07:46:10.306: INFO: Waiting for observed generation 2
Feb 23 07:46:12.315: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 23 07:46:12.318: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 23 07:46:12.321: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 23 07:46:12.332: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 23 07:46:12.332: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 23 07:46:12.334: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 23 07:46:12.340: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 23 07:46:12.340: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 23 07:46:12.348: INFO: Updating deployment nginx-deployment
Feb 23 07:46:12.348: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 23 07:46:12.354: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 23 07:46:12.360: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 23 07:46:12.383: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-hk97f,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hk97f/deployments/nginx-deployment,UID:139ad4b0-373f-11e9-8d12-02951b420040,ResourceVersion:11834,Generation:3,CreationTimestamp:2019-02-23 07:46:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-02-23 07:46:10 +0000 UTC 2019-02-23 07:46:06 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.} {Available False 2019-02-23 07:46:12 +0000 UTC 2019-02-23 07:46:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 23 07:46:12.397: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-hk97f,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hk97f/replicasets/nginx-deployment-65bbdb5f8,UID:16043573-373f-11e9-8d12-02951b420040,ResourceVersion:11829,Generation:3,CreationTimestamp:2019-02-23 07:46:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 139ad4b0-373f-11e9-8d12-02951b420040 0xc002c7a677 0xc002c7a678}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 23 07:46:12.398: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 23 07:46:12.398: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-hk97f,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hk97f/replicasets/nginx-deployment-555b55d965,UID:139c405d-373f-11e9-8d12-02951b420040,ResourceVersion:11828,Generation:3,CreationTimestamp:2019-02-23 07:46:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 139ad4b0-373f-11e9-8d12-02951b420040 0xc002c7a5b7 0xc002c7a5b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 23 07:46:12.428: INFO: Pod "nginx-deployment-555b55d965-67vxw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-67vxw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-67vxw,UID:173e2d11-373f-11e9-8d12-02951b420040,ResourceVersion:11872,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc0020cfc87 0xc0020cfc88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cfd00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cfd20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.103,PodIP:,StartTime:2019-02-23 07:46:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.429: INFO: Pod "nginx-deployment-555b55d965-6d4gg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6d4gg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-6d4gg,UID:13a17ea6-373f-11e9-8d12-02951b420040,ResourceVersion:11703,Generation:0,CreationTimestamp:2019-02-23 07:46:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc0020cfdd7 0xc0020cfdd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cfe50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cfe70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.102,PodIP:10.40.0.7,StartTime:2019-02-23 07:46:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-23 07:46:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://cbc0e69cf36cfb2bf71ba103cf79be5de827955b90251c79862b492d43986930}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.429: INFO: Pod "nginx-deployment-555b55d965-8plht" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-8plht,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-8plht,UID:139efffe-373f-11e9-8d12-02951b420040,ResourceVersion:11680,Generation:0,CreationTimestamp:2019-02-23 07:46:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc0020cff30 0xc0020cff31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cffa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cffc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.103,PodIP:10.42.0.6,StartTime:2019-02-23 07:46:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-23 07:46:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://50140e2c17a5bf58c5822bcabdd5b90f2d9e547c71acb343fcedd3ab77200e22}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.429: INFO: Pod "nginx-deployment-555b55d965-9zdjj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9zdjj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-9zdjj,UID:13a3d310-373f-11e9-8d12-02951b420040,ResourceVersion:11734,Generation:0,CreationTimestamp:2019-02-23 07:46:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc002d28440 0xc002d28441}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d28870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d288a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.102,PodIP:10.40.0.10,StartTime:2019-02-23 07:46:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-23 07:46:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://d7523e17e9bff00d907bfca3454fb67c3fad249aaa0efc34a3e1f88938cffa78}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.430: INFO: Pod "nginx-deployment-555b55d965-b6r9f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-b6r9f,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-b6r9f,UID:173fef07-373f-11e9-8d12-02951b420040,ResourceVersion:11867,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc002d289b0 0xc002d289b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d28a20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d28be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.430: INFO: Pod "nginx-deployment-555b55d965-f5kg7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-f5kg7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-f5kg7,UID:173fcf6f-373f-11e9-8d12-02951b420040,ResourceVersion:11859,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc002d28c60 0xc002d28c61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d28e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d28e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.430: INFO: Pod "nginx-deployment-555b55d965-fdlrf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-fdlrf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-fdlrf,UID:17441a7b-373f-11e9-8d12-02951b420040,ResourceVersion:11864,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc002d28ec0 0xc002d28ec1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d291e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d29280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.435: INFO: Pod "nginx-deployment-555b55d965-gffg7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gffg7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-gffg7,UID:13a3f33d-373f-11e9-8d12-02951b420040,ResourceVersion:11725,Generation:0,CreationTimestamp:2019-02-23 07:46:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc002d292d7 0xc002d292d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d29510} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d29530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.103,PodIP:10.42.0.10,StartTime:2019-02-23 07:46:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-23 07:46:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://86e96328f31ea4633df5fe64beaa5d2c676caf52b99d1e7fa1cd48e34f6c8b79}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.435: INFO: Pod "nginx-deployment-555b55d965-kd9lj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-kd9lj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-kd9lj,UID:139fe352-373f-11e9-8d12-02951b420040,ResourceVersion:11719,Generation:0,CreationTimestamp:2019-02-23 07:46:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc002d297b0 0xc002d297b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d29820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d29840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.103,PodIP:10.42.0.9,StartTime:2019-02-23 07:46:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-23 07:46:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://5d90aac383928910726eb8fc651dcf14ec5c7e3a44b4b61e6235ff68fe3f695b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.435: INFO: Pod "nginx-deployment-555b55d965-lmrhg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lmrhg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-lmrhg,UID:139fcb6f-373f-11e9-8d12-02951b420040,ResourceVersion:11731,Generation:0,CreationTimestamp:2019-02-23 07:46:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc002d299a0 0xc002d299a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d29a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d29aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.102,PodIP:10.40.0.6,StartTime:2019-02-23 07:46:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-23 07:46:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://74f6bec84d879a377125cea4caf72c2f60366123271c0e408ff58fa3f67e6661}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.435: INFO: Pod "nginx-deployment-555b55d965-pdkcl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-pdkcl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-pdkcl,UID:173ff98f-373f-11e9-8d12-02951b420040,ResourceVersion:11868,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc002d29b60 0xc002d29b61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d29f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d29f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.435: INFO: Pod "nginx-deployment-555b55d965-qb5vz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qb5vz,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-qb5vz,UID:13a1c048-373f-11e9-8d12-02951b420040,ResourceVersion:11710,Generation:0,CreationTimestamp:2019-02-23 07:46:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc001d98010 0xc001d98011}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d980c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d980e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.102,PodIP:10.40.0.9,StartTime:2019-02-23 07:46:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-23 07:46:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://ecb53d07482f5e7ce5f30bcca2dad629dc053d3c71ab3059a0cafd9439838849}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.435: INFO: Pod "nginx-deployment-555b55d965-qndkk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qndkk,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-qndkk,UID:17442912-373f-11e9-8d12-02951b420040,ResourceVersion:11871,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc001d98270 0xc001d98271}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d982d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d982f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.436: INFO: Pod "nginx-deployment-555b55d965-sdhsv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sdhsv,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-sdhsv,UID:174421be-373f-11e9-8d12-02951b420040,ResourceVersion:11870,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc001d98347 0xc001d98348}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d983b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d983d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.436: INFO: Pod "nginx-deployment-555b55d965-spkzm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-spkzm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-spkzm,UID:173cad5a-373f-11e9-8d12-02951b420040,ResourceVersion:11851,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc001d984c7 0xc001d984c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d98540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d98560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.103,PodIP:,StartTime:2019-02-23 07:46:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.436: INFO: Pod "nginx-deployment-555b55d965-w6xnf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-w6xnf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-w6xnf,UID:173e1f02-373f-11e9-8d12-02951b420040,ResourceVersion:11849,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc001d986d7 0xc001d986d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d98750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d98770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.436: INFO: Pod "nginx-deployment-555b55d965-wvbmf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-wvbmf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-wvbmf,UID:17400156-373f-11e9-8d12-02951b420040,ResourceVersion:11860,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc001d987e0 0xc001d987e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d98940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d98960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.436: INFO: Pod "nginx-deployment-555b55d965-wz4qm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-wz4qm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-wz4qm,UID:1743e4ad-373f-11e9-8d12-02951b420040,ResourceVersion:11869,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc001d989d0 0xc001d989d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d98a30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d98a50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.436: INFO: Pod "nginx-deployment-555b55d965-zsdsn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zsdsn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-zsdsn,UID:174412fd-373f-11e9-8d12-02951b420040,ResourceVersion:11866,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc001d98bf7 0xc001d98bf8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d98cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d98cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.436: INFO: Pod "nginx-deployment-555b55d965-zz8dl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zz8dl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-555b55d965-zz8dl,UID:13a4018a-373f-11e9-8d12-02951b420040,ResourceVersion:11705,Generation:0,CreationTimestamp:2019-02-23 07:46:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 139c405d-373f-11e9-8d12-02951b420040 0xc001d98d47 0xc001d98d48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d98ea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d98ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:06 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.102,PodIP:10.40.0.8,StartTime:2019-02-23 07:46:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-23 07:46:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://2d4e2b15d28098723fb1015402b91c57ef77ce07871f804e11a7603b17eefb54}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.436: INFO: Pod "nginx-deployment-65bbdb5f8-287nm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-287nm,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-287nm,UID:174173ff-373f-11e9-8d12-02951b420040,ResourceVersion:11853,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d99000 0xc001d99001}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d990e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d99100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.437: INFO: Pod "nginx-deployment-65bbdb5f8-52bq8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-52bq8,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-52bq8,UID:173d85fe-373f-11e9-8d12-02951b420040,ResourceVersion:11862,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d99207 0xc001d99208}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d992f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d99310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.102,PodIP:,StartTime:2019-02-23 07:46:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.438: INFO: Pod "nginx-deployment-65bbdb5f8-5l5r9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-5l5r9,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-5l5r9,UID:173ef0a2-373f-11e9-8d12-02951b420040,ResourceVersion:11857,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d993d0 0xc001d993d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d99570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d99590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.438: INFO: Pod "nginx-deployment-65bbdb5f8-6446g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6446g,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-6446g,UID:17419fb4-373f-11e9-8d12-02951b420040,ResourceVersion:11854,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d99600 0xc001d99601}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d99670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d99690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.438: INFO: Pod "nginx-deployment-65bbdb5f8-6k299" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6k299,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-6k299,UID:160cd979-373f-11e9-8d12-02951b420040,ResourceVersion:11817,Generation:0,CreationTimestamp:2019-02-23 07:46:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d996e7 0xc001d996e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d997c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d997e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.102,PodIP:10.40.0.12,StartTime:2019-02-23 07:46:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.438: INFO: Pod "nginx-deployment-65bbdb5f8-7472m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-7472m,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-7472m,UID:160dd5ea-373f-11e9-8d12-02951b420040,ResourceVersion:11812,Generation:0,CreationTimestamp:2019-02-23 07:46:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d998c0 0xc001d998c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d99950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d99970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.103,PodIP:10.42.0.11,StartTime:2019-02-23 07:46:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.438: INFO: Pod "nginx-deployment-65bbdb5f8-bxr25" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-bxr25,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-bxr25,UID:1741b288-373f-11e9-8d12-02951b420040,ResourceVersion:11855,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d99a50 0xc001d99a51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d99ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d99ae0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.438: INFO: Pod "nginx-deployment-65bbdb5f8-p2p2b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-p2p2b,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-p2p2b,UID:173eff43-373f-11e9-8d12-02951b420040,ResourceVersion:11858,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d99b37 0xc001d99b38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d99bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d99bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:12 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.438: INFO: Pod "nginx-deployment-65bbdb5f8-qfljp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-qfljp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-qfljp,UID:17440822-373f-11e9-8d12-02951b420040,ResourceVersion:11865,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d99c40 0xc001d99c41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d99cb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d99cd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.439: INFO: Pod "nginx-deployment-65bbdb5f8-rshmq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rshmq,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-rshmq,UID:160682f5-373f-11e9-8d12-02951b420040,ResourceVersion:11820,Generation:0,CreationTimestamp:2019-02-23 07:46:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d99d27 0xc001d99d28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d99da0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d99dc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.102,PodIP:10.40.0.11,StartTime:2019-02-23 07:46:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.439: INFO: Pod "nginx-deployment-65bbdb5f8-stc2r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-stc2r,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-stc2r,UID:1741ab16-373f-11e9-8d12-02951b420040,ResourceVersion:11856,Generation:0,CreationTimestamp:2019-02-23 07:46:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d99ea0 0xc001d99ea1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d99f10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d99f30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.439: INFO: Pod "nginx-deployment-65bbdb5f8-svczf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-svczf,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-svczf,UID:1605210e-373f-11e9-8d12-02951b420040,ResourceVersion:11826,Generation:0,CreationTimestamp:2019-02-23 07:46:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001d99f87 0xc001d99f88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001fb60a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001fb60c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.103,PodIP:10.42.0.8,StartTime:2019-02-23 07:46:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 23 07:46:12.439: INFO: Pod "nginx-deployment-65bbdb5f8-zpwwr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-zpwwr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hk97f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk97f/pods/nginx-deployment-65bbdb5f8-zpwwr,UID:1606d14f-373f-11e9-8d12-02951b420040,ResourceVersion:11810,Generation:0,CreationTimestamp:2019-02-23 07:46:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 16043573-373f-11e9-8d12-02951b420040 0xc001fb6450 0xc001fb6451}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d6f45 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6f45,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6f45 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001fb64d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001fb6660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 07:46:10 +0000 UTC  }],Message:,Reason:,HostIP:172.17.10.103,PodIP:10.42.0.7,StartTime:2019-02-23 07:46:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:46:12.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-hk97f" for this suite.
Feb 23 07:46:20.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:46:20.483: INFO: namespace: e2e-tests-deployment-hk97f, resource: bindings, ignored listing per whitelist
Feb 23 07:46:20.579: INFO: namespace e2e-tests-deployment-hk97f deletion completed in 8.12674342s

• [SLOW TEST:14.403 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:46:20.580: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:46:20.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-ldkxf" for this suite.
Feb 23 07:46:26.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:46:26.780: INFO: namespace: e2e-tests-kubelet-test-ldkxf, resource: bindings, ignored listing per whitelist
Feb 23 07:46:26.803: INFO: namespace e2e-tests-kubelet-test-ldkxf deletion completed in 6.133407394s

• [SLOW TEST:6.223 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:46:26.803: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-1fdebe2e-373f-11e9-a63e-52687f37ce9e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:46:28.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vjls7" for this suite.
Feb 23 07:46:50.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:46:50.952: INFO: namespace: e2e-tests-configmap-vjls7, resource: bindings, ignored listing per whitelist
Feb 23 07:46:51.042: INFO: namespace e2e-tests-configmap-vjls7 deletion completed in 22.133966539s

• [SLOW TEST:24.240 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:46:51.042: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 23 07:46:51.125: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-wp9fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-wp9fj/configmaps/e2e-watch-test-label-changed,UID:2e5644b2-373f-11e9-8d12-02951b420040,ResourceVersion:12248,Generation:0,CreationTimestamp:2019-02-23 07:46:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 07:46:51.125: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-wp9fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-wp9fj/configmaps/e2e-watch-test-label-changed,UID:2e5644b2-373f-11e9-8d12-02951b420040,ResourceVersion:12249,Generation:0,CreationTimestamp:2019-02-23 07:46:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 23 07:46:51.125: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-wp9fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-wp9fj/configmaps/e2e-watch-test-label-changed,UID:2e5644b2-373f-11e9-8d12-02951b420040,ResourceVersion:12250,Generation:0,CreationTimestamp:2019-02-23 07:46:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 23 07:47:01.166: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-wp9fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-wp9fj/configmaps/e2e-watch-test-label-changed,UID:2e5644b2-373f-11e9-8d12-02951b420040,ResourceVersion:12270,Generation:0,CreationTimestamp:2019-02-23 07:46:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 07:47:01.166: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-wp9fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-wp9fj/configmaps/e2e-watch-test-label-changed,UID:2e5644b2-373f-11e9-8d12-02951b420040,ResourceVersion:12271,Generation:0,CreationTimestamp:2019-02-23 07:46:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 23 07:47:01.166: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-wp9fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-wp9fj/configmaps/e2e-watch-test-label-changed,UID:2e5644b2-373f-11e9-8d12-02951b420040,ResourceVersion:12273,Generation:0,CreationTimestamp:2019-02-23 07:46:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:47:01.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-wp9fj" for this suite.
Feb 23 07:47:07.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:47:07.281: INFO: namespace: e2e-tests-watch-wp9fj, resource: bindings, ignored listing per whitelist
Feb 23 07:47:07.300: INFO: namespace e2e-tests-watch-wp9fj deletion completed in 6.128319669s

• [SLOW TEST:16.258 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:47:07.301: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 23 07:47:07.379: INFO: Waiting up to 5m0s for pod "pod-38036f62-373f-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-d8h9v" to be "success or failure"
Feb 23 07:47:07.383: INFO: Pod "pod-38036f62-373f-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.227334ms
Feb 23 07:47:09.387: INFO: Pod "pod-38036f62-373f-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007905922s
STEP: Saw pod success
Feb 23 07:47:09.387: INFO: Pod "pod-38036f62-373f-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:47:09.391: INFO: Trying to get logs from node kube-node-01 pod pod-38036f62-373f-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 07:47:09.419: INFO: Waiting for pod pod-38036f62-373f-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:47:09.422: INFO: Pod pod-38036f62-373f-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:47:09.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-d8h9v" for this suite.
Feb 23 07:47:15.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:47:15.560: INFO: namespace: e2e-tests-emptydir-d8h9v, resource: bindings, ignored listing per whitelist
Feb 23 07:47:15.648: INFO: namespace e2e-tests-emptydir-d8h9v deletion completed in 6.21980462s

• [SLOW TEST:8.347 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:47:15.648: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 23 07:47:16.265: INFO: Waiting up to 5m0s for pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-sl4bf" in namespace "e2e-tests-svcaccounts-fv86s" to be "success or failure"
Feb 23 07:47:16.272: INFO: Pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-sl4bf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.13775ms
Feb 23 07:47:18.280: INFO: Pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-sl4bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015006706s
STEP: Saw pod success
Feb 23 07:47:18.280: INFO: Pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-sl4bf" satisfied condition "success or failure"
Feb 23 07:47:18.284: INFO: Trying to get logs from node kube-node-02 pod pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-sl4bf container token-test: <nil>
STEP: delete the pod
Feb 23 07:47:18.312: INFO: Waiting for pod pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-sl4bf to disappear
Feb 23 07:47:18.316: INFO: Pod pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-sl4bf no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 23 07:47:18.323: INFO: Waiting up to 5m0s for pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-fwjp4" in namespace "e2e-tests-svcaccounts-fv86s" to be "success or failure"
Feb 23 07:47:18.329: INFO: Pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-fwjp4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.475858ms
Feb 23 07:47:20.332: INFO: Pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-fwjp4": Phase="Running", Reason="", readiness=false. Elapsed: 2.009384655s
Feb 23 07:47:22.336: INFO: Pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-fwjp4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013221107s
STEP: Saw pod success
Feb 23 07:47:22.336: INFO: Pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-fwjp4" satisfied condition "success or failure"
Feb 23 07:47:22.339: INFO: Trying to get logs from node kube-node-01 pod pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-fwjp4 container root-ca-test: <nil>
STEP: delete the pod
Feb 23 07:47:22.360: INFO: Waiting for pod pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-fwjp4 to disappear
Feb 23 07:47:22.364: INFO: Pod pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-fwjp4 no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 23 07:47:22.369: INFO: Waiting up to 5m0s for pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-nknx2" in namespace "e2e-tests-svcaccounts-fv86s" to be "success or failure"
Feb 23 07:47:22.373: INFO: Pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-nknx2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.111816ms
Feb 23 07:47:24.377: INFO: Pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-nknx2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008276773s
Feb 23 07:47:26.382: INFO: Pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-nknx2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013665229s
STEP: Saw pod success
Feb 23 07:47:26.383: INFO: Pod "pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-nknx2" satisfied condition "success or failure"
Feb 23 07:47:26.387: INFO: Trying to get logs from node kube-node-02 pod pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-nknx2 container namespace-test: <nil>
STEP: delete the pod
Feb 23 07:47:26.410: INFO: Waiting for pod pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-nknx2 to disappear
Feb 23 07:47:26.414: INFO: Pod pod-service-account-3d4e8181-373f-11e9-a63e-52687f37ce9e-nknx2 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:47:26.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-fv86s" for this suite.
Feb 23 07:47:32.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:47:32.539: INFO: namespace: e2e-tests-svcaccounts-fv86s, resource: bindings, ignored listing per whitelist
Feb 23 07:47:32.606: INFO: namespace e2e-tests-svcaccounts-fv86s deletion completed in 6.185307559s

• [SLOW TEST:16.958 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:47:32.606: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:47:32.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-471ad8c8-373f-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-jrwsh" to be "success or failure"
Feb 23 07:47:32.707: INFO: Pod "downwardapi-volume-471ad8c8-373f-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.026713ms
Feb 23 07:47:34.711: INFO: Pod "downwardapi-volume-471ad8c8-373f-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009442538s
STEP: Saw pod success
Feb 23 07:47:34.711: INFO: Pod "downwardapi-volume-471ad8c8-373f-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:47:34.716: INFO: Trying to get logs from node kube-node-01 pod downwardapi-volume-471ad8c8-373f-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:47:34.744: INFO: Waiting for pod downwardapi-volume-471ad8c8-373f-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:47:34.747: INFO: Pod downwardapi-volume-471ad8c8-373f-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:47:34.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jrwsh" for this suite.
Feb 23 07:47:40.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:47:40.808: INFO: namespace: e2e-tests-projected-jrwsh, resource: bindings, ignored listing per whitelist
Feb 23 07:47:40.879: INFO: namespace e2e-tests-projected-jrwsh deletion completed in 6.126218608s

• [SLOW TEST:8.273 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:47:40.879: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4c05f152-373f-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 07:47:40.953: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4c067d2b-373f-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-mc859" to be "success or failure"
Feb 23 07:47:40.957: INFO: Pod "pod-projected-configmaps-4c067d2b-373f-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.523303ms
Feb 23 07:47:42.961: INFO: Pod "pod-projected-configmaps-4c067d2b-373f-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007629468s
STEP: Saw pod success
Feb 23 07:47:42.961: INFO: Pod "pod-projected-configmaps-4c067d2b-373f-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:47:42.964: INFO: Trying to get logs from node kube-node-02 pod pod-projected-configmaps-4c067d2b-373f-11e9-a63e-52687f37ce9e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 07:47:42.984: INFO: Waiting for pod pod-projected-configmaps-4c067d2b-373f-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:47:42.990: INFO: Pod pod-projected-configmaps-4c067d2b-373f-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:47:42.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mc859" for this suite.
Feb 23 07:47:49.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:47:49.041: INFO: namespace: e2e-tests-projected-mc859, resource: bindings, ignored listing per whitelist
Feb 23 07:47:49.111: INFO: namespace e2e-tests-projected-mc859 deletion completed in 6.117315564s

• [SLOW TEST:8.231 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:47:49.111: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Feb 23 07:47:59.207: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:47:59.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-9g4qn" for this suite.
Feb 23 07:48:05.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:48:05.308: INFO: namespace: e2e-tests-gc-9g4qn, resource: bindings, ignored listing per whitelist
Feb 23 07:48:05.418: INFO: namespace e2e-tests-gc-9g4qn deletion completed in 6.206772332s

• [SLOW TEST:16.307 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:48:05.418: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 23 07:48:05.536: INFO: Waiting up to 5m0s for pod "pod-5aad81f9-373f-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-k4qbb" to be "success or failure"
Feb 23 07:48:05.543: INFO: Pod "pod-5aad81f9-373f-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.471834ms
Feb 23 07:48:07.549: INFO: Pod "pod-5aad81f9-373f-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012484802s
STEP: Saw pod success
Feb 23 07:48:07.549: INFO: Pod "pod-5aad81f9-373f-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:48:07.553: INFO: Trying to get logs from node kube-node-01 pod pod-5aad81f9-373f-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 07:48:07.575: INFO: Waiting for pod pod-5aad81f9-373f-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:48:07.579: INFO: Pod pod-5aad81f9-373f-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:48:07.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-k4qbb" for this suite.
Feb 23 07:48:13.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:48:13.641: INFO: namespace: e2e-tests-emptydir-k4qbb, resource: bindings, ignored listing per whitelist
Feb 23 07:48:13.778: INFO: namespace e2e-tests-emptydir-k4qbb deletion completed in 6.189555378s

• [SLOW TEST:8.359 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:48:13.778: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-pmrq
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 07:48:13.894: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pmrq" in namespace "e2e-tests-subpath-w98nd" to be "success or failure"
Feb 23 07:48:13.899: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.368259ms
Feb 23 07:48:15.904: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Running", Reason="", readiness=false. Elapsed: 2.009555838s
Feb 23 07:48:17.908: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Running", Reason="", readiness=false. Elapsed: 4.013426082s
Feb 23 07:48:19.912: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Running", Reason="", readiness=false. Elapsed: 6.017559267s
Feb 23 07:48:21.917: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Running", Reason="", readiness=false. Elapsed: 8.022704329s
Feb 23 07:48:23.921: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Running", Reason="", readiness=false. Elapsed: 10.02705776s
Feb 23 07:48:25.926: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Running", Reason="", readiness=false. Elapsed: 12.031106405s
Feb 23 07:48:27.929: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Running", Reason="", readiness=false. Elapsed: 14.034825217s
Feb 23 07:48:29.933: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Running", Reason="", readiness=false. Elapsed: 16.03826424s
Feb 23 07:48:31.937: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Running", Reason="", readiness=false. Elapsed: 18.042242935s
Feb 23 07:48:33.942: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Running", Reason="", readiness=false. Elapsed: 20.047175989s
Feb 23 07:48:35.945: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Running", Reason="", readiness=false. Elapsed: 22.05080417s
Feb 23 07:48:37.949: INFO: Pod "pod-subpath-test-configmap-pmrq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.054304099s
STEP: Saw pod success
Feb 23 07:48:37.949: INFO: Pod "pod-subpath-test-configmap-pmrq" satisfied condition "success or failure"
Feb 23 07:48:37.953: INFO: Trying to get logs from node kube-node-02 pod pod-subpath-test-configmap-pmrq container test-container-subpath-configmap-pmrq: <nil>
STEP: delete the pod
Feb 23 07:48:37.975: INFO: Waiting for pod pod-subpath-test-configmap-pmrq to disappear
Feb 23 07:48:37.980: INFO: Pod pod-subpath-test-configmap-pmrq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pmrq
Feb 23 07:48:37.980: INFO: Deleting pod "pod-subpath-test-configmap-pmrq" in namespace "e2e-tests-subpath-w98nd"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:48:37.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-w98nd" for this suite.
Feb 23 07:48:44.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:48:44.102: INFO: namespace: e2e-tests-subpath-w98nd, resource: bindings, ignored listing per whitelist
Feb 23 07:48:44.113: INFO: namespace e2e-tests-subpath-w98nd deletion completed in 6.122924827s

• [SLOW TEST:30.335 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:48:44.113: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-71b89872-373f-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 07:48:44.204: INFO: Waiting up to 5m0s for pod "pod-secrets-71b96efe-373f-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-secrets-hnvw4" to be "success or failure"
Feb 23 07:48:44.208: INFO: Pod "pod-secrets-71b96efe-373f-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.207297ms
Feb 23 07:48:46.212: INFO: Pod "pod-secrets-71b96efe-373f-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008118828s
STEP: Saw pod success
Feb 23 07:48:46.212: INFO: Pod "pod-secrets-71b96efe-373f-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:48:46.214: INFO: Trying to get logs from node kube-node-01 pod pod-secrets-71b96efe-373f-11e9-a63e-52687f37ce9e container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 07:48:46.234: INFO: Waiting for pod pod-secrets-71b96efe-373f-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:48:46.238: INFO: Pod pod-secrets-71b96efe-373f-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:48:46.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hnvw4" for this suite.
Feb 23 07:48:52.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:48:52.308: INFO: namespace: e2e-tests-secrets-hnvw4, resource: bindings, ignored listing per whitelist
Feb 23 07:48:52.379: INFO: namespace e2e-tests-secrets-hnvw4 deletion completed in 6.13759494s

• [SLOW TEST:8.266 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:48:52.379: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:48:52.447: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76a37b71-373f-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-8ltr8" to be "success or failure"
Feb 23 07:48:52.452: INFO: Pod "downwardapi-volume-76a37b71-373f-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.1755ms
Feb 23 07:48:54.456: INFO: Pod "downwardapi-volume-76a37b71-373f-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008174891s
STEP: Saw pod success
Feb 23 07:48:54.456: INFO: Pod "downwardapi-volume-76a37b71-373f-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:48:54.459: INFO: Trying to get logs from node kube-node-02 pod downwardapi-volume-76a37b71-373f-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:48:54.478: INFO: Waiting for pod downwardapi-volume-76a37b71-373f-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:48:54.480: INFO: Pod downwardapi-volume-76a37b71-373f-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:48:54.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8ltr8" for this suite.
Feb 23 07:49:00.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:49:00.506: INFO: namespace: e2e-tests-downward-api-8ltr8, resource: bindings, ignored listing per whitelist
Feb 23 07:49:00.632: INFO: namespace e2e-tests-downward-api-8ltr8 deletion completed in 6.148089878s

• [SLOW TEST:8.253 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:49:00.632: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:49:00.751: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b966f5c-373f-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-hnckq" to be "success or failure"
Feb 23 07:49:00.756: INFO: Pod "downwardapi-volume-7b966f5c-373f-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.075231ms
Feb 23 07:49:02.763: INFO: Pod "downwardapi-volume-7b966f5c-373f-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012191095s
STEP: Saw pod success
Feb 23 07:49:02.764: INFO: Pod "downwardapi-volume-7b966f5c-373f-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:49:02.772: INFO: Trying to get logs from node kube-node-01 pod downwardapi-volume-7b966f5c-373f-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:49:02.809: INFO: Waiting for pod downwardapi-volume-7b966f5c-373f-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:49:02.834: INFO: Pod downwardapi-volume-7b966f5c-373f-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:49:02.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hnckq" for this suite.
Feb 23 07:49:08.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:49:08.895: INFO: namespace: e2e-tests-downward-api-hnckq, resource: bindings, ignored listing per whitelist
Feb 23 07:49:09.070: INFO: namespace e2e-tests-downward-api-hnckq deletion completed in 6.225569853s

• [SLOW TEST:8.438 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:49:09.070: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-809c1f73-373f-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 07:49:09.183: INFO: Waiting up to 5m0s for pod "pod-configmaps-809d13d8-373f-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-configmap-fwf4k" to be "success or failure"
Feb 23 07:49:09.190: INFO: Pod "pod-configmaps-809d13d8-373f-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.293433ms
Feb 23 07:49:11.194: INFO: Pod "pod-configmaps-809d13d8-373f-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010289126s
STEP: Saw pod success
Feb 23 07:49:11.194: INFO: Pod "pod-configmaps-809d13d8-373f-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:49:11.197: INFO: Trying to get logs from node kube-node-02 pod pod-configmaps-809d13d8-373f-11e9-a63e-52687f37ce9e container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 07:49:11.219: INFO: Waiting for pod pod-configmaps-809d13d8-373f-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:49:11.222: INFO: Pod pod-configmaps-809d13d8-373f-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:49:11.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-fwf4k" for this suite.
Feb 23 07:49:17.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:49:17.433: INFO: namespace: e2e-tests-configmap-fwf4k, resource: bindings, ignored listing per whitelist
Feb 23 07:49:17.462: INFO: namespace e2e-tests-configmap-fwf4k deletion completed in 6.236446475s

• [SLOW TEST:8.392 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:49:17.462: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:49:17.571: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:49:19.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-8gv6h" for this suite.
Feb 23 07:49:57.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:49:57.646: INFO: namespace: e2e-tests-pods-8gv6h, resource: bindings, ignored listing per whitelist
Feb 23 07:49:57.741: INFO: namespace e2e-tests-pods-8gv6h deletion completed in 38.127659527s

• [SLOW TEST:40.279 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:49:57.741: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:50:57.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-k89hs" for this suite.
Feb 23 07:51:19.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:51:19.945: INFO: namespace: e2e-tests-container-probe-k89hs, resource: bindings, ignored listing per whitelist
Feb 23 07:51:20.002: INFO: namespace e2e-tests-container-probe-k89hs deletion completed in 22.184248445s

• [SLOW TEST:82.261 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:51:20.002: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-94mn2 in namespace e2e-tests-proxy-vfgbf
I0223 07:51:20.119221      16 runners.go:184] Created replication controller with name: proxy-service-94mn2, namespace: e2e-tests-proxy-vfgbf, replica count: 1
I0223 07:51:21.169669      16 runners.go:184] proxy-service-94mn2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0223 07:51:22.169874      16 runners.go:184] proxy-service-94mn2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0223 07:51:23.170084      16 runners.go:184] proxy-service-94mn2 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0223 07:51:24.170300      16 runners.go:184] proxy-service-94mn2 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0223 07:51:25.170533      16 runners.go:184] proxy-service-94mn2 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0223 07:51:26.170736      16 runners.go:184] proxy-service-94mn2 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 07:51:26.174: INFO: setup took 6.086113523s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 23 07:51:26.188: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 13.448938ms)
Feb 23 07:51:26.192: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 18.324555ms)
Feb 23 07:51:26.193: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 18.19515ms)
Feb 23 07:51:26.193: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 18.365332ms)
Feb 23 07:51:26.193: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 18.588771ms)
Feb 23 07:51:26.195: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 20.932073ms)
Feb 23 07:51:26.196: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 21.525713ms)
Feb 23 07:51:26.196: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 21.317212ms)
Feb 23 07:51:26.196: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 21.378082ms)
Feb 23 07:51:26.196: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 21.344702ms)
Feb 23 07:51:26.199: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 25.086715ms)
Feb 23 07:51:26.205: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 31.320625ms)
Feb 23 07:51:26.205: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 31.272765ms)
Feb 23 07:51:26.205: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 31.275611ms)
Feb 23 07:51:26.208: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 33.233951ms)
Feb 23 07:51:26.208: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 33.207589ms)
Feb 23 07:51:26.220: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 11.900539ms)
Feb 23 07:51:26.222: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 13.884031ms)
Feb 23 07:51:26.222: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 14.021026ms)
Feb 23 07:51:26.222: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 14.068358ms)
Feb 23 07:51:26.222: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 14.199285ms)
Feb 23 07:51:26.222: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 14.42812ms)
Feb 23 07:51:26.223: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 14.557696ms)
Feb 23 07:51:26.223: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 14.643096ms)
Feb 23 07:51:26.223: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 14.241898ms)
Feb 23 07:51:26.223: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 14.740527ms)
Feb 23 07:51:26.225: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 16.808663ms)
Feb 23 07:51:26.225: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 16.954051ms)
Feb 23 07:51:26.271: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 62.33233ms)
Feb 23 07:51:26.273: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 64.577207ms)
Feb 23 07:51:26.273: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 64.107565ms)
Feb 23 07:51:26.273: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 64.833867ms)
Feb 23 07:51:26.278: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 5.715214ms)
Feb 23 07:51:26.281: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 7.562437ms)
Feb 23 07:51:26.281: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 7.741799ms)
Feb 23 07:51:26.281: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 7.796904ms)
Feb 23 07:51:26.281: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 7.929355ms)
Feb 23 07:51:26.281: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 7.694658ms)
Feb 23 07:51:26.283: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 9.423806ms)
Feb 23 07:51:26.283: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 9.411406ms)
Feb 23 07:51:26.283: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 9.653859ms)
Feb 23 07:51:26.285: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 11.561719ms)
Feb 23 07:51:26.287: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 13.488566ms)
Feb 23 07:51:26.287: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 13.430066ms)
Feb 23 07:51:26.287: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 13.413477ms)
Feb 23 07:51:26.287: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 13.38395ms)
Feb 23 07:51:26.287: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 13.432462ms)
Feb 23 07:51:26.287: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 13.607074ms)
Feb 23 07:51:26.294: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 7.649403ms)
Feb 23 07:51:26.294: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 7.704866ms)
Feb 23 07:51:26.295: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 7.742911ms)
Feb 23 07:51:26.296: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 9.367386ms)
Feb 23 07:51:26.296: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 9.551501ms)
Feb 23 07:51:26.297: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 9.545633ms)
Feb 23 07:51:26.297: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 9.765832ms)
Feb 23 07:51:26.297: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 9.65841ms)
Feb 23 07:51:26.297: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 9.761398ms)
Feb 23 07:51:26.297: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 9.895459ms)
Feb 23 07:51:26.299: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 11.480123ms)
Feb 23 07:51:26.299: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 11.726566ms)
Feb 23 07:51:26.299: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 11.619546ms)
Feb 23 07:51:26.299: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 11.743972ms)
Feb 23 07:51:26.299: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 11.897872ms)
Feb 23 07:51:26.299: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 11.860107ms)
Feb 23 07:51:26.305: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 5.776008ms)
Feb 23 07:51:26.307: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 7.921377ms)
Feb 23 07:51:26.307: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 7.222646ms)
Feb 23 07:51:26.307: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 7.793248ms)
Feb 23 07:51:26.307: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 7.731758ms)
Feb 23 07:51:26.307: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 7.881225ms)
Feb 23 07:51:26.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 9.249724ms)
Feb 23 07:51:26.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 9.653045ms)
Feb 23 07:51:26.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 9.36629ms)
Feb 23 07:51:26.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 9.511285ms)
Feb 23 07:51:26.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 9.646481ms)
Feb 23 07:51:26.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 9.197696ms)
Feb 23 07:51:26.311: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 11.200613ms)
Feb 23 07:51:26.311: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 11.219381ms)
Feb 23 07:51:26.311: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 11.49448ms)
Feb 23 07:51:26.311: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 11.705873ms)
Feb 23 07:51:26.317: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 5.920168ms)
Feb 23 07:51:26.317: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 5.726806ms)
Feb 23 07:51:26.319: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 7.752477ms)
Feb 23 07:51:26.319: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 7.76159ms)
Feb 23 07:51:26.319: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 7.688063ms)
Feb 23 07:51:26.319: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 7.859723ms)
Feb 23 07:51:26.321: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 9.877336ms)
Feb 23 07:51:26.321: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 9.723442ms)
Feb 23 07:51:26.321: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 9.685162ms)
Feb 23 07:51:26.321: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 9.73375ms)
Feb 23 07:51:26.321: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 10.111827ms)
Feb 23 07:51:26.324: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 12.49304ms)
Feb 23 07:51:26.324: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 12.548676ms)
Feb 23 07:51:26.324: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 12.643036ms)
Feb 23 07:51:26.324: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 12.546466ms)
Feb 23 07:51:26.324: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 12.712892ms)
Feb 23 07:51:26.334: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 9.503026ms)
Feb 23 07:51:26.334: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 10.117845ms)
Feb 23 07:51:26.334: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 9.698368ms)
Feb 23 07:51:26.336: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 12.195419ms)
Feb 23 07:51:26.336: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 12.021785ms)
Feb 23 07:51:26.336: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 12.339184ms)
Feb 23 07:51:26.336: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 11.995349ms)
Feb 23 07:51:26.337: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 11.854392ms)
Feb 23 07:51:26.337: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 12.353489ms)
Feb 23 07:51:26.362: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 38.081418ms)
Feb 23 07:51:26.363: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 38.183207ms)
Feb 23 07:51:26.363: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 37.760943ms)
Feb 23 07:51:26.363: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 38.243566ms)
Feb 23 07:51:26.363: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 37.893991ms)
Feb 23 07:51:26.363: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 37.75757ms)
Feb 23 07:51:26.363: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 37.86538ms)
Feb 23 07:51:26.369: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 6.053872ms)
Feb 23 07:51:26.371: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 8.085423ms)
Feb 23 07:51:26.371: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 7.987451ms)
Feb 23 07:51:26.371: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 8.06914ms)
Feb 23 07:51:26.373: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 10.073522ms)
Feb 23 07:51:26.373: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 10.085919ms)
Feb 23 07:51:26.373: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 10.063574ms)
Feb 23 07:51:26.373: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 10.532032ms)
Feb 23 07:51:26.374: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 10.895316ms)
Feb 23 07:51:26.374: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 10.607112ms)
Feb 23 07:51:26.374: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 10.081149ms)
Feb 23 07:51:26.375: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 12.003476ms)
Feb 23 07:51:26.375: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 12.001955ms)
Feb 23 07:51:26.376: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 12.303666ms)
Feb 23 07:51:26.376: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 11.937379ms)
Feb 23 07:51:26.376: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 12.375426ms)
Feb 23 07:51:26.382: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 6.319248ms)
Feb 23 07:51:26.384: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 7.909433ms)
Feb 23 07:51:26.384: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 7.889965ms)
Feb 23 07:51:26.384: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 8.278128ms)
Feb 23 07:51:26.384: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 8.191359ms)
Feb 23 07:51:26.384: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 8.290439ms)
Feb 23 07:51:26.384: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 8.133678ms)
Feb 23 07:51:26.386: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 10.059825ms)
Feb 23 07:51:26.386: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 10.254262ms)
Feb 23 07:51:26.386: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 9.975771ms)
Feb 23 07:51:26.386: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 10.100774ms)
Feb 23 07:51:26.386: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 10.702831ms)
Feb 23 07:51:26.386: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 10.786946ms)
Feb 23 07:51:26.387: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 10.112582ms)
Feb 23 07:51:26.388: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 12.153017ms)
Feb 23 07:51:26.388: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 12.244775ms)
Feb 23 07:51:26.395: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 6.225432ms)
Feb 23 07:51:26.397: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 8.10409ms)
Feb 23 07:51:26.397: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 8.255317ms)
Feb 23 07:51:26.397: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 7.817117ms)
Feb 23 07:51:26.399: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 10.00989ms)
Feb 23 07:51:26.399: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 10.448129ms)
Feb 23 07:51:26.399: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 10.088039ms)
Feb 23 07:51:26.399: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 10.457945ms)
Feb 23 07:51:26.399: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 9.962199ms)
Feb 23 07:51:26.399: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 10.04148ms)
Feb 23 07:51:26.399: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 10.423516ms)
Feb 23 07:51:26.399: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 10.490876ms)
Feb 23 07:51:26.399: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 10.390189ms)
Feb 23 07:51:26.399: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 10.357877ms)
Feb 23 07:51:26.401: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 12.160189ms)
Feb 23 07:51:26.401: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 12.827756ms)
Feb 23 07:51:26.406: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 4.57713ms)
Feb 23 07:51:26.408: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 6.646419ms)
Feb 23 07:51:26.408: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 6.870883ms)
Feb 23 07:51:26.408: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 6.749637ms)
Feb 23 07:51:26.412: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 10.605921ms)
Feb 23 07:51:26.412: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 10.729606ms)
Feb 23 07:51:26.412: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 10.69958ms)
Feb 23 07:51:26.412: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 10.666793ms)
Feb 23 07:51:26.413: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 11.065158ms)
Feb 23 07:51:26.413: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 11.041191ms)
Feb 23 07:51:26.414: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 12.946318ms)
Feb 23 07:51:26.415: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 12.806045ms)
Feb 23 07:51:26.415: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 12.806155ms)
Feb 23 07:51:26.415: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 13.169154ms)
Feb 23 07:51:26.415: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 12.879556ms)
Feb 23 07:51:26.415: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 12.939913ms)
Feb 23 07:51:26.422: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 7.473459ms)
Feb 23 07:51:26.423: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 7.718269ms)
Feb 23 07:51:26.423: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 7.677443ms)
Feb 23 07:51:26.424: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 9.67849ms)
Feb 23 07:51:26.425: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 9.568377ms)
Feb 23 07:51:26.425: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 9.685516ms)
Feb 23 07:51:26.425: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 9.898777ms)
Feb 23 07:51:26.425: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 9.787227ms)
Feb 23 07:51:26.425: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 9.713668ms)
Feb 23 07:51:26.425: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 9.88063ms)
Feb 23 07:51:26.425: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 10.070274ms)
Feb 23 07:51:26.427: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 11.512784ms)
Feb 23 07:51:26.427: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 11.699014ms)
Feb 23 07:51:26.427: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 11.604984ms)
Feb 23 07:51:26.427: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 11.881491ms)
Feb 23 07:51:26.427: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 11.785292ms)
Feb 23 07:51:26.436: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 9.528565ms)
Feb 23 07:51:26.436: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 9.404813ms)
Feb 23 07:51:26.436: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 9.209225ms)
Feb 23 07:51:26.436: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 9.171369ms)
Feb 23 07:51:26.436: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 9.358283ms)
Feb 23 07:51:26.436: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 9.113778ms)
Feb 23 07:51:26.437: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 9.214017ms)
Feb 23 07:51:26.437: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 9.094825ms)
Feb 23 07:51:26.437: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 9.819814ms)
Feb 23 07:51:26.439: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 11.038356ms)
Feb 23 07:51:26.439: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 10.95766ms)
Feb 23 07:51:26.440: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 13.401182ms)
Feb 23 07:51:26.441: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 13.082204ms)
Feb 23 07:51:26.441: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 13.514455ms)
Feb 23 07:51:26.441: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 13.344971ms)
Feb 23 07:51:26.441: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 13.143382ms)
Feb 23 07:51:26.451: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 9.667104ms)
Feb 23 07:51:26.451: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 9.771299ms)
Feb 23 07:51:26.451: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 9.403455ms)
Feb 23 07:51:26.453: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 11.691666ms)
Feb 23 07:51:26.453: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 11.350963ms)
Feb 23 07:51:26.453: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 12.20572ms)
Feb 23 07:51:26.453: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 12.176689ms)
Feb 23 07:51:26.453: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 12.328441ms)
Feb 23 07:51:26.453: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 11.77561ms)
Feb 23 07:51:26.453: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 12.45646ms)
Feb 23 07:51:26.453: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 12.025889ms)
Feb 23 07:51:26.455: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 13.798189ms)
Feb 23 07:51:26.455: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 14.231605ms)
Feb 23 07:51:26.455: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 13.974518ms)
Feb 23 07:51:26.455: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 13.8869ms)
Feb 23 07:51:26.455: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 13.789776ms)
Feb 23 07:51:26.461: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 5.981008ms)
Feb 23 07:51:26.462: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 6.048795ms)
Feb 23 07:51:26.462: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 6.11862ms)
Feb 23 07:51:26.464: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 8.09884ms)
Feb 23 07:51:26.464: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 8.049767ms)
Feb 23 07:51:26.464: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 7.954568ms)
Feb 23 07:51:26.466: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 9.790959ms)
Feb 23 07:51:26.466: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 10.201916ms)
Feb 23 07:51:26.466: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 10.094773ms)
Feb 23 07:51:26.466: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 9.984189ms)
Feb 23 07:51:26.468: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 11.926517ms)
Feb 23 07:51:26.468: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 12.205279ms)
Feb 23 07:51:26.468: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 12.182122ms)
Feb 23 07:51:26.468: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 12.190102ms)
Feb 23 07:51:26.468: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 12.146773ms)
Feb 23 07:51:26.468: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 12.170323ms)
Feb 23 07:51:26.474: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 5.894967ms)
Feb 23 07:51:26.476: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 7.589068ms)
Feb 23 07:51:26.476: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 7.674889ms)
Feb 23 07:51:26.478: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 9.158721ms)
Feb 23 07:51:26.478: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 9.399824ms)
Feb 23 07:51:26.478: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 9.051426ms)
Feb 23 07:51:26.478: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 9.767029ms)
Feb 23 07:51:26.478: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 9.383511ms)
Feb 23 07:51:26.478: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 10.041812ms)
Feb 23 07:51:26.478: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 9.88984ms)
Feb 23 07:51:26.478: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 9.809558ms)
Feb 23 07:51:26.484: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 15.75837ms)
Feb 23 07:51:26.484: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 15.32951ms)
Feb 23 07:51:26.484: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 15.304269ms)
Feb 23 07:51:26.484: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 15.382299ms)
Feb 23 07:51:26.484: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 15.24721ms)
Feb 23 07:51:26.490: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 5.7954ms)
Feb 23 07:51:26.490: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 5.64637ms)
Feb 23 07:51:26.492: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 7.239246ms)
Feb 23 07:51:26.492: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 7.882421ms)
Feb 23 07:51:26.492: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 7.718342ms)
Feb 23 07:51:26.492: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 7.681031ms)
Feb 23 07:51:26.492: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 7.792326ms)
Feb 23 07:51:26.492: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 7.451925ms)
Feb 23 07:51:26.492: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 7.875849ms)
Feb 23 07:51:26.492: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 7.946099ms)
Feb 23 07:51:26.494: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 10.129965ms)
Feb 23 07:51:26.494: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 10.050591ms)
Feb 23 07:51:26.496: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 11.273741ms)
Feb 23 07:51:26.496: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 11.576147ms)
Feb 23 07:51:26.496: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 11.664177ms)
Feb 23 07:51:26.496: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 11.57292ms)
Feb 23 07:51:26.502: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 5.824975ms)
Feb 23 07:51:26.502: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 5.596621ms)
Feb 23 07:51:26.507: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 10.329588ms)
Feb 23 07:51:26.507: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 10.253113ms)
Feb 23 07:51:26.507: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 9.988656ms)
Feb 23 07:51:26.507: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 10.803688ms)
Feb 23 07:51:26.507: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 10.735275ms)
Feb 23 07:51:26.507: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 10.094153ms)
Feb 23 07:51:26.507: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 10.713824ms)
Feb 23 07:51:26.507: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 10.635063ms)
Feb 23 07:51:26.507: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 10.544242ms)
Feb 23 07:51:26.507: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 10.345905ms)
Feb 23 07:51:26.509: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 12.119696ms)
Feb 23 07:51:26.509: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 12.454692ms)
Feb 23 07:51:26.509: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 12.427941ms)
Feb 23 07:51:26.509: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 12.574123ms)
Feb 23 07:51:26.518: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 8.133492ms)
Feb 23 07:51:26.518: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 8.169928ms)
Feb 23 07:51:26.518: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 8.154581ms)
Feb 23 07:51:26.518: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 8.090371ms)
Feb 23 07:51:26.520: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 10.077742ms)
Feb 23 07:51:26.520: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 10.310708ms)
Feb 23 07:51:26.520: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 10.314028ms)
Feb 23 07:51:26.520: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 10.298325ms)
Feb 23 07:51:26.520: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 10.421292ms)
Feb 23 07:51:26.520: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 10.325692ms)
Feb 23 07:51:26.522: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 12.770246ms)
Feb 23 07:51:26.524: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 14.901436ms)
Feb 23 07:51:26.525: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 14.860505ms)
Feb 23 07:51:26.525: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 15.042188ms)
Feb 23 07:51:26.525: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 15.042294ms)
Feb 23 07:51:26.525: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 14.976031ms)
Feb 23 07:51:26.533: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:162/proxy/: bar (200; 8.458033ms)
Feb 23 07:51:26.533: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5/proxy/rewriteme"... (200; 8.356189ms)
Feb 23 07:51:26.533: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:160/proxy/: foo (200; 8.460274ms)
Feb 23 07:51:26.533: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:460/proxy/: tls baz (200; 8.549319ms)
Feb 23 07:51:26.533: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:443/proxy/... (200; 8.479319ms)
Feb 23 07:51:26.533: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/https:proxy-service-94mn2-zmck5:462/proxy/: tls qux (200; 8.709528ms)
Feb 23 07:51:26.533: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:162/proxy/: bar (200; 8.437036ms)
Feb 23 07:51:26.533: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:1080/proxy/... (200; 8.629242ms)
Feb 23 07:51:26.533: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/http:proxy-service-94mn2-zmck5:160/proxy/: foo (200; 8.727289ms)
Feb 23 07:51:26.533: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-vfgbf/pods/proxy-service-94mn2-zmck5:1080/proxy/rewri... (200; 8.509857ms)
Feb 23 07:51:26.536: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname1/proxy/: foo (200; 10.677326ms)
Feb 23 07:51:26.536: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname1/proxy/: foo (200; 10.721049ms)
Feb 23 07:51:26.536: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/http:proxy-service-94mn2:portname2/proxy/: bar (200; 10.923761ms)
Feb 23 07:51:26.536: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/proxy-service-94mn2:portname2/proxy/: bar (200; 10.685413ms)
Feb 23 07:51:26.540: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname2/proxy/: tls qux (200; 14.757783ms)
Feb 23 07:51:26.540: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-vfgbf/services/https:proxy-service-94mn2:tlsportname1/proxy/: tls baz (200; 14.985077ms)
STEP: deleting ReplicationController proxy-service-94mn2 in namespace e2e-tests-proxy-vfgbf, will wait for the garbage collector to delete the pods
Feb 23 07:51:26.621: INFO: Deleting ReplicationController proxy-service-94mn2 took: 23.317575ms
Feb 23 07:51:26.721: INFO: Terminating ReplicationController proxy-service-94mn2 pods took: 100.238741ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:51:31.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-vfgbf" for this suite.
Feb 23 07:51:37.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:51:37.856: INFO: namespace: e2e-tests-proxy-vfgbf, resource: bindings, ignored listing per whitelist
Feb 23 07:51:38.061: INFO: namespace e2e-tests-proxy-vfgbf deletion completed in 6.232745305s

• [SLOW TEST:18.059 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:51:38.061: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 23 07:51:38.174: INFO: Waiting up to 5m0s for pod "pod-d96b572d-373f-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-9jrpb" to be "success or failure"
Feb 23 07:51:38.177: INFO: Pod "pod-d96b572d-373f-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.028256ms
Feb 23 07:51:40.183: INFO: Pod "pod-d96b572d-373f-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009353s
STEP: Saw pod success
Feb 23 07:51:40.183: INFO: Pod "pod-d96b572d-373f-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:51:40.187: INFO: Trying to get logs from node kube-node-02 pod pod-d96b572d-373f-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 07:51:40.214: INFO: Waiting for pod pod-d96b572d-373f-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:51:40.217: INFO: Pod pod-d96b572d-373f-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:51:40.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9jrpb" for this suite.
Feb 23 07:51:46.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:51:46.312: INFO: namespace: e2e-tests-emptydir-9jrpb, resource: bindings, ignored listing per whitelist
Feb 23 07:51:46.371: INFO: namespace e2e-tests-emptydir-9jrpb deletion completed in 6.148167391s

• [SLOW TEST:8.310 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:51:46.372: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:51:46.495: INFO: (0) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.540546ms)
Feb 23 07:51:46.498: INFO: (1) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.792421ms)
Feb 23 07:51:46.502: INFO: (2) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.379811ms)
Feb 23 07:51:46.505: INFO: (3) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.266805ms)
Feb 23 07:51:46.509: INFO: (4) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.390775ms)
Feb 23 07:51:46.512: INFO: (5) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.505986ms)
Feb 23 07:51:46.516: INFO: (6) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.395511ms)
Feb 23 07:51:46.519: INFO: (7) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.250688ms)
Feb 23 07:51:46.522: INFO: (8) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.305525ms)
Feb 23 07:51:46.526: INFO: (9) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.56999ms)
Feb 23 07:51:46.529: INFO: (10) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.570679ms)
Feb 23 07:51:46.533: INFO: (11) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.329955ms)
Feb 23 07:51:46.536: INFO: (12) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.413166ms)
Feb 23 07:51:46.540: INFO: (13) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.654213ms)
Feb 23 07:51:46.545: INFO: (14) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.973243ms)
Feb 23 07:51:46.548: INFO: (15) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.691449ms)
Feb 23 07:51:46.554: INFO: (16) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.260418ms)
Feb 23 07:51:46.557: INFO: (17) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.337337ms)
Feb 23 07:51:46.561: INFO: (18) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.55681ms)
Feb 23 07:51:46.564: INFO: (19) /api/v1/nodes/kube-node-01:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 3.524826ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:51:46.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-4xxhp" for this suite.
Feb 23 07:51:52.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:51:52.653: INFO: namespace: e2e-tests-proxy-4xxhp, resource: bindings, ignored listing per whitelist
Feb 23 07:51:52.690: INFO: namespace e2e-tests-proxy-4xxhp deletion completed in 6.122049228s

• [SLOW TEST:6.318 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:51:52.690: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-e21d81f4-373f-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 07:51:52.768: INFO: Waiting up to 5m0s for pod "pod-secrets-e21e4d2b-373f-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-secrets-6mddk" to be "success or failure"
Feb 23 07:51:52.771: INFO: Pod "pod-secrets-e21e4d2b-373f-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148864ms
Feb 23 07:51:54.775: INFO: Pod "pod-secrets-e21e4d2b-373f-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007089933s
STEP: Saw pod success
Feb 23 07:51:54.775: INFO: Pod "pod-secrets-e21e4d2b-373f-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:51:54.778: INFO: Trying to get logs from node kube-node-01 pod pod-secrets-e21e4d2b-373f-11e9-a63e-52687f37ce9e container secret-env-test: <nil>
STEP: delete the pod
Feb 23 07:51:54.798: INFO: Waiting for pod pod-secrets-e21e4d2b-373f-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:51:54.801: INFO: Pod pod-secrets-e21e4d2b-373f-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:51:54.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6mddk" for this suite.
Feb 23 07:52:00.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:52:00.868: INFO: namespace: e2e-tests-secrets-6mddk, resource: bindings, ignored listing per whitelist
Feb 23 07:52:00.931: INFO: namespace e2e-tests-secrets-6mddk deletion completed in 6.125553241s

• [SLOW TEST:8.241 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:52:00.931: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 07:52:01.001: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e7066433-373f-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-tp48x" to be "success or failure"
Feb 23 07:52:01.006: INFO: Pod "downwardapi-volume-e7066433-373f-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.3223ms
Feb 23 07:52:03.015: INFO: Pod "downwardapi-volume-e7066433-373f-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013891712s
STEP: Saw pod success
Feb 23 07:52:03.030: INFO: Pod "downwardapi-volume-e7066433-373f-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:52:03.039: INFO: Trying to get logs from node kube-node-02 pod downwardapi-volume-e7066433-373f-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 07:52:03.071: INFO: Waiting for pod downwardapi-volume-e7066433-373f-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:52:03.074: INFO: Pod downwardapi-volume-e7066433-373f-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:52:03.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tp48x" for this suite.
Feb 23 07:52:09.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:52:09.176: INFO: namespace: e2e-tests-projected-tp48x, resource: bindings, ignored listing per whitelist
Feb 23 07:52:09.280: INFO: namespace e2e-tests-projected-tp48x deletion completed in 6.200346497s

• [SLOW TEST:8.349 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:52:09.280: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:52:11.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-mhmsn" for this suite.
Feb 23 07:52:49.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:52:49.476: INFO: namespace: e2e-tests-kubelet-test-mhmsn, resource: bindings, ignored listing per whitelist
Feb 23 07:52:49.544: INFO: namespace e2e-tests-kubelet-test-mhmsn deletion completed in 38.117364107s

• [SLOW TEST:40.264 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:52:49.544: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-04010119-3740-11e9-a63e-52687f37ce9e
STEP: Creating secret with name s-test-opt-upd-0401015e-3740-11e9-a63e-52687f37ce9e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-04010119-3740-11e9-a63e-52687f37ce9e
STEP: Updating secret s-test-opt-upd-0401015e-3740-11e9-a63e-52687f37ce9e
STEP: Creating secret with name s-test-opt-create-04010175-3740-11e9-a63e-52687f37ce9e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:52:53.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-sfx8b" for this suite.
Feb 23 07:53:15.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:53:15.812: INFO: namespace: e2e-tests-secrets-sfx8b, resource: bindings, ignored listing per whitelist
Feb 23 07:53:15.846: INFO: namespace e2e-tests-secrets-sfx8b deletion completed in 22.133713655s

• [SLOW TEST:26.302 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:53:15.846: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 23 07:53:15.935: INFO: Waiting up to 5m0s for pod "pod-13b02ffc-3740-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-xk6wf" to be "success or failure"
Feb 23 07:53:15.939: INFO: Pod "pod-13b02ffc-3740-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.895078ms
Feb 23 07:53:17.943: INFO: Pod "pod-13b02ffc-3740-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007566672s
STEP: Saw pod success
Feb 23 07:53:17.943: INFO: Pod "pod-13b02ffc-3740-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:53:17.945: INFO: Trying to get logs from node kube-node-01 pod pod-13b02ffc-3740-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 07:53:17.966: INFO: Waiting for pod pod-13b02ffc-3740-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:53:17.969: INFO: Pod pod-13b02ffc-3740-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:53:17.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xk6wf" for this suite.
Feb 23 07:53:23.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:53:24.062: INFO: namespace: e2e-tests-emptydir-xk6wf, resource: bindings, ignored listing per whitelist
Feb 23 07:53:24.102: INFO: namespace e2e-tests-emptydir-xk6wf deletion completed in 6.128547239s

• [SLOW TEST:8.256 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:53:24.102: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-mxhw
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 07:53:24.192: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mxhw" in namespace "e2e-tests-subpath-7bkqh" to be "success or failure"
Feb 23 07:53:24.196: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.616358ms
Feb 23 07:53:26.200: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007623743s
Feb 23 07:53:28.205: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Running", Reason="", readiness=false. Elapsed: 4.013113465s
Feb 23 07:53:30.209: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Running", Reason="", readiness=false. Elapsed: 6.017051384s
Feb 23 07:53:32.213: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Running", Reason="", readiness=false. Elapsed: 8.020668095s
Feb 23 07:53:34.217: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Running", Reason="", readiness=false. Elapsed: 10.02460342s
Feb 23 07:53:36.221: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Running", Reason="", readiness=false. Elapsed: 12.028620395s
Feb 23 07:53:38.225: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Running", Reason="", readiness=false. Elapsed: 14.032423211s
Feb 23 07:53:40.228: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Running", Reason="", readiness=false. Elapsed: 16.036075195s
Feb 23 07:53:42.232: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Running", Reason="", readiness=false. Elapsed: 18.03989203s
Feb 23 07:53:44.236: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Running", Reason="", readiness=false. Elapsed: 20.043622444s
Feb 23 07:53:46.240: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Running", Reason="", readiness=false. Elapsed: 22.047719889s
Feb 23 07:53:48.244: INFO: Pod "pod-subpath-test-downwardapi-mxhw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.05224587s
STEP: Saw pod success
Feb 23 07:53:48.244: INFO: Pod "pod-subpath-test-downwardapi-mxhw" satisfied condition "success or failure"
Feb 23 07:53:48.247: INFO: Trying to get logs from node kube-node-01 pod pod-subpath-test-downwardapi-mxhw container test-container-subpath-downwardapi-mxhw: <nil>
STEP: delete the pod
Feb 23 07:53:48.269: INFO: Waiting for pod pod-subpath-test-downwardapi-mxhw to disappear
Feb 23 07:53:48.273: INFO: Pod pod-subpath-test-downwardapi-mxhw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mxhw
Feb 23 07:53:48.273: INFO: Deleting pod "pod-subpath-test-downwardapi-mxhw" in namespace "e2e-tests-subpath-7bkqh"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:53:48.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-7bkqh" for this suite.
Feb 23 07:53:54.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:53:54.330: INFO: namespace: e2e-tests-subpath-7bkqh, resource: bindings, ignored listing per whitelist
Feb 23 07:53:54.412: INFO: namespace e2e-tests-subpath-7bkqh deletion completed in 6.131736096s

• [SLOW TEST:30.310 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:53:54.412: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:53:56.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-hmfps" for this suite.
Feb 23 07:54:48.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:54:48.535: INFO: namespace: e2e-tests-kubelet-test-hmfps, resource: bindings, ignored listing per whitelist
Feb 23 07:54:48.627: INFO: namespace e2e-tests-kubelet-test-hmfps deletion completed in 52.112994175s

• [SLOW TEST:54.215 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:54:48.628: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 23 07:54:48.701: INFO: Waiting up to 5m0s for pod "pod-4afb67f8-3740-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-k9xsq" to be "success or failure"
Feb 23 07:54:48.706: INFO: Pod "pod-4afb67f8-3740-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.048134ms
Feb 23 07:54:50.710: INFO: Pod "pod-4afb67f8-3740-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008762232s
STEP: Saw pod success
Feb 23 07:54:50.710: INFO: Pod "pod-4afb67f8-3740-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:54:50.713: INFO: Trying to get logs from node kube-node-02 pod pod-4afb67f8-3740-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 07:54:50.736: INFO: Waiting for pod pod-4afb67f8-3740-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:54:50.738: INFO: Pod pod-4afb67f8-3740-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:54:50.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-k9xsq" for this suite.
Feb 23 07:54:56.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:54:56.768: INFO: namespace: e2e-tests-emptydir-k9xsq, resource: bindings, ignored listing per whitelist
Feb 23 07:54:56.855: INFO: namespace e2e-tests-emptydir-k9xsq deletion completed in 6.112912397s

• [SLOW TEST:8.228 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:54:56.856: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 23 07:54:56.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:54:57.327: INFO: stderr: ""
Feb 23 07:54:57.327: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 07:54:57.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:54:57.418: INFO: stderr: ""
Feb 23 07:54:57.418: INFO: stdout: "update-demo-nautilus-kklqd update-demo-nautilus-rr2r7 "
Feb 23 07:54:57.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-kklqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:54:57.488: INFO: stderr: ""
Feb 23 07:54:57.488: INFO: stdout: ""
Feb 23 07:54:57.488: INFO: update-demo-nautilus-kklqd is created but not running
Feb 23 07:55:02.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:02.711: INFO: stderr: ""
Feb 23 07:55:02.711: INFO: stdout: "update-demo-nautilus-kklqd update-demo-nautilus-rr2r7 "
Feb 23 07:55:02.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-kklqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:02.881: INFO: stderr: ""
Feb 23 07:55:02.881: INFO: stdout: "true"
Feb 23 07:55:02.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-kklqd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:03.050: INFO: stderr: ""
Feb 23 07:55:03.050: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 07:55:03.050: INFO: validating pod update-demo-nautilus-kklqd
Feb 23 07:55:03.059: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 07:55:03.059: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 07:55:03.059: INFO: update-demo-nautilus-kklqd is verified up and running
Feb 23 07:55:03.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-rr2r7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:03.221: INFO: stderr: ""
Feb 23 07:55:03.221: INFO: stdout: "true"
Feb 23 07:55:03.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-rr2r7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:03.397: INFO: stderr: ""
Feb 23 07:55:03.397: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 07:55:03.397: INFO: validating pod update-demo-nautilus-rr2r7
Feb 23 07:55:03.405: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 07:55:03.405: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 07:55:03.405: INFO: update-demo-nautilus-rr2r7 is verified up and running
STEP: scaling down the replication controller
Feb 23 07:55:03.407: INFO: scanned /root for discovery docs: <nil>
Feb 23 07:55:03.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:03.566: INFO: stderr: ""
Feb 23 07:55:03.566: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 07:55:03.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:03.637: INFO: stderr: ""
Feb 23 07:55:03.637: INFO: stdout: "update-demo-nautilus-kklqd update-demo-nautilus-rr2r7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 23 07:55:08.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:08.712: INFO: stderr: ""
Feb 23 07:55:08.712: INFO: stdout: "update-demo-nautilus-kklqd update-demo-nautilus-rr2r7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 23 07:55:13.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:13.783: INFO: stderr: ""
Feb 23 07:55:13.783: INFO: stdout: "update-demo-nautilus-rr2r7 "
Feb 23 07:55:13.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-rr2r7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:13.847: INFO: stderr: ""
Feb 23 07:55:13.847: INFO: stdout: "true"
Feb 23 07:55:13.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-rr2r7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:13.917: INFO: stderr: ""
Feb 23 07:55:13.917: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 07:55:13.917: INFO: validating pod update-demo-nautilus-rr2r7
Feb 23 07:55:13.922: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 07:55:13.922: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 07:55:13.922: INFO: update-demo-nautilus-rr2r7 is verified up and running
STEP: scaling up the replication controller
Feb 23 07:55:13.923: INFO: scanned /root for discovery docs: <nil>
Feb 23 07:55:13.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:15.039: INFO: stderr: ""
Feb 23 07:55:15.039: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 07:55:15.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:15.108: INFO: stderr: ""
Feb 23 07:55:15.108: INFO: stdout: "update-demo-nautilus-lzpkn update-demo-nautilus-rr2r7 "
Feb 23 07:55:15.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-lzpkn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:15.172: INFO: stderr: ""
Feb 23 07:55:15.172: INFO: stdout: "true"
Feb 23 07:55:15.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-lzpkn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:15.234: INFO: stderr: ""
Feb 23 07:55:15.234: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 07:55:15.234: INFO: validating pod update-demo-nautilus-lzpkn
Feb 23 07:55:15.240: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 07:55:15.240: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 07:55:15.240: INFO: update-demo-nautilus-lzpkn is verified up and running
Feb 23 07:55:15.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-rr2r7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:15.305: INFO: stderr: ""
Feb 23 07:55:15.305: INFO: stdout: "true"
Feb 23 07:55:15.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-rr2r7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:15.371: INFO: stderr: ""
Feb 23 07:55:15.371: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 07:55:15.371: INFO: validating pod update-demo-nautilus-rr2r7
Feb 23 07:55:15.375: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 07:55:15.375: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 07:55:15.375: INFO: update-demo-nautilus-rr2r7 is verified up and running
STEP: using delete to clean up resources
Feb 23 07:55:15.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:15.455: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 07:55:15.455: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 23 07:55:15.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-5fgpw'
Feb 23 07:55:15.533: INFO: stderr: "No resources found.\n"
Feb 23 07:55:15.533: INFO: stdout: ""
Feb 23 07:55:15.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -l name=update-demo --namespace=e2e-tests-kubectl-5fgpw -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 07:55:15.600: INFO: stderr: ""
Feb 23 07:55:15.600: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:55:15.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5fgpw" for this suite.
Feb 23 07:55:37.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:55:37.691: INFO: namespace: e2e-tests-kubectl-5fgpw, resource: bindings, ignored listing per whitelist
Feb 23 07:55:37.723: INFO: namespace e2e-tests-kubectl-5fgpw deletion completed in 22.118722425s

• [SLOW TEST:40.868 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:55:37.723: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 07:55:37.802: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 23 07:55:37.812: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:37.812: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:37.812: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:37.816: INFO: Number of nodes with available pods: 0
Feb 23 07:55:37.816: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 07:55:38.821: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:38.821: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:38.821: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:38.824: INFO: Number of nodes with available pods: 0
Feb 23 07:55:38.824: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 07:55:39.821: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:39.821: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:39.821: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:39.825: INFO: Number of nodes with available pods: 2
Feb 23 07:55:39.825: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 23 07:55:39.858: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:39.858: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:39.862: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:39.862: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:39.862: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:40.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:40.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:40.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:40.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:40.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:41.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:41.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:41.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:41.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:41.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:42.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:42.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:42.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:42.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:42.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:43.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:43.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:43.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:43.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:43.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:44.871: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:44.871: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:44.875: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:44.875: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:44.875: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:45.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:45.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:45.875: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:45.875: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:45.875: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:46.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:46.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:46.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:46.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:46.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:47.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:47.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:47.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:47.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:47.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:48.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:48.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:48.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:48.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:48.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:49.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:49.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:49.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:49.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:49.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:50.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:50.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:50.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:50.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:50.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:51.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:51.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:51.872: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:51.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:51.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:52.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:52.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:52.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:52.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:52.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:53.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:53.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:53.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:53.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:53.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:54.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:54.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:54.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:54.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:54.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:55.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:55.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:55.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:55.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:55.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:56.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:56.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:56.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:56.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:56.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:57.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:57.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:57.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:57.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:57.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:58.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:58.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:58.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:58.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:58.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:59.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:59.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:55:59.872: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:59.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:55:59.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:00.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:00.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:00.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:00.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:00.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:01.872: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:01.875: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:01.891: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:01.891: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:01.891: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:02.873: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:02.873: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:02.878: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:02.878: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:02.878: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:03.869: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:03.869: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:03.873: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:03.873: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:03.873: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:04.875: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:04.875: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:04.880: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:04.880: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:04.880: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:05.868: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:05.868: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:05.872: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:05.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:05.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:06.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:06.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:06.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:06.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:06.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:07.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:07.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:07.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:07.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:07.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:08.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:08.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:08.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:08.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:08.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:09.868: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:09.868: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:09.873: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:09.873: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:09.873: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:10.873: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:10.873: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:10.877: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:10.877: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:10.877: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:11.871: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:11.871: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:11.878: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:11.878: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:11.878: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:12.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:12.867: INFO: Pod daemon-set-6wf28 is not available
Feb 23 07:56:12.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:12.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:12.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:12.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:13.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:13.866: INFO: Pod daemon-set-6wf28 is not available
Feb 23 07:56:13.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:13.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:13.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:13.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:14.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:14.867: INFO: Pod daemon-set-6wf28 is not available
Feb 23 07:56:14.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:14.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:14.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:14.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:15.866: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:15.866: INFO: Pod daemon-set-6wf28 is not available
Feb 23 07:56:15.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:15.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:15.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:15.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:16.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:16.867: INFO: Pod daemon-set-6wf28 is not available
Feb 23 07:56:16.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:16.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:16.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:16.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:17.867: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:17.867: INFO: Pod daemon-set-6wf28 is not available
Feb 23 07:56:17.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:17.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:17.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:17.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:18.868: INFO: Wrong image for pod: daemon-set-6wf28. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:18.868: INFO: Pod daemon-set-6wf28 is not available
Feb 23 07:56:18.868: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:18.873: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:18.873: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:18.873: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:19.866: INFO: Pod daemon-set-5m8pl is not available
Feb 23 07:56:19.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:19.874: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:19.874: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:19.874: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:20.866: INFO: Pod daemon-set-5m8pl is not available
Feb 23 07:56:20.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:20.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:20.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:20.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:21.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:21.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:21.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:21.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:22.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:22.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:22.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:22.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:23.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:23.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:23.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:23.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:24.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:24.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:24.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:24.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:25.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:25.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:25.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:25.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:26.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:26.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:26.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:26.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:27.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:27.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:27.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:27.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:28.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:28.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:28.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:28.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:29.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:29.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:29.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:29.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:30.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:30.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:30.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:30.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:31.868: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:31.873: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:31.873: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:31.873: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:32.871: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:32.875: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:32.875: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:32.875: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:33.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:33.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:33.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:33.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:34.868: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:34.872: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:34.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:34.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:35.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:35.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:35.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:35.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:36.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:36.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:36.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:36.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:37.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:37.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:37.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:37.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:38.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:38.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:38.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:38.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:39.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:39.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:39.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:39.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:40.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:40.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:40.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:40.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:41.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:41.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:41.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:41.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:42.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:42.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:42.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:42.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:43.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:43.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:43.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:43.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:44.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:44.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:44.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:44.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:45.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:45.872: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:45.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:45.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:46.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:46.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:46.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:46.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:47.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:47.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:47.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:47.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:48.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:48.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:48.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:48.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:49.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:49.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:49.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:49.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:50.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:50.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:50.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:50.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:51.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:51.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:51.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:51.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:52.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:52.867: INFO: Pod daemon-set-dlxkr is not available
Feb 23 07:56:52.872: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:52.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:52.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:53.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:53.866: INFO: Pod daemon-set-dlxkr is not available
Feb 23 07:56:53.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:53.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:53.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:54.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:54.866: INFO: Pod daemon-set-dlxkr is not available
Feb 23 07:56:54.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:54.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:54.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:55.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:55.866: INFO: Pod daemon-set-dlxkr is not available
Feb 23 07:56:55.873: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:55.873: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:55.873: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:56.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:56.866: INFO: Pod daemon-set-dlxkr is not available
Feb 23 07:56:56.870: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:56.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:56.870: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:57.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:57.866: INFO: Pod daemon-set-dlxkr is not available
Feb 23 07:56:57.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:57.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:57.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:58.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:58.867: INFO: Pod daemon-set-dlxkr is not available
Feb 23 07:56:58.872: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:58.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:58.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:59.866: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:56:59.866: INFO: Pod daemon-set-dlxkr is not available
Feb 23 07:56:59.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:59.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:56:59.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:00.867: INFO: Wrong image for pod: daemon-set-dlxkr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 23 07:57:00.867: INFO: Pod daemon-set-dlxkr is not available
Feb 23 07:57:00.871: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:00.871: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:00.872: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:01.876: INFO: Pod daemon-set-dnpgr is not available
Feb 23 07:57:01.899: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:01.900: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:01.900: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 23 07:57:01.907: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:01.907: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:01.907: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:01.915: INFO: Number of nodes with available pods: 1
Feb 23 07:57:01.915: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 07:57:02.968: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:02.968: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:02.968: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 07:57:02.979: INFO: Number of nodes with available pods: 2
Feb 23 07:57:02.979: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-pfkkv, will wait for the garbage collector to delete the pods
Feb 23 07:57:03.201: INFO: Deleting DaemonSet.extensions daemon-set took: 10.906316ms
Feb 23 07:57:03.301: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.196202ms
Feb 23 07:57:09.705: INFO: Number of nodes with available pods: 0
Feb 23 07:57:09.705: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 07:57:09.712: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-pfkkv/daemonsets","resourceVersion":"14295"},"items":null}

Feb 23 07:57:09.714: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-pfkkv/pods","resourceVersion":"14295"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:57:09.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-pfkkv" for this suite.
Feb 23 07:57:15.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:57:15.807: INFO: namespace: e2e-tests-daemonsets-pfkkv, resource: bindings, ignored listing per whitelist
Feb 23 07:57:15.855: INFO: namespace e2e-tests-daemonsets-pfkkv deletion completed in 6.127644247s

• [SLOW TEST:98.132 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:57:15.855: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 23 07:57:15.923: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 07:57:15.931: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 07:57:15.934: INFO: 
Logging pods the kubelet thinks is on node kube-node-01 before test
Feb 23 07:57:15.942: INFO: rook-api-77bc4484c6-dfhs4 from rook started at 2019-02-23 07:04:58 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.942: INFO: 	Container rook-api ready: true, restart count 0
Feb 23 07:57:15.942: INFO: rook-ceph-osd-fxwdx from rook started at 2019-02-23 07:05:00 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.942: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Feb 23 07:57:15.942: INFO: sonobuoy-systemd-logs-daemon-set-b1c5aeee47bb4835-tp9jc from heptio-sonobuoy started at 2019-02-23 07:11:00 +0000 UTC (2 container statuses recorded)
Feb 23 07:57:15.942: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb 23 07:57:15.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 07:57:15.942: INFO: weave-net-vxfzf from kube-system started at 2019-02-23 07:03:02 +0000 UTC (2 container statuses recorded)
Feb 23 07:57:15.942: INFO: 	Container weave ready: true, restart count 1
Feb 23 07:57:15.942: INFO: 	Container weave-npc ready: true, restart count 0
Feb 23 07:57:15.942: INFO: rook-ceph-mon0-ljg4v from rook started at 2019-02-23 07:03:45 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.942: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Feb 23 07:57:15.942: INFO: rook-agent-bm2h9 from rook-system started at 2019-02-23 07:03:42 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.942: INFO: 	Container rook-agent ready: true, restart count 0
Feb 23 07:57:15.942: INFO: rook-ceph-mon2-jdx9z from rook started at 2019-02-23 07:04:44 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.942: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Feb 23 07:57:15.942: INFO: rook-ceph-mgr0-65cb98fd4f-swbqs from rook started at 2019-02-23 07:04:58 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.942: INFO: 	Container rook-ceph-mgr0 ready: true, restart count 0
Feb 23 07:57:15.942: INFO: kube-proxy-622xh from kube-system started at 2019-02-23 07:03:02 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.942: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 07:57:15.942: INFO: 
Logging pods the kubelet thinks is on node kube-node-02 before test
Feb 23 07:57:15.950: INFO: rook-operator-98c6d84c5-8dvfs from rook-system started at 2019-02-23 07:02:59 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.950: INFO: 	Container rook-operator ready: true, restart count 0
Feb 23 07:57:15.950: INFO: rook-agent-8lrhx from rook-system started at 2019-02-23 07:03:29 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.950: INFO: 	Container rook-agent ready: true, restart count 0
Feb 23 07:57:15.950: INFO: rook-ceph-mon1-cpwgw from rook started at 2019-02-23 07:04:38 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.950: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Feb 23 07:57:15.950: INFO: weave-net-psnqt from kube-system started at 2019-02-23 07:02:59 +0000 UTC (2 container statuses recorded)
Feb 23 07:57:15.950: INFO: 	Container weave ready: true, restart count 0
Feb 23 07:57:15.950: INFO: 	Container weave-npc ready: true, restart count 0
Feb 23 07:57:15.950: INFO: sonobuoy-systemd-logs-daemon-set-b1c5aeee47bb4835-42s5h from heptio-sonobuoy started at 2019-02-23 07:11:00 +0000 UTC (2 container statuses recorded)
Feb 23 07:57:15.950: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb 23 07:57:15.950: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 07:57:15.950: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-23 07:10:57 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.950: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 23 07:57:15.950: INFO: tiller-deploy-dbb85cb99-9gspb from kube-system started at 2019-02-23 07:02:59 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.950: INFO: 	Container tiller ready: true, restart count 0
Feb 23 07:57:15.950: INFO: kube-proxy-lc5r2 from kube-system started at 2019-02-23 07:02:59 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.950: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 07:57:15.950: INFO: rook-ceph-osd-mfkgj from rook started at 2019-02-23 07:05:00 +0000 UTC (1 container statuses recorded)
Feb 23 07:57:15.950: INFO: 	Container rook-ceph-osd ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1585ef2ff911fcdc], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:57:16.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-5rjgm" for this suite.
Feb 23 07:57:23.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:57:23.084: INFO: namespace: e2e-tests-sched-pred-5rjgm, resource: bindings, ignored listing per whitelist
Feb 23 07:57:23.113: INFO: namespace e2e-tests-sched-pred-5rjgm deletion completed in 6.121599454s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.259 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:57:23.114: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-a70f6399-3740-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 07:57:23.186: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7100b95-3740-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-configmap-rwwq9" to be "success or failure"
Feb 23 07:57:23.192: INFO: Pod "pod-configmaps-a7100b95-3740-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.541528ms
Feb 23 07:57:25.204: INFO: Pod "pod-configmaps-a7100b95-3740-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018137546s
STEP: Saw pod success
Feb 23 07:57:25.204: INFO: Pod "pod-configmaps-a7100b95-3740-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:57:25.208: INFO: Trying to get logs from node kube-node-02 pod pod-configmaps-a7100b95-3740-11e9-a63e-52687f37ce9e container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 07:57:25.230: INFO: Waiting for pod pod-configmaps-a7100b95-3740-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:57:25.233: INFO: Pod pod-configmaps-a7100b95-3740-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:57:25.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rwwq9" for this suite.
Feb 23 07:57:31.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:57:31.289: INFO: namespace: e2e-tests-configmap-rwwq9, resource: bindings, ignored listing per whitelist
Feb 23 07:57:31.360: INFO: namespace e2e-tests-configmap-rwwq9 deletion completed in 6.122501844s

• [SLOW TEST:8.246 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:57:31.360: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Feb 23 07:57:33.532: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:57:57.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-zqwgl" for this suite.
Feb 23 07:58:03.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:58:03.622: INFO: namespace: e2e-tests-namespaces-zqwgl, resource: bindings, ignored listing per whitelist
Feb 23 07:58:03.804: INFO: namespace e2e-tests-namespaces-zqwgl deletion completed in 6.219980451s
STEP: Destroying namespace "e2e-tests-nsdeletetest-qd6l9" for this suite.
Feb 23 07:58:03.808: INFO: Namespace e2e-tests-nsdeletetest-qd6l9 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-7wsxf" for this suite.
Feb 23 07:58:09.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:58:09.879: INFO: namespace: e2e-tests-nsdeletetest-7wsxf, resource: bindings, ignored listing per whitelist
Feb 23 07:58:09.988: INFO: namespace e2e-tests-nsdeletetest-7wsxf deletion completed in 6.179543475s

• [SLOW TEST:38.628 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:58:09.988: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 23 07:58:10.061: INFO: PodSpec: initContainers in spec.initContainers
Feb 23 07:58:52.129: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c301bf66-3740-11e9-a63e-52687f37ce9e", GenerateName:"", Namespace:"e2e-tests-init-container-97rnj", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-97rnj/pods/pod-init-c301bf66-3740-11e9-a63e-52687f37ce9e", UID:"c307ee5d-3740-11e9-8d12-02951b420040", ResourceVersion:"14646", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686505490, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"61711565"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-n65qp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001294cc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-n65qp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-n65qp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}, "cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-n65qp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001d07848), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kube-node-01", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001a95f80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001d078d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001d07960)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001d07968), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001d0796c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686505490, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686505490, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686505490, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686505490, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.17.10.102", PodIP:"10.40.0.6", StartTime:(*v1.Time)(0xc001050060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0021b3c00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0021b3c70)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://685e1f857d2fcb35b1fe8f089bfc4e1b5e8b7c5409182d06f0b33efe7d47312b"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0010500a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001050080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:58:52.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-97rnj" for this suite.
Feb 23 07:59:14.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:59:14.254: INFO: namespace: e2e-tests-init-container-97rnj, resource: bindings, ignored listing per whitelist
Feb 23 07:59:14.328: INFO: namespace e2e-tests-init-container-97rnj deletion completed in 22.192938901s

• [SLOW TEST:64.340 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:59:14.328: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 23 07:59:14.444: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-6hg2x,SelfLink:/api/v1/namespaces/e2e-tests-watch-6hg2x/configmaps/e2e-watch-test-resource-version,UID:e9631d9f-3740-11e9-8d12-02951b420040,ResourceVersion:14709,Generation:0,CreationTimestamp:2019-02-23 07:59:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 07:59:14.444: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-6hg2x,SelfLink:/api/v1/namespaces/e2e-tests-watch-6hg2x/configmaps/e2e-watch-test-resource-version,UID:e9631d9f-3740-11e9-8d12-02951b420040,ResourceVersion:14710,Generation:0,CreationTimestamp:2019-02-23 07:59:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:59:14.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-6hg2x" for this suite.
Feb 23 07:59:20.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:59:20.564: INFO: namespace: e2e-tests-watch-6hg2x, resource: bindings, ignored listing per whitelist
Feb 23 07:59:20.585: INFO: namespace e2e-tests-watch-6hg2x deletion completed in 6.136827134s

• [SLOW TEST:6.257 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:59:20.585: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-4wv8d
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4wv8d to expose endpoints map[]
Feb 23 07:59:20.669: INFO: Get endpoints failed (3.002368ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb 23 07:59:21.673: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4wv8d exposes endpoints map[] (1.006541814s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-4wv8d
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4wv8d to expose endpoints map[pod1:[100]]
Feb 23 07:59:23.699: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4wv8d exposes endpoints map[pod1:[100]] (2.019586598s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-4wv8d
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4wv8d to expose endpoints map[pod1:[100] pod2:[101]]
Feb 23 07:59:25.734: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4wv8d exposes endpoints map[pod2:[101] pod1:[100]] (2.029888165s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-4wv8d
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4wv8d to expose endpoints map[pod2:[101]]
Feb 23 07:59:26.756: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4wv8d exposes endpoints map[pod2:[101]] (1.01470634s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-4wv8d
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4wv8d to expose endpoints map[]
Feb 23 07:59:26.769: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4wv8d exposes endpoints map[] (6.283828ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:59:26.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-4wv8d" for this suite.
Feb 23 07:59:32.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:59:32.824: INFO: namespace: e2e-tests-services-4wv8d, resource: bindings, ignored listing per whitelist
Feb 23 07:59:32.953: INFO: namespace e2e-tests-services-4wv8d deletion completed in 6.151172743s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:12.368 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:59:32.953: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-f4742a39-3740-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 07:59:33.030: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f474c32d-3740-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-qlnq6" to be "success or failure"
Feb 23 07:59:33.036: INFO: Pod "pod-projected-secrets-f474c32d-3740-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.315789ms
Feb 23 07:59:35.039: INFO: Pod "pod-projected-secrets-f474c32d-3740-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009008415s
STEP: Saw pod success
Feb 23 07:59:35.039: INFO: Pod "pod-projected-secrets-f474c32d-3740-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 07:59:35.042: INFO: Trying to get logs from node kube-node-02 pod pod-projected-secrets-f474c32d-3740-11e9-a63e-52687f37ce9e container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 07:59:35.063: INFO: Waiting for pod pod-projected-secrets-f474c32d-3740-11e9-a63e-52687f37ce9e to disappear
Feb 23 07:59:35.065: INFO: Pod pod-projected-secrets-f474c32d-3740-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:59:35.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qlnq6" for this suite.
Feb 23 07:59:41.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 07:59:41.107: INFO: namespace: e2e-tests-projected-qlnq6, resource: bindings, ignored listing per whitelist
Feb 23 07:59:41.221: INFO: namespace e2e-tests-projected-qlnq6 deletion completed in 6.151437615s

• [SLOW TEST:8.267 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 07:59:41.221: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f96109da-3740-11e9-a63e-52687f37ce9e
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f96109da-3740-11e9-a63e-52687f37ce9e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 07:59:45.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2lktq" for this suite.
Feb 23 08:00:07.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:00:07.363: INFO: namespace: e2e-tests-projected-2lktq, resource: bindings, ignored listing per whitelist
Feb 23 08:00:07.499: INFO: namespace e2e-tests-projected-2lktq deletion completed in 22.158801948s

• [SLOW TEST:26.278 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:00:07.499: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 23 08:00:07.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-7kg76'
Feb 23 08:00:07.657: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 08:00:07.657: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Feb 23 08:00:07.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-7kg76'
Feb 23 08:00:07.741: INFO: stderr: ""
Feb 23 08:00:07.741: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:00:07.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7kg76" for this suite.
Feb 23 08:00:29.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:00:29.844: INFO: namespace: e2e-tests-kubectl-7kg76, resource: bindings, ignored listing per whitelist
Feb 23 08:00:29.920: INFO: namespace e2e-tests-kubectl-7kg76 deletion completed in 22.174338861s

• [SLOW TEST:22.421 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:00:29.920: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-166da8d2-3741-11e9-a63e-52687f37ce9e
STEP: Creating configMap with name cm-test-opt-upd-166da915-3741-11e9-a63e-52687f37ce9e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-166da8d2-3741-11e9-a63e-52687f37ce9e
STEP: Updating configmap cm-test-opt-upd-166da915-3741-11e9-a63e-52687f37ce9e
STEP: Creating configMap with name cm-test-opt-create-166da92d-3741-11e9-a63e-52687f37ce9e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:00:34.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9ws9x" for this suite.
Feb 23 08:00:56.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:00:56.203: INFO: namespace: e2e-tests-projected-9ws9x, resource: bindings, ignored listing per whitelist
Feb 23 08:00:56.327: INFO: namespace e2e-tests-projected-9ws9x deletion completed in 22.194267025s

• [SLOW TEST:26.407 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:00:56.327: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 23 08:00:56.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-p9l77'
Feb 23 08:00:56.574: INFO: stderr: ""
Feb 23 08:00:56.574: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 08:00:56.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-p9l77'
Feb 23 08:00:56.668: INFO: stderr: ""
Feb 23 08:00:56.668: INFO: stdout: "update-demo-nautilus-64vks update-demo-nautilus-hqfsg "
Feb 23 08:00:56.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-64vks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p9l77'
Feb 23 08:00:56.732: INFO: stderr: ""
Feb 23 08:00:56.732: INFO: stdout: ""
Feb 23 08:00:56.732: INFO: update-demo-nautilus-64vks is created but not running
Feb 23 08:01:01.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-p9l77'
Feb 23 08:01:02.021: INFO: stderr: ""
Feb 23 08:01:02.021: INFO: stdout: "update-demo-nautilus-64vks update-demo-nautilus-hqfsg "
Feb 23 08:01:02.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-64vks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p9l77'
Feb 23 08:01:02.186: INFO: stderr: ""
Feb 23 08:01:02.186: INFO: stdout: "true"
Feb 23 08:01:02.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-64vks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p9l77'
Feb 23 08:01:02.265: INFO: stderr: ""
Feb 23 08:01:02.265: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 08:01:02.265: INFO: validating pod update-demo-nautilus-64vks
Feb 23 08:01:02.273: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 08:01:02.273: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 08:01:02.273: INFO: update-demo-nautilus-64vks is verified up and running
Feb 23 08:01:02.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-hqfsg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p9l77'
Feb 23 08:01:02.344: INFO: stderr: ""
Feb 23 08:01:02.344: INFO: stdout: "true"
Feb 23 08:01:02.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-hqfsg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p9l77'
Feb 23 08:01:02.411: INFO: stderr: ""
Feb 23 08:01:02.411: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 08:01:02.411: INFO: validating pod update-demo-nautilus-hqfsg
Feb 23 08:01:02.421: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 08:01:02.421: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 08:01:02.421: INFO: update-demo-nautilus-hqfsg is verified up and running
STEP: using delete to clean up resources
Feb 23 08:01:02.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-p9l77'
Feb 23 08:01:02.517: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 08:01:02.517: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 23 08:01:02.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-p9l77'
Feb 23 08:01:02.612: INFO: stderr: "No resources found.\n"
Feb 23 08:01:02.612: INFO: stdout: ""
Feb 23 08:01:02.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -l name=update-demo --namespace=e2e-tests-kubectl-p9l77 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 08:01:02.693: INFO: stderr: ""
Feb 23 08:01:02.693: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:01:02.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-p9l77" for this suite.
Feb 23 08:01:24.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:01:24.807: INFO: namespace: e2e-tests-kubectl-p9l77, resource: bindings, ignored listing per whitelist
Feb 23 08:01:24.829: INFO: namespace e2e-tests-kubectl-p9l77 deletion completed in 22.131416631s

• [SLOW TEST:28.503 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:01:24.829: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-pwcl
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 08:01:24.911: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pwcl" in namespace "e2e-tests-subpath-hznhh" to be "success or failure"
Feb 23 08:01:24.914: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.312736ms
Feb 23 08:01:26.919: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Running", Reason="", readiness=false. Elapsed: 2.007405652s
Feb 23 08:01:28.923: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Running", Reason="", readiness=false. Elapsed: 4.011526947s
Feb 23 08:01:30.927: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Running", Reason="", readiness=false. Elapsed: 6.015727239s
Feb 23 08:01:32.931: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Running", Reason="", readiness=false. Elapsed: 8.019494797s
Feb 23 08:01:34.934: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Running", Reason="", readiness=false. Elapsed: 10.02316236s
Feb 23 08:01:36.938: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Running", Reason="", readiness=false. Elapsed: 12.026985914s
Feb 23 08:01:38.942: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Running", Reason="", readiness=false. Elapsed: 14.031034004s
Feb 23 08:01:40.946: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Running", Reason="", readiness=false. Elapsed: 16.035261118s
Feb 23 08:01:42.950: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Running", Reason="", readiness=false. Elapsed: 18.039339518s
Feb 23 08:01:44.955: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Running", Reason="", readiness=false. Elapsed: 20.043681642s
Feb 23 08:01:46.959: INFO: Pod "pod-subpath-test-configmap-pwcl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.047825576s
STEP: Saw pod success
Feb 23 08:01:46.959: INFO: Pod "pod-subpath-test-configmap-pwcl" satisfied condition "success or failure"
Feb 23 08:01:46.962: INFO: Trying to get logs from node kube-node-02 pod pod-subpath-test-configmap-pwcl container test-container-subpath-configmap-pwcl: <nil>
STEP: delete the pod
Feb 23 08:01:46.984: INFO: Waiting for pod pod-subpath-test-configmap-pwcl to disappear
Feb 23 08:01:46.987: INFO: Pod pod-subpath-test-configmap-pwcl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pwcl
Feb 23 08:01:46.987: INFO: Deleting pod "pod-subpath-test-configmap-pwcl" in namespace "e2e-tests-subpath-hznhh"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:01:46.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-hznhh" for this suite.
Feb 23 08:01:53.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:01:53.025: INFO: namespace: e2e-tests-subpath-hznhh, resource: bindings, ignored listing per whitelist
Feb 23 08:01:53.115: INFO: namespace e2e-tests-subpath-hznhh deletion completed in 6.120847093s

• [SLOW TEST:28.285 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:01:53.115: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 08:01:53.179: INFO: Creating ReplicaSet my-hostname-basic-47fed2d0-3741-11e9-a63e-52687f37ce9e
Feb 23 08:01:53.187: INFO: Pod name my-hostname-basic-47fed2d0-3741-11e9-a63e-52687f37ce9e: Found 0 pods out of 1
Feb 23 08:01:58.191: INFO: Pod name my-hostname-basic-47fed2d0-3741-11e9-a63e-52687f37ce9e: Found 1 pods out of 1
Feb 23 08:01:58.191: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-47fed2d0-3741-11e9-a63e-52687f37ce9e" is running
Feb 23 08:01:58.194: INFO: Pod "my-hostname-basic-47fed2d0-3741-11e9-a63e-52687f37ce9e-bg8ws" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-23 08:01:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-23 08:01:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-23 08:01:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-23 08:01:53 +0000 UTC Reason: Message:}])
Feb 23 08:01:58.194: INFO: Trying to dial the pod
Feb 23 08:02:03.208: INFO: Controller my-hostname-basic-47fed2d0-3741-11e9-a63e-52687f37ce9e: Got expected result from replica 1 [my-hostname-basic-47fed2d0-3741-11e9-a63e-52687f37ce9e-bg8ws]: "my-hostname-basic-47fed2d0-3741-11e9-a63e-52687f37ce9e-bg8ws", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:02:03.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-gpx5q" for this suite.
Feb 23 08:02:09.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:02:09.258: INFO: namespace: e2e-tests-replicaset-gpx5q, resource: bindings, ignored listing per whitelist
Feb 23 08:02:09.338: INFO: namespace e2e-tests-replicaset-gpx5q deletion completed in 6.125017355s

• [SLOW TEST:16.223 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:02:09.338: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 23 08:02:09.402: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:02:13.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-zlzz4" for this suite.
Feb 23 08:02:19.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:02:19.260: INFO: namespace: e2e-tests-init-container-zlzz4, resource: bindings, ignored listing per whitelist
Feb 23 08:02:19.297: INFO: namespace e2e-tests-init-container-zlzz4 deletion completed in 6.121927899s

• [SLOW TEST:9.960 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:02:19.297: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-5799b61d-3741-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 08:02:19.371: INFO: Waiting up to 5m0s for pod "pod-secrets-579a4ad7-3741-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-secrets-6tkmm" to be "success or failure"
Feb 23 08:02:19.378: INFO: Pod "pod-secrets-579a4ad7-3741-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.914513ms
Feb 23 08:02:21.382: INFO: Pod "pod-secrets-579a4ad7-3741-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010424743s
STEP: Saw pod success
Feb 23 08:02:21.382: INFO: Pod "pod-secrets-579a4ad7-3741-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:02:21.385: INFO: Trying to get logs from node kube-node-01 pod pod-secrets-579a4ad7-3741-11e9-a63e-52687f37ce9e container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 08:02:21.408: INFO: Waiting for pod pod-secrets-579a4ad7-3741-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:02:21.411: INFO: Pod pod-secrets-579a4ad7-3741-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:02:21.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6tkmm" for this suite.
Feb 23 08:02:27.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:02:27.467: INFO: namespace: e2e-tests-secrets-6tkmm, resource: bindings, ignored listing per whitelist
Feb 23 08:02:27.536: INFO: namespace e2e-tests-secrets-6tkmm deletion completed in 6.121240527s

• [SLOW TEST:8.239 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:02:27.537: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 23 08:02:27.607: INFO: Waiting up to 5m0s for pod "var-expansion-5c82f6c3-3741-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-var-expansion-92wpk" to be "success or failure"
Feb 23 08:02:27.613: INFO: Pod "var-expansion-5c82f6c3-3741-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.791336ms
Feb 23 08:02:29.617: INFO: Pod "var-expansion-5c82f6c3-3741-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00952469s
STEP: Saw pod success
Feb 23 08:02:29.617: INFO: Pod "var-expansion-5c82f6c3-3741-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:02:29.619: INFO: Trying to get logs from node kube-node-02 pod var-expansion-5c82f6c3-3741-11e9-a63e-52687f37ce9e container dapi-container: <nil>
STEP: delete the pod
Feb 23 08:02:29.642: INFO: Waiting for pod var-expansion-5c82f6c3-3741-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:02:29.645: INFO: Pod var-expansion-5c82f6c3-3741-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:02:29.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-92wpk" for this suite.
Feb 23 08:02:35.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:02:35.753: INFO: namespace: e2e-tests-var-expansion-92wpk, resource: bindings, ignored listing per whitelist
Feb 23 08:02:35.769: INFO: namespace e2e-tests-var-expansion-92wpk deletion completed in 6.119872596s

• [SLOW TEST:8.232 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:02:35.769: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 23 08:02:35.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-vp4w7'
Feb 23 08:02:35.914: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 08:02:35.914: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Feb 23 08:02:39.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-vp4w7'
Feb 23 08:02:40.004: INFO: stderr: ""
Feb 23 08:02:40.004: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:02:40.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vp4w7" for this suite.
Feb 23 08:03:02.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:03:02.204: INFO: namespace: e2e-tests-kubectl-vp4w7, resource: bindings, ignored listing per whitelist
Feb 23 08:03:02.436: INFO: namespace e2e-tests-kubectl-vp4w7 deletion completed in 22.427381198s

• [SLOW TEST:26.667 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:03:02.436: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 08:03:02.596: INFO: Waiting up to 5m0s for pod "downwardapi-volume-715dc63a-3741-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-8qlw4" to be "success or failure"
Feb 23 08:03:02.603: INFO: Pod "downwardapi-volume-715dc63a-3741-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.029374ms
Feb 23 08:03:04.607: INFO: Pod "downwardapi-volume-715dc63a-3741-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011071267s
STEP: Saw pod success
Feb 23 08:03:04.607: INFO: Pod "downwardapi-volume-715dc63a-3741-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:03:04.610: INFO: Trying to get logs from node kube-node-02 pod downwardapi-volume-715dc63a-3741-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 08:03:04.634: INFO: Waiting for pod downwardapi-volume-715dc63a-3741-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:03:04.637: INFO: Pod downwardapi-volume-715dc63a-3741-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:03:04.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8qlw4" for this suite.
Feb 23 08:03:10.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:03:10.703: INFO: namespace: e2e-tests-downward-api-8qlw4, resource: bindings, ignored listing per whitelist
Feb 23 08:03:10.805: INFO: namespace e2e-tests-downward-api-8qlw4 deletion completed in 6.160328282s

• [SLOW TEST:8.369 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:03:10.805: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 23 08:03:10.878: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-klc9n" to be "success or failure"
Feb 23 08:03:10.884: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.850927ms
Feb 23 08:03:12.888: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009792769s
STEP: Saw pod success
Feb 23 08:03:12.888: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 23 08:03:12.891: INFO: Trying to get logs from node kube-node-01 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 23 08:03:12.916: INFO: Waiting for pod pod-host-path-test to disappear
Feb 23 08:03:12.919: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:03:12.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-klc9n" for this suite.
Feb 23 08:03:18.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:03:19.013: INFO: namespace: e2e-tests-hostpath-klc9n, resource: bindings, ignored listing per whitelist
Feb 23 08:03:19.111: INFO: namespace e2e-tests-hostpath-klc9n deletion completed in 6.18736654s

• [SLOW TEST:8.306 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:03:19.111: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb 23 08:03:59.233: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:03:59.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-5lq6x" for this suite.
Feb 23 08:04:05.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:04:05.365: INFO: namespace: e2e-tests-gc-5lq6x, resource: bindings, ignored listing per whitelist
Feb 23 08:04:05.428: INFO: namespace e2e-tests-gc-5lq6x deletion completed in 6.188825432s

• [SLOW TEST:46.317 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:04:05.428: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 23 08:04:05.514: INFO: Waiting up to 5m0s for pod "client-containers-96de531a-3741-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-containers-wd97r" to be "success or failure"
Feb 23 08:04:05.523: INFO: Pod "client-containers-96de531a-3741-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.879432ms
Feb 23 08:04:07.528: INFO: Pod "client-containers-96de531a-3741-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014499882s
Feb 23 08:04:09.539: INFO: Pod "client-containers-96de531a-3741-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025494204s
STEP: Saw pod success
Feb 23 08:04:09.539: INFO: Pod "client-containers-96de531a-3741-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:04:09.542: INFO: Trying to get logs from node kube-node-02 pod client-containers-96de531a-3741-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 08:04:09.564: INFO: Waiting for pod client-containers-96de531a-3741-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:04:09.567: INFO: Pod client-containers-96de531a-3741-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:04:09.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-wd97r" for this suite.
Feb 23 08:04:15.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:04:15.856: INFO: namespace: e2e-tests-containers-wd97r, resource: bindings, ignored listing per whitelist
Feb 23 08:04:15.957: INFO: namespace e2e-tests-containers-wd97r deletion completed in 6.382612467s

• [SLOW TEST:10.529 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:04:15.957: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-vfdzk
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 08:04:16.150: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 08:04:34.237: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.42.0.6 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-vfdzk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 08:04:34.237: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 08:04:35.306: INFO: Found all expected endpoints: [netserver-0]
Feb 23 08:04:35.310: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.40.0.6 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-vfdzk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 08:04:35.310: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 08:04:36.372: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:04:36.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-vfdzk" for this suite.
Feb 23 08:04:58.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:04:58.470: INFO: namespace: e2e-tests-pod-network-test-vfdzk, resource: bindings, ignored listing per whitelist
Feb 23 08:04:58.526: INFO: namespace e2e-tests-pod-network-test-vfdzk deletion completed in 22.149086419s

• [SLOW TEST:42.569 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:04:58.526: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 08:04:58.609: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6841dd7-3741-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-x86fp" to be "success or failure"
Feb 23 08:04:58.613: INFO: Pod "downwardapi-volume-b6841dd7-3741-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.730767ms
Feb 23 08:05:00.617: INFO: Pod "downwardapi-volume-b6841dd7-3741-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007836728s
STEP: Saw pod success
Feb 23 08:05:00.617: INFO: Pod "downwardapi-volume-b6841dd7-3741-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:05:00.620: INFO: Trying to get logs from node kube-node-01 pod downwardapi-volume-b6841dd7-3741-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 08:05:00.643: INFO: Waiting for pod downwardapi-volume-b6841dd7-3741-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:05:00.646: INFO: Pod downwardapi-volume-b6841dd7-3741-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:05:00.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x86fp" for this suite.
Feb 23 08:05:06.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:05:06.671: INFO: namespace: e2e-tests-projected-x86fp, resource: bindings, ignored listing per whitelist
Feb 23 08:05:06.817: INFO: namespace e2e-tests-projected-x86fp deletion completed in 6.166718395s

• [SLOW TEST:8.290 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:05:06.817: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 23 08:05:16.967: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:05:16.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-d45lj" for this suite.
Feb 23 08:05:22.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:05:23.042: INFO: namespace: e2e-tests-gc-d45lj, resource: bindings, ignored listing per whitelist
Feb 23 08:05:23.148: INFO: namespace e2e-tests-gc-d45lj deletion completed in 6.177111507s

• [SLOW TEST:16.332 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:05:23.148: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 23 08:05:23.245: INFO: Waiting up to 5m0s for pod "downward-api-c5323ff6-3741-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-mx57v" to be "success or failure"
Feb 23 08:05:23.253: INFO: Pod "downward-api-c5323ff6-3741-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.457409ms
Feb 23 08:05:25.256: INFO: Pod "downward-api-c5323ff6-3741-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011011816s
STEP: Saw pod success
Feb 23 08:05:25.256: INFO: Pod "downward-api-c5323ff6-3741-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:05:25.260: INFO: Trying to get logs from node kube-node-02 pod downward-api-c5323ff6-3741-11e9-a63e-52687f37ce9e container dapi-container: <nil>
STEP: delete the pod
Feb 23 08:05:25.283: INFO: Waiting for pod downward-api-c5323ff6-3741-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:05:25.286: INFO: Pod downward-api-c5323ff6-3741-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:05:25.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mx57v" for this suite.
Feb 23 08:05:31.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:05:31.318: INFO: namespace: e2e-tests-downward-api-mx57v, resource: bindings, ignored listing per whitelist
Feb 23 08:05:31.444: INFO: namespace e2e-tests-downward-api-mx57v deletion completed in 6.154050769s

• [SLOW TEST:8.296 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:05:31.444: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 23 08:05:31.509: INFO: Waiting up to 5m0s for pod "var-expansion-ca201dd2-3741-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-var-expansion-mwmtz" to be "success or failure"
Feb 23 08:05:31.513: INFO: Pod "var-expansion-ca201dd2-3741-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097144ms
Feb 23 08:05:33.518: INFO: Pod "var-expansion-ca201dd2-3741-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009040546s
STEP: Saw pod success
Feb 23 08:05:33.518: INFO: Pod "var-expansion-ca201dd2-3741-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:05:33.521: INFO: Trying to get logs from node kube-node-01 pod var-expansion-ca201dd2-3741-11e9-a63e-52687f37ce9e container dapi-container: <nil>
STEP: delete the pod
Feb 23 08:05:33.541: INFO: Waiting for pod var-expansion-ca201dd2-3741-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:05:33.545: INFO: Pod var-expansion-ca201dd2-3741-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:05:33.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-mwmtz" for this suite.
Feb 23 08:05:39.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:05:39.670: INFO: namespace: e2e-tests-var-expansion-mwmtz, resource: bindings, ignored listing per whitelist
Feb 23 08:05:39.708: INFO: namespace e2e-tests-var-expansion-mwmtz deletion completed in 6.159287094s

• [SLOW TEST:8.264 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:05:39.708: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 08:05:39.793: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 23 08:05:39.805: INFO: Number of nodes with available pods: 0
Feb 23 08:05:39.805: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 23 08:05:39.828: INFO: Number of nodes with available pods: 0
Feb 23 08:05:39.828: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:40.831: INFO: Number of nodes with available pods: 0
Feb 23 08:05:40.831: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:41.831: INFO: Number of nodes with available pods: 1
Feb 23 08:05:41.831: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 23 08:05:41.852: INFO: Number of nodes with available pods: 1
Feb 23 08:05:41.852: INFO: Number of running nodes: 0, number of available pods: 1
Feb 23 08:05:42.856: INFO: Number of nodes with available pods: 0
Feb 23 08:05:42.856: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 23 08:05:42.865: INFO: Number of nodes with available pods: 0
Feb 23 08:05:42.865: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:43.869: INFO: Number of nodes with available pods: 0
Feb 23 08:05:43.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:44.869: INFO: Number of nodes with available pods: 0
Feb 23 08:05:44.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:45.870: INFO: Number of nodes with available pods: 0
Feb 23 08:05:45.870: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:46.869: INFO: Number of nodes with available pods: 0
Feb 23 08:05:46.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:47.869: INFO: Number of nodes with available pods: 0
Feb 23 08:05:47.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:48.869: INFO: Number of nodes with available pods: 0
Feb 23 08:05:48.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:49.869: INFO: Number of nodes with available pods: 0
Feb 23 08:05:49.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:50.869: INFO: Number of nodes with available pods: 0
Feb 23 08:05:50.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:51.869: INFO: Number of nodes with available pods: 0
Feb 23 08:05:51.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:52.870: INFO: Number of nodes with available pods: 0
Feb 23 08:05:52.870: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:53.869: INFO: Number of nodes with available pods: 0
Feb 23 08:05:53.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:54.869: INFO: Number of nodes with available pods: 0
Feb 23 08:05:54.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:55.873: INFO: Number of nodes with available pods: 0
Feb 23 08:05:55.873: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:56.870: INFO: Number of nodes with available pods: 0
Feb 23 08:05:56.870: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:57.869: INFO: Number of nodes with available pods: 0
Feb 23 08:05:57.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:58.870: INFO: Number of nodes with available pods: 0
Feb 23 08:05:58.870: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:05:59.871: INFO: Number of nodes with available pods: 0
Feb 23 08:05:59.871: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:00.869: INFO: Number of nodes with available pods: 0
Feb 23 08:06:00.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:01.887: INFO: Number of nodes with available pods: 0
Feb 23 08:06:01.887: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:02.870: INFO: Number of nodes with available pods: 0
Feb 23 08:06:02.870: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:03.869: INFO: Number of nodes with available pods: 0
Feb 23 08:06:03.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:04.870: INFO: Number of nodes with available pods: 0
Feb 23 08:06:04.870: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:05.869: INFO: Number of nodes with available pods: 0
Feb 23 08:06:05.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:06.869: INFO: Number of nodes with available pods: 0
Feb 23 08:06:06.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:07.870: INFO: Number of nodes with available pods: 0
Feb 23 08:06:07.870: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:08.869: INFO: Number of nodes with available pods: 0
Feb 23 08:06:08.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:09.869: INFO: Number of nodes with available pods: 0
Feb 23 08:06:09.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:10.869: INFO: Number of nodes with available pods: 0
Feb 23 08:06:10.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:11.869: INFO: Number of nodes with available pods: 0
Feb 23 08:06:11.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:12.869: INFO: Number of nodes with available pods: 0
Feb 23 08:06:12.869: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:13.870: INFO: Number of nodes with available pods: 0
Feb 23 08:06:13.870: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:14.869: INFO: Number of nodes with available pods: 0
Feb 23 08:06:14.870: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:06:15.869: INFO: Number of nodes with available pods: 1
Feb 23 08:06:15.869: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-dhf4l, will wait for the garbage collector to delete the pods
Feb 23 08:06:15.936: INFO: Deleting DaemonSet.extensions daemon-set took: 8.013295ms
Feb 23 08:06:16.036: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.243246ms
Feb 23 08:06:49.940: INFO: Number of nodes with available pods: 0
Feb 23 08:06:49.940: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 08:06:49.943: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-dhf4l/daemonsets","resourceVersion":"16666"},"items":null}

Feb 23 08:06:49.946: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-dhf4l/pods","resourceVersion":"16666"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:06:49.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-dhf4l" for this suite.
Feb 23 08:06:55.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:06:56.058: INFO: namespace: e2e-tests-daemonsets-dhf4l, resource: bindings, ignored listing per whitelist
Feb 23 08:06:56.125: INFO: namespace e2e-tests-daemonsets-dhf4l deletion completed in 6.154886351s

• [SLOW TEST:76.417 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:06:56.125: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-fc9b900a-3741-11e9-a63e-52687f37ce9e
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-fc9b900a-3741-11e9-a63e-52687f37ce9e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:08:04.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bftjk" for this suite.
Feb 23 08:08:26.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:08:26.721: INFO: namespace: e2e-tests-configmap-bftjk, resource: bindings, ignored listing per whitelist
Feb 23 08:08:26.810: INFO: namespace e2e-tests-configmap-bftjk deletion completed in 22.143558045s

• [SLOW TEST:90.685 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:08:26.810: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 23 08:08:30.923: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:30.927: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:32.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:32.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:34.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:34.938: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:36.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:36.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:38.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:38.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:40.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:40.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:42.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:42.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:44.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:44.932: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:46.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:46.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:48.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:48.930: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:50.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:50.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:52.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:52.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:54.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:54.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:56.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:56.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:08:58.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:08:58.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 08:09:00.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 08:09:00.935: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:09:00.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-gpgmx" for this suite.
Feb 23 08:09:22.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:09:23.069: INFO: namespace: e2e-tests-container-lifecycle-hook-gpgmx, resource: bindings, ignored listing per whitelist
Feb 23 08:09:23.097: INFO: namespace e2e-tests-container-lifecycle-hook-gpgmx deletion completed in 22.148411644s

• [SLOW TEST:56.287 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:09:23.098: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 08:09:25.221: INFO: Waiting up to 5m0s for pod "client-envvars-556df624-3742-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-pods-zcdn2" to be "success or failure"
Feb 23 08:09:25.227: INFO: Pod "client-envvars-556df624-3742-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096241ms
Feb 23 08:09:27.231: INFO: Pod "client-envvars-556df624-3742-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009943867s
STEP: Saw pod success
Feb 23 08:09:27.231: INFO: Pod "client-envvars-556df624-3742-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:09:27.234: INFO: Trying to get logs from node kube-node-02 pod client-envvars-556df624-3742-11e9-a63e-52687f37ce9e container env3cont: <nil>
STEP: delete the pod
Feb 23 08:09:27.254: INFO: Waiting for pod client-envvars-556df624-3742-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:09:27.258: INFO: Pod client-envvars-556df624-3742-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:09:27.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zcdn2" for this suite.
Feb 23 08:10:13.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:10:13.306: INFO: namespace: e2e-tests-pods-zcdn2, resource: bindings, ignored listing per whitelist
Feb 23 08:10:13.417: INFO: namespace e2e-tests-pods-zcdn2 deletion completed in 46.155294581s

• [SLOW TEST:50.320 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:10:13.417: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-72338f9c-3742-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 08:10:13.497: INFO: Waiting up to 5m0s for pod "pod-secrets-723432e5-3742-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-secrets-8wjdw" to be "success or failure"
Feb 23 08:10:13.502: INFO: Pod "pod-secrets-723432e5-3742-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.308699ms
Feb 23 08:10:15.506: INFO: Pod "pod-secrets-723432e5-3742-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009356464s
STEP: Saw pod success
Feb 23 08:10:15.506: INFO: Pod "pod-secrets-723432e5-3742-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:10:15.509: INFO: Trying to get logs from node kube-node-01 pod pod-secrets-723432e5-3742-11e9-a63e-52687f37ce9e container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 08:10:15.529: INFO: Waiting for pod pod-secrets-723432e5-3742-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:10:15.532: INFO: Pod pod-secrets-723432e5-3742-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:10:15.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8wjdw" for this suite.
Feb 23 08:10:21.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:10:21.578: INFO: namespace: e2e-tests-secrets-8wjdw, resource: bindings, ignored listing per whitelist
Feb 23 08:10:21.684: INFO: namespace e2e-tests-secrets-8wjdw deletion completed in 6.148119109s

• [SLOW TEST:8.267 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:10:21.685: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-771fd40c-3742-11e9-a63e-52687f37ce9e
Feb 23 08:10:21.756: INFO: Pod name my-hostname-basic-771fd40c-3742-11e9-a63e-52687f37ce9e: Found 0 pods out of 1
Feb 23 08:10:26.760: INFO: Pod name my-hostname-basic-771fd40c-3742-11e9-a63e-52687f37ce9e: Found 1 pods out of 1
Feb 23 08:10:26.760: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-771fd40c-3742-11e9-a63e-52687f37ce9e" are running
Feb 23 08:10:26.763: INFO: Pod "my-hostname-basic-771fd40c-3742-11e9-a63e-52687f37ce9e-qqr64" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-23 08:10:21 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-23 08:10:23 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-23 08:10:23 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-23 08:10:21 +0000 UTC Reason: Message:}])
Feb 23 08:10:26.763: INFO: Trying to dial the pod
Feb 23 08:10:31.777: INFO: Controller my-hostname-basic-771fd40c-3742-11e9-a63e-52687f37ce9e: Got expected result from replica 1 [my-hostname-basic-771fd40c-3742-11e9-a63e-52687f37ce9e-qqr64]: "my-hostname-basic-771fd40c-3742-11e9-a63e-52687f37ce9e-qqr64", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:10:31.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-hxmbs" for this suite.
Feb 23 08:10:37.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:10:37.883: INFO: namespace: e2e-tests-replication-controller-hxmbs, resource: bindings, ignored listing per whitelist
Feb 23 08:10:37.930: INFO: namespace e2e-tests-replication-controller-hxmbs deletion completed in 6.148419493s

• [SLOW TEST:16.246 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:10:37.931: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 23 08:10:38.000: INFO: namespace e2e-tests-kubectl-6vdjt
Feb 23 08:10:38.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-6vdjt'
Feb 23 08:10:38.348: INFO: stderr: ""
Feb 23 08:10:38.348: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 23 08:10:39.352: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 08:10:39.352: INFO: Found 0 / 1
Feb 23 08:10:40.352: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 08:10:40.352: INFO: Found 1 / 1
Feb 23 08:10:40.352: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 23 08:10:40.355: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 08:10:40.355: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 23 08:10:40.355: INFO: wait on redis-master startup in e2e-tests-kubectl-6vdjt 
Feb 23 08:10:40.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 logs redis-master-f2k9p redis-master --namespace=e2e-tests-kubectl-6vdjt'
Feb 23 08:10:40.439: INFO: stderr: ""
Feb 23 08:10:40.439: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Feb 08:10:39.103 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Feb 08:10:39.103 # Server started, Redis version 3.2.12\n1:M 23 Feb 08:10:39.103 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Feb 08:10:39.103 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 23 08:10:40.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-6vdjt'
Feb 23 08:10:40.529: INFO: stderr: ""
Feb 23 08:10:40.529: INFO: stdout: "service/rm2 exposed\n"
Feb 23 08:10:40.537: INFO: Service rm2 in namespace e2e-tests-kubectl-6vdjt found.
STEP: exposing service
Feb 23 08:10:42.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-6vdjt'
Feb 23 08:10:42.635: INFO: stderr: ""
Feb 23 08:10:42.635: INFO: stdout: "service/rm3 exposed\n"
Feb 23 08:10:42.638: INFO: Service rm3 in namespace e2e-tests-kubectl-6vdjt found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:10:44.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6vdjt" for this suite.
Feb 23 08:11:06.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:11:06.676: INFO: namespace: e2e-tests-kubectl-6vdjt, resource: bindings, ignored listing per whitelist
Feb 23 08:11:06.794: INFO: namespace e2e-tests-kubectl-6vdjt deletion completed in 22.14548583s

• [SLOW TEST:28.864 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:11:06.794: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-9gm8w
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-9gm8w
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-9gm8w
Feb 23 08:11:06.880: INFO: Found 0 stateful pods, waiting for 1
Feb 23 08:11:16.884: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 23 08:11:16.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 23 08:11:17.025: INFO: stderr: ""
Feb 23 08:11:17.025: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 23 08:11:17.025: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 23 08:11:17.029: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 23 08:11:27.033: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 08:11:27.033: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 08:11:27.052: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 23 08:11:27.052: INFO: ss-0  kube-node-02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  }]
Feb 23 08:11:27.052: INFO: 
Feb 23 08:11:27.052: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 23 08:11:28.057: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995307649s
Feb 23 08:11:29.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990180268s
Feb 23 08:11:30.066: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985599479s
Feb 23 08:11:31.076: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980870179s
Feb 23 08:11:32.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970878543s
Feb 23 08:11:33.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96631446s
Feb 23 08:11:34.092: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959050019s
Feb 23 08:11:35.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954580518s
Feb 23 08:11:36.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.170703ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-9gm8w
Feb 23 08:11:37.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:11:37.238: INFO: stderr: ""
Feb 23 08:11:37.238: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 23 08:11:37.238: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 23 08:11:37.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:11:37.393: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 23 08:11:37.394: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 23 08:11:37.394: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 23 08:11:37.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:11:37.542: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 23 08:11:37.542: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 23 08:11:37.542: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 23 08:11:37.546: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 23 08:11:47.551: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 08:11:47.551: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 08:11:47.551: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 23 08:11:47.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 23 08:11:47.690: INFO: stderr: ""
Feb 23 08:11:47.690: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 23 08:11:47.691: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 23 08:11:47.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 23 08:11:47.835: INFO: stderr: ""
Feb 23 08:11:47.835: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 23 08:11:47.835: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 23 08:11:47.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 23 08:11:47.994: INFO: stderr: ""
Feb 23 08:11:47.994: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 23 08:11:47.994: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 23 08:11:47.994: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 08:11:47.999: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 23 08:11:58.007: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 08:11:58.007: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 08:11:58.007: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 08:11:58.020: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 23 08:11:58.020: INFO: ss-0  kube-node-02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  }]
Feb 23 08:11:58.020: INFO: ss-1  kube-node-01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:11:58.020: INFO: ss-2  kube-node-02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:11:58.020: INFO: 
Feb 23 08:11:58.020: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 23 08:11:59.024: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 23 08:11:59.024: INFO: ss-0  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  }]
Feb 23 08:11:59.024: INFO: ss-1  kube-node-01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:11:59.024: INFO: ss-2  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:11:59.024: INFO: 
Feb 23 08:11:59.024: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 23 08:12:00.028: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 23 08:12:00.028: INFO: ss-0  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  }]
Feb 23 08:12:00.028: INFO: ss-1  kube-node-01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:12:00.028: INFO: ss-2  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:12:00.028: INFO: 
Feb 23 08:12:00.028: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 23 08:12:01.033: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 23 08:12:01.033: INFO: ss-0  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  }]
Feb 23 08:12:01.033: INFO: ss-1  kube-node-01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:12:01.033: INFO: ss-2  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:12:01.033: INFO: 
Feb 23 08:12:01.033: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 23 08:12:02.040: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 23 08:12:02.040: INFO: ss-0  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  }]
Feb 23 08:12:02.040: INFO: ss-2  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:12:02.040: INFO: 
Feb 23 08:12:02.040: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 23 08:12:03.045: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 23 08:12:03.045: INFO: ss-0  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  }]
Feb 23 08:12:03.045: INFO: ss-2  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:12:03.045: INFO: 
Feb 23 08:12:03.045: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 23 08:12:04.049: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 23 08:12:04.049: INFO: ss-0  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  }]
Feb 23 08:12:04.049: INFO: ss-2  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:12:04.049: INFO: 
Feb 23 08:12:04.049: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 23 08:12:05.053: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 23 08:12:05.053: INFO: ss-0  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  }]
Feb 23 08:12:05.053: INFO: ss-2  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:12:05.053: INFO: 
Feb 23 08:12:05.053: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 23 08:12:06.057: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 23 08:12:06.057: INFO: ss-0  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  }]
Feb 23 08:12:06.057: INFO: ss-2  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:12:06.057: INFO: 
Feb 23 08:12:06.057: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 23 08:12:07.062: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 23 08:12:07.062: INFO: ss-0  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:06 +0000 UTC  }]
Feb 23 08:12:07.062: INFO: ss-2  kube-node-02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-23 08:11:27 +0000 UTC  }]
Feb 23 08:12:07.062: INFO: 
Feb 23 08:12:07.062: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-9gm8w
Feb 23 08:12:08.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:12:08.156: INFO: rc: 1
Feb 23 08:12:08.156: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001bda9c0 exit status 1 <nil> <nil> true [0xc001c132e0 0xc001c132f8 0xc001c13310] [0xc001c132e0 0xc001c132f8 0xc001c13310] [0xc001c132f0 0xc001c13308] [0x92f8e0 0x92f8e0] 0xc0016f8c00 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 23 08:12:18.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:12:18.219: INFO: rc: 1
Feb 23 08:12:18.219: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bdaf00 exit status 1 <nil> <nil> true [0xc001c13318 0xc001c13330 0xc001c13348] [0xc001c13318 0xc001c13330 0xc001c13348] [0xc001c13328 0xc001c13340] [0x92f8e0 0x92f8e0] 0xc0016f9e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:12:28.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:12:28.284: INFO: rc: 1
Feb 23 08:12:28.284: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c8a690 exit status 1 <nil> <nil> true [0xc0025a4e08 0xc0025a4e20 0xc0025a4e38] [0xc0025a4e08 0xc0025a4e20 0xc0025a4e38] [0xc0025a4e18 0xc0025a4e30] [0x92f8e0 0x92f8e0] 0xc001368840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:12:38.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:12:38.346: INFO: rc: 1
Feb 23 08:12:38.346: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c8ab70 exit status 1 <nil> <nil> true [0xc0025a4e40 0xc0025a4e58 0xc0025a4e70] [0xc0025a4e40 0xc0025a4e58 0xc0025a4e70] [0xc0025a4e50 0xc0025a4e68] [0x92f8e0 0x92f8e0] 0xc001369b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:12:48.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:12:48.408: INFO: rc: 1
Feb 23 08:12:48.408: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bdb3b0 exit status 1 <nil> <nil> true [0xc001c13350 0xc001c13368 0xc001c13380] [0xc001c13350 0xc001c13368 0xc001c13380] [0xc001c13360 0xc001c13378] [0x92f8e0 0x92f8e0] 0xc001800ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:12:58.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:12:58.486: INFO: rc: 1
Feb 23 08:12:58.486: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bdb830 exit status 1 <nil> <nil> true [0xc001c13388 0xc001c133a0 0xc001c133b8] [0xc001c13388 0xc001c133a0 0xc001c133b8] [0xc001c13398 0xc001c133b0] [0x92f8e0 0x92f8e0] 0xc0018016e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:13:08.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:13:08.552: INFO: rc: 1
Feb 23 08:13:08.552: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011ae3c0 exit status 1 <nil> <nil> true [0xc0025a4008 0xc0025a4020 0xc0025a4038] [0xc0025a4008 0xc0025a4020 0xc0025a4038] [0xc0025a4018 0xc0025a4030] [0x92f8e0 0x92f8e0] 0xc001368a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:13:18.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:13:18.617: INFO: rc: 1
Feb 23 08:13:18.617: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011ae8d0 exit status 1 <nil> <nil> true [0xc0025a4040 0xc0025a4058 0xc0025a4070] [0xc0025a4040 0xc0025a4058 0xc0025a4070] [0xc0025a4050 0xc0025a4068] [0x92f8e0 0x92f8e0] 0xc001369f80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:13:28.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:13:28.682: INFO: rc: 1
Feb 23 08:13:28.682: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011aef30 exit status 1 <nil> <nil> true [0xc0025a4078 0xc0025a4090 0xc0025a40a8] [0xc0025a4078 0xc0025a4090 0xc0025a40a8] [0xc0025a4088 0xc0025a40a0] [0x92f8e0 0x92f8e0] 0xc0016f95c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:13:38.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:13:38.758: INFO: rc: 1
Feb 23 08:13:38.758: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011af350 exit status 1 <nil> <nil> true [0xc0025a40b0 0xc0025a40c8 0xc0025a40e0] [0xc0025a40b0 0xc0025a40c8 0xc0025a40e0] [0xc0025a40c0 0xc0025a40d8] [0x92f8e0 0x92f8e0] 0xc00144aa80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:13:48.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:13:48.843: INFO: rc: 1
Feb 23 08:13:48.843: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011af710 exit status 1 <nil> <nil> true [0xc0025a40e8 0xc0025a4100 0xc0025a4118] [0xc0025a40e8 0xc0025a4100 0xc0025a4118] [0xc0025a40f8 0xc0025a4110] [0x92f8e0 0x92f8e0] 0xc001745380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:13:58.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:13:58.908: INFO: rc: 1
Feb 23 08:13:58.908: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001da8450 exit status 1 <nil> <nil> true [0xc001c12010 0xc001c12188 0xc001c121a0] [0xc001c12010 0xc001c12188 0xc001c121a0] [0xc001c120b8 0xc001c12198] [0x92f8e0 0x92f8e0] 0xc000a5cf00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:14:08.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:14:08.971: INFO: rc: 1
Feb 23 08:14:08.971: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001da8840 exit status 1 <nil> <nil> true [0xc001c121b0 0xc001c121f8 0xc001c12238] [0xc001c121b0 0xc001c121f8 0xc001c12238] [0xc001c121e8 0xc001c12218] [0x92f8e0 0x92f8e0] 0xc0017a67e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:14:18.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:14:19.033: INFO: rc: 1
Feb 23 08:14:19.033: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001da8c90 exit status 1 <nil> <nil> true [0xc001c12258 0xc001c122a8 0xc001c122d8] [0xc001c12258 0xc001c122a8 0xc001c122d8] [0xc001c12290 0xc001c122c8] [0x92f8e0 0x92f8e0] 0xc001b83740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:14:29.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:14:29.097: INFO: rc: 1
Feb 23 08:14:29.097: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001da9170 exit status 1 <nil> <nil> true [0xc001c122e0 0xc001c12318 0xc001c12368] [0xc001c122e0 0xc001c12318 0xc001c12368] [0xc001c122f8 0xc001c12348] [0x92f8e0 0x92f8e0] 0xc001c1bec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:14:39.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:14:39.167: INFO: rc: 1
Feb 23 08:14:39.167: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001da96b0 exit status 1 <nil> <nil> true [0xc001c12380 0xc001c12398 0xc001c123d0] [0xc001c12380 0xc001c12398 0xc001c123d0] [0xc001c12390 0xc001c123a8] [0x92f8e0 0x92f8e0] 0xc001bc9c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:14:49.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:14:49.226: INFO: rc: 1
Feb 23 08:14:49.226: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011afbc0 exit status 1 <nil> <nil> true [0xc0025a4120 0xc0025a4138 0xc0025a4150] [0xc0025a4120 0xc0025a4138 0xc0025a4150] [0xc0025a4130 0xc0025a4148] [0x92f8e0 0x92f8e0] 0xc000aa3d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:14:59.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:14:59.289: INFO: rc: 1
Feb 23 08:14:59.289: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000b960f0 exit status 1 <nil> <nil> true [0xc0025a4158 0xc0025a4170 0xc0025a4188] [0xc0025a4158 0xc0025a4170 0xc0025a4188] [0xc0025a4168 0xc0025a4180] [0x92f8e0 0x92f8e0] 0xc000b9ab40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:15:09.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:15:09.392: INFO: rc: 1
Feb 23 08:15:09.393: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011ae420 exit status 1 <nil> <nil> true [0xc001c12058 0xc001c12190 0xc001c121b0] [0xc001c12058 0xc001c12190 0xc001c121b0] [0xc001c12188 0xc001c121a0] [0x92f8e0 0x92f8e0] 0xc00169f0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:15:19.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:15:19.458: INFO: rc: 1
Feb 23 08:15:19.458: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001da83c0 exit status 1 <nil> <nil> true [0xc0025a4000 0xc0025a4018 0xc0025a4030] [0xc0025a4000 0xc0025a4018 0xc0025a4030] [0xc0025a4010 0xc0025a4028] [0x92f8e0 0x92f8e0] 0xc001875860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:15:29.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:15:29.535: INFO: rc: 1
Feb 23 08:15:29.535: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011ae930 exit status 1 <nil> <nil> true [0xc001c121d0 0xc001c12210 0xc001c12258] [0xc001c121d0 0xc001c12210 0xc001c12258] [0xc001c121f8 0xc001c12238] [0x92f8e0 0x92f8e0] 0xc001c1bec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:15:39.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:15:39.600: INFO: rc: 1
Feb 23 08:15:39.600: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011aefc0 exit status 1 <nil> <nil> true [0xc001c12278 0xc001c122c0 0xc001c122e0] [0xc001c12278 0xc001c122c0 0xc001c122e0] [0xc001c122a8 0xc001c122d8] [0x92f8e0 0x92f8e0] 0xc0017a63c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:15:49.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:15:49.667: INFO: rc: 1
Feb 23 08:15:49.667: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011af410 exit status 1 <nil> <nil> true [0xc001c122e8 0xc001c12330 0xc001c12380] [0xc001c122e8 0xc001c12330 0xc001c12380] [0xc001c12318 0xc001c12368] [0x92f8e0 0x92f8e0] 0xc000af8c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:15:59.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:15:59.733: INFO: rc: 1
Feb 23 08:15:59.733: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011af830 exit status 1 <nil> <nil> true [0xc001c12388 0xc001c123a0 0xc001c123f0] [0xc001c12388 0xc001c123a0 0xc001c123f0] [0xc001c12398 0xc001c123d0] [0x92f8e0 0x92f8e0] 0xc0014b5da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:16:09.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:16:09.813: INFO: rc: 1
Feb 23 08:16:09.813: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011afce0 exit status 1 <nil> <nil> true [0xc001c12438 0xc001c12480 0xc001c124e8] [0xc001c12438 0xc001c12480 0xc001c124e8] [0xc001c12468 0xc001c124d0] [0x92f8e0 0x92f8e0] 0xc00144a6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:16:19.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:16:19.877: INFO: rc: 1
Feb 23 08:16:19.877: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001da87e0 exit status 1 <nil> <nil> true [0xc0025a4038 0xc0025a4050 0xc0025a4068] [0xc0025a4038 0xc0025a4050 0xc0025a4068] [0xc0025a4048 0xc0025a4060] [0x92f8e0 0x92f8e0] 0xc0016f9080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:16:29.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:16:29.939: INFO: rc: 1
Feb 23 08:16:29.939: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000b962a0 exit status 1 <nil> <nil> true [0xc001c124f0 0xc001c12508 0xc001c12520] [0xc001c124f0 0xc001c12508 0xc001c12520] [0xc001c12500 0xc001c12518] [0x92f8e0 0x92f8e0] 0xc001368180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:16:39.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:16:40.008: INFO: rc: 1
Feb 23 08:16:40.008: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000b96690 exit status 1 <nil> <nil> true [0xc001c12528 0xc001c12578 0xc001c125d8] [0xc001c12528 0xc001c12578 0xc001c125d8] [0xc001c12558 0xc001c125b8] [0x92f8e0 0x92f8e0] 0xc0013691a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:16:50.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:16:50.070: INFO: rc: 1
Feb 23 08:16:50.070: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000b96a50 exit status 1 <nil> <nil> true [0xc001c125f0 0xc001c12608 0xc001c12628] [0xc001c125f0 0xc001c12608 0xc001c12628] [0xc001c12600 0xc001c12620] [0x92f8e0 0x92f8e0] 0xc000b9a720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:17:00.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:17:00.131: INFO: rc: 1
Feb 23 08:17:00.131: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001da8c60 exit status 1 <nil> <nil> true [0xc0025a4070 0xc0025a4088 0xc0025a40a0] [0xc0025a4070 0xc0025a4088 0xc0025a40a0] [0xc0025a4080 0xc0025a4098] [0x92f8e0 0x92f8e0] 0xc0019dc0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 23 08:17:10.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 exec --namespace=e2e-tests-statefulset-9gm8w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 23 08:17:10.198: INFO: rc: 1
Feb 23 08:17:10.198: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Feb 23 08:17:10.198: INFO: Scaling statefulset ss to 0
Feb 23 08:17:10.207: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 23 08:17:10.210: INFO: Deleting all statefulset in ns e2e-tests-statefulset-9gm8w
Feb 23 08:17:10.212: INFO: Scaling statefulset ss to 0
Feb 23 08:17:10.221: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 08:17:10.223: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:17:10.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-9gm8w" for this suite.
Feb 23 08:17:16.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:17:16.384: INFO: namespace: e2e-tests-statefulset-9gm8w, resource: bindings, ignored listing per whitelist
Feb 23 08:17:16.395: INFO: namespace e2e-tests-statefulset-9gm8w deletion completed in 6.15319557s

• [SLOW TEST:369.601 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:17:16.395: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-6e52ac26-3743-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 08:17:16.487: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6e534fbb-3743-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-ndqqf" to be "success or failure"
Feb 23 08:17:16.492: INFO: Pod "pod-projected-secrets-6e534fbb-3743-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.774473ms
Feb 23 08:17:18.496: INFO: Pod "pod-projected-secrets-6e534fbb-3743-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008438998s
STEP: Saw pod success
Feb 23 08:17:18.496: INFO: Pod "pod-projected-secrets-6e534fbb-3743-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:17:18.499: INFO: Trying to get logs from node kube-node-01 pod pod-projected-secrets-6e534fbb-3743-11e9-a63e-52687f37ce9e container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 08:17:18.522: INFO: Waiting for pod pod-projected-secrets-6e534fbb-3743-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:17:18.525: INFO: Pod pod-projected-secrets-6e534fbb-3743-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:17:18.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ndqqf" for this suite.
Feb 23 08:17:24.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:17:24.560: INFO: namespace: e2e-tests-projected-ndqqf, resource: bindings, ignored listing per whitelist
Feb 23 08:17:24.671: INFO: namespace e2e-tests-projected-ndqqf deletion completed in 6.14159363s

• [SLOW TEST:8.276 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:17:24.671: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 08:17:24.761: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"73456ce4-3743-11e9-8d12-02951b420040", Controller:(*bool)(0xc00092b416), BlockOwnerDeletion:(*bool)(0xc00092b417)}}
Feb 23 08:17:24.773: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"73434bda-3743-11e9-8d12-02951b420040", Controller:(*bool)(0xc0017ad3de), BlockOwnerDeletion:(*bool)(0xc0017ad3df)}}
Feb 23 08:17:24.779: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"734459d8-3743-11e9-8d12-02951b420040", Controller:(*bool)(0xc00092b616), BlockOwnerDeletion:(*bool)(0xc00092b617)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:17:29.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-vh7lk" for this suite.
Feb 23 08:17:35.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:17:35.873: INFO: namespace: e2e-tests-gc-vh7lk, resource: bindings, ignored listing per whitelist
Feb 23 08:17:35.949: INFO: namespace e2e-tests-gc-vh7lk deletion completed in 6.152188461s

• [SLOW TEST:11.278 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:17:35.949: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-brsfp.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-brsfp.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-brsfp.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-brsfp.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-brsfp.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-brsfp.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 08:17:46.044: INFO: Unable to read wheezy_udp@kubernetes.default from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.048: INFO: Unable to read wheezy_tcp@kubernetes.default from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.052: INFO: Unable to read wheezy_udp@kubernetes.default.svc from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.055: INFO: Unable to read wheezy_tcp@kubernetes.default.svc from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.060: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.064: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.067: INFO: Unable to read wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-brsfp.svc.cluster.local from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.071: INFO: Unable to read wheezy_hosts@dns-querier-1 from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.075: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.078: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.082: INFO: Unable to read jessie_udp@kubernetes.default from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.086: INFO: Unable to read jessie_tcp@kubernetes.default from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.089: INFO: Unable to read jessie_udp@kubernetes.default.svc from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.093: INFO: Unable to read jessie_tcp@kubernetes.default.svc from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.096: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.100: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.104: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-brsfp.svc.cluster.local from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.108: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.114: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.117: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e: the server could not find the requested resource (get pods dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e)
Feb 23 08:17:46.117: INFO: Lookups using e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e failed for: [wheezy_udp@kubernetes.default wheezy_tcp@kubernetes.default wheezy_udp@kubernetes.default.svc wheezy_tcp@kubernetes.default.svc wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-brsfp.svc.cluster.local wheezy_hosts@dns-querier-1 wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@kubernetes.default jessie_tcp@kubernetes.default jessie_udp@kubernetes.default.svc jessie_tcp@kubernetes.default.svc jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-brsfp.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 23 08:17:51.201: INFO: DNS probes using e2e-tests-dns-brsfp/dns-test-79f7db1a-3743-11e9-a63e-52687f37ce9e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:17:51.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-brsfp" for this suite.
Feb 23 08:17:57.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:17:57.286: INFO: namespace: e2e-tests-dns-brsfp, resource: bindings, ignored listing per whitelist
Feb 23 08:17:57.363: INFO: namespace e2e-tests-dns-brsfp deletion completed in 6.13885676s

• [SLOW TEST:21.415 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:17:57.364: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Feb 23 08:17:57.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-4zpld'
Feb 23 08:17:57.864: INFO: stderr: ""
Feb 23 08:17:57.864: INFO: stdout: "pod/pause created\n"
Feb 23 08:17:57.864: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 23 08:17:57.864: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-4zpld" to be "running and ready"
Feb 23 08:17:57.868: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.760657ms
Feb 23 08:17:59.872: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.00772823s
Feb 23 08:17:59.872: INFO: Pod "pause" satisfied condition "running and ready"
Feb 23 08:17:59.872: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 23 08:17:59.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-4zpld'
Feb 23 08:17:59.944: INFO: stderr: ""
Feb 23 08:17:59.944: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 23 08:17:59.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pod pause -L testing-label --namespace=e2e-tests-kubectl-4zpld'
Feb 23 08:18:00.185: INFO: stderr: ""
Feb 23 08:18:00.185: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 23 08:18:00.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 label pods pause testing-label- --namespace=e2e-tests-kubectl-4zpld'
Feb 23 08:18:00.594: INFO: stderr: ""
Feb 23 08:18:00.594: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 23 08:18:00.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pod pause -L testing-label --namespace=e2e-tests-kubectl-4zpld'
Feb 23 08:18:00.664: INFO: stderr: ""
Feb 23 08:18:00.664: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Feb 23 08:18:00.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4zpld'
Feb 23 08:18:00.742: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 08:18:00.742: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 23 08:18:00.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-4zpld'
Feb 23 08:18:00.820: INFO: stderr: "No resources found.\n"
Feb 23 08:18:00.821: INFO: stdout: ""
Feb 23 08:18:00.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -l name=pause --namespace=e2e-tests-kubectl-4zpld -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 08:18:00.886: INFO: stderr: ""
Feb 23 08:18:00.886: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:18:00.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4zpld" for this suite.
Feb 23 08:18:06.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:18:07.483: INFO: namespace: e2e-tests-kubectl-4zpld, resource: bindings, ignored listing per whitelist
Feb 23 08:18:07.497: INFO: namespace e2e-tests-kubectl-4zpld deletion completed in 6.607146304s

• [SLOW TEST:10.133 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:18:07.497: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 23 08:18:07.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:07.708: INFO: stderr: ""
Feb 23 08:18:07.709: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 08:18:07.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:07.796: INFO: stderr: ""
Feb 23 08:18:07.796: INFO: stdout: "update-demo-nautilus-cks98 update-demo-nautilus-h5dwk "
Feb 23 08:18:07.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-cks98 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:07.870: INFO: stderr: ""
Feb 23 08:18:07.871: INFO: stdout: ""
Feb 23 08:18:07.871: INFO: update-demo-nautilus-cks98 is created but not running
Feb 23 08:18:12.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:12.947: INFO: stderr: ""
Feb 23 08:18:12.947: INFO: stdout: "update-demo-nautilus-cks98 update-demo-nautilus-h5dwk "
Feb 23 08:18:12.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-cks98 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:13.017: INFO: stderr: ""
Feb 23 08:18:13.017: INFO: stdout: "true"
Feb 23 08:18:13.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-cks98 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:13.083: INFO: stderr: ""
Feb 23 08:18:13.083: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 08:18:13.083: INFO: validating pod update-demo-nautilus-cks98
Feb 23 08:18:13.090: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 08:18:13.090: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 08:18:13.090: INFO: update-demo-nautilus-cks98 is verified up and running
Feb 23 08:18:13.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-h5dwk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:13.158: INFO: stderr: ""
Feb 23 08:18:13.158: INFO: stdout: "true"
Feb 23 08:18:13.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-nautilus-h5dwk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:13.227: INFO: stderr: ""
Feb 23 08:18:13.227: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 08:18:13.227: INFO: validating pod update-demo-nautilus-h5dwk
Feb 23 08:18:13.234: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 08:18:13.234: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 08:18:13.234: INFO: update-demo-nautilus-h5dwk is verified up and running
STEP: rolling-update to new replication controller
Feb 23 08:18:13.236: INFO: scanned /root for discovery docs: <nil>
Feb 23 08:18:13.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:36.490: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 23 08:18:36.490: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 08:18:36.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:36.568: INFO: stderr: ""
Feb 23 08:18:36.568: INFO: stdout: "update-demo-kitten-6jbbf update-demo-kitten-6zmxk "
Feb 23 08:18:36.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-kitten-6jbbf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:36.644: INFO: stderr: ""
Feb 23 08:18:36.644: INFO: stdout: "true"
Feb 23 08:18:36.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-kitten-6jbbf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:36.708: INFO: stderr: ""
Feb 23 08:18:36.708: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 23 08:18:36.708: INFO: validating pod update-demo-kitten-6jbbf
Feb 23 08:18:36.715: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 23 08:18:36.715: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 23 08:18:36.715: INFO: update-demo-kitten-6jbbf is verified up and running
Feb 23 08:18:36.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-kitten-6zmxk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:36.780: INFO: stderr: ""
Feb 23 08:18:36.780: INFO: stdout: "true"
Feb 23 08:18:36.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods update-demo-kitten-6zmxk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rcfqv'
Feb 23 08:18:36.845: INFO: stderr: ""
Feb 23 08:18:36.845: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 23 08:18:36.845: INFO: validating pod update-demo-kitten-6zmxk
Feb 23 08:18:36.852: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 23 08:18:36.852: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 23 08:18:36.852: INFO: update-demo-kitten-6zmxk is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:18:36.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rcfqv" for this suite.
Feb 23 08:18:58.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:18:58.877: INFO: namespace: e2e-tests-kubectl-rcfqv, resource: bindings, ignored listing per whitelist
Feb 23 08:18:59.032: INFO: namespace e2e-tests-kubectl-rcfqv deletion completed in 22.176090587s

• [SLOW TEST:51.535 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:18:59.032: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-xdvks
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-xdvks
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-xdvks
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-xdvks
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-xdvks
Feb 23 08:19:03.173: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-xdvks, name: ss-0, uid: ad9a2057-3743-11e9-8d12-02951b420040, status phase: Failed. Waiting for statefulset controller to delete.
Feb 23 08:19:03.180: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-xdvks
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-xdvks
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-xdvks and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 23 08:19:07.238: INFO: Deleting all statefulset in ns e2e-tests-statefulset-xdvks
Feb 23 08:19:07.241: INFO: Scaling statefulset ss to 0
Feb 23 08:19:17.265: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 08:19:17.271: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:19:17.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-xdvks" for this suite.
Feb 23 08:19:23.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:19:23.520: INFO: namespace: e2e-tests-statefulset-xdvks, resource: bindings, ignored listing per whitelist
Feb 23 08:19:23.520: INFO: namespace e2e-tests-statefulset-xdvks deletion completed in 6.221306159s

• [SLOW TEST:24.488 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:19:23.521: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-ba1d1d9e-3743-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 08:19:23.644: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba1dbf7e-3743-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-configmap-tm6fc" to be "success or failure"
Feb 23 08:19:23.649: INFO: Pod "pod-configmaps-ba1dbf7e-3743-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.880067ms
Feb 23 08:19:25.653: INFO: Pod "pod-configmaps-ba1dbf7e-3743-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008735283s
STEP: Saw pod success
Feb 23 08:19:25.653: INFO: Pod "pod-configmaps-ba1dbf7e-3743-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:19:25.656: INFO: Trying to get logs from node kube-node-01 pod pod-configmaps-ba1dbf7e-3743-11e9-a63e-52687f37ce9e container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 08:19:25.676: INFO: Waiting for pod pod-configmaps-ba1dbf7e-3743-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:19:25.679: INFO: Pod pod-configmaps-ba1dbf7e-3743-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:19:25.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-tm6fc" for this suite.
Feb 23 08:19:31.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:19:31.818: INFO: namespace: e2e-tests-configmap-tm6fc, resource: bindings, ignored listing per whitelist
Feb 23 08:19:31.883: INFO: namespace e2e-tests-configmap-tm6fc deletion completed in 6.198774158s

• [SLOW TEST:8.362 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:19:31.883: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 23 08:19:32.057: INFO: Waiting up to 5m0s for pod "downward-api-bf216b4d-3743-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-gx6kf" to be "success or failure"
Feb 23 08:19:32.062: INFO: Pod "downward-api-bf216b4d-3743-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.511702ms
Feb 23 08:19:34.067: INFO: Pod "downward-api-bf216b4d-3743-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010386519s
STEP: Saw pod success
Feb 23 08:19:34.067: INFO: Pod "downward-api-bf216b4d-3743-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:19:34.071: INFO: Trying to get logs from node kube-node-02 pod downward-api-bf216b4d-3743-11e9-a63e-52687f37ce9e container dapi-container: <nil>
STEP: delete the pod
Feb 23 08:19:34.094: INFO: Waiting for pod downward-api-bf216b4d-3743-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:19:34.096: INFO: Pod downward-api-bf216b4d-3743-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:19:34.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gx6kf" for this suite.
Feb 23 08:19:40.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:19:40.245: INFO: namespace: e2e-tests-downward-api-gx6kf, resource: bindings, ignored listing per whitelist
Feb 23 08:19:40.284: INFO: namespace e2e-tests-downward-api-gx6kf deletion completed in 6.183434866s

• [SLOW TEST:8.401 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:19:40.285: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 08:19:40.363: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:19:42.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4cq48" for this suite.
Feb 23 08:20:22.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:20:22.586: INFO: namespace: e2e-tests-pods-4cq48, resource: bindings, ignored listing per whitelist
Feb 23 08:20:22.626: INFO: namespace e2e-tests-pods-4cq48 deletion completed in 40.16761433s

• [SLOW TEST:42.341 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:20:22.626: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 23 08:20:24.745: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-dd54601d-3743-11e9-a63e-52687f37ce9e", GenerateName:"", Namespace:"e2e-tests-pods-mq8wd", SelfLink:"/api/v1/namespaces/e2e-tests-pods-mq8wd/pods/pod-submit-remove-dd54601d-3743-11e9-a63e-52687f37ce9e", UID:"dd58d739-3743-11e9-8d12-02951b420040", ResourceVersion:"19103", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686506822, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"714171452"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-t6748", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0019a7340), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-t6748", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001fad0d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kube-node-02", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001569140), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001fad120)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001fad140)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001fad148), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001fad14c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686506822, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686506823, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686506823, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686506822, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.17.10.103", PodIP:"10.42.0.6", StartTime:(*v1.Time)(0xc001e1dd20), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001e1dd40), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://6f79c3f89f357d2204277f1cc9799045c0ce537adb9dc5386a82b4d2c6137a54"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 23 08:20:29.771: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:20:29.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-mq8wd" for this suite.
Feb 23 08:20:35.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:20:35.939: INFO: namespace: e2e-tests-pods-mq8wd, resource: bindings, ignored listing per whitelist
Feb 23 08:20:35.945: INFO: namespace e2e-tests-pods-mq8wd deletion completed in 6.166045256s

• [SLOW TEST:13.319 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:20:35.945: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 08:20:36.040: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e5447b32-3743-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-downward-api-ff9r4" to be "success or failure"
Feb 23 08:20:36.046: INFO: Pod "downwardapi-volume-e5447b32-3743-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.419022ms
Feb 23 08:20:38.050: INFO: Pod "downwardapi-volume-e5447b32-3743-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009525618s
STEP: Saw pod success
Feb 23 08:20:38.050: INFO: Pod "downwardapi-volume-e5447b32-3743-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:20:38.053: INFO: Trying to get logs from node kube-node-01 pod downwardapi-volume-e5447b32-3743-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 08:20:38.074: INFO: Waiting for pod downwardapi-volume-e5447b32-3743-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:20:38.078: INFO: Pod downwardapi-volume-e5447b32-3743-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:20:38.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ff9r4" for this suite.
Feb 23 08:20:44.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:20:44.173: INFO: namespace: e2e-tests-downward-api-ff9r4, resource: bindings, ignored listing per whitelist
Feb 23 08:20:44.271: INFO: namespace e2e-tests-downward-api-ff9r4 deletion completed in 6.184204918s

• [SLOW TEST:8.326 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:20:44.271: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-ea3cb7db-3743-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 08:20:44.386: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ea3daaf5-3743-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-22vtr" to be "success or failure"
Feb 23 08:20:44.392: INFO: Pod "pod-projected-configmaps-ea3daaf5-3743-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033232ms
Feb 23 08:20:46.396: INFO: Pod "pod-projected-configmaps-ea3daaf5-3743-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010067586s
STEP: Saw pod success
Feb 23 08:20:46.396: INFO: Pod "pod-projected-configmaps-ea3daaf5-3743-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:20:46.399: INFO: Trying to get logs from node kube-node-02 pod pod-projected-configmaps-ea3daaf5-3743-11e9-a63e-52687f37ce9e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 08:20:46.420: INFO: Waiting for pod pod-projected-configmaps-ea3daaf5-3743-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:20:46.423: INFO: Pod pod-projected-configmaps-ea3daaf5-3743-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:20:46.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-22vtr" for this suite.
Feb 23 08:20:52.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:20:52.522: INFO: namespace: e2e-tests-projected-22vtr, resource: bindings, ignored listing per whitelist
Feb 23 08:20:52.627: INFO: namespace e2e-tests-projected-22vtr deletion completed in 6.199180654s

• [SLOW TEST:8.356 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:20:52.627: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 23 08:20:54.742: INFO: Pod pod-hostip-ef367faa-3743-11e9-a63e-52687f37ce9e has hostIP: 172.17.10.102
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:20:54.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-fg4vp" for this suite.
Feb 23 08:21:16.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:21:16.859: INFO: namespace: e2e-tests-pods-fg4vp, resource: bindings, ignored listing per whitelist
Feb 23 08:21:16.946: INFO: namespace e2e-tests-pods-fg4vp deletion completed in 22.198431249s

• [SLOW TEST:24.319 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:21:16.946: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 23 08:21:17.055: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-a,UID:fdba386e-3743-11e9-8d12-02951b420040,ResourceVersion:19293,Generation:0,CreationTimestamp:2019-02-23 08:21:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 08:21:17.055: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-a,UID:fdba386e-3743-11e9-8d12-02951b420040,ResourceVersion:19293,Generation:0,CreationTimestamp:2019-02-23 08:21:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 23 08:21:27.065: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-a,UID:fdba386e-3743-11e9-8d12-02951b420040,ResourceVersion:19313,Generation:0,CreationTimestamp:2019-02-23 08:21:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 23 08:21:27.065: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-a,UID:fdba386e-3743-11e9-8d12-02951b420040,ResourceVersion:19313,Generation:0,CreationTimestamp:2019-02-23 08:21:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 23 08:21:37.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-a,UID:fdba386e-3743-11e9-8d12-02951b420040,ResourceVersion:19332,Generation:0,CreationTimestamp:2019-02-23 08:21:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 08:21:37.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-a,UID:fdba386e-3743-11e9-8d12-02951b420040,ResourceVersion:19332,Generation:0,CreationTimestamp:2019-02-23 08:21:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 23 08:21:47.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-a,UID:fdba386e-3743-11e9-8d12-02951b420040,ResourceVersion:19351,Generation:0,CreationTimestamp:2019-02-23 08:21:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 08:21:47.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-a,UID:fdba386e-3743-11e9-8d12-02951b420040,ResourceVersion:19351,Generation:0,CreationTimestamp:2019-02-23 08:21:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 23 08:21:57.092: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-b,UID:1596fc74-3744-11e9-8d12-02951b420040,ResourceVersion:19370,Generation:0,CreationTimestamp:2019-02-23 08:21:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 08:21:57.092: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-b,UID:1596fc74-3744-11e9-8d12-02951b420040,ResourceVersion:19370,Generation:0,CreationTimestamp:2019-02-23 08:21:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 23 08:22:07.101: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-b,UID:1596fc74-3744-11e9-8d12-02951b420040,ResourceVersion:19389,Generation:0,CreationTimestamp:2019-02-23 08:21:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 08:22:07.101: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-t4lp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4lp2/configmaps/e2e-watch-test-configmap-b,UID:1596fc74-3744-11e9-8d12-02951b420040,ResourceVersion:19389,Generation:0,CreationTimestamp:2019-02-23 08:21:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:22:17.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-t4lp2" for this suite.
Feb 23 08:22:23.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:22:23.232: INFO: namespace: e2e-tests-watch-t4lp2, resource: bindings, ignored listing per whitelist
Feb 23 08:22:23.239: INFO: namespace e2e-tests-watch-t4lp2 deletion completed in 6.132148889s

• [SLOW TEST:66.292 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:22:23.239: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-22cwf
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-22cwf to expose endpoints map[]
Feb 23 08:22:23.333: INFO: Get endpoints failed (5.713335ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb 23 08:22:24.336: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-22cwf exposes endpoints map[] (1.009332988s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-22cwf
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-22cwf to expose endpoints map[pod1:[80]]
Feb 23 08:22:26.366: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-22cwf exposes endpoints map[pod1:[80]] (2.022163605s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-22cwf
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-22cwf to expose endpoints map[pod1:[80] pod2:[80]]
Feb 23 08:22:28.404: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-22cwf exposes endpoints map[pod1:[80] pod2:[80]] (2.031699881s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-22cwf
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-22cwf to expose endpoints map[pod2:[80]]
Feb 23 08:22:29.426: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-22cwf exposes endpoints map[pod2:[80]] (1.015053552s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-22cwf
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-22cwf to expose endpoints map[]
Feb 23 08:22:30.439: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-22cwf exposes endpoints map[] (1.006523936s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:22:30.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-22cwf" for this suite.
Feb 23 08:22:52.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:22:52.647: INFO: namespace: e2e-tests-services-22cwf, resource: bindings, ignored listing per whitelist
Feb 23 08:22:52.650: INFO: namespace e2e-tests-services-22cwf deletion completed in 22.168190985s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:29.411 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:22:52.650: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 23 08:22:55.259: INFO: Successfully updated pod "labelsupdate36be0496-3744-11e9-a63e-52687f37ce9e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:22:59.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rmqzp" for this suite.
Feb 23 08:23:21.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:23:21.420: INFO: namespace: e2e-tests-downward-api-rmqzp, resource: bindings, ignored listing per whitelist
Feb 23 08:23:21.438: INFO: namespace e2e-tests-downward-api-rmqzp deletion completed in 22.149925876s

• [SLOW TEST:28.787 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:23:21.438: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 23 08:23:21.518: INFO: Waiting up to 5m0s for pod "pod-47e6bbd6-3744-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-mjsql" to be "success or failure"
Feb 23 08:23:21.522: INFO: Pod "pod-47e6bbd6-3744-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.644823ms
Feb 23 08:23:23.526: INFO: Pod "pod-47e6bbd6-3744-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008671759s
STEP: Saw pod success
Feb 23 08:23:23.526: INFO: Pod "pod-47e6bbd6-3744-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:23:23.530: INFO: Trying to get logs from node kube-node-01 pod pod-47e6bbd6-3744-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 08:23:23.555: INFO: Waiting for pod pod-47e6bbd6-3744-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:23:23.558: INFO: Pod pod-47e6bbd6-3744-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:23:23.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mjsql" for this suite.
Feb 23 08:23:29.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:23:29.616: INFO: namespace: e2e-tests-emptydir-mjsql, resource: bindings, ignored listing per whitelist
Feb 23 08:23:29.693: INFO: namespace e2e-tests-emptydir-mjsql deletion completed in 6.131123533s

• [SLOW TEST:8.256 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:23:29.694: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-tbqzx
Feb 23 08:23:31.773: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-tbqzx
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 08:23:31.776: INFO: Initial restart count of pod liveness-exec is 0
Feb 23 08:24:25.900: INFO: Restart count of pod e2e-tests-container-probe-tbqzx/liveness-exec is now 1 (54.123758925s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:24:25.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-tbqzx" for this suite.
Feb 23 08:24:31.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:24:31.958: INFO: namespace: e2e-tests-container-probe-tbqzx, resource: bindings, ignored listing per whitelist
Feb 23 08:24:32.048: INFO: namespace e2e-tests-container-probe-tbqzx deletion completed in 6.122782491s

• [SLOW TEST:62.354 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:24:32.048: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 08:24:32.119: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71fb8749-3744-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-6f6p7" to be "success or failure"
Feb 23 08:24:32.123: INFO: Pod "downwardapi-volume-71fb8749-3744-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.404651ms
Feb 23 08:24:34.127: INFO: Pod "downwardapi-volume-71fb8749-3744-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008437476s
STEP: Saw pod success
Feb 23 08:24:34.127: INFO: Pod "downwardapi-volume-71fb8749-3744-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:24:34.130: INFO: Trying to get logs from node kube-node-01 pod downwardapi-volume-71fb8749-3744-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 08:24:34.150: INFO: Waiting for pod downwardapi-volume-71fb8749-3744-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:24:34.153: INFO: Pod downwardapi-volume-71fb8749-3744-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:24:34.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6f6p7" for this suite.
Feb 23 08:24:40.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:24:40.236: INFO: namespace: e2e-tests-projected-6f6p7, resource: bindings, ignored listing per whitelist
Feb 23 08:24:40.278: INFO: namespace e2e-tests-projected-6f6p7 deletion completed in 6.120418305s

• [SLOW TEST:8.230 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:24:40.278: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 23 08:24:44.387: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 08:24:44.390: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 08:24:46.390: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 08:24:46.394: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 08:24:48.390: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 08:24:48.394: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 08:24:50.390: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 08:24:50.396: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 08:24:52.390: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 08:24:52.394: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:24:52.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-bc6sc" for this suite.
Feb 23 08:25:14.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:25:14.512: INFO: namespace: e2e-tests-container-lifecycle-hook-bc6sc, resource: bindings, ignored listing per whitelist
Feb 23 08:25:14.543: INFO: namespace e2e-tests-container-lifecycle-hook-bc6sc deletion completed in 22.143652704s

• [SLOW TEST:34.265 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:25:14.544: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 23 08:27:56.672: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:27:56.675: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:27:58.675: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:27:58.679: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:00.676: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:00.680: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:02.677: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:02.686: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:04.675: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:04.680: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:06.676: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:06.680: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:08.675: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:08.684: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:10.676: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:10.680: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:12.676: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:12.680: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:14.675: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:14.680: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:16.676: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:16.680: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:18.675: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:18.679: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:20.675: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:20.679: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 08:28:22.675: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 08:28:22.679: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:28:22.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-tn5dm" for this suite.
Feb 23 08:28:44.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:28:44.794: INFO: namespace: e2e-tests-container-lifecycle-hook-tn5dm, resource: bindings, ignored listing per whitelist
Feb 23 08:28:44.813: INFO: namespace e2e-tests-container-lifecycle-hook-tn5dm deletion completed in 22.129855756s

• [SLOW TEST:210.269 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:28:44.813: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:28:48.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-xgmpt" for this suite.
Feb 23 08:28:54.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:28:54.998: INFO: namespace: e2e-tests-kubelet-test-xgmpt, resource: bindings, ignored listing per whitelist
Feb 23 08:28:55.028: INFO: namespace e2e-tests-kubelet-test-xgmpt deletion completed in 6.127658481s

• [SLOW TEST:10.215 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:28:55.028: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-m87dh
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 08:28:55.093: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 08:29:11.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.7:8080/dial?request=hostName&protocol=http&host=10.40.0.6&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-m87dh PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 08:29:11.167: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 08:29:11.238: INFO: Waiting for endpoints: map[]
Feb 23 08:29:11.242: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.7:8080/dial?request=hostName&protocol=http&host=10.42.0.6&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-m87dh PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 08:29:11.242: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 08:29:11.333: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:29:11.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-m87dh" for this suite.
Feb 23 08:29:33.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:29:33.457: INFO: namespace: e2e-tests-pod-network-test-m87dh, resource: bindings, ignored listing per whitelist
Feb 23 08:29:33.462: INFO: namespace e2e-tests-pod-network-test-m87dh deletion completed in 22.123894192s

• [SLOW TEST:38.434 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:29:33.463: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-25a3d25a-3745-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 08:29:33.544: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-25a46253-3745-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-svhwd" to be "success or failure"
Feb 23 08:29:33.548: INFO: Pod "pod-projected-configmaps-25a46253-3745-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.697769ms
Feb 23 08:29:35.554: INFO: Pod "pod-projected-configmaps-25a46253-3745-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010542227s
STEP: Saw pod success
Feb 23 08:29:35.554: INFO: Pod "pod-projected-configmaps-25a46253-3745-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:29:35.558: INFO: Trying to get logs from node kube-node-01 pod pod-projected-configmaps-25a46253-3745-11e9-a63e-52687f37ce9e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 08:29:35.582: INFO: Waiting for pod pod-projected-configmaps-25a46253-3745-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:29:35.585: INFO: Pod pod-projected-configmaps-25a46253-3745-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:29:35.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-svhwd" for this suite.
Feb 23 08:29:41.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:29:41.631: INFO: namespace: e2e-tests-projected-svhwd, resource: bindings, ignored listing per whitelist
Feb 23 08:29:41.719: INFO: namespace e2e-tests-projected-svhwd deletion completed in 6.129682036s

• [SLOW TEST:8.257 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:29:41.719: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-2a966c0c-3745-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 08:29:41.843: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2a979d2c-3745-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-hqkxx" to be "success or failure"
Feb 23 08:29:41.847: INFO: Pod "pod-projected-configmaps-2a979d2c-3745-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.490051ms
Feb 23 08:29:43.851: INFO: Pod "pod-projected-configmaps-2a979d2c-3745-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008423311s
STEP: Saw pod success
Feb 23 08:29:43.851: INFO: Pod "pod-projected-configmaps-2a979d2c-3745-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:29:43.854: INFO: Trying to get logs from node kube-node-02 pod pod-projected-configmaps-2a979d2c-3745-11e9-a63e-52687f37ce9e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 08:29:43.880: INFO: Waiting for pod pod-projected-configmaps-2a979d2c-3745-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:29:43.883: INFO: Pod pod-projected-configmaps-2a979d2c-3745-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:29:43.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hqkxx" for this suite.
Feb 23 08:29:49.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:29:49.998: INFO: namespace: e2e-tests-projected-hqkxx, resource: bindings, ignored listing per whitelist
Feb 23 08:29:50.031: INFO: namespace e2e-tests-projected-hqkxx deletion completed in 6.135584235s

• [SLOW TEST:8.311 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:29:50.031: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-2f8456c9-3745-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 08:29:50.110: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2f8526da-3745-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-rklcl" to be "success or failure"
Feb 23 08:29:50.116: INFO: Pod "pod-projected-configmaps-2f8526da-3745-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083906ms
Feb 23 08:29:52.120: INFO: Pod "pod-projected-configmaps-2f8526da-3745-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010126305s
STEP: Saw pod success
Feb 23 08:29:52.120: INFO: Pod "pod-projected-configmaps-2f8526da-3745-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:29:52.123: INFO: Trying to get logs from node kube-node-01 pod pod-projected-configmaps-2f8526da-3745-11e9-a63e-52687f37ce9e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 08:29:52.145: INFO: Waiting for pod pod-projected-configmaps-2f8526da-3745-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:29:52.148: INFO: Pod pod-projected-configmaps-2f8526da-3745-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:29:52.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rklcl" for this suite.
Feb 23 08:29:58.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:29:58.216: INFO: namespace: e2e-tests-projected-rklcl, resource: bindings, ignored listing per whitelist
Feb 23 08:29:58.278: INFO: namespace e2e-tests-projected-rklcl deletion completed in 6.125623738s

• [SLOW TEST:8.247 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:29:58.278: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 23 08:29:58.392: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 23 08:29:58.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:29:58.713: INFO: stderr: ""
Feb 23 08:29:58.713: INFO: stdout: "service/redis-slave created\n"
Feb 23 08:29:58.713: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 23 08:29:58.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:29:59.010: INFO: stderr: ""
Feb 23 08:29:59.010: INFO: stdout: "service/redis-master created\n"
Feb 23 08:29:59.010: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 23 08:29:59.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:29:59.186: INFO: stderr: ""
Feb 23 08:29:59.186: INFO: stdout: "service/frontend created\n"
Feb 23 08:29:59.186: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 23 08:29:59.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:29:59.358: INFO: stderr: ""
Feb 23 08:29:59.358: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 23 08:29:59.358: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 23 08:29:59.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:29:59.522: INFO: stderr: ""
Feb 23 08:29:59.523: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 23 08:29:59.523: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 23 08:29:59.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:29:59.690: INFO: stderr: ""
Feb 23 08:29:59.690: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 23 08:29:59.690: INFO: Waiting for all frontend pods to be Running.
Feb 23 08:30:14.741: INFO: Waiting for frontend to serve content.
Feb 23 08:30:19.764: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 23 08:30:24.786: INFO: Trying to add a new entry to the guestbook.
Feb 23 08:30:24.803: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 23 08:30:24.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:30:24.902: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 08:30:24.902: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 08:30:24.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:30:25.008: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 08:30:25.008: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 08:30:25.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:30:25.131: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 08:30:25.131: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 08:30:25.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:30:25.219: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 08:30:25.219: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 08:30:25.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:30:25.307: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 08:30:25.307: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 08:30:25.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-9qt79'
Feb 23 08:30:25.390: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 08:30:25.390: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:30:25.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9qt79" for this suite.
Feb 23 08:31:03.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:31:03.543: INFO: namespace: e2e-tests-kubectl-9qt79, resource: bindings, ignored listing per whitelist
Feb 23 08:31:03.552: INFO: namespace e2e-tests-kubectl-9qt79 deletion completed in 38.155249949s

• [SLOW TEST:65.274 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:31:03.552: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 23 08:31:03.628: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 08:31:03.638: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 08:31:03.641: INFO: 
Logging pods the kubelet thinks is on node kube-node-01 before test
Feb 23 08:31:03.648: INFO: weave-net-vxfzf from kube-system started at 2019-02-23 07:03:02 +0000 UTC (2 container statuses recorded)
Feb 23 08:31:03.648: INFO: 	Container weave ready: true, restart count 1
Feb 23 08:31:03.648: INFO: 	Container weave-npc ready: true, restart count 0
Feb 23 08:31:03.648: INFO: rook-ceph-osd-fxwdx from rook started at 2019-02-23 07:05:00 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.648: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Feb 23 08:31:03.648: INFO: sonobuoy-systemd-logs-daemon-set-b1c5aeee47bb4835-tp9jc from heptio-sonobuoy started at 2019-02-23 07:11:00 +0000 UTC (2 container statuses recorded)
Feb 23 08:31:03.648: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb 23 08:31:03.648: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 23 08:31:03.648: INFO: rook-ceph-mon0-ljg4v from rook started at 2019-02-23 07:03:45 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.648: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Feb 23 08:31:03.648: INFO: rook-ceph-mgr0-65cb98fd4f-swbqs from rook started at 2019-02-23 07:04:58 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.648: INFO: 	Container rook-ceph-mgr0 ready: true, restart count 0
Feb 23 08:31:03.648: INFO: kube-proxy-622xh from kube-system started at 2019-02-23 07:03:02 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.648: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 08:31:03.648: INFO: rook-agent-bm2h9 from rook-system started at 2019-02-23 07:03:42 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.648: INFO: 	Container rook-agent ready: true, restart count 0
Feb 23 08:31:03.648: INFO: rook-ceph-mon2-jdx9z from rook started at 2019-02-23 07:04:44 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.648: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Feb 23 08:31:03.648: INFO: rook-api-77bc4484c6-dfhs4 from rook started at 2019-02-23 07:04:58 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.648: INFO: 	Container rook-api ready: true, restart count 0
Feb 23 08:31:03.648: INFO: 
Logging pods the kubelet thinks is on node kube-node-02 before test
Feb 23 08:31:03.657: INFO: tiller-deploy-dbb85cb99-9gspb from kube-system started at 2019-02-23 07:02:59 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.657: INFO: 	Container tiller ready: true, restart count 0
Feb 23 08:31:03.657: INFO: kube-proxy-lc5r2 from kube-system started at 2019-02-23 07:02:59 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.657: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 08:31:03.657: INFO: rook-ceph-osd-mfkgj from rook started at 2019-02-23 07:05:00 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.657: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Feb 23 08:31:03.657: INFO: rook-operator-98c6d84c5-8dvfs from rook-system started at 2019-02-23 07:02:59 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.657: INFO: 	Container rook-operator ready: true, restart count 0
Feb 23 08:31:03.657: INFO: rook-agent-8lrhx from rook-system started at 2019-02-23 07:03:29 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.657: INFO: 	Container rook-agent ready: true, restart count 0
Feb 23 08:31:03.657: INFO: rook-ceph-mon1-cpwgw from rook started at 2019-02-23 07:04:38 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.657: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Feb 23 08:31:03.657: INFO: weave-net-psnqt from kube-system started at 2019-02-23 07:02:59 +0000 UTC (2 container statuses recorded)
Feb 23 08:31:03.657: INFO: 	Container weave ready: true, restart count 0
Feb 23 08:31:03.657: INFO: 	Container weave-npc ready: true, restart count 0
Feb 23 08:31:03.657: INFO: sonobuoy-systemd-logs-daemon-set-b1c5aeee47bb4835-42s5h from heptio-sonobuoy started at 2019-02-23 07:11:00 +0000 UTC (2 container statuses recorded)
Feb 23 08:31:03.657: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb 23 08:31:03.657: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 23 08:31:03.657: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-23 07:10:57 +0000 UTC (1 container statuses recorded)
Feb 23 08:31:03.657: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node kube-node-01
STEP: verifying the node has the label node kube-node-02
Feb 23 08:31:03.710: INFO: Pod sonobuoy requesting resource cpu=0m on Node kube-node-02
Feb 23 08:31:03.710: INFO: Pod sonobuoy-systemd-logs-daemon-set-b1c5aeee47bb4835-42s5h requesting resource cpu=0m on Node kube-node-02
Feb 23 08:31:03.710: INFO: Pod sonobuoy-systemd-logs-daemon-set-b1c5aeee47bb4835-tp9jc requesting resource cpu=0m on Node kube-node-01
Feb 23 08:31:03.710: INFO: Pod kube-proxy-622xh requesting resource cpu=0m on Node kube-node-01
Feb 23 08:31:03.710: INFO: Pod kube-proxy-lc5r2 requesting resource cpu=0m on Node kube-node-02
Feb 23 08:31:03.710: INFO: Pod tiller-deploy-dbb85cb99-9gspb requesting resource cpu=0m on Node kube-node-02
Feb 23 08:31:03.710: INFO: Pod weave-net-psnqt requesting resource cpu=20m on Node kube-node-02
Feb 23 08:31:03.710: INFO: Pod weave-net-vxfzf requesting resource cpu=20m on Node kube-node-01
Feb 23 08:31:03.710: INFO: Pod rook-agent-8lrhx requesting resource cpu=0m on Node kube-node-02
Feb 23 08:31:03.710: INFO: Pod rook-agent-bm2h9 requesting resource cpu=0m on Node kube-node-01
Feb 23 08:31:03.710: INFO: Pod rook-operator-98c6d84c5-8dvfs requesting resource cpu=0m on Node kube-node-02
Feb 23 08:31:03.710: INFO: Pod rook-api-77bc4484c6-dfhs4 requesting resource cpu=0m on Node kube-node-01
Feb 23 08:31:03.710: INFO: Pod rook-ceph-mgr0-65cb98fd4f-swbqs requesting resource cpu=0m on Node kube-node-01
Feb 23 08:31:03.710: INFO: Pod rook-ceph-mon0-ljg4v requesting resource cpu=0m on Node kube-node-01
Feb 23 08:31:03.710: INFO: Pod rook-ceph-mon1-cpwgw requesting resource cpu=0m on Node kube-node-02
Feb 23 08:31:03.710: INFO: Pod rook-ceph-mon2-jdx9z requesting resource cpu=0m on Node kube-node-01
Feb 23 08:31:03.710: INFO: Pod rook-ceph-osd-fxwdx requesting resource cpu=0m on Node kube-node-01
Feb 23 08:31:03.710: INFO: Pod rook-ceph-osd-mfkgj requesting resource cpu=0m on Node kube-node-02
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b64b354-3745-11e9-a63e-52687f37ce9e.1585f108160b231a], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-hp8sv/filler-pod-5b64b354-3745-11e9-a63e-52687f37ce9e to kube-node-01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b64b354-3745-11e9-a63e-52687f37ce9e.1585f10839dba122], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b64b354-3745-11e9-a63e-52687f37ce9e.1585f1083cfcf3be], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b64b354-3745-11e9-a63e-52687f37ce9e.1585f10841dc011e], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b65e8b1-3745-11e9-a63e-52687f37ce9e.1585f108167d9356], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-hp8sv/filler-pod-5b65e8b1-3745-11e9-a63e-52687f37ce9e to kube-node-02]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b65e8b1-3745-11e9-a63e-52687f37ce9e.1585f1083dec3fa4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b65e8b1-3745-11e9-a63e-52687f37ce9e.1585f108431d54f4], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b65e8b1-3745-11e9-a63e-52687f37ce9e.1585f1084a3e1cae], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1585f1088eaa591b], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node kube-node-01
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kube-node-02
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:31:06.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-hp8sv" for this suite.
Feb 23 08:31:12.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:31:12.912: INFO: namespace: e2e-tests-sched-pred-hp8sv, resource: bindings, ignored listing per whitelist
Feb 23 08:31:12.931: INFO: namespace e2e-tests-sched-pred-hp8sv deletion completed in 6.128868678s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.379 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:31:12.931: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-60ecc3b7-3745-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume secrets
Feb 23 08:31:13.004: INFO: Waiting up to 5m0s for pod "pod-secrets-60ed5e90-3745-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-secrets-gtqsd" to be "success or failure"
Feb 23 08:31:13.010: INFO: Pod "pod-secrets-60ed5e90-3745-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.976163ms
Feb 23 08:31:15.013: INFO: Pod "pod-secrets-60ed5e90-3745-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009683443s
STEP: Saw pod success
Feb 23 08:31:15.013: INFO: Pod "pod-secrets-60ed5e90-3745-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:31:15.019: INFO: Trying to get logs from node kube-node-02 pod pod-secrets-60ed5e90-3745-11e9-a63e-52687f37ce9e container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 08:31:15.041: INFO: Waiting for pod pod-secrets-60ed5e90-3745-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:31:15.045: INFO: Pod pod-secrets-60ed5e90-3745-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:31:15.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-gtqsd" for this suite.
Feb 23 08:31:21.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:31:21.095: INFO: namespace: e2e-tests-secrets-gtqsd, resource: bindings, ignored listing per whitelist
Feb 23 08:31:21.171: INFO: namespace e2e-tests-secrets-gtqsd deletion completed in 6.120354332s

• [SLOW TEST:8.240 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:31:21.171: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:31:21.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-fbnfh" for this suite.
Feb 23 08:31:43.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:31:43.421: INFO: namespace: e2e-tests-pods-fbnfh, resource: bindings, ignored listing per whitelist
Feb 23 08:31:43.476: INFO: namespace e2e-tests-pods-fbnfh deletion completed in 22.222049009s

• [SLOW TEST:22.305 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:31:43.476: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:31:46.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-2zsrk" for this suite.
Feb 23 08:32:08.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:32:08.691: INFO: namespace: e2e-tests-replication-controller-2zsrk, resource: bindings, ignored listing per whitelist
Feb 23 08:32:08.759: INFO: namespace e2e-tests-replication-controller-2zsrk deletion completed in 22.126880715s

• [SLOW TEST:25.283 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:32:08.759: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 23 08:32:08.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-8gnz8'
Feb 23 08:32:08.902: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 08:32:08.902: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 23 08:32:08.911: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Feb 23 08:32:08.913: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 23 08:32:08.927: INFO: scanned /root for discovery docs: <nil>
Feb 23 08:32:08.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-8gnz8'
Feb 23 08:32:24.711: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 23 08:32:24.711: INFO: stdout: "Created e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0\nScaling up e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 23 08:32:24.711: INFO: stdout: "Created e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0\nScaling up e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 23 08:32:24.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-8gnz8'
Feb 23 08:32:24.793: INFO: stderr: ""
Feb 23 08:32:24.793: INFO: stdout: "e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0-zt4qq "
Feb 23 08:32:24.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0-zt4qq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8gnz8'
Feb 23 08:32:24.860: INFO: stderr: ""
Feb 23 08:32:24.860: INFO: stdout: "true"
Feb 23 08:32:24.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 get pods e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0-zt4qq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-8gnz8'
Feb 23 08:32:24.925: INFO: stderr: ""
Feb 23 08:32:24.925: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb 23 08:32:24.925: INFO: e2e-test-nginx-rc-dfcfa9e949d23e53f5236c3993ec82c0-zt4qq is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Feb 23 08:32:24.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-8gnz8'
Feb 23 08:32:24.995: INFO: stderr: ""
Feb 23 08:32:24.995: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:32:24.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8gnz8" for this suite.
Feb 23 08:32:31.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:32:31.047: INFO: namespace: e2e-tests-kubectl-8gnz8, resource: bindings, ignored listing per whitelist
Feb 23 08:32:31.230: INFO: namespace e2e-tests-kubectl-8gnz8 deletion completed in 6.230466946s

• [SLOW TEST:22.471 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:32:31.231: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-8f9f6194-3745-11e9-a63e-52687f37ce9e
STEP: Creating a pod to test consume configMaps
Feb 23 08:32:31.352: INFO: Waiting up to 5m0s for pod "pod-configmaps-8fa06096-3745-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-configmap-j2545" to be "success or failure"
Feb 23 08:32:31.356: INFO: Pod "pod-configmaps-8fa06096-3745-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.189918ms
Feb 23 08:32:33.360: INFO: Pod "pod-configmaps-8fa06096-3745-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008315256s
STEP: Saw pod success
Feb 23 08:32:33.360: INFO: Pod "pod-configmaps-8fa06096-3745-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:32:33.363: INFO: Trying to get logs from node kube-node-02 pod pod-configmaps-8fa06096-3745-11e9-a63e-52687f37ce9e container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 08:32:33.385: INFO: Waiting for pod pod-configmaps-8fa06096-3745-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:32:33.388: INFO: Pod pod-configmaps-8fa06096-3745-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:32:33.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-j2545" for this suite.
Feb 23 08:32:39.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:32:39.607: INFO: namespace: e2e-tests-configmap-j2545, resource: bindings, ignored listing per whitelist
Feb 23 08:32:39.611: INFO: namespace e2e-tests-configmap-j2545 deletion completed in 6.219505178s

• [SLOW TEST:8.381 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:32:39.612: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 23 08:32:45.813: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:32:45.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-blpdh" for this suite.
Feb 23 08:32:51.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:32:51.860: INFO: namespace: e2e-tests-gc-blpdh, resource: bindings, ignored listing per whitelist
Feb 23 08:32:52.030: INFO: namespace e2e-tests-gc-blpdh deletion completed in 6.213405758s

• [SLOW TEST:12.419 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:32:52.030: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-fbq6q
Feb 23 08:32:56.151: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-fbq6q
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 08:32:56.155: INFO: Initial restart count of pod liveness-http is 0
Feb 23 08:33:08.190: INFO: Restart count of pod e2e-tests-container-probe-fbq6q/liveness-http is now 1 (12.035343376s elapsed)
Feb 23 08:33:28.237: INFO: Restart count of pod e2e-tests-container-probe-fbq6q/liveness-http is now 2 (32.08190479s elapsed)
Feb 23 08:33:48.282: INFO: Restart count of pod e2e-tests-container-probe-fbq6q/liveness-http is now 3 (52.127713592s elapsed)
Feb 23 08:34:08.335: INFO: Restart count of pod e2e-tests-container-probe-fbq6q/liveness-http is now 4 (1m12.180540336s elapsed)
Feb 23 08:35:18.535: INFO: Restart count of pod e2e-tests-container-probe-fbq6q/liveness-http is now 5 (2m22.38036854s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:35:18.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-fbq6q" for this suite.
Feb 23 08:35:24.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:35:24.665: INFO: namespace: e2e-tests-container-probe-fbq6q, resource: bindings, ignored listing per whitelist
Feb 23 08:35:24.780: INFO: namespace e2e-tests-container-probe-fbq6q deletion completed in 6.228186643s

• [SLOW TEST:152.750 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:35:24.780: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:35:26.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-lv472" for this suite.
Feb 23 08:35:32.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:35:33.169: INFO: namespace: e2e-tests-emptydir-wrapper-lv472, resource: bindings, ignored listing per whitelist
Feb 23 08:35:33.173: INFO: namespace e2e-tests-emptydir-wrapper-lv472 deletion completed in 6.210756465s

• [SLOW TEST:8.393 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:35:33.173: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 23 08:35:33.274: INFO: Waiting up to 5m0s for pod "pod-fc0f5352-3745-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-xnfzz" to be "success or failure"
Feb 23 08:35:33.279: INFO: Pod "pod-fc0f5352-3745-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.74459ms
Feb 23 08:35:35.283: INFO: Pod "pod-fc0f5352-3745-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009090817s
STEP: Saw pod success
Feb 23 08:35:35.283: INFO: Pod "pod-fc0f5352-3745-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:35:35.287: INFO: Trying to get logs from node kube-node-02 pod pod-fc0f5352-3745-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 08:35:35.309: INFO: Waiting for pod pod-fc0f5352-3745-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:35:35.313: INFO: Pod pod-fc0f5352-3745-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:35:35.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xnfzz" for this suite.
Feb 23 08:35:41.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:35:41.446: INFO: namespace: e2e-tests-emptydir-xnfzz, resource: bindings, ignored listing per whitelist
Feb 23 08:35:41.485: INFO: namespace e2e-tests-emptydir-xnfzz deletion completed in 6.16623413s

• [SLOW TEST:8.312 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:35:41.486: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 23 08:35:41.587: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:41.587: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:41.587: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:41.590: INFO: Number of nodes with available pods: 0
Feb 23 08:35:41.590: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:35:42.600: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:42.600: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:42.600: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:42.604: INFO: Number of nodes with available pods: 0
Feb 23 08:35:42.604: INFO: Node kube-node-01 is running more than one daemon pod
Feb 23 08:35:43.599: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:43.599: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:43.599: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:43.603: INFO: Number of nodes with available pods: 2
Feb 23 08:35:43.603: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 23 08:35:43.631: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:43.631: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:43.631: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:43.639: INFO: Number of nodes with available pods: 1
Feb 23 08:35:43.639: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:44.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:44.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:44.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:44.648: INFO: Number of nodes with available pods: 1
Feb 23 08:35:44.648: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:45.647: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:45.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:45.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:45.650: INFO: Number of nodes with available pods: 1
Feb 23 08:35:45.650: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:46.647: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:46.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:46.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:46.656: INFO: Number of nodes with available pods: 1
Feb 23 08:35:46.656: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:47.646: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:47.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:47.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:47.651: INFO: Number of nodes with available pods: 1
Feb 23 08:35:47.651: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:48.643: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:48.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:48.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:48.647: INFO: Number of nodes with available pods: 1
Feb 23 08:35:48.647: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:49.645: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:49.645: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:49.645: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:49.648: INFO: Number of nodes with available pods: 1
Feb 23 08:35:49.648: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:50.645: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:50.645: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:50.645: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:50.651: INFO: Number of nodes with available pods: 1
Feb 23 08:35:50.651: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:51.647: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:51.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:51.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:51.651: INFO: Number of nodes with available pods: 1
Feb 23 08:35:51.651: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:52.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:52.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:52.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:52.648: INFO: Number of nodes with available pods: 1
Feb 23 08:35:52.648: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:53.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:53.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:53.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:53.647: INFO: Number of nodes with available pods: 1
Feb 23 08:35:53.647: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:54.646: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:54.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:54.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:54.654: INFO: Number of nodes with available pods: 1
Feb 23 08:35:54.654: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:55.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:55.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:55.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:55.647: INFO: Number of nodes with available pods: 1
Feb 23 08:35:55.647: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:56.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:56.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:56.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:56.647: INFO: Number of nodes with available pods: 1
Feb 23 08:35:56.647: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:57.645: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:57.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:57.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:57.649: INFO: Number of nodes with available pods: 1
Feb 23 08:35:57.649: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:58.647: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:58.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:58.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:58.653: INFO: Number of nodes with available pods: 1
Feb 23 08:35:58.653: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:35:59.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:59.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:59.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:35:59.648: INFO: Number of nodes with available pods: 1
Feb 23 08:35:59.648: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:00.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:00.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:00.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:00.647: INFO: Number of nodes with available pods: 1
Feb 23 08:36:00.647: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:01.648: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:01.648: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:01.648: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:01.654: INFO: Number of nodes with available pods: 1
Feb 23 08:36:01.654: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:02.670: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:02.670: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:02.670: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:02.686: INFO: Number of nodes with available pods: 1
Feb 23 08:36:02.687: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:03.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:03.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:03.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:03.647: INFO: Number of nodes with available pods: 1
Feb 23 08:36:03.647: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:04.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:04.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:04.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:04.647: INFO: Number of nodes with available pods: 1
Feb 23 08:36:04.647: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:05.647: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:05.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:05.647: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:05.653: INFO: Number of nodes with available pods: 1
Feb 23 08:36:05.653: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:06.646: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:06.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:06.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:06.650: INFO: Number of nodes with available pods: 1
Feb 23 08:36:06.650: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:07.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:07.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:07.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:07.647: INFO: Number of nodes with available pods: 1
Feb 23 08:36:07.647: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:08.649: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:08.649: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:08.649: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:08.653: INFO: Number of nodes with available pods: 1
Feb 23 08:36:08.653: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:09.649: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:09.649: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:09.649: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:09.653: INFO: Number of nodes with available pods: 1
Feb 23 08:36:09.653: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:10.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:10.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:10.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:10.647: INFO: Number of nodes with available pods: 1
Feb 23 08:36:10.647: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:11.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:11.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:11.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:11.648: INFO: Number of nodes with available pods: 1
Feb 23 08:36:11.648: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:12.646: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:12.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:12.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:12.650: INFO: Number of nodes with available pods: 1
Feb 23 08:36:12.650: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:13.648: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:13.648: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:13.648: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:13.652: INFO: Number of nodes with available pods: 1
Feb 23 08:36:13.652: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:14.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:14.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:14.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:14.648: INFO: Number of nodes with available pods: 1
Feb 23 08:36:14.648: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:15.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:15.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:15.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:15.648: INFO: Number of nodes with available pods: 1
Feb 23 08:36:15.648: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:16.646: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:16.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:16.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:16.653: INFO: Number of nodes with available pods: 1
Feb 23 08:36:16.653: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:17.646: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:17.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:17.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:17.650: INFO: Number of nodes with available pods: 1
Feb 23 08:36:17.650: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:18.644: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:18.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:18.644: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:18.647: INFO: Number of nodes with available pods: 1
Feb 23 08:36:18.647: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:19.643: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:19.643: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:19.643: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:19.647: INFO: Number of nodes with available pods: 1
Feb 23 08:36:19.647: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:20.646: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:20.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:20.646: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:20.650: INFO: Number of nodes with available pods: 1
Feb 23 08:36:20.650: INFO: Node kube-node-02 is running more than one daemon pod
Feb 23 08:36:21.645: INFO: DaemonSet pods can't tolerate node kube-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:21.645: INFO: DaemonSet pods can't tolerate node kube-replica-master-01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:21.645: INFO: DaemonSet pods can't tolerate node kube-replica-master-02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 23 08:36:21.662: INFO: Number of nodes with available pods: 2
Feb 23 08:36:21.662: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-ls5ps, will wait for the garbage collector to delete the pods
Feb 23 08:36:21.731: INFO: Deleting DaemonSet.extensions daemon-set took: 8.894861ms
Feb 23 08:36:21.831: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.232625ms
Feb 23 08:37:01.945: INFO: Number of nodes with available pods: 0
Feb 23 08:37:01.945: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 08:37:01.967: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-ls5ps/daemonsets","resourceVersion":"22253"},"items":null}

Feb 23 08:37:01.972: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-ls5ps/pods","resourceVersion":"22253"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:37:02.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-ls5ps" for this suite.
Feb 23 08:37:08.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:37:08.156: INFO: namespace: e2e-tests-daemonsets-ls5ps, resource: bindings, ignored listing per whitelist
Feb 23 08:37:08.194: INFO: namespace e2e-tests-daemonsets-ls5ps deletion completed in 6.176626459s

• [SLOW TEST:86.708 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:37:08.194: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 23 08:37:08.274: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34afa9b2-3746-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-projected-744sf" to be "success or failure"
Feb 23 08:37:08.283: INFO: Pod "downwardapi-volume-34afa9b2-3746-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.960772ms
Feb 23 08:37:10.287: INFO: Pod "downwardapi-volume-34afa9b2-3746-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013062245s
STEP: Saw pod success
Feb 23 08:37:10.287: INFO: Pod "downwardapi-volume-34afa9b2-3746-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:37:10.290: INFO: Trying to get logs from node kube-node-01 pod downwardapi-volume-34afa9b2-3746-11e9-a63e-52687f37ce9e container client-container: <nil>
STEP: delete the pod
Feb 23 08:37:10.318: INFO: Waiting for pod downwardapi-volume-34afa9b2-3746-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:37:10.321: INFO: Pod downwardapi-volume-34afa9b2-3746-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:37:10.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-744sf" for this suite.
Feb 23 08:37:16.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:37:16.445: INFO: namespace: e2e-tests-projected-744sf, resource: bindings, ignored listing per whitelist
Feb 23 08:37:16.459: INFO: namespace e2e-tests-projected-744sf deletion completed in 6.131968869s

• [SLOW TEST:8.265 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:37:16.459: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 23 08:37:16.537: INFO: Waiting up to 5m0s for pod "pod-399baed2-3746-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-lvn7r" to be "success or failure"
Feb 23 08:37:16.542: INFO: Pod "pod-399baed2-3746-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089943ms
Feb 23 08:37:18.546: INFO: Pod "pod-399baed2-3746-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008001222s
STEP: Saw pod success
Feb 23 08:37:18.546: INFO: Pod "pod-399baed2-3746-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:37:18.548: INFO: Trying to get logs from node kube-node-02 pod pod-399baed2-3746-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 08:37:18.572: INFO: Waiting for pod pod-399baed2-3746-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:37:18.575: INFO: Pod pod-399baed2-3746-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:37:18.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lvn7r" for this suite.
Feb 23 08:37:24.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:37:24.631: INFO: namespace: e2e-tests-emptydir-lvn7r, resource: bindings, ignored listing per whitelist
Feb 23 08:37:24.705: INFO: namespace e2e-tests-emptydir-lvn7r deletion completed in 6.126400964s

• [SLOW TEST:8.246 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:37:24.705: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:37:24.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-4scmg" for this suite.
Feb 23 08:37:30.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:37:30.811: INFO: namespace: e2e-tests-services-4scmg, resource: bindings, ignored listing per whitelist
Feb 23 08:37:30.901: INFO: namespace e2e-tests-services-4scmg deletion completed in 6.122654717s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.196 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:37:30.901: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 23 08:37:30.968: INFO: Waiting up to 5m0s for pod "client-containers-42369d24-3746-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-containers-7kx75" to be "success or failure"
Feb 23 08:37:30.971: INFO: Pod "client-containers-42369d24-3746-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.711185ms
Feb 23 08:37:32.975: INFO: Pod "client-containers-42369d24-3746-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006918159s
STEP: Saw pod success
Feb 23 08:37:32.975: INFO: Pod "client-containers-42369d24-3746-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:37:32.978: INFO: Trying to get logs from node kube-node-02 pod client-containers-42369d24-3746-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 08:37:32.999: INFO: Waiting for pod client-containers-42369d24-3746-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:37:33.002: INFO: Pod client-containers-42369d24-3746-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:37:33.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-7kx75" for this suite.
Feb 23 08:37:39.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:37:39.126: INFO: namespace: e2e-tests-containers-7kx75, resource: bindings, ignored listing per whitelist
Feb 23 08:37:39.146: INFO: namespace e2e-tests-containers-7kx75 deletion completed in 6.139747871s

• [SLOW TEST:8.245 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:37:39.147: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 23 08:37:39.223: INFO: Waiting up to 5m0s for pod "client-containers-47219ed6-3746-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-containers-jlqq6" to be "success or failure"
Feb 23 08:37:39.226: INFO: Pod "client-containers-47219ed6-3746-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.983464ms
Feb 23 08:37:41.230: INFO: Pod "client-containers-47219ed6-3746-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00661966s
STEP: Saw pod success
Feb 23 08:37:41.230: INFO: Pod "client-containers-47219ed6-3746-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:37:41.233: INFO: Trying to get logs from node kube-node-02 pod client-containers-47219ed6-3746-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 08:37:41.252: INFO: Waiting for pod client-containers-47219ed6-3746-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:37:41.255: INFO: Pod client-containers-47219ed6-3746-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:37:41.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-jlqq6" for this suite.
Feb 23 08:37:47.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:37:47.592: INFO: namespace: e2e-tests-containers-jlqq6, resource: bindings, ignored listing per whitelist
Feb 23 08:37:47.604: INFO: namespace e2e-tests-containers-jlqq6 deletion completed in 6.344813159s

• [SLOW TEST:8.458 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:37:47.605: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 23 08:37:47.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 version --client'
Feb 23 08:37:47.855: INFO: stderr: ""
Feb 23 08:37:47.855: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 23 08:37:47.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-m9t5c'
Feb 23 08:37:48.109: INFO: stderr: ""
Feb 23 08:37:48.109: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 23 08:37:48.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 create -f - --namespace=e2e-tests-kubectl-m9t5c'
Feb 23 08:37:48.425: INFO: stderr: ""
Feb 23 08:37:48.425: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 23 08:37:49.439: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 08:37:49.439: INFO: Found 0 / 1
Feb 23 08:37:50.429: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 08:37:50.429: INFO: Found 1 / 1
Feb 23 08:37:50.429: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 23 08:37:50.432: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 08:37:50.432: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 23 08:37:50.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 describe pod redis-master-44297 --namespace=e2e-tests-kubectl-m9t5c'
Feb 23 08:37:50.511: INFO: stderr: ""
Feb 23 08:37:50.511: INFO: stdout: "Name:               redis-master-44297\nNamespace:          e2e-tests-kubectl-m9t5c\nPriority:           0\nPriorityClassName:  <none>\nNode:               kube-node-01/172.17.10.102\nStart Time:         Sat, 23 Feb 2019 08:37:48 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.40.0.6\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://7b0fac35f597155dc538b814936e8056dfcff145ed1a510dd499db7aedff072f\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 23 Feb 2019 08:37:48 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jkbvx (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-jkbvx:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-jkbvx\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                   Message\n  ----    ------     ----  ----                   -------\n  Normal  Scheduled  2s    default-scheduler      Successfully assigned e2e-tests-kubectl-m9t5c/redis-master-44297 to kube-node-01\n  Normal  Pulled     2s    kubelet, kube-node-01  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, kube-node-01  Created container\n  Normal  Started    2s    kubelet, kube-node-01  Started container\n"
Feb 23 08:37:50.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 describe rc redis-master --namespace=e2e-tests-kubectl-m9t5c'
Feb 23 08:37:50.600: INFO: stderr: ""
Feb 23 08:37:50.600: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-m9t5c\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-44297\n"
Feb 23 08:37:50.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 describe service redis-master --namespace=e2e-tests-kubectl-m9t5c'
Feb 23 08:37:50.677: INFO: stderr: ""
Feb 23 08:37:50.677: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-m9t5c\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.101.201.35\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.40.0.6:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 23 08:37:50.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 describe node kube-master'
Feb 23 08:37:50.796: INFO: stderr: ""
Feb 23 08:37:50.796: INFO: stdout: "Name:               kube-master\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=kube-master\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 23 Feb 2019 07:02:17 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 23 Feb 2019 07:02:23 +0000   Sat, 23 Feb 2019 07:02:23 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Sat, 23 Feb 2019 08:37:44 +0000   Sat, 23 Feb 2019 07:02:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 23 Feb 2019 08:37:44 +0000   Sat, 23 Feb 2019 07:02:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 23 Feb 2019 08:37:44 +0000   Sat, 23 Feb 2019 07:02:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 23 Feb 2019 08:37:44 +0000   Sat, 23 Feb 2019 07:02:27 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.17.10.101\n  Hostname:    kube-master\nCapacity:\n cpu:                2\n ephemeral-storage:  8065444Ki\n hugepages-2Mi:      0\n memory:             4045212Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  7433113179\n hugepages-2Mi:      0\n memory:             3942812Ki\n pods:               110\nSystem Info:\n Machine ID:                 384f5e57b169476bb92d67fcd0875241\n System UUID:                EC247998-1732-C7BF-F217-17CCD4F4487F\n Boot ID:                    b32c1e36-2053-4c99-8615-e15e08e695d3\n Kernel Version:             4.4.0-1022-aws\n OS Image:                   Ubuntu 16.04.2 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://17.3.0\n Kubelet Version:            v1.13.2\n Kube-Proxy Version:         v1.13.2\nPodCIDR:                     10.32.2.0/24\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-e2e-job-2d3c3694e963446a                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         86m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-b1c5aeee47bb4835-66ngg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         86m\n  kube-system                kube-apiserver-kube-master                                 250m (12%)    0 (0%)      0 (0%)           0 (0%)         94m\n  kube-system                kube-controller-manager-kube-master                        200m (10%)    0 (0%)      0 (0%)           0 (0%)         94m\n  kube-system                kube-proxy-vr5kf                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         95m\n  kube-system                kube-scheduler-kube-master                                 100m (5%)     0 (0%)      0 (0%)           0 (0%)         94m\n  kube-system                weave-net-nz67n                                            20m (1%)      0 (0%)      0 (0%)           0 (0%)         95m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                570m (28%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Feb 23 08:37:50.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-203633079 describe namespace e2e-tests-kubectl-m9t5c'
Feb 23 08:37:50.870: INFO: stderr: ""
Feb 23 08:37:50.870: INFO: stdout: "Name:         e2e-tests-kubectl-m9t5c\nLabels:       e2e-framework=kubectl\n              e2e-run=396235de-373a-11e9-a63e-52687f37ce9e\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:37:50.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-m9t5c" for this suite.
Feb 23 08:38:12.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:38:12.907: INFO: namespace: e2e-tests-kubectl-m9t5c, resource: bindings, ignored listing per whitelist
Feb 23 08:38:13.028: INFO: namespace e2e-tests-kubectl-m9t5c deletion completed in 22.15308647s

• [SLOW TEST:25.423 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:38:13.028: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-srzd6
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 08:38:13.092: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 08:38:35.173: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.40.0.6:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-srzd6 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 08:38:35.173: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 08:38:35.250: INFO: Found all expected endpoints: [netserver-0]
Feb 23 08:38:35.254: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.0.6:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-srzd6 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 08:38:35.254: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
Feb 23 08:38:35.319: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:38:35.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-srzd6" for this suite.
Feb 23 08:38:57.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:38:57.401: INFO: namespace: e2e-tests-pod-network-test-srzd6, resource: bindings, ignored listing per whitelist
Feb 23 08:38:57.457: INFO: namespace e2e-tests-pod-network-test-srzd6 deletion completed in 22.133167859s

• [SLOW TEST:44.429 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:38:57.458: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 23 08:38:57.530: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-203633079 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:38:57.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lhhdv" for this suite.
Feb 23 08:39:03.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:39:03.673: INFO: namespace: e2e-tests-kubectl-lhhdv, resource: bindings, ignored listing per whitelist
Feb 23 08:39:03.744: INFO: namespace e2e-tests-kubectl-lhhdv deletion completed in 6.145591324s

• [SLOW TEST:6.287 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:39:03.744: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 23 08:39:03.821: INFO: Waiting up to 5m0s for pod "pod-798e8441-3746-11e9-a63e-52687f37ce9e" in namespace "e2e-tests-emptydir-tjpqz" to be "success or failure"
Feb 23 08:39:03.826: INFO: Pod "pod-798e8441-3746-11e9-a63e-52687f37ce9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.847012ms
Feb 23 08:39:05.830: INFO: Pod "pod-798e8441-3746-11e9-a63e-52687f37ce9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009887166s
STEP: Saw pod success
Feb 23 08:39:05.831: INFO: Pod "pod-798e8441-3746-11e9-a63e-52687f37ce9e" satisfied condition "success or failure"
Feb 23 08:39:05.833: INFO: Trying to get logs from node kube-node-02 pod pod-798e8441-3746-11e9-a63e-52687f37ce9e container test-container: <nil>
STEP: delete the pod
Feb 23 08:39:05.857: INFO: Waiting for pod pod-798e8441-3746-11e9-a63e-52687f37ce9e to disappear
Feb 23 08:39:05.866: INFO: Pod pod-798e8441-3746-11e9-a63e-52687f37ce9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:39:05.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tjpqz" for this suite.
Feb 23 08:39:11.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:39:11.908: INFO: namespace: e2e-tests-emptydir-tjpqz, resource: bindings, ignored listing per whitelist
Feb 23 08:39:12.003: INFO: namespace e2e-tests-emptydir-tjpqz deletion completed in 6.128220431s

• [SLOW TEST:8.258 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:39:12.003: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-7e7afd41-3746-11e9-a63e-52687f37ce9e
STEP: Creating configMap with name cm-test-opt-upd-7e7afd8a-3746-11e9-a63e-52687f37ce9e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7e7afd41-3746-11e9-a63e-52687f37ce9e
STEP: Updating configmap cm-test-opt-upd-7e7afd8a-3746-11e9-a63e-52687f37ce9e
STEP: Creating configMap with name cm-test-opt-create-7e7afda3-3746-11e9-a63e-52687f37ce9e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:39:16.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-wd6bp" for this suite.
Feb 23 08:39:38.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:39:38.226: INFO: namespace: e2e-tests-configmap-wd6bp, resource: bindings, ignored listing per whitelist
Feb 23 08:39:38.334: INFO: namespace e2e-tests-configmap-wd6bp deletion completed in 22.140089335s

• [SLOW TEST:26.331 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 23 08:39:38.334: INFO: >>> kubeConfig: /tmp/kubeconfig-203633079
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-qd5w5
Feb 23 08:39:40.419: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-qd5w5
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 08:39:40.422: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 23 08:43:41.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-qd5w5" for this suite.
Feb 23 08:43:47.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 08:43:47.113: INFO: namespace: e2e-tests-container-probe-qd5w5, resource: bindings, ignored listing per whitelist
Feb 23 08:43:47.217: INFO: namespace e2e-tests-container-probe-qd5w5 deletion completed in 6.160168968s

• [SLOW TEST:248.883 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSFeb 23 08:43:47.217: INFO: Running AfterSuite actions on all nodes
Feb 23 08:43:47.217: INFO: Running AfterSuite actions on node 1
Feb 23 08:43:47.217: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5543.882 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h32m25.100499209s
Test Suite Passed
