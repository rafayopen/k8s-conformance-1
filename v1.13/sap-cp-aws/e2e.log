Conformance test: not doing test setup.
I0228 07:24:40.151522   30448 e2e.go:224] Starting e2e run "e8b8954a-3b29-11e9-8ad6-42906abdb246" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1551338679 - Will randomize all specs
Will run 201 of 2161 specs

Feb 28 07:24:40.362: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:24:40.365: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 28 07:24:40.585: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 28 07:24:40.818: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 28 07:24:40.818: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb 28 07:24:40.818: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 28 07:24:40.878: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 28 07:24:40.878: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 28 07:24:40.878: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb 28 07:24:40.878: INFO: e2e test version: v1.13.3
Feb 28 07:24:40.931: INFO: kube-apiserver version: v1.13.3
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:24:40.931: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
Feb 28 07:24:42.887: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 28 07:24:43.057: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vmkl8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 28 07:24:43.371: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-vmkl8'
Feb 28 07:24:43.974: INFO: stderr: ""
Feb 28 07:24:43.974: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 07:24:43.974: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vmkl8'
Feb 28 07:24:44.282: INFO: stderr: ""
Feb 28 07:24:44.282: INFO: stdout: "update-demo-nautilus-gzbkz update-demo-nautilus-zwlr6 "
Feb 28 07:24:44.282: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-gzbkz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vmkl8'
Feb 28 07:24:44.594: INFO: stderr: ""
Feb 28 07:24:44.594: INFO: stdout: ""
Feb 28 07:24:44.594: INFO: update-demo-nautilus-gzbkz is created but not running
Feb 28 07:24:49.594: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vmkl8'
Feb 28 07:24:49.892: INFO: stderr: ""
Feb 28 07:24:49.893: INFO: stdout: "update-demo-nautilus-gzbkz update-demo-nautilus-zwlr6 "
Feb 28 07:24:49.893: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-gzbkz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vmkl8'
Feb 28 07:24:50.188: INFO: stderr: ""
Feb 28 07:24:50.188: INFO: stdout: "true"
Feb 28 07:24:50.188: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-gzbkz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vmkl8'
Feb 28 07:24:50.474: INFO: stderr: ""
Feb 28 07:24:50.474: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:24:50.474: INFO: validating pod update-demo-nautilus-gzbkz
Feb 28 07:24:50.618: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:24:50.618: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:24:50.618: INFO: update-demo-nautilus-gzbkz is verified up and running
Feb 28 07:24:50.618: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-zwlr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vmkl8'
Feb 28 07:24:50.923: INFO: stderr: ""
Feb 28 07:24:50.923: INFO: stdout: "true"
Feb 28 07:24:50.923: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-zwlr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vmkl8'
Feb 28 07:24:51.234: INFO: stderr: ""
Feb 28 07:24:51.234: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:24:51.234: INFO: validating pod update-demo-nautilus-zwlr6
Feb 28 07:24:51.377: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:24:51.377: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:24:51.377: INFO: update-demo-nautilus-zwlr6 is verified up and running
STEP: using delete to clean up resources
Feb 28 07:24:51.377: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vmkl8'
Feb 28 07:24:51.730: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:24:51.730: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 28 07:24:51.730: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-vmkl8'
Feb 28 07:24:52.072: INFO: stderr: "No resources found.\n"
Feb 28 07:24:52.072: INFO: stdout: ""
Feb 28 07:24:52.072: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-vmkl8 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 07:24:52.373: INFO: stderr: ""
Feb 28 07:24:52.373: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:24:52.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vmkl8" for this suite.
Feb 28 07:24:58.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:25:00.400: INFO: namespace: e2e-tests-kubectl-vmkl8, resource: bindings, ignored listing per whitelist
Feb 28 07:25:00.669: INFO: namespace e2e-tests-kubectl-vmkl8 deletion completed in 8.242055435s

• [SLOW TEST:19.738 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:25:00.670: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-ljh52
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 28 07:25:03.071: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:25:03.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-ljh52" for this suite.
Feb 28 07:25:09.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:25:10.987: INFO: namespace: e2e-tests-replication-controller-ljh52, resource: bindings, ignored listing per whitelist
Feb 28 07:25:11.574: INFO: namespace e2e-tests-replication-controller-ljh52 deletion completed in 8.286559659s

• [SLOW TEST:10.904 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:25:11.574: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-bkwkv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-fd19c9a5-3b29-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 07:25:13.863: INFO: Waiting up to 5m0s for pod "pod-configmaps-fd21ffe5-3b29-11e9-8ad6-42906abdb246" in namespace "e2e-tests-configmap-bkwkv" to be "success or failure"
Feb 28 07:25:13.919: INFO: Pod "pod-configmaps-fd21ffe5-3b29-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 55.396488ms
Feb 28 07:25:15.973: INFO: Pod "pod-configmaps-fd21ffe5-3b29-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109795954s
Feb 28 07:25:18.028: INFO: Pod "pod-configmaps-fd21ffe5-3b29-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.165245768s
STEP: Saw pod success
Feb 28 07:25:18.028: INFO: Pod "pod-configmaps-fd21ffe5-3b29-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:25:18.082: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-configmaps-fd21ffe5-3b29-11e9-8ad6-42906abdb246 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:25:18.331: INFO: Waiting for pod pod-configmaps-fd21ffe5-3b29-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:25:18.385: INFO: Pod pod-configmaps-fd21ffe5-3b29-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:25:18.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bkwkv" for this suite.
Feb 28 07:25:24.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:25:25.628: INFO: namespace: e2e-tests-configmap-bkwkv, resource: bindings, ignored listing per whitelist
Feb 28 07:25:26.653: INFO: namespace e2e-tests-configmap-bkwkv deletion completed in 8.214512992s

• [SLOW TEST:15.081 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:25:26.658: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-btr96
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-g42j
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 07:25:28.949: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-g42j" in namespace "e2e-tests-subpath-btr96" to be "success or failure"
Feb 28 07:25:29.003: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Pending", Reason="", readiness=false. Elapsed: 53.401761ms
Feb 28 07:25:31.059: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109099116s
Feb 28 07:25:33.113: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Running", Reason="", readiness=false. Elapsed: 4.163859752s
Feb 28 07:25:35.169: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Running", Reason="", readiness=false. Elapsed: 6.219498352s
Feb 28 07:25:37.224: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Running", Reason="", readiness=false. Elapsed: 8.275008633s
Feb 28 07:25:39.279: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Running", Reason="", readiness=false. Elapsed: 10.329587939s
Feb 28 07:25:41.334: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Running", Reason="", readiness=false. Elapsed: 12.384696698s
Feb 28 07:25:43.389: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Running", Reason="", readiness=false. Elapsed: 14.439822632s
Feb 28 07:25:45.444: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Running", Reason="", readiness=false. Elapsed: 16.494667106s
Feb 28 07:25:47.499: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Running", Reason="", readiness=false. Elapsed: 18.549868106s
Feb 28 07:25:49.554: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Running", Reason="", readiness=false. Elapsed: 20.604299235s
Feb 28 07:25:51.608: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Running", Reason="", readiness=false. Elapsed: 22.658630024s
Feb 28 07:25:53.662: INFO: Pod "pod-subpath-test-secret-g42j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.7128585s
STEP: Saw pod success
Feb 28 07:25:53.662: INFO: Pod "pod-subpath-test-secret-g42j" satisfied condition "success or failure"
Feb 28 07:25:53.716: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-subpath-test-secret-g42j container test-container-subpath-secret-g42j: <nil>
STEP: delete the pod
Feb 28 07:25:53.831: INFO: Waiting for pod pod-subpath-test-secret-g42j to disappear
Feb 28 07:25:53.886: INFO: Pod pod-subpath-test-secret-g42j no longer exists
STEP: Deleting pod pod-subpath-test-secret-g42j
Feb 28 07:25:53.886: INFO: Deleting pod "pod-subpath-test-secret-g42j" in namespace "e2e-tests-subpath-btr96"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:25:53.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-btr96" for this suite.
Feb 28 07:26:00.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:26:01.175: INFO: namespace: e2e-tests-subpath-btr96, resource: bindings, ignored listing per whitelist
Feb 28 07:26:02.225: INFO: namespace e2e-tests-subpath-btr96 deletion completed in 8.230842759s

• [SLOW TEST:35.567 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:26:02.225: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-ncf4j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 07:26:04.373: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:26:09.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-ncf4j" for this suite.
Feb 28 07:26:31.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:26:32.968: INFO: namespace: e2e-tests-init-container-ncf4j, resource: bindings, ignored listing per whitelist
Feb 28 07:26:33.614: INFO: namespace e2e-tests-init-container-ncf4j deletion completed in 24.215984121s

• [SLOW TEST:31.389 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:26:33.615: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-rsgst
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-rsgst
Feb 28 07:26:37.915: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-rsgst
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 07:26:37.968: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:30:38.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-rsgst" for this suite.
Feb 28 07:30:44.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:30:46.559: INFO: namespace: e2e-tests-container-probe-rsgst, resource: bindings, ignored listing per whitelist
Feb 28 07:30:46.665: INFO: namespace e2e-tests-container-probe-rsgst deletion completed in 8.193384107s

• [SLOW TEST:253.050 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:30:46.666: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-n886h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:30:48.907: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4d5da16-3b2a-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-n886h" to be "success or failure"
Feb 28 07:30:48.961: INFO: Pod "downwardapi-volume-c4d5da16-3b2a-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.369041ms
Feb 28 07:30:51.015: INFO: Pod "downwardapi-volume-c4d5da16-3b2a-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107873761s
STEP: Saw pod success
Feb 28 07:30:51.015: INFO: Pod "downwardapi-volume-c4d5da16-3b2a-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:30:51.069: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-c4d5da16-3b2a-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 07:30:51.188: INFO: Waiting for pod downwardapi-volume-c4d5da16-3b2a-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:30:51.241: INFO: Pod downwardapi-volume-c4d5da16-3b2a-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:30:51.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-n886h" for this suite.
Feb 28 07:30:57.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:30:58.054: INFO: namespace: e2e-tests-downward-api-n886h, resource: bindings, ignored listing per whitelist
Feb 28 07:30:59.493: INFO: namespace e2e-tests-downward-api-n886h deletion completed in 8.197675316s

• [SLOW TEST:12.827 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:30:59.493: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-vc8lm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 07:31:01.760: INFO: Waiting up to 5m0s for pod "downward-api-cc7eeb1a-3b2a-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-vc8lm" to be "success or failure"
Feb 28 07:31:01.813: INFO: Pod "downward-api-cc7eeb1a-3b2a-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.462962ms
Feb 28 07:31:03.867: INFO: Pod "downward-api-cc7eeb1a-3b2a-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.10743136s
STEP: Saw pod success
Feb 28 07:31:03.867: INFO: Pod "downward-api-cc7eeb1a-3b2a-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:31:03.921: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downward-api-cc7eeb1a-3b2a-11e9-8ad6-42906abdb246 container dapi-container: <nil>
STEP: delete the pod
Feb 28 07:31:04.040: INFO: Waiting for pod downward-api-cc7eeb1a-3b2a-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:31:04.093: INFO: Pod downward-api-cc7eeb1a-3b2a-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:31:04.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vc8lm" for this suite.
Feb 28 07:31:10.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:31:12.238: INFO: namespace: e2e-tests-downward-api-vc8lm, resource: bindings, ignored listing per whitelist
Feb 28 07:31:12.345: INFO: namespace e2e-tests-downward-api-vc8lm deletion completed in 8.198067692s

• [SLOW TEST:12.852 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:31:12.345: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-zvx5v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:31:14.515: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d419519a-3b2a-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-zvx5v" to be "success or failure"
Feb 28 07:31:14.569: INFO: Pod "downwardapi-volume-d419519a-3b2a-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.983571ms
Feb 28 07:31:16.623: INFO: Pod "downwardapi-volume-d419519a-3b2a-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107617331s
STEP: Saw pod success
Feb 28 07:31:16.623: INFO: Pod "downwardapi-volume-d419519a-3b2a-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:31:16.686: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-d419519a-3b2a-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 07:31:16.800: INFO: Waiting for pod downwardapi-volume-d419519a-3b2a-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:31:16.855: INFO: Pod downwardapi-volume-d419519a-3b2a-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:31:16.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zvx5v" for this suite.
Feb 28 07:31:23.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:31:24.521: INFO: namespace: e2e-tests-downward-api-zvx5v, resource: bindings, ignored listing per whitelist
Feb 28 07:31:25.106: INFO: namespace e2e-tests-downward-api-zvx5v deletion completed in 8.196407199s

• [SLOW TEST:12.761 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:31:25.106: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-p6sps
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 07:31:27.254: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-p6sps'
Feb 28 07:31:27.950: INFO: stderr: ""
Feb 28 07:31:27.950: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Feb 28 07:31:28.003: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-p6sps'
Feb 28 07:31:30.210: INFO: stderr: ""
Feb 28 07:31:30.210: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:31:30.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-p6sps" for this suite.
Feb 28 07:31:36.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:31:36.968: INFO: namespace: e2e-tests-kubectl-p6sps, resource: bindings, ignored listing per whitelist
Feb 28 07:31:38.477: INFO: namespace e2e-tests-kubectl-p6sps deletion completed in 8.213104851s

• [SLOW TEST:13.371 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:31:38.477: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8t528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 07:31:43.563: INFO: Successfully updated pod "labelsupdatee3b82980-3b2a-11e9-8ad6-42906abdb246"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:31:45.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8t528" for this suite.
Feb 28 07:32:07.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:32:09.344: INFO: namespace: e2e-tests-projected-8t528, resource: bindings, ignored listing per whitelist
Feb 28 07:32:09.932: INFO: namespace e2e-tests-projected-8t528 deletion completed in 24.197561608s

• [SLOW TEST:31.455 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:32:09.932: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-k4wjp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-fkqr
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 07:32:12.321: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fkqr" in namespace "e2e-tests-subpath-k4wjp" to be "success or failure"
Feb 28 07:32:12.375: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Pending", Reason="", readiness=false. Elapsed: 53.923736ms
Feb 28 07:32:14.430: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109174766s
Feb 28 07:32:16.486: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Running", Reason="", readiness=false. Elapsed: 4.164743902s
Feb 28 07:32:18.540: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Running", Reason="", readiness=false. Elapsed: 6.218643992s
Feb 28 07:32:20.594: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Running", Reason="", readiness=false. Elapsed: 8.27303852s
Feb 28 07:32:22.648: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Running", Reason="", readiness=false. Elapsed: 10.326875609s
Feb 28 07:32:24.702: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Running", Reason="", readiness=false. Elapsed: 12.381099917s
Feb 28 07:32:26.756: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Running", Reason="", readiness=false. Elapsed: 14.435220134s
Feb 28 07:32:28.811: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Running", Reason="", readiness=false. Elapsed: 16.4905466s
Feb 28 07:32:30.868: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Running", Reason="", readiness=false. Elapsed: 18.54670665s
Feb 28 07:32:32.922: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Running", Reason="", readiness=false. Elapsed: 20.601221902s
Feb 28 07:32:34.976: INFO: Pod "pod-subpath-test-configmap-fkqr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.65547714s
STEP: Saw pod success
Feb 28 07:32:34.976: INFO: Pod "pod-subpath-test-configmap-fkqr" satisfied condition "success or failure"
Feb 28 07:32:35.030: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-subpath-test-configmap-fkqr container test-container-subpath-configmap-fkqr: <nil>
STEP: delete the pod
Feb 28 07:32:35.147: INFO: Waiting for pod pod-subpath-test-configmap-fkqr to disappear
Feb 28 07:32:35.201: INFO: Pod pod-subpath-test-configmap-fkqr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fkqr
Feb 28 07:32:35.201: INFO: Deleting pod "pod-subpath-test-configmap-fkqr" in namespace "e2e-tests-subpath-k4wjp"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:32:35.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-k4wjp" for this suite.
Feb 28 07:32:41.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:32:42.704: INFO: namespace: e2e-tests-subpath-k4wjp, resource: bindings, ignored listing per whitelist
Feb 28 07:32:43.635: INFO: namespace e2e-tests-subpath-k4wjp deletion completed in 8.323256546s

• [SLOW TEST:33.703 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:32:43.635: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-l7mbj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 28 07:32:52.346: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 07:32:52.407: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 07:32:54.407: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 07:32:54.460: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 07:32:56.407: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 07:32:56.461: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:32:56.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-l7mbj" for this suite.
Feb 28 07:33:18.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:33:19.432: INFO: namespace: e2e-tests-container-lifecycle-hook-l7mbj, resource: bindings, ignored listing per whitelist
Feb 28 07:33:20.841: INFO: namespace e2e-tests-container-lifecycle-hook-l7mbj deletion completed in 24.266214269s

• [SLOW TEST:37.206 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:33:20.841: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-nzfdl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 07:33:23.547: INFO: Number of nodes with available pods: 0
Feb 28 07:33:23.547: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:24.655: INFO: Number of nodes with available pods: 0
Feb 28 07:33:24.655: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:25.655: INFO: Number of nodes with available pods: 0
Feb 28 07:33:25.655: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:26.662: INFO: Number of nodes with available pods: 2
Feb 28 07:33:26.662: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 28 07:33:26.934: INFO: Number of nodes with available pods: 1
Feb 28 07:33:26.934: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:28.046: INFO: Number of nodes with available pods: 1
Feb 28 07:33:28.046: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:29.045: INFO: Number of nodes with available pods: 1
Feb 28 07:33:29.045: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:30.043: INFO: Number of nodes with available pods: 1
Feb 28 07:33:30.043: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:31.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:31.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:32.043: INFO: Number of nodes with available pods: 1
Feb 28 07:33:32.043: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:33.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:33.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:34.043: INFO: Number of nodes with available pods: 1
Feb 28 07:33:34.043: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:35.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:35.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:36.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:36.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:37.056: INFO: Number of nodes with available pods: 1
Feb 28 07:33:37.056: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:38.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:38.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:39.041: INFO: Number of nodes with available pods: 1
Feb 28 07:33:39.041: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:40.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:40.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:41.043: INFO: Number of nodes with available pods: 1
Feb 28 07:33:41.043: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:42.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:42.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:43.047: INFO: Number of nodes with available pods: 1
Feb 28 07:33:43.047: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:44.044: INFO: Number of nodes with available pods: 1
Feb 28 07:33:44.044: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:45.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:45.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:46.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:46.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:47.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:47.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:48.043: INFO: Number of nodes with available pods: 1
Feb 28 07:33:48.043: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:49.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:49.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:50.046: INFO: Number of nodes with available pods: 1
Feb 28 07:33:50.046: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:51.041: INFO: Number of nodes with available pods: 1
Feb 28 07:33:51.041: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:52.045: INFO: Number of nodes with available pods: 1
Feb 28 07:33:52.045: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:53.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:53.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:54.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:54.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:55.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:55.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:56.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:56.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:57.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:57.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:58.042: INFO: Number of nodes with available pods: 1
Feb 28 07:33:58.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:33:59.041: INFO: Number of nodes with available pods: 1
Feb 28 07:33:59.041: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:34:00.047: INFO: Number of nodes with available pods: 1
Feb 28 07:34:00.047: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:34:01.041: INFO: Number of nodes with available pods: 1
Feb 28 07:34:01.041: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:34:02.055: INFO: Number of nodes with available pods: 1
Feb 28 07:34:02.055: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:34:03.041: INFO: Number of nodes with available pods: 1
Feb 28 07:34:03.041: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:34:04.045: INFO: Number of nodes with available pods: 1
Feb 28 07:34:04.045: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:34:05.045: INFO: Number of nodes with available pods: 1
Feb 28 07:34:05.045: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:34:06.042: INFO: Number of nodes with available pods: 1
Feb 28 07:34:06.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:34:07.042: INFO: Number of nodes with available pods: 1
Feb 28 07:34:07.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:34:08.042: INFO: Number of nodes with available pods: 1
Feb 28 07:34:08.042: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:34:09.041: INFO: Number of nodes with available pods: 2
Feb 28 07:34:09.041: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-nzfdl, will wait for the garbage collector to delete the pods
Feb 28 07:34:09.319: INFO: Deleting DaemonSet.extensions daemon-set took: 55.18855ms
Feb 28 07:34:09.419: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.244675ms
Feb 28 07:34:47.872: INFO: Number of nodes with available pods: 0
Feb 28 07:34:47.872: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 07:34:47.927: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-nzfdl/daemonsets","resourceVersion":"3732"},"items":null}

Feb 28 07:34:47.981: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-nzfdl/pods","resourceVersion":"3732"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:34:48.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-nzfdl" for this suite.
Feb 28 07:34:54.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:34:56.061: INFO: namespace: e2e-tests-daemonsets-nzfdl, resource: bindings, ignored listing per whitelist
Feb 28 07:34:56.379: INFO: namespace e2e-tests-daemonsets-nzfdl deletion completed in 8.182330551s

• [SLOW TEST:95.537 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:34:56.379: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-n2gt2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 28 07:35:02.984: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-n2gt2 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:35:02.984: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:35:03.833: INFO: Exec stderr: ""
Feb 28 07:35:03.833: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-n2gt2 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:35:03.833: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:35:04.511: INFO: Exec stderr: ""
Feb 28 07:35:04.511: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-n2gt2 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:35:04.511: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:35:05.153: INFO: Exec stderr: ""
Feb 28 07:35:05.153: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-n2gt2 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:35:05.153: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:35:05.835: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 28 07:35:05.835: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-n2gt2 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:35:05.835: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:35:06.505: INFO: Exec stderr: ""
Feb 28 07:35:06.505: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-n2gt2 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:35:06.505: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:35:07.218: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 28 07:35:07.218: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-n2gt2 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:35:07.218: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:35:07.887: INFO: Exec stderr: ""
Feb 28 07:35:07.887: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-n2gt2 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:35:07.887: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:35:08.551: INFO: Exec stderr: ""
Feb 28 07:35:08.551: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-n2gt2 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:35:08.551: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:35:09.207: INFO: Exec stderr: ""
Feb 28 07:35:09.207: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-n2gt2 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:35:09.208: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:35:09.953: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:35:09.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-n2gt2" for this suite.
Feb 28 07:36:02.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:36:04.041: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-n2gt2, resource: bindings, ignored listing per whitelist
Feb 28 07:36:04.220: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-n2gt2 deletion completed in 54.208090807s

• [SLOW TEST:67.841 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:36:04.220: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-7jd46
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0228 07:36:46.869160   30448 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 07:36:46.882: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:36:46.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-7jd46" for this suite.
Feb 28 07:36:53.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:36:54.649: INFO: namespace: e2e-tests-gc-7jd46, resource: bindings, ignored listing per whitelist
Feb 28 07:36:55.147: INFO: namespace e2e-tests-gc-7jd46 deletion completed in 8.210186033s

• [SLOW TEST:50.927 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:36:55.147: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-jzwtq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 28 07:37:01.738: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:37:01.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-jzwtq" for this suite.
Feb 28 07:37:24.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:37:25.344: INFO: namespace: e2e-tests-replicaset-jzwtq, resource: bindings, ignored listing per whitelist
Feb 28 07:37:26.213: INFO: namespace e2e-tests-replicaset-jzwtq deletion completed in 24.260286638s

• [SLOW TEST:31.067 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:37:26.231: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-gvvj5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:37:28.620: INFO: (0) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 57.595438ms)
Feb 28 07:37:28.675: INFO: (1) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.823397ms)
Feb 28 07:37:28.827: INFO: (2) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.59421ms)
Feb 28 07:37:28.883: INFO: (3) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.331994ms)
Feb 28 07:37:28.938: INFO: (4) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 54.982396ms)
Feb 28 07:37:28.994: INFO: (5) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.986184ms)
Feb 28 07:37:29.049: INFO: (6) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.161194ms)
Feb 28 07:37:29.104: INFO: (7) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.159165ms)
Feb 28 07:37:29.160: INFO: (8) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.403774ms)
Feb 28 07:37:29.215: INFO: (9) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.56858ms)
Feb 28 07:37:29.270: INFO: (10) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.088305ms)
Feb 28 07:37:29.326: INFO: (11) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.460695ms)
Feb 28 07:37:29.384: INFO: (12) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 57.843372ms)
Feb 28 07:37:29.439: INFO: (13) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.12039ms)
Feb 28 07:37:29.494: INFO: (14) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.224793ms)
Feb 28 07:37:29.550: INFO: (15) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.393769ms)
Feb 28 07:37:29.606: INFO: (16) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.925195ms)
Feb 28 07:37:29.661: INFO: (17) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.722973ms)
Feb 28 07:37:29.717: INFO: (18) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.64268ms)
Feb 28 07:37:29.774: INFO: (19) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 56.805602ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:37:29.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-gvvj5" for this suite.
Feb 28 07:37:36.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:37:37.181: INFO: namespace: e2e-tests-proxy-gvvj5, resource: bindings, ignored listing per whitelist
Feb 28 07:37:38.054: INFO: namespace e2e-tests-proxy-gvvj5 deletion completed in 8.223974441s

• [SLOW TEST:11.824 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:37:38.054: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-knx5p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-knx5p
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-knx5p
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-knx5p
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-knx5p
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-knx5p
Feb 28 07:37:42.561: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-knx5p, name: ss-0, uid: bb6666e2-3b2b-11e9-ac40-06fd5a6cf138, status phase: Pending. Waiting for statefulset controller to delete.
Feb 28 07:37:42.562: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-knx5p, name: ss-0, uid: bb6666e2-3b2b-11e9-ac40-06fd5a6cf138, status phase: Failed. Waiting for statefulset controller to delete.
Feb 28 07:37:42.604: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-knx5p, name: ss-0, uid: bb6666e2-3b2b-11e9-ac40-06fd5a6cf138, status phase: Failed. Waiting for statefulset controller to delete.
Feb 28 07:37:42.605: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-knx5p
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-knx5p
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-knx5p and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 07:37:44.767: INFO: Deleting all statefulset in ns e2e-tests-statefulset-knx5p
Feb 28 07:37:44.843: INFO: Scaling statefulset ss to 0
Feb 28 07:37:55.061: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:37:55.132: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:37:55.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-knx5p" for this suite.
Feb 28 07:38:01.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:38:03.210: INFO: namespace: e2e-tests-statefulset-knx5p, resource: bindings, ignored listing per whitelist
Feb 28 07:38:03.688: INFO: namespace e2e-tests-statefulset-knx5p deletion completed in 8.340454314s

• [SLOW TEST:25.634 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:38:03.688: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-q7bw8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:38:06.124: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 07:38:06.286: INFO: Number of nodes with available pods: 0
Feb 28 07:38:06.287: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:38:07.394: INFO: Number of nodes with available pods: 0
Feb 28 07:38:07.394: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:38:08.394: INFO: Number of nodes with available pods: 2
Feb 28 07:38:08.394: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 28 07:38:08.726: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:08.726: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:09.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:09.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:10.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:10.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:11.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:11.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:12.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:12.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:13.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:13.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:14.836: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:14.836: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:15.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:15.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:16.836: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:16.836: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:17.836: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:17.836: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:18.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:18.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:19.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:19.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:20.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:20.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:21.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:21.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:22.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:22.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:23.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:23.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:24.861: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:24.861: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:25.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:25.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:26.842: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:26.842: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:27.837: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:27.837: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:28.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:28.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:29.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:29.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:30.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:30.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:31.837: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:31.837: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:32.837: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:32.837: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:33.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:33.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:34.836: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:34.836: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:35.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:35.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:36.841: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:36.841: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:37.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:37.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:38.837: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:38.837: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:39.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:39.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:40.834: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:40.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:41.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:41.835: INFO: Pod daemon-set-hlfn5 is not available
Feb 28 07:38:41.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:42.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:42.835: INFO: Pod daemon-set-hlfn5 is not available
Feb 28 07:38:42.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:43.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:43.835: INFO: Pod daemon-set-hlfn5 is not available
Feb 28 07:38:43.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:44.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:44.835: INFO: Pod daemon-set-hlfn5 is not available
Feb 28 07:38:44.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:45.835: INFO: Wrong image for pod: daemon-set-hlfn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:45.835: INFO: Pod daemon-set-hlfn5 is not available
Feb 28 07:38:45.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:46.835: INFO: Pod daemon-set-kggfz is not available
Feb 28 07:38:46.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:47.834: INFO: Pod daemon-set-kggfz is not available
Feb 28 07:38:47.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:48.834: INFO: Pod daemon-set-kggfz is not available
Feb 28 07:38:48.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:49.839: INFO: Pod daemon-set-kggfz is not available
Feb 28 07:38:49.839: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:50.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:51.836: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:52.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:53.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:54.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:55.837: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:56.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:57.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:58.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:38:59.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:00.838: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:01.837: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:02.836: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:03.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:04.840: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:05.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:06.842: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:07.838: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:08.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:09.840: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:10.839: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:11.848: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:12.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:13.840: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:14.836: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:15.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:16.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:17.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:18.834: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:19.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:20.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:21.835: INFO: Wrong image for pod: daemon-set-tdr96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 07:39:21.835: INFO: Pod daemon-set-tdr96 is not available
Feb 28 07:39:22.836: INFO: Pod daemon-set-xnnlh is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 28 07:39:22.997: INFO: Number of nodes with available pods: 1
Feb 28 07:39:22.997: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:39:24.116: INFO: Number of nodes with available pods: 1
Feb 28 07:39:24.116: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:39:25.104: INFO: Number of nodes with available pods: 1
Feb 28 07:39:25.104: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:39:26.105: INFO: Number of nodes with available pods: 2
Feb 28 07:39:26.105: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-q7bw8, will wait for the garbage collector to delete the pods
Feb 28 07:39:26.583: INFO: Deleting DaemonSet.extensions daemon-set took: 54.785224ms
Feb 28 07:39:26.696: INFO: Terminating DaemonSet.extensions daemon-set pods took: 112.939978ms
Feb 28 07:39:37.550: INFO: Number of nodes with available pods: 0
Feb 28 07:39:37.550: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 07:39:37.603: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-q7bw8/daemonsets","resourceVersion":"4653"},"items":null}

Feb 28 07:39:37.675: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-q7bw8/pods","resourceVersion":"4653"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:39:37.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-q7bw8" for this suite.
Feb 28 07:39:44.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:39:44.478: INFO: namespace: e2e-tests-daemonsets-q7bw8, resource: bindings, ignored listing per whitelist
Feb 28 07:39:46.129: INFO: namespace e2e-tests-daemonsets-q7bw8 deletion completed in 8.23901156s

• [SLOW TEST:102.441 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:39:46.130: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-qttbt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 28 07:39:50.533: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-06598f9e-3b2c-11e9-8ad6-42906abdb246,GenerateName:,Namespace:e2e-tests-events-qttbt,SelfLink:/api/v1/namespaces/e2e-tests-events-qttbt/pods/send-events-06598f9e-3b2c-11e9-8ad6-42906abdb246,UID:065cc4fe-3b2c-11e9-ac40-06fd5a6cf138,ResourceVersion:4692,Generation:0,CreationTimestamp:2019-02-28 07:39:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 264340753,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.32/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fz9vd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fz9vd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-fz9vd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001354c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001354cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:39:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:39:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:39:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:39:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:100.96.1.32,StartTime:2019-02-28 07:39:48 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-28 07:39:49 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://142600c087f46cae962fb0096d2cded3d9ab7bc7f2652367b88bebda02b3cbee}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 28 07:39:52.589: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 28 07:39:54.644: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:39:54.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-qttbt" for this suite.
Feb 28 07:40:40.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:40:42.819: INFO: namespace: e2e-tests-events-qttbt, resource: bindings, ignored listing per whitelist
Feb 28 07:40:43.035: INFO: namespace e2e-tests-events-qttbt deletion completed in 48.275748301s

• [SLOW TEST:56.905 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:40:43.035: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xt6dx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-28424e56-3b2c-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 07:40:45.263: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-284a7ef2-3b2c-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-xt6dx" to be "success or failure"
Feb 28 07:40:45.317: INFO: Pod "pod-projected-secrets-284a7ef2-3b2c-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.603911ms
Feb 28 07:40:47.371: INFO: Pod "pod-projected-secrets-284a7ef2-3b2c-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107715814s
STEP: Saw pod success
Feb 28 07:40:47.371: INFO: Pod "pod-projected-secrets-284a7ef2-3b2c-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:40:47.424: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-secrets-284a7ef2-3b2c-11e9-8ad6-42906abdb246 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:40:47.542: INFO: Waiting for pod pod-projected-secrets-284a7ef2-3b2c-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:40:47.595: INFO: Pod pod-projected-secrets-284a7ef2-3b2c-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:40:47.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xt6dx" for this suite.
Feb 28 07:40:53.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:40:55.470: INFO: namespace: e2e-tests-projected-xt6dx, resource: bindings, ignored listing per whitelist
Feb 28 07:40:55.843: INFO: namespace e2e-tests-projected-xt6dx deletion completed in 8.194082684s

• [SLOW TEST:12.808 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:40:55.844: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-v6rwg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-j4zkc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-rfrkz
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:41:05.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-v6rwg" for this suite.
Feb 28 07:41:11.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:41:11.448: INFO: namespace: e2e-tests-namespaces-v6rwg, resource: bindings, ignored listing per whitelist
Feb 28 07:41:13.370: INFO: namespace e2e-tests-namespaces-v6rwg deletion completed in 8.26195192s
STEP: Destroying namespace "e2e-tests-nsdeletetest-j4zkc" for this suite.
Feb 28 07:41:13.423: INFO: Namespace e2e-tests-nsdeletetest-j4zkc was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-rfrkz" for this suite.
Feb 28 07:41:19.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:41:20.170: INFO: namespace: e2e-tests-nsdeletetest-rfrkz, resource: bindings, ignored listing per whitelist
Feb 28 07:41:21.610: INFO: namespace e2e-tests-nsdeletetest-rfrkz deletion completed in 8.186813433s

• [SLOW TEST:25.767 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:41:21.611: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-8s2sf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0228 07:41:34.545577   30448 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 07:41:34.545: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:41:34.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-8s2sf" for this suite.
Feb 28 07:41:40.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:41:41.349: INFO: namespace: e2e-tests-gc-8s2sf, resource: bindings, ignored listing per whitelist
Feb 28 07:41:42.807: INFO: namespace e2e-tests-gc-8s2sf deletion completed in 8.205622807s

• [SLOW TEST:21.196 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:41:42.812: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-w7n5s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-4be6d767-3b2c-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 07:41:45.065: INFO: Waiting up to 5m0s for pod "pod-secrets-4bef2639-3b2c-11e9-8ad6-42906abdb246" in namespace "e2e-tests-secrets-w7n5s" to be "success or failure"
Feb 28 07:41:45.119: INFO: Pod "pod-secrets-4bef2639-3b2c-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.812575ms
Feb 28 07:41:47.174: INFO: Pod "pod-secrets-4bef2639-3b2c-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108508445s
STEP: Saw pod success
Feb 28 07:41:47.174: INFO: Pod "pod-secrets-4bef2639-3b2c-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:41:47.227: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-secrets-4bef2639-3b2c-11e9-8ad6-42906abdb246 container secret-env-test: <nil>
STEP: delete the pod
Feb 28 07:41:47.341: INFO: Waiting for pod pod-secrets-4bef2639-3b2c-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:41:47.395: INFO: Pod pod-secrets-4bef2639-3b2c-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:41:47.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-w7n5s" for this suite.
Feb 28 07:41:53.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:41:54.736: INFO: namespace: e2e-tests-secrets-w7n5s, resource: bindings, ignored listing per whitelist
Feb 28 07:41:55.689: INFO: namespace e2e-tests-secrets-w7n5s deletion completed in 8.240496343s

• [SLOW TEST:12.879 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:41:55.690: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-bqhm9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 28 07:41:58.624: INFO: created pod pod-service-account-defaultsa
Feb 28 07:41:58.624: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 28 07:41:58.678: INFO: created pod pod-service-account-mountsa
Feb 28 07:41:58.678: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 28 07:41:58.732: INFO: created pod pod-service-account-nomountsa
Feb 28 07:41:58.732: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 28 07:41:58.786: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 28 07:41:58.786: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 28 07:41:58.840: INFO: created pod pod-service-account-mountsa-mountspec
Feb 28 07:41:58.840: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 28 07:41:58.894: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 28 07:41:58.894: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 28 07:41:58.948: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 28 07:41:58.948: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 28 07:41:59.002: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 28 07:41:59.002: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 28 07:41:59.057: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 28 07:41:59.057: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:41:59.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-bqhm9" for this suite.
Feb 28 07:42:05.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:42:05.436: INFO: namespace: e2e-tests-svcaccounts-bqhm9, resource: bindings, ignored listing per whitelist
Feb 28 07:42:07.320: INFO: namespace e2e-tests-svcaccounts-bqhm9 deletion completed in 8.208585926s

• [SLOW TEST:11.630 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:42:07.320: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-hh5lk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 07:42:09.508: INFO: Waiting up to 5m0s for pod "downward-api-5a81191e-3b2c-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-hh5lk" to be "success or failure"
Feb 28 07:42:09.561: INFO: Pod "downward-api-5a81191e-3b2c-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.356064ms
Feb 28 07:42:11.621: INFO: Pod "downward-api-5a81191e-3b2c-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.113377319s
STEP: Saw pod success
Feb 28 07:42:11.621: INFO: Pod "downward-api-5a81191e-3b2c-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:42:11.680: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downward-api-5a81191e-3b2c-11e9-8ad6-42906abdb246 container dapi-container: <nil>
STEP: delete the pod
Feb 28 07:42:11.806: INFO: Waiting for pod downward-api-5a81191e-3b2c-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:42:11.866: INFO: Pod downward-api-5a81191e-3b2c-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:42:11.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hh5lk" for this suite.
Feb 28 07:42:18.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:42:20.010: INFO: namespace: e2e-tests-downward-api-hh5lk, resource: bindings, ignored listing per whitelist
Feb 28 07:42:20.117: INFO: namespace e2e-tests-downward-api-hh5lk deletion completed in 8.196105895s

• [SLOW TEST:12.797 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:42:20.117: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-zgsdv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:43:22.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-zgsdv" for this suite.
Feb 28 07:43:46.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:43:47.246: INFO: namespace: e2e-tests-container-probe-zgsdv, resource: bindings, ignored listing per whitelist
Feb 28 07:43:48.641: INFO: namespace e2e-tests-container-probe-zgsdv deletion completed in 26.209460878s

• [SLOW TEST:88.524 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:43:48.641: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-cgjxm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0228 07:44:21.186346   30448 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 07:44:21.186: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:44:21.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-cgjxm" for this suite.
Feb 28 07:44:27.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:44:28.797: INFO: namespace: e2e-tests-gc-cgjxm, resource: bindings, ignored listing per whitelist
Feb 28 07:44:29.452: INFO: namespace e2e-tests-gc-cgjxm deletion completed in 8.211956918s

• [SLOW TEST:40.811 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:44:29.452: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-wkmkk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 28 07:44:31.707: INFO: Waiting up to 5m0s for pod "var-expansion-af42c5a0-3b2c-11e9-8ad6-42906abdb246" in namespace "e2e-tests-var-expansion-wkmkk" to be "success or failure"
Feb 28 07:44:31.760: INFO: Pod "var-expansion-af42c5a0-3b2c-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.453795ms
Feb 28 07:44:33.814: INFO: Pod "var-expansion-af42c5a0-3b2c-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107093455s
STEP: Saw pod success
Feb 28 07:44:33.814: INFO: Pod "var-expansion-af42c5a0-3b2c-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:44:33.867: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod var-expansion-af42c5a0-3b2c-11e9-8ad6-42906abdb246 container dapi-container: <nil>
STEP: delete the pod
Feb 28 07:44:33.981: INFO: Waiting for pod var-expansion-af42c5a0-3b2c-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:44:34.036: INFO: Pod var-expansion-af42c5a0-3b2c-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:44:34.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-wkmkk" for this suite.
Feb 28 07:44:40.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:44:40.680: INFO: namespace: e2e-tests-var-expansion-wkmkk, resource: bindings, ignored listing per whitelist
Feb 28 07:44:42.285: INFO: namespace e2e-tests-var-expansion-wkmkk deletion completed in 8.194528457s

• [SLOW TEST:12.832 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:44:42.285: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-7h2dx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 28 07:44:44.511: INFO: Waiting up to 5m0s for pod "pod-b6e47034-3b2c-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-7h2dx" to be "success or failure"
Feb 28 07:44:44.565: INFO: Pod "pod-b6e47034-3b2c-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.747426ms
Feb 28 07:44:46.619: INFO: Pod "pod-b6e47034-3b2c-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10809168s
Feb 28 07:44:48.673: INFO: Pod "pod-b6e47034-3b2c-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.162315279s
STEP: Saw pod success
Feb 28 07:44:48.673: INFO: Pod "pod-b6e47034-3b2c-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:44:48.727: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-b6e47034-3b2c-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 07:44:48.841: INFO: Waiting for pod pod-b6e47034-3b2c-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:44:48.894: INFO: Pod pod-b6e47034-3b2c-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:44:48.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7h2dx" for this suite.
Feb 28 07:44:55.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:44:56.489: INFO: namespace: e2e-tests-emptydir-7h2dx, resource: bindings, ignored listing per whitelist
Feb 28 07:44:57.189: INFO: namespace e2e-tests-emptydir-7h2dx deletion completed in 8.240457966s

• [SLOW TEST:14.904 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:44:57.190: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-bbtbw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-bbtbw
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 07:44:59.374: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 07:45:24.352: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.54:8080/dial?request=hostName&protocol=http&host=100.96.0.25&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-bbtbw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:45:24.353: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:45:25.124: INFO: Waiting for endpoints: map[]
Feb 28 07:45:25.178: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.54:8080/dial?request=hostName&protocol=http&host=100.96.1.53&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-bbtbw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:45:25.178: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:45:25.838: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:45:25.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-bbtbw" for this suite.
Feb 28 07:45:48.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:45:48.216: INFO: namespace: e2e-tests-pod-network-test-bbtbw, resource: bindings, ignored listing per whitelist
Feb 28 07:45:50.091: INFO: namespace e2e-tests-pod-network-test-bbtbw deletion completed in 24.19775674s

• [SLOW TEST:52.901 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:45:50.091: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-w8fq8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-df4f8805-3b2c-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 07:45:52.374: INFO: Waiting up to 5m0s for pod "pod-configmaps-df57b580-3b2c-11e9-8ad6-42906abdb246" in namespace "e2e-tests-configmap-w8fq8" to be "success or failure"
Feb 28 07:45:52.427: INFO: Pod "pod-configmaps-df57b580-3b2c-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.434529ms
Feb 28 07:45:54.482: INFO: Pod "pod-configmaps-df57b580-3b2c-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107554426s
STEP: Saw pod success
Feb 28 07:45:54.482: INFO: Pod "pod-configmaps-df57b580-3b2c-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:45:54.536: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-configmaps-df57b580-3b2c-11e9-8ad6-42906abdb246 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:45:54.654: INFO: Waiting for pod pod-configmaps-df57b580-3b2c-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:45:54.707: INFO: Pod pod-configmaps-df57b580-3b2c-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:45:54.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-w8fq8" for this suite.
Feb 28 07:46:00.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:46:02.122: INFO: namespace: e2e-tests-configmap-w8fq8, resource: bindings, ignored listing per whitelist
Feb 28 07:46:03.041: INFO: namespace e2e-tests-configmap-w8fq8 deletion completed in 8.279843205s

• [SLOW TEST:12.950 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:46:03.041: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-bvwmz
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 28 07:46:05.233: INFO: Waiting up to 5m0s for pod "pod-e701f7e4-3b2c-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-bvwmz" to be "success or failure"
Feb 28 07:46:05.290: INFO: Pod "pod-e701f7e4-3b2c-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 56.87651ms
Feb 28 07:46:07.345: INFO: Pod "pod-e701f7e4-3b2c-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.111052899s
STEP: Saw pod success
Feb 28 07:46:07.345: INFO: Pod "pod-e701f7e4-3b2c-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:46:07.398: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-e701f7e4-3b2c-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 07:46:07.516: INFO: Waiting for pod pod-e701f7e4-3b2c-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:46:07.570: INFO: Pod pod-e701f7e4-3b2c-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:46:07.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bvwmz" for this suite.
Feb 28 07:46:13.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:46:14.055: INFO: namespace: e2e-tests-emptydir-bvwmz, resource: bindings, ignored listing per whitelist
Feb 28 07:46:15.830: INFO: namespace e2e-tests-emptydir-bvwmz deletion completed in 8.206283893s

• [SLOW TEST:12.789 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:46:15.832: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-4l8lq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 28 07:46:18.030: INFO: Waiting up to 5m0s for pod "client-containers-eea2a45f-3b2c-11e9-8ad6-42906abdb246" in namespace "e2e-tests-containers-4l8lq" to be "success or failure"
Feb 28 07:46:18.084: INFO: Pod "client-containers-eea2a45f-3b2c-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 54.145875ms
Feb 28 07:46:20.138: INFO: Pod "client-containers-eea2a45f-3b2c-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10806038s
Feb 28 07:46:22.192: INFO: Pod "client-containers-eea2a45f-3b2c-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.162279445s
STEP: Saw pod success
Feb 28 07:46:22.192: INFO: Pod "client-containers-eea2a45f-3b2c-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:46:22.246: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod client-containers-eea2a45f-3b2c-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 07:46:22.363: INFO: Waiting for pod client-containers-eea2a45f-3b2c-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:46:22.416: INFO: Pod client-containers-eea2a45f-3b2c-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:46:22.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-4l8lq" for this suite.
Feb 28 07:46:28.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:46:30.466: INFO: namespace: e2e-tests-containers-4l8lq, resource: bindings, ignored listing per whitelist
Feb 28 07:46:30.680: INFO: namespace e2e-tests-containers-4l8lq deletion completed in 8.210412511s

• [SLOW TEST:14.849 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:46:30.681: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-9v76h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-9v76h
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-9v76h
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-9v76h
Feb 28 07:46:33.014: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 28 07:46:43.070: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 28 07:46:43.128: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-9v76h ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:46:44.282: INFO: stderr: ""
Feb 28 07:46:44.282: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:46:44.283: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:46:44.338: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 28 07:46:54.394: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:46:54.394: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:46:54.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999225s
Feb 28 07:46:55.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.945182975s
Feb 28 07:46:56.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.889865963s
Feb 28 07:46:57.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.835508188s
Feb 28 07:46:58.831: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.781276778s
Feb 28 07:46:59.890: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.725142354s
Feb 28 07:47:00.944: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.666547527s
Feb 28 07:47:01.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.612104963s
Feb 28 07:47:03.054: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.557011868s
Feb 28 07:47:04.108: INFO: Verifying statefulset ss doesn't scale past 3 for another 502.505777ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-9v76h
Feb 28 07:47:05.163: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-9v76h ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:47:06.130: INFO: stderr: ""
Feb 28 07:47:06.130: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 07:47:06.130: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 07:47:06.130: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-9v76h ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:47:07.098: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 28 07:47:07.098: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 07:47:07.098: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 07:47:07.099: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-9v76h ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:47:08.050: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 28 07:47:08.050: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 07:47:08.050: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 07:47:08.104: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:47:08.105: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:47:08.105: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 28 07:47:08.158: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-9v76h ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:47:09.144: INFO: stderr: ""
Feb 28 07:47:09.144: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:47:09.144: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:47:09.144: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-9v76h ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:47:10.289: INFO: stderr: ""
Feb 28 07:47:10.289: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:47:10.289: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:47:10.289: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-9v76h ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:47:11.267: INFO: stderr: ""
Feb 28 07:47:11.267: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:47:11.267: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:47:11.267: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:47:11.321: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 28 07:47:21.429: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:47:21.429: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:47:21.429: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:47:21.591: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 28 07:47:21.591: INFO: ss-0  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  }]
Feb 28 07:47:21.591: INFO: ss-1  ip-10-250-20-37.eu-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  }]
Feb 28 07:47:21.591: INFO: ss-2  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  }]
Feb 28 07:47:21.591: INFO: 
Feb 28 07:47:21.591: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:47:22.646: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 28 07:47:22.646: INFO: ss-0  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  }]
Feb 28 07:47:22.646: INFO: ss-1  ip-10-250-20-37.eu-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  }]
Feb 28 07:47:22.646: INFO: ss-2  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  }]
Feb 28 07:47:22.646: INFO: 
Feb 28 07:47:22.646: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:47:23.701: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 28 07:47:23.702: INFO: ss-0  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  }]
Feb 28 07:47:23.702: INFO: ss-1  ip-10-250-20-37.eu-west-1.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  }]
Feb 28 07:47:23.702: INFO: ss-2  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  }]
Feb 28 07:47:23.702: INFO: 
Feb 28 07:47:23.702: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:47:24.757: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 28 07:47:24.757: INFO: ss-0  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  }]
Feb 28 07:47:24.757: INFO: ss-1  ip-10-250-20-37.eu-west-1.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  }]
Feb 28 07:47:24.757: INFO: ss-2  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  }]
Feb 28 07:47:24.757: INFO: 
Feb 28 07:47:24.757: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:47:25.811: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 28 07:47:25.812: INFO: ss-0  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  }]
Feb 28 07:47:25.812: INFO: ss-1  ip-10-250-20-37.eu-west-1.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  }]
Feb 28 07:47:25.812: INFO: ss-2  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  }]
Feb 28 07:47:25.812: INFO: 
Feb 28 07:47:25.812: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:47:26.866: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 28 07:47:26.866: INFO: ss-0  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:32 +0000 UTC  }]
Feb 28 07:47:26.866: INFO: ss-2  ip-10-250-15-222.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:46:54 +0000 UTC  }]
Feb 28 07:47:26.867: INFO: 
Feb 28 07:47:26.867: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 07:47:27.928: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.669979454s
Feb 28 07:47:28.983: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.608491228s
Feb 28 07:47:30.037: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.55392008s
Feb 28 07:47:31.091: INFO: Verifying statefulset ss doesn't scale past 0 for another 499.100054ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-9v76h
Feb 28 07:47:32.146: INFO: Scaling statefulset ss to 0
Feb 28 07:47:32.308: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 07:47:32.361: INFO: Deleting all statefulset in ns e2e-tests-statefulset-9v76h
Feb 28 07:47:32.415: INFO: Scaling statefulset ss to 0
Feb 28 07:47:32.576: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:47:32.629: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:47:32.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-9v76h" for this suite.
Feb 28 07:47:39.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:47:39.554: INFO: namespace: e2e-tests-statefulset-9v76h, resource: bindings, ignored listing per whitelist
Feb 28 07:47:41.048: INFO: namespace e2e-tests-statefulset-9v76h deletion completed in 8.201770561s

• [SLOW TEST:70.367 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:47:41.048: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-nkslh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 07:47:43.256: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-nkslh'
Feb 28 07:47:46.008: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 07:47:46.008: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Feb 28 07:47:46.065: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-nkslh'
Feb 28 07:47:46.452: INFO: stderr: ""
Feb 28 07:47:46.452: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:47:46.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nkslh" for this suite.
Feb 28 07:47:52.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:47:52.929: INFO: namespace: e2e-tests-kubectl-nkslh, resource: bindings, ignored listing per whitelist
Feb 28 07:47:54.751: INFO: namespace e2e-tests-kubectl-nkslh deletion completed in 8.244363895s

• [SLOW TEST:13.703 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:47:54.752: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-9mj92
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-5thpq
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Feb 28 07:48:06.638: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-nrrqg
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:48:24.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-9mj92" for this suite.
Feb 28 07:48:30.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:48:32.396: INFO: namespace: e2e-tests-namespaces-9mj92, resource: bindings, ignored listing per whitelist
Feb 28 07:48:32.450: INFO: namespace e2e-tests-namespaces-9mj92 deletion completed in 8.198719158s
STEP: Destroying namespace "e2e-tests-nsdeletetest-5thpq" for this suite.
Feb 28 07:48:32.503: INFO: Namespace e2e-tests-nsdeletetest-5thpq was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-nrrqg" for this suite.
Feb 28 07:48:38.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:48:39.748: INFO: namespace: e2e-tests-nsdeletetest-nrrqg, resource: bindings, ignored listing per whitelist
Feb 28 07:48:40.709: INFO: namespace e2e-tests-nsdeletetest-nrrqg deletion completed in 8.205903537s

• [SLOW TEST:45.958 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:48:40.710: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-l8klr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-l8klr
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l8klr to expose endpoints map[]
Feb 28 07:48:42.966: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l8klr exposes endpoints map[] (53.570033ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-l8klr
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l8klr to expose endpoints map[pod1:[100]]
Feb 28 07:48:45.350: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l8klr exposes endpoints map[pod1:[100]] (2.327306903s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-l8klr
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l8klr to expose endpoints map[pod1:[100] pod2:[101]]
Feb 28 07:48:49.055: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l8klr exposes endpoints map[pod1:[100] pod2:[101]] (3.649834211s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-l8klr
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l8klr to expose endpoints map[pod2:[101]]
Feb 28 07:48:49.218: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l8klr exposes endpoints map[pod2:[101]] (107.289096ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-l8klr
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l8klr to expose endpoints map[]
Feb 28 07:48:49.331: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l8klr exposes endpoints map[] (54.413387ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:48:49.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-l8klr" for this suite.
Feb 28 07:49:11.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:49:13.434: INFO: namespace: e2e-tests-services-l8klr, resource: bindings, ignored listing per whitelist
Feb 28 07:49:13.648: INFO: namespace e2e-tests-services-l8klr deletion completed in 24.203264427s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:32.938 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:49:13.648: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-lt7lg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 28 07:49:20.260: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:20.314: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:22.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:22.368: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:24.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:24.370: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:26.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:26.369: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:28.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:28.368: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:30.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:30.370: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:32.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:32.368: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:34.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:34.369: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:36.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:36.369: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:38.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:38.369: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:40.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:40.369: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:42.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:42.368: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:44.315: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:44.373: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:46.315: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:46.370: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:49:48.314: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:49:48.369: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:49:48.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-lt7lg" for this suite.
Feb 28 07:50:10.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:50:10.972: INFO: namespace: e2e-tests-container-lifecycle-hook-lt7lg, resource: bindings, ignored listing per whitelist
Feb 28 07:50:12.699: INFO: namespace e2e-tests-container-lifecycle-hook-lt7lg deletion completed in 24.213862042s

• [SLOW TEST:59.050 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:50:12.699: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-p77tt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:50:14.906: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bd319fa-3b2d-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-p77tt" to be "success or failure"
Feb 28 07:50:14.960: INFO: Pod "downwardapi-volume-7bd319fa-3b2d-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.705684ms
Feb 28 07:50:17.015: INFO: Pod "downwardapi-volume-7bd319fa-3b2d-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.10810163s
STEP: Saw pod success
Feb 28 07:50:17.015: INFO: Pod "downwardapi-volume-7bd319fa-3b2d-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:50:17.069: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-7bd319fa-3b2d-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 07:50:17.191: INFO: Waiting for pod downwardapi-volume-7bd319fa-3b2d-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:50:17.251: INFO: Pod downwardapi-volume-7bd319fa-3b2d-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:50:17.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p77tt" for this suite.
Feb 28 07:50:23.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:50:23.914: INFO: namespace: e2e-tests-projected-p77tt, resource: bindings, ignored listing per whitelist
Feb 28 07:50:25.533: INFO: namespace e2e-tests-projected-p77tt deletion completed in 8.220607647s

• [SLOW TEST:12.834 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:50:25.533: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-xsc28
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:50:27.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-xsc28" for this suite.
Feb 28 07:50:50.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:50:52.004: INFO: namespace: e2e-tests-kubelet-test-xsc28, resource: bindings, ignored listing per whitelist
Feb 28 07:50:52.171: INFO: namespace e2e-tests-kubelet-test-xsc28 deletion completed in 24.257640177s

• [SLOW TEST:26.638 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:50:52.172: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-x59hf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 28 07:50:54.614: INFO: Waiting up to 5m0s for pod "var-expansion-937e04de-3b2d-11e9-8ad6-42906abdb246" in namespace "e2e-tests-var-expansion-x59hf" to be "success or failure"
Feb 28 07:50:54.668: INFO: Pod "var-expansion-937e04de-3b2d-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 54.139917ms
Feb 28 07:50:56.723: INFO: Pod "var-expansion-937e04de-3b2d-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.109373334s
STEP: Saw pod success
Feb 28 07:50:56.724: INFO: Pod "var-expansion-937e04de-3b2d-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:50:56.777: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod var-expansion-937e04de-3b2d-11e9-8ad6-42906abdb246 container dapi-container: <nil>
STEP: delete the pod
Feb 28 07:50:56.897: INFO: Waiting for pod var-expansion-937e04de-3b2d-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:50:56.950: INFO: Pod var-expansion-937e04de-3b2d-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:50:56.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-x59hf" for this suite.
Feb 28 07:51:03.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:51:04.830: INFO: namespace: e2e-tests-var-expansion-x59hf, resource: bindings, ignored listing per whitelist
Feb 28 07:51:05.205: INFO: namespace e2e-tests-var-expansion-x59hf deletion completed in 8.200828875s

• [SLOW TEST:13.033 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:51:05.205: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-95jsz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 28 07:51:07.372: INFO: namespace e2e-tests-kubectl-95jsz
Feb 28 07:51:07.372: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-95jsz'
Feb 28 07:51:07.986: INFO: stderr: ""
Feb 28 07:51:07.986: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 07:51:09.040: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:51:09.040: INFO: Found 0 / 1
Feb 28 07:51:10.048: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:51:10.049: INFO: Found 1 / 1
Feb 28 07:51:10.049: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 07:51:10.103: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:51:10.103: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 07:51:10.103: INFO: wait on redis-master startup in e2e-tests-kubectl-95jsz 
Feb 28 07:51:10.103: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml logs redis-master-vpz9j redis-master --namespace=e2e-tests-kubectl-95jsz'
Feb 28 07:51:10.482: INFO: stderr: ""
Feb 28 07:51:10.483: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 07:51:08.770 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 07:51:08.770 # Server started, Redis version 3.2.12\n1:M 28 Feb 07:51:08.770 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 07:51:08.770 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 28 07:51:10.483: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-95jsz'
Feb 28 07:51:10.874: INFO: stderr: ""
Feb 28 07:51:10.874: INFO: stdout: "service/rm2 exposed\n"
Feb 28 07:51:10.927: INFO: Service rm2 in namespace e2e-tests-kubectl-95jsz found.
STEP: exposing service
Feb 28 07:51:13.036: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-95jsz'
Feb 28 07:51:13.415: INFO: stderr: ""
Feb 28 07:51:13.416: INFO: stdout: "service/rm3 exposed\n"
Feb 28 07:51:13.470: INFO: Service rm3 in namespace e2e-tests-kubectl-95jsz found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:51:15.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-95jsz" for this suite.
Feb 28 07:51:37.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:51:38.067: INFO: namespace: e2e-tests-kubectl-95jsz, resource: bindings, ignored listing per whitelist
Feb 28 07:51:39.882: INFO: namespace e2e-tests-kubectl-95jsz deletion completed in 24.245378737s

• [SLOW TEST:34.677 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:51:39.882: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6bwss
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-afccd6c8-3b2d-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 07:51:42.160: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-afd5093d-3b2d-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-6bwss" to be "success or failure"
Feb 28 07:51:42.214: INFO: Pod "pod-projected-secrets-afd5093d-3b2d-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.661657ms
Feb 28 07:51:44.270: INFO: Pod "pod-projected-secrets-afd5093d-3b2d-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.10989299s
STEP: Saw pod success
Feb 28 07:51:44.270: INFO: Pod "pod-projected-secrets-afd5093d-3b2d-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:51:44.325: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-secrets-afd5093d-3b2d-11e9-8ad6-42906abdb246 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:51:44.440: INFO: Waiting for pod pod-projected-secrets-afd5093d-3b2d-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:51:44.494: INFO: Pod pod-projected-secrets-afd5093d-3b2d-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:51:44.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6bwss" for this suite.
Feb 28 07:51:50.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:51:51.023: INFO: namespace: e2e-tests-projected-6bwss, resource: bindings, ignored listing per whitelist
Feb 28 07:51:52.792: INFO: namespace e2e-tests-projected-6bwss deletion completed in 8.243238713s

• [SLOW TEST:12.910 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:51:52.793: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-dtm8z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:51:55.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b77c9d46-3b2d-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-dtm8z" to be "success or failure"
Feb 28 07:51:55.056: INFO: Pod "downwardapi-volume-b77c9d46-3b2d-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.589477ms
Feb 28 07:51:57.111: INFO: Pod "downwardapi-volume-b77c9d46-3b2d-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108618667s
STEP: Saw pod success
Feb 28 07:51:57.111: INFO: Pod "downwardapi-volume-b77c9d46-3b2d-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:51:57.165: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-b77c9d46-3b2d-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 07:51:57.283: INFO: Waiting for pod downwardapi-volume-b77c9d46-3b2d-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:51:57.337: INFO: Pod downwardapi-volume-b77c9d46-3b2d-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:51:57.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dtm8z" for this suite.
Feb 28 07:52:03.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:52:05.705: INFO: namespace: e2e-tests-downward-api-dtm8z, resource: bindings, ignored listing per whitelist
Feb 28 07:52:05.763: INFO: namespace e2e-tests-downward-api-dtm8z deletion completed in 8.371482516s

• [SLOW TEST:12.970 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:52:05.763: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-2tqqv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 07:52:08.468: INFO: Number of nodes with available pods: 0
Feb 28 07:52:08.468: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:52:09.590: INFO: Number of nodes with available pods: 0
Feb 28 07:52:09.590: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:52:10.586: INFO: Number of nodes with available pods: 2
Feb 28 07:52:10.586: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 28 07:52:10.870: INFO: Number of nodes with available pods: 1
Feb 28 07:52:10.870: INFO: Node ip-10-250-20-37.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:52:11.990: INFO: Number of nodes with available pods: 1
Feb 28 07:52:11.990: INFO: Node ip-10-250-20-37.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 07:52:12.981: INFO: Number of nodes with available pods: 2
Feb 28 07:52:12.981: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-2tqqv, will wait for the garbage collector to delete the pods
Feb 28 07:52:13.301: INFO: Deleting DaemonSet.extensions daemon-set took: 56.793896ms
Feb 28 07:52:13.401: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.284785ms
Feb 28 07:52:56.855: INFO: Number of nodes with available pods: 0
Feb 28 07:52:56.855: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 07:52:56.909: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-2tqqv/daemonsets","resourceVersion":"7047"},"items":null}

Feb 28 07:52:56.963: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-2tqqv/pods","resourceVersion":"7047"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:52:57.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-2tqqv" for this suite.
Feb 28 07:53:03.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:04.728: INFO: namespace: e2e-tests-daemonsets-2tqqv, resource: bindings, ignored listing per whitelist
Feb 28 07:53:05.423: INFO: namespace e2e-tests-daemonsets-2tqqv deletion completed in 8.243177996s

• [SLOW TEST:59.660 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:53:05.423: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-4cj9m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 28 07:53:08.213: INFO: Waiting up to 5m0s for pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-d4mzj" in namespace "e2e-tests-svcaccounts-4cj9m" to be "success or failure"
Feb 28 07:53:08.267: INFO: Pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-d4mzj": Phase="Pending", Reason="", readiness=false. Elapsed: 53.825949ms
Feb 28 07:53:10.322: INFO: Pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-d4mzj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108383813s
STEP: Saw pod success
Feb 28 07:53:10.322: INFO: Pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-d4mzj" satisfied condition "success or failure"
Feb 28 07:53:10.376: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-d4mzj container token-test: <nil>
STEP: delete the pod
Feb 28 07:53:10.493: INFO: Waiting for pod pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-d4mzj to disappear
Feb 28 07:53:10.546: INFO: Pod pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-d4mzj no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 28 07:53:10.601: INFO: Waiting up to 5m0s for pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-pggxd" in namespace "e2e-tests-svcaccounts-4cj9m" to be "success or failure"
Feb 28 07:53:10.654: INFO: Pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-pggxd": Phase="Pending", Reason="", readiness=false. Elapsed: 53.537225ms
Feb 28 07:53:12.708: INFO: Pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-pggxd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107640375s
STEP: Saw pod success
Feb 28 07:53:12.709: INFO: Pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-pggxd" satisfied condition "success or failure"
Feb 28 07:53:12.762: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-pggxd container root-ca-test: <nil>
STEP: delete the pod
Feb 28 07:53:12.879: INFO: Waiting for pod pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-pggxd to disappear
Feb 28 07:53:12.933: INFO: Pod pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-pggxd no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 28 07:53:12.988: INFO: Waiting up to 5m0s for pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-wtw4p" in namespace "e2e-tests-svcaccounts-4cj9m" to be "success or failure"
Feb 28 07:53:13.041: INFO: Pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-wtw4p": Phase="Pending", Reason="", readiness=false. Elapsed: 53.350048ms
Feb 28 07:53:15.096: INFO: Pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-wtw4p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.10848048s
STEP: Saw pod success
Feb 28 07:53:15.096: INFO: Pod "pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-wtw4p" satisfied condition "success or failure"
Feb 28 07:53:15.150: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-wtw4p container namespace-test: <nil>
STEP: delete the pod
Feb 28 07:53:15.267: INFO: Waiting for pod pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-wtw4p to disappear
Feb 28 07:53:15.321: INFO: Pod pod-service-account-e31fa44c-3b2d-11e9-8ad6-42906abdb246-wtw4p no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:53:15.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-4cj9m" for this suite.
Feb 28 07:53:21.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:22.203: INFO: namespace: e2e-tests-svcaccounts-4cj9m, resource: bindings, ignored listing per whitelist
Feb 28 07:53:23.714: INFO: namespace e2e-tests-svcaccounts-4cj9m deletion completed in 8.339020002s

• [SLOW TEST:18.291 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:53:23.714: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-6jh6s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:53:26.342: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 28 07:53:26.450: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-6jh6s/daemonsets","resourceVersion":"7148"},"items":null}

Feb 28 07:53:26.504: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-6jh6s/pods","resourceVersion":"7148"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:53:26.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-6jh6s" for this suite.
Feb 28 07:53:32.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:34.076: INFO: namespace: e2e-tests-daemonsets-6jh6s, resource: bindings, ignored listing per whitelist
Feb 28 07:53:34.941: INFO: namespace e2e-tests-daemonsets-6jh6s deletion completed in 8.220158396s

S [SKIPPING] [11.227 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 28 07:53:26.342: Requires at least 2 nodes (not -1)

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:53:34.941: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-mvkj4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:53:37.179: INFO: (0) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 57.156083ms)
Feb 28 07:53:37.237: INFO: (1) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 57.153516ms)
Feb 28 07:53:37.292: INFO: (2) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.514483ms)
Feb 28 07:53:37.348: INFO: (3) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.561581ms)
Feb 28 07:53:37.403: INFO: (4) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.339022ms)
Feb 28 07:53:37.459: INFO: (5) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.776782ms)
Feb 28 07:53:37.515: INFO: (6) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.582442ms)
Feb 28 07:53:37.570: INFO: (7) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.474815ms)
Feb 28 07:53:37.626: INFO: (8) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.753152ms)
Feb 28 07:53:37.681: INFO: (9) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.224708ms)
Feb 28 07:53:37.741: INFO: (10) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 58.942457ms)
Feb 28 07:53:37.796: INFO: (11) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.836775ms)
Feb 28 07:53:37.852: INFO: (12) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.360223ms)
Feb 28 07:53:37.908: INFO: (13) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.683192ms)
Feb 28 07:53:37.963: INFO: (14) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.356768ms)
Feb 28 07:53:38.020: INFO: (15) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 56.824962ms)
Feb 28 07:53:38.078: INFO: (16) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 57.911005ms)
Feb 28 07:53:38.134: INFO: (17) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 56.056672ms)
Feb 28 07:53:38.190: INFO: (18) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.89911ms)
Feb 28 07:53:38.245: INFO: (19) /api/v1/nodes/ip-10-250-15-222.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.300484ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:53:38.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-mvkj4" for this suite.
Feb 28 07:53:44.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:45.218: INFO: namespace: e2e-tests-proxy-mvkj4, resource: bindings, ignored listing per whitelist
Feb 28 07:53:46.505: INFO: namespace e2e-tests-proxy-mvkj4 deletion completed in 8.20510655s

• [SLOW TEST:11.564 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:53:46.506: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-pgzq5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 28 07:53:48.707: INFO: Waiting up to 5m0s for pod "pod-fb428ca7-3b2d-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-pgzq5" to be "success or failure"
Feb 28 07:53:48.762: INFO: Pod "pod-fb428ca7-3b2d-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 54.510926ms
Feb 28 07:53:50.816: INFO: Pod "pod-fb428ca7-3b2d-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108401964s
STEP: Saw pod success
Feb 28 07:53:50.816: INFO: Pod "pod-fb428ca7-3b2d-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:53:50.869: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-fb428ca7-3b2d-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 07:53:50.984: INFO: Waiting for pod pod-fb428ca7-3b2d-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:53:51.038: INFO: Pod pod-fb428ca7-3b2d-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:53:51.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pgzq5" for this suite.
Feb 28 07:53:57.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:58.526: INFO: namespace: e2e-tests-emptydir-pgzq5, resource: bindings, ignored listing per whitelist
Feb 28 07:53:59.330: INFO: namespace e2e-tests-emptydir-pgzq5 deletion completed in 8.237934733s

• [SLOW TEST:12.825 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:53:59.331: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-q7pjs
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-02fc551d-3b2e-11e9-8ad6-42906abdb246
STEP: Creating configMap with name cm-test-opt-upd-02fc5562-3b2e-11e9-8ad6-42906abdb246
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-02fc551d-3b2e-11e9-8ad6-42906abdb246
STEP: Updating configmap cm-test-opt-upd-02fc5562-3b2e-11e9-8ad6-42906abdb246
STEP: Creating configMap with name cm-test-opt-create-02fc5579-3b2e-11e9-8ad6-42906abdb246
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:54:06.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-q7pjs" for this suite.
Feb 28 07:54:28.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:54:30.481: INFO: namespace: e2e-tests-projected-q7pjs, resource: bindings, ignored listing per whitelist
Feb 28 07:54:30.855: INFO: namespace e2e-tests-projected-q7pjs deletion completed in 24.2780388s

• [SLOW TEST:31.524 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:54:30.855: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5tbfg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-15ad566c-3b2e-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 07:54:33.082: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-15b58623-3b2e-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-5tbfg" to be "success or failure"
Feb 28 07:54:33.136: INFO: Pod "pod-projected-configmaps-15b58623-3b2e-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 54.068133ms
Feb 28 07:54:35.193: INFO: Pod "pod-projected-configmaps-15b58623-3b2e-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11069704s
STEP: Saw pod success
Feb 28 07:54:35.193: INFO: Pod "pod-projected-configmaps-15b58623-3b2e-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:54:35.246: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-configmaps-15b58623-3b2e-11e9-8ad6-42906abdb246 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:54:35.364: INFO: Waiting for pod pod-projected-configmaps-15b58623-3b2e-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:54:35.417: INFO: Pod pod-projected-configmaps-15b58623-3b2e-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:54:35.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5tbfg" for this suite.
Feb 28 07:54:41.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:54:43.495: INFO: namespace: e2e-tests-projected-5tbfg, resource: bindings, ignored listing per whitelist
Feb 28 07:54:43.712: INFO: namespace e2e-tests-projected-5tbfg deletion completed in 8.239604906s

• [SLOW TEST:12.857 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:54:43.713: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-ln6pf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-ln6pf
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 07:54:45.851: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 07:55:06.775: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.78:8080/dial?request=hostName&protocol=udp&host=100.96.1.77&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-ln6pf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:55:06.775: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:55:07.566: INFO: Waiting for endpoints: map[]
Feb 28 07:55:07.636: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.78:8080/dial?request=hostName&protocol=udp&host=100.96.0.30&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-ln6pf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:55:07.636: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:55:08.331: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:55:08.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-ln6pf" for this suite.
Feb 28 07:55:30.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:32.427: INFO: namespace: e2e-tests-pod-network-test-ln6pf, resource: bindings, ignored listing per whitelist
Feb 28 07:55:32.587: INFO: namespace e2e-tests-pod-network-test-ln6pf deletion completed in 24.200490613s

• [SLOW TEST:48.874 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:55:32.587: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-tx7hz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 28 07:55:35.082: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tx7hz,SelfLink:/api/v1/namespaces/e2e-tests-watch-tx7hz/configmaps/e2e-watch-test-label-changed,UID:3a8cdf2c-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:7516,Generation:0,CreationTimestamp:2019-02-28 07:55:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 07:55:35.159: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tx7hz,SelfLink:/api/v1/namespaces/e2e-tests-watch-tx7hz/configmaps/e2e-watch-test-label-changed,UID:3a8cdf2c-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:7517,Generation:0,CreationTimestamp:2019-02-28 07:55:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 28 07:55:35.160: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tx7hz,SelfLink:/api/v1/namespaces/e2e-tests-watch-tx7hz/configmaps/e2e-watch-test-label-changed,UID:3a8cdf2c-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:7518,Generation:0,CreationTimestamp:2019-02-28 07:55:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 28 07:55:45.539: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tx7hz,SelfLink:/api/v1/namespaces/e2e-tests-watch-tx7hz/configmaps/e2e-watch-test-label-changed,UID:3a8cdf2c-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:7539,Generation:0,CreationTimestamp:2019-02-28 07:55:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 07:55:45.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tx7hz,SelfLink:/api/v1/namespaces/e2e-tests-watch-tx7hz/configmaps/e2e-watch-test-label-changed,UID:3a8cdf2c-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:7540,Generation:0,CreationTimestamp:2019-02-28 07:55:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 28 07:55:45.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tx7hz,SelfLink:/api/v1/namespaces/e2e-tests-watch-tx7hz/configmaps/e2e-watch-test-label-changed,UID:3a8cdf2c-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:7541,Generation:0,CreationTimestamp:2019-02-28 07:55:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:55:45.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-tx7hz" for this suite.
Feb 28 07:55:51.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:53.188: INFO: namespace: e2e-tests-watch-tx7hz, resource: bindings, ignored listing per whitelist
Feb 28 07:55:53.833: INFO: namespace e2e-tests-watch-tx7hz deletion completed in 8.237398052s

• [SLOW TEST:21.246 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:55:53.833: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ntdn8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 28 07:55:55.951: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-ntdn8'
Feb 28 07:55:56.407: INFO: stderr: ""
Feb 28 07:55:56.407: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 07:55:57.461: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:55:57.461: INFO: Found 0 / 1
Feb 28 07:55:58.462: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:55:58.462: INFO: Found 1 / 1
Feb 28 07:55:58.462: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 28 07:55:58.516: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:55:58.516: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 07:55:58.516: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml patch pod redis-master-7fh7c --namespace=e2e-tests-kubectl-ntdn8 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 28 07:55:58.874: INFO: stderr: ""
Feb 28 07:55:58.874: INFO: stdout: "pod/redis-master-7fh7c patched\n"
STEP: checking annotations
Feb 28 07:55:58.929: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:55:58.929: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:55:58.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ntdn8" for this suite.
Feb 28 07:56:21.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:22.683: INFO: namespace: e2e-tests-kubectl-ntdn8, resource: bindings, ignored listing per whitelist
Feb 28 07:56:23.219: INFO: namespace e2e-tests-kubectl-ntdn8 deletion completed in 24.235411849s

• [SLOW TEST:29.386 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:56:23.219: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-n7hr5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:56:25.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-n7hr5" for this suite.
Feb 28 07:56:47.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:47.912: INFO: namespace: e2e-tests-pods-n7hr5, resource: bindings, ignored listing per whitelist
Feb 28 07:56:49.750: INFO: namespace e2e-tests-pods-n7hr5 deletion completed in 24.227955907s

• [SLOW TEST:26.531 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:56:49.751: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-l4zsd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 28 07:56:52.017: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-l4zsd" to be "success or failure"
Feb 28 07:56:52.071: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 53.717008ms
Feb 28 07:56:54.125: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107943603s
STEP: Saw pod success
Feb 28 07:56:54.125: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 28 07:56:54.178: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 28 07:56:54.296: INFO: Waiting for pod pod-host-path-test to disappear
Feb 28 07:56:54.349: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:56:54.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-l4zsd" for this suite.
Feb 28 07:57:00.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:57:00.887: INFO: namespace: e2e-tests-hostpath-l4zsd, resource: bindings, ignored listing per whitelist
Feb 28 07:57:02.614: INFO: namespace e2e-tests-hostpath-l4zsd deletion completed in 8.210669844s

• [SLOW TEST:12.863 seconds]
[sig-storage] HostPath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:57:02.614: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-7445h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0228 07:57:15.087067   30448 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 07:57:15.087: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:57:15.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-7445h" for this suite.
Feb 28 07:57:21.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:57:22.647: INFO: namespace: e2e-tests-gc-7445h, resource: bindings, ignored listing per whitelist
Feb 28 07:57:23.405: INFO: namespace e2e-tests-gc-7445h deletion completed in 8.264421472s

• [SLOW TEST:20.791 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:57:23.406: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-dnp9f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:57:25.569: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 28 07:57:25.677: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 07:57:27.800: INFO: Creating deployment "test-rolling-update-deployment"
Feb 28 07:57:27.854: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 28 07:57:28.026: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 28 07:57:28.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937447, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937447, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937447, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937447, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 07:57:30.135: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 07:57:30.331: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-dnp9f,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dnp9f/deployments/test-rolling-update-deployment,UID:7de567b8-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:7870,Generation:1,CreationTimestamp:2019-02-28 07:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-28 07:57:27 +0000 UTC 2019-02-28 07:57:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-28 07:57:29 +0000 UTC 2019-02-28 07:57:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 07:57:30.385: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-dnp9f,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dnp9f/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:7de6ebd5-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:7863,Generation:1,CreationTimestamp:2019-02-28 07:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 7de567b8-3b2e-11e9-ac40-06fd5a6cf138 0xc001f11e07 0xc001f11e08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 07:57:30.386: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 28 07:57:30.386: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-dnp9f,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dnp9f/replicasets/test-rolling-update-controller,UID:7c910abf-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:7869,Generation:2,CreationTimestamp:2019-02-28 07:57:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 7de567b8-3b2e-11e9-ac40-06fd5a6cf138 0xc001f11bef 0xc001f11c00}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 07:57:30.440: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-kfndq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-kfndq,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-dnp9f,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dnp9f/pods/test-rolling-update-deployment-68b55d7bc6-kfndq,UID:7de7378e-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:7862,Generation:0,CreationTimestamp:2019-02-28 07:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.84/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 7de6ebd5-3b2e-11e9-ac40-06fd5a6cf138 0xc001f948a7 0xc001f948a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-hn7mb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hn7mb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-hn7mb true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f94930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f94950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:57:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:57:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:57:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:57:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:100.96.1.84,StartTime:2019-02-28 07:57:27 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-28 07:57:28 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://f367217e3ded6170eda5e1251c3c2e866748e49e4c7df9b042b54f5dd9272fcb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:57:30.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-dnp9f" for this suite.
Feb 28 07:57:36.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:57:37.726: INFO: namespace: e2e-tests-deployment-dnp9f, resource: bindings, ignored listing per whitelist
Feb 28 07:57:38.745: INFO: namespace e2e-tests-deployment-dnp9f deletion completed in 8.251304595s

• [SLOW TEST:15.340 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:57:38.746: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-4qhzq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-4qhzq
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-4qhzq to expose endpoints map[]
Feb 28 07:57:40.975: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-4qhzq exposes endpoints map[] (53.245785ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-4qhzq
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-4qhzq to expose endpoints map[pod1:[80]]
Feb 28 07:57:42.246: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-4qhzq exposes endpoints map[pod1:[80]] (1.214756509s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-4qhzq
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-4qhzq to expose endpoints map[pod1:[80] pod2:[80]]
Feb 28 07:57:44.786: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-4qhzq exposes endpoints map[pod1:[80] pod2:[80]] (2.48553802s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-4qhzq
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-4qhzq to expose endpoints map[pod2:[80]]
Feb 28 07:57:44.949: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-4qhzq exposes endpoints map[pod2:[80]] (107.512546ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-4qhzq
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-4qhzq to expose endpoints map[]
Feb 28 07:57:45.056: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-4qhzq exposes endpoints map[] (53.344548ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:57:45.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-4qhzq" for this suite.
Feb 28 07:58:07.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:58:09.087: INFO: namespace: e2e-tests-services-4qhzq, resource: bindings, ignored listing per whitelist
Feb 28 07:58:09.407: INFO: namespace e2e-tests-services-4qhzq deletion completed in 24.238586555s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:30.662 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:58:09.408: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-9s6ld
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 28 07:58:11.570: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 07:58:11.684: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 07:58:11.739: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-15-222.eu-west-1.compute.internal before test
Feb 28 07:58:11.814: INFO: kube-proxy-bvswv from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.814: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 07:58:11.814: INFO: node-exporter-bsxsh from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.814: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 07:58:11.814: INFO: calico-node-9s67j from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.814: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 07:58:11.814: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-20-37.eu-west-1.compute.internal before test
Feb 28 07:58:11.977: INFO: addons-nginx-ingress-controller-85496c84cf-v44st from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.977: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 07:58:11.977: INFO: blackbox-exporter-86f6cf4cb7-q64tx from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.977: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 28 07:58:11.977: INFO: calico-node-clpwj from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.977: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 07:58:11.977: INFO: vpn-shoot-5fcfddb796-m7gkh from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.977: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 28 07:58:11.977: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-f4cl8 from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.977: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 07:58:11.977: INFO: addons-kube-lego-69bbdc96b6-fspjg from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.977: INFO: 	Container kube-lego ready: true, restart count 0
Feb 28 07:58:11.977: INFO: node-exporter-kn6kk from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.977: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 07:58:11.977: INFO: coredns-67df79bbdd-7spjs from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.977: INFO: 	Container coredns ready: true, restart count 0
Feb 28 07:58:11.977: INFO: addons-kubernetes-dashboard-6579b646c5-mljz4 from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.977: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 07:58:11.977: INFO: kube-proxy-bp6q8 from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.977: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 07:58:11.977: INFO: metrics-server-56cb9fb9c6-bjqsq from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 07:58:11.977: INFO: 	Container metrics-server ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-99932433-3b2e-11e9-8ad6-42906abdb246 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-99932433-3b2e-11e9-8ad6-42906abdb246 off the node ip-10-250-15-222.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-99932433-3b2e-11e9-8ad6-42906abdb246
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:58:16.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-9s6ld" for this suite.
Feb 28 07:58:30.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:58:31.912: INFO: namespace: e2e-tests-sched-pred-9s6ld, resource: bindings, ignored listing per whitelist
Feb 28 07:58:33.041: INFO: namespace e2e-tests-sched-pred-9s6ld deletion completed in 16.216144531s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:23.633 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:58:33.041: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-mjwlp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 28 07:58:35.385: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-mjwlp,SelfLink:/api/v1/namespaces/e2e-tests-watch-mjwlp/configmaps/e2e-watch-test-watch-closed,UID:a6157a46-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:8078,Generation:0,CreationTimestamp:2019-02-28 07:58:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 07:58:35.385: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-mjwlp,SelfLink:/api/v1/namespaces/e2e-tests-watch-mjwlp/configmaps/e2e-watch-test-watch-closed,UID:a6157a46-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:8079,Generation:0,CreationTimestamp:2019-02-28 07:58:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 28 07:58:35.601: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-mjwlp,SelfLink:/api/v1/namespaces/e2e-tests-watch-mjwlp/configmaps/e2e-watch-test-watch-closed,UID:a6157a46-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:8080,Generation:0,CreationTimestamp:2019-02-28 07:58:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 07:58:35.601: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-mjwlp,SelfLink:/api/v1/namespaces/e2e-tests-watch-mjwlp/configmaps/e2e-watch-test-watch-closed,UID:a6157a46-3b2e-11e9-ac40-06fd5a6cf138,ResourceVersion:8081,Generation:0,CreationTimestamp:2019-02-28 07:58:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:58:35.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-mjwlp" for this suite.
Feb 28 07:58:41.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:58:43.754: INFO: namespace: e2e-tests-watch-mjwlp, resource: bindings, ignored listing per whitelist
Feb 28 07:58:43.861: INFO: namespace e2e-tests-watch-mjwlp deletion completed in 8.20530324s

• [SLOW TEST:10.820 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:58:43.861: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-wkwb2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-ac87617d-3b2e-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 07:58:46.174: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac8ff749-3b2e-11e9-8ad6-42906abdb246" in namespace "e2e-tests-configmap-wkwb2" to be "success or failure"
Feb 28 07:58:46.228: INFO: Pod "pod-configmaps-ac8ff749-3b2e-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 54.307948ms
Feb 28 07:58:48.282: INFO: Pod "pod-configmaps-ac8ff749-3b2e-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108238012s
STEP: Saw pod success
Feb 28 07:58:48.282: INFO: Pod "pod-configmaps-ac8ff749-3b2e-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 07:58:48.336: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-configmaps-ac8ff749-3b2e-11e9-8ad6-42906abdb246 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:58:48.453: INFO: Waiting for pod pod-configmaps-ac8ff749-3b2e-11e9-8ad6-42906abdb246 to disappear
Feb 28 07:58:48.506: INFO: Pod pod-configmaps-ac8ff749-3b2e-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:58:48.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-wkwb2" for this suite.
Feb 28 07:58:54.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:58:56.253: INFO: namespace: e2e-tests-configmap-wkwb2, resource: bindings, ignored listing per whitelist
Feb 28 07:58:56.787: INFO: namespace e2e-tests-configmap-wkwb2 deletion completed in 8.226555017s

• [SLOW TEST:12.926 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:58:56.787: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-6kks5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-6kks5
Feb 28 07:59:01.121: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-6kks5
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 07:59:01.175: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:03:01.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-6kks5" for this suite.
Feb 28 08:03:08.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:03:08.494: INFO: namespace: e2e-tests-container-probe-6kks5, resource: bindings, ignored listing per whitelist
Feb 28 08:03:10.476: INFO: namespace e2e-tests-container-probe-6kks5 deletion completed in 8.549455137s

• [SLOW TEST:253.689 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:03:10.476: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-66p2b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:03:12.831: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b801e9c-3b2f-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-66p2b" to be "success or failure"
Feb 28 08:03:12.890: INFO: Pod "downwardapi-volume-4b801e9c-3b2f-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 58.76934ms
Feb 28 08:03:14.950: INFO: Pod "downwardapi-volume-4b801e9c-3b2f-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118930366s
STEP: Saw pod success
Feb 28 08:03:14.950: INFO: Pod "downwardapi-volume-4b801e9c-3b2f-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:03:15.012: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-4b801e9c-3b2f-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 08:03:15.134: INFO: Waiting for pod downwardapi-volume-4b801e9c-3b2f-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:03:15.191: INFO: Pod downwardapi-volume-4b801e9c-3b2f-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:03:15.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-66p2b" for this suite.
Feb 28 08:03:21.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:03:23.178: INFO: namespace: e2e-tests-downward-api-66p2b, resource: bindings, ignored listing per whitelist
Feb 28 08:03:23.705: INFO: namespace e2e-tests-downward-api-66p2b deletion completed in 8.455974961s

• [SLOW TEST:13.229 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:03:23.706: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-lrckl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-536e2505-3b2f-11e9-8ad6-42906abdb246
Feb 28 08:03:26.184: INFO: Pod name my-hostname-basic-536e2505-3b2f-11e9-8ad6-42906abdb246: Found 1 pods out of 1
Feb 28 08:03:26.184: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-536e2505-3b2f-11e9-8ad6-42906abdb246" are running
Feb 28 08:03:28.554: INFO: Pod "my-hostname-basic-536e2505-3b2f-11e9-8ad6-42906abdb246-fqgk8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:03:26 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:03:26 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-536e2505-3b2f-11e9-8ad6-42906abdb246]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:03:26 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-536e2505-3b2f-11e9-8ad6-42906abdb246]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:03:26 +0000 UTC Reason: Message:}])
Feb 28 08:03:28.554: INFO: Trying to dial the pod
Feb 28 08:03:33.814: INFO: Controller my-hostname-basic-536e2505-3b2f-11e9-8ad6-42906abdb246: Got expected result from replica 1 [my-hostname-basic-536e2505-3b2f-11e9-8ad6-42906abdb246-fqgk8]: "my-hostname-basic-536e2505-3b2f-11e9-8ad6-42906abdb246-fqgk8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:03:33.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-lrckl" for this suite.
Feb 28 08:03:40.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:03:41.766: INFO: namespace: e2e-tests-replication-controller-lrckl, resource: bindings, ignored listing per whitelist
Feb 28 08:03:42.294: INFO: namespace e2e-tests-replication-controller-lrckl deletion completed in 8.362781647s

• [SLOW TEST:18.589 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:03:42.295: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-d79ht
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-d79ht
Feb 28 08:03:46.618: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-d79ht
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:03:46.673: INFO: Initial restart count of pod liveness-exec is 0
Feb 28 08:04:40.208: INFO: Restart count of pod e2e-tests-container-probe-d79ht/liveness-exec is now 1 (53.534197349s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:04:40.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-d79ht" for this suite.
Feb 28 08:04:46.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:04:48.086: INFO: namespace: e2e-tests-container-probe-d79ht, resource: bindings, ignored listing per whitelist
Feb 28 08:04:48.692: INFO: namespace e2e-tests-container-probe-d79ht deletion completed in 8.364010809s

• [SLOW TEST:66.397 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:04:48.692: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-2rcbp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 28 08:04:51.058: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 08:04:51.171: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 08:04:51.228: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-15-222.eu-west-1.compute.internal before test
Feb 28 08:04:51.291: INFO: node-exporter-bsxsh from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.291: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 08:04:51.291: INFO: calico-node-9s67j from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.291: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 08:04:51.291: INFO: kube-proxy-bvswv from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.291: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 08:04:51.291: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-20-37.eu-west-1.compute.internal before test
Feb 28 08:04:51.418: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-f4cl8 from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.418: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 08:04:51.418: INFO: addons-kube-lego-69bbdc96b6-fspjg from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.418: INFO: 	Container kube-lego ready: true, restart count 0
Feb 28 08:04:51.418: INFO: node-exporter-kn6kk from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.418: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 08:04:51.418: INFO: coredns-67df79bbdd-7spjs from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.418: INFO: 	Container coredns ready: true, restart count 0
Feb 28 08:04:51.418: INFO: addons-kubernetes-dashboard-6579b646c5-mljz4 from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.418: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 08:04:51.419: INFO: kube-proxy-bp6q8 from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.419: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 08:04:51.419: INFO: metrics-server-56cb9fb9c6-bjqsq from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.419: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 08:04:51.419: INFO: addons-nginx-ingress-controller-85496c84cf-v44st from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.419: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 08:04:51.419: INFO: blackbox-exporter-86f6cf4cb7-q64tx from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.419: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 28 08:04:51.419: INFO: calico-node-clpwj from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.419: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 08:04:51.419: INFO: vpn-shoot-5fcfddb796-m7gkh from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:04:51.419: INFO: 	Container vpn-shoot ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node ip-10-250-15-222.eu-west-1.compute.internal
STEP: verifying the node has the label node ip-10-250-20-37.eu-west-1.compute.internal
Feb 28 08:04:51.776: INFO: Pod addons-kube-lego-69bbdc96b6-fspjg requesting resource cpu=20m on Node ip-10-250-20-37.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod addons-kubernetes-dashboard-6579b646c5-mljz4 requesting resource cpu=50m on Node ip-10-250-20-37.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod addons-nginx-ingress-controller-85496c84cf-v44st requesting resource cpu=100m on Node ip-10-250-20-37.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-f4cl8 requesting resource cpu=0m on Node ip-10-250-20-37.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod blackbox-exporter-86f6cf4cb7-q64tx requesting resource cpu=5m on Node ip-10-250-20-37.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod calico-node-9s67j requesting resource cpu=100m on Node ip-10-250-15-222.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod calico-node-clpwj requesting resource cpu=100m on Node ip-10-250-20-37.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod coredns-67df79bbdd-7spjs requesting resource cpu=50m on Node ip-10-250-20-37.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod kube-proxy-bp6q8 requesting resource cpu=20m on Node ip-10-250-20-37.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod kube-proxy-bvswv requesting resource cpu=20m on Node ip-10-250-15-222.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod metrics-server-56cb9fb9c6-bjqsq requesting resource cpu=20m on Node ip-10-250-20-37.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod node-exporter-bsxsh requesting resource cpu=5m on Node ip-10-250-15-222.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod node-exporter-kn6kk requesting resource cpu=5m on Node ip-10-250-20-37.eu-west-1.compute.internal
Feb 28 08:04:51.777: INFO: Pod vpn-shoot-5fcfddb796-m7gkh requesting resource cpu=50m on Node ip-10-250-20-37.eu-west-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-86836868-3b2f-11e9-8ad6-42906abdb246.15877880ee791ecd], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-2rcbp/filler-pod-86836868-3b2f-11e9-8ad6-42906abdb246 to ip-10-250-15-222.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-86836868-3b2f-11e9-8ad6-42906abdb246.15877881162f9e45], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-86836868-3b2f-11e9-8ad6-42906abdb246.1587788119049b44], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-86836868-3b2f-11e9-8ad6-42906abdb246.15877881203cb6b4], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-868cb393-3b2f-11e9-8ad6-42906abdb246.15877880f216296f], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-2rcbp/filler-pod-868cb393-3b2f-11e9-8ad6-42906abdb246 to ip-10-250-20-37.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-868cb393-3b2f-11e9-8ad6-42906abdb246.158778811c646a1a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-868cb393-3b2f-11e9-8ad6-42906abdb246.158778811f2af09a], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-868cb393-3b2f-11e9-8ad6-42906abdb246.15877881274f2480], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.158778817e606685], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node ip-10-250-15-222.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-250-20-37.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:04:55.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-2rcbp" for this suite.
Feb 28 08:05:01.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:05:02.602: INFO: namespace: e2e-tests-sched-pred-2rcbp, resource: bindings, ignored listing per whitelist
Feb 28 08:05:04.143: INFO: namespace e2e-tests-sched-pred-2rcbp deletion completed in 8.407182724s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:15.451 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:05:04.143: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-n9v2d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 28 08:05:06.794: INFO: Waiting up to 5m0s for pod "client-containers-8f6e0539-3b2f-11e9-8ad6-42906abdb246" in namespace "e2e-tests-containers-n9v2d" to be "success or failure"
Feb 28 08:05:06.851: INFO: Pod "client-containers-8f6e0539-3b2f-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 56.853845ms
Feb 28 08:05:08.909: INFO: Pod "client-containers-8f6e0539-3b2f-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.114263869s
STEP: Saw pod success
Feb 28 08:05:08.909: INFO: Pod "client-containers-8f6e0539-3b2f-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:05:08.963: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod client-containers-8f6e0539-3b2f-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 08:05:09.080: INFO: Waiting for pod client-containers-8f6e0539-3b2f-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:05:09.135: INFO: Pod client-containers-8f6e0539-3b2f-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:05:09.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-n9v2d" for this suite.
Feb 28 08:05:15.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:05:16.295: INFO: namespace: e2e-tests-containers-n9v2d, resource: bindings, ignored listing per whitelist
Feb 28 08:05:17.563: INFO: namespace e2e-tests-containers-n9v2d deletion completed in 8.371060614s

• [SLOW TEST:13.420 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:05:17.564: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-nswtv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-nswtv/configmap-test-97412a1b-3b2f-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 08:05:19.978: INFO: Waiting up to 5m0s for pod "pod-configmaps-9749d2f8-3b2f-11e9-8ad6-42906abdb246" in namespace "e2e-tests-configmap-nswtv" to be "success or failure"
Feb 28 08:05:20.032: INFO: Pod "pod-configmaps-9749d2f8-3b2f-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 54.591872ms
Feb 28 08:05:22.092: INFO: Pod "pod-configmaps-9749d2f8-3b2f-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.114561953s
STEP: Saw pod success
Feb 28 08:05:22.093: INFO: Pod "pod-configmaps-9749d2f8-3b2f-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:05:22.154: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-configmaps-9749d2f8-3b2f-11e9-8ad6-42906abdb246 container env-test: <nil>
STEP: delete the pod
Feb 28 08:05:22.285: INFO: Waiting for pod pod-configmaps-9749d2f8-3b2f-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:05:22.345: INFO: Pod pod-configmaps-9749d2f8-3b2f-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:05:22.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-nswtv" for this suite.
Feb 28 08:05:28.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:05:28.950: INFO: namespace: e2e-tests-configmap-nswtv, resource: bindings, ignored listing per whitelist
Feb 28 08:05:30.681: INFO: namespace e2e-tests-configmap-nswtv deletion completed in 8.278432449s

• [SLOW TEST:13.118 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:05:30.682: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-xtrks
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-9f0d2f79-3b2f-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 08:05:33.054: INFO: Waiting up to 5m0s for pod "pod-secrets-9f154fc1-3b2f-11e9-8ad6-42906abdb246" in namespace "e2e-tests-secrets-xtrks" to be "success or failure"
Feb 28 08:05:33.107: INFO: Pod "pod-secrets-9f154fc1-3b2f-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.378612ms
Feb 28 08:05:35.163: INFO: Pod "pod-secrets-9f154fc1-3b2f-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.109044603s
STEP: Saw pod success
Feb 28 08:05:35.163: INFO: Pod "pod-secrets-9f154fc1-3b2f-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:05:35.217: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-secrets-9f154fc1-3b2f-11e9-8ad6-42906abdb246 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:05:35.334: INFO: Waiting for pod pod-secrets-9f154fc1-3b2f-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:05:35.387: INFO: Pod pod-secrets-9f154fc1-3b2f-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:05:35.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-xtrks" for this suite.
Feb 28 08:05:41.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:05:43.149: INFO: namespace: e2e-tests-secrets-xtrks, resource: bindings, ignored listing per whitelist
Feb 28 08:05:43.741: INFO: namespace e2e-tests-secrets-xtrks deletion completed in 8.299180823s

• [SLOW TEST:13.060 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:05:43.741: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-nd87z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 28 08:05:46.037: INFO: Waiting up to 5m0s for pod "pod-a6d1fc83-3b2f-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-nd87z" to be "success or failure"
Feb 28 08:05:46.092: INFO: Pod "pod-a6d1fc83-3b2f-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 54.182989ms
Feb 28 08:05:48.152: INFO: Pod "pod-a6d1fc83-3b2f-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.113992573s
STEP: Saw pod success
Feb 28 08:05:48.152: INFO: Pod "pod-a6d1fc83-3b2f-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:05:48.212: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-a6d1fc83-3b2f-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 08:05:48.339: INFO: Waiting for pod pod-a6d1fc83-3b2f-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:05:48.399: INFO: Pod pod-a6d1fc83-3b2f-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:05:48.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nd87z" for this suite.
Feb 28 08:05:54.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:05:55.852: INFO: namespace: e2e-tests-emptydir-nd87z, resource: bindings, ignored listing per whitelist
Feb 28 08:05:56.757: INFO: namespace e2e-tests-emptydir-nd87z deletion completed in 8.299425098s

• [SLOW TEST:13.016 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:05:56.758: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-nst4c
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-aeb5be72-3b2f-11e9-8ad6-42906abdb246
STEP: Creating secret with name s-test-opt-upd-aeb5beea-3b2f-11e9-8ad6-42906abdb246
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-aeb5be72-3b2f-11e9-8ad6-42906abdb246
STEP: Updating secret s-test-opt-upd-aeb5beea-3b2f-11e9-8ad6-42906abdb246
STEP: Creating secret with name s-test-opt-create-aeb5bf34-3b2f-11e9-8ad6-42906abdb246
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:06:06.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-nst4c" for this suite.
Feb 28 08:06:28.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:06:30.307: INFO: namespace: e2e-tests-secrets-nst4c, resource: bindings, ignored listing per whitelist
Feb 28 08:06:30.584: INFO: namespace e2e-tests-secrets-nst4c deletion completed in 24.299619771s

• [SLOW TEST:33.827 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:06:30.584: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-795zj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 28 08:06:32.814: INFO: Waiting up to 5m0s for pod "pod-c2b3d24b-3b2f-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-795zj" to be "success or failure"
Feb 28 08:06:32.868: INFO: Pod "pod-c2b3d24b-3b2f-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 54.528114ms
Feb 28 08:06:34.923: INFO: Pod "pod-c2b3d24b-3b2f-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108828795s
STEP: Saw pod success
Feb 28 08:06:34.923: INFO: Pod "pod-c2b3d24b-3b2f-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:06:34.976: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-c2b3d24b-3b2f-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 08:06:35.092: INFO: Waiting for pod pod-c2b3d24b-3b2f-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:06:35.149: INFO: Pod pod-c2b3d24b-3b2f-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:06:35.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-795zj" for this suite.
Feb 28 08:06:41.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:06:42.714: INFO: namespace: e2e-tests-emptydir-795zj, resource: bindings, ignored listing per whitelist
Feb 28 08:06:43.478: INFO: namespace e2e-tests-emptydir-795zj deletion completed in 8.27118144s

• [SLOW TEST:12.894 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:06:43.479: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qdnxr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:06:45.724: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca6565e9-3b2f-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-qdnxr" to be "success or failure"
Feb 28 08:06:45.781: INFO: Pod "downwardapi-volume-ca6565e9-3b2f-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 56.875853ms
Feb 28 08:06:47.841: INFO: Pod "downwardapi-volume-ca6565e9-3b2f-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117131834s
STEP: Saw pod success
Feb 28 08:06:47.842: INFO: Pod "downwardapi-volume-ca6565e9-3b2f-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:06:47.900: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-ca6565e9-3b2f-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 08:06:48.028: INFO: Waiting for pod downwardapi-volume-ca6565e9-3b2f-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:06:48.087: INFO: Pod downwardapi-volume-ca6565e9-3b2f-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:06:48.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qdnxr" for this suite.
Feb 28 08:06:54.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:06:55.403: INFO: namespace: e2e-tests-downward-api-qdnxr, resource: bindings, ignored listing per whitelist
Feb 28 08:06:56.393: INFO: namespace e2e-tests-downward-api-qdnxr deletion completed in 8.247665872s

• [SLOW TEST:12.915 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:06:56.393: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-8tpqm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 28 08:06:58.619: INFO: Waiting up to 5m0s for pod "pod-d2155a31-3b2f-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-8tpqm" to be "success or failure"
Feb 28 08:06:58.674: INFO: Pod "pod-d2155a31-3b2f-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 54.950731ms
Feb 28 08:07:00.731: INFO: Pod "pod-d2155a31-3b2f-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.112048708s
STEP: Saw pod success
Feb 28 08:07:00.731: INFO: Pod "pod-d2155a31-3b2f-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:07:00.790: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-d2155a31-3b2f-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 08:07:01.173: INFO: Waiting for pod pod-d2155a31-3b2f-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:07:01.230: INFO: Pod pod-d2155a31-3b2f-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:07:01.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8tpqm" for this suite.
Feb 28 08:07:07.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:07:08.043: INFO: namespace: e2e-tests-emptydir-8tpqm, resource: bindings, ignored listing per whitelist
Feb 28 08:07:09.612: INFO: namespace e2e-tests-emptydir-8tpqm deletion completed in 8.318537335s

• [SLOW TEST:13.218 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:07:09.612: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-qsxwq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 28 08:07:14.738: INFO: Successfully updated pod "pod-update-activedeadlineseconds-da011541-3b2f-11e9-8ad6-42906abdb246"
Feb 28 08:07:14.738: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-da011541-3b2f-11e9-8ad6-42906abdb246" in namespace "e2e-tests-pods-qsxwq" to be "terminated due to deadline exceeded"
Feb 28 08:07:14.791: INFO: Pod "pod-update-activedeadlineseconds-da011541-3b2f-11e9-8ad6-42906abdb246": Phase="Running", Reason="", readiness=true. Elapsed: 53.510343ms
Feb 28 08:07:16.856: INFO: Pod "pod-update-activedeadlineseconds-da011541-3b2f-11e9-8ad6-42906abdb246": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.118705108s
Feb 28 08:07:16.857: INFO: Pod "pod-update-activedeadlineseconds-da011541-3b2f-11e9-8ad6-42906abdb246" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:07:16.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-qsxwq" for this suite.
Feb 28 08:07:23.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:07:24.222: INFO: namespace: e2e-tests-pods-qsxwq, resource: bindings, ignored listing per whitelist
Feb 28 08:07:25.135: INFO: namespace e2e-tests-pods-qsxwq deletion completed in 8.217920597s

• [SLOW TEST:15.523 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:07:25.136: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-2rb4t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:07:27.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-2rb4t" for this suite.
Feb 28 08:07:33.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:07:35.167: INFO: namespace: e2e-tests-services-2rb4t, resource: bindings, ignored listing per whitelist
Feb 28 08:07:35.730: INFO: namespace e2e-tests-services-2rb4t deletion completed in 8.323796179s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:10.595 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:07:35.731: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5lfcr
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-e997be29-3b2f-11e9-8ad6-42906abdb246
STEP: Creating secret with name s-test-opt-upd-e997bef7-3b2f-11e9-8ad6-42906abdb246
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e997be29-3b2f-11e9-8ad6-42906abdb246
STEP: Updating secret s-test-opt-upd-e997bef7-3b2f-11e9-8ad6-42906abdb246
STEP: Creating secret with name s-test-opt-create-e997bf20-3b2f-11e9-8ad6-42906abdb246
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:07:44.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5lfcr" for this suite.
Feb 28 08:08:07.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:09.121: INFO: namespace: e2e-tests-projected-5lfcr, resource: bindings, ignored listing per whitelist
Feb 28 08:08:09.401: INFO: namespace e2e-tests-projected-5lfcr deletion completed in 24.355019192s

• [SLOW TEST:33.670 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:08:09.401: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mc4gp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-fdae99be-3b2f-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 08:08:11.828: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fdb7a50c-3b2f-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-mc4gp" to be "success or failure"
Feb 28 08:08:11.888: INFO: Pod "pod-projected-configmaps-fdb7a50c-3b2f-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 58.915975ms
Feb 28 08:08:13.949: INFO: Pod "pod-projected-configmaps-fdb7a50c-3b2f-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.119873029s
STEP: Saw pod success
Feb 28 08:08:13.949: INFO: Pod "pod-projected-configmaps-fdb7a50c-3b2f-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:08:14.008: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-configmaps-fdb7a50c-3b2f-11e9-8ad6-42906abdb246 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:08:14.132: INFO: Waiting for pod pod-projected-configmaps-fdb7a50c-3b2f-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:08:14.189: INFO: Pod pod-projected-configmaps-fdb7a50c-3b2f-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:08:14.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mc4gp" for this suite.
Feb 28 08:08:20.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:21.111: INFO: namespace: e2e-tests-projected-mc4gp, resource: bindings, ignored listing per whitelist
Feb 28 08:08:22.603: INFO: namespace e2e-tests-projected-mc4gp deletion completed in 8.355603119s

• [SLOW TEST:13.202 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:08:22.603: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-ssgqr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-ssgqr
Feb 28 08:08:29.405: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-ssgqr
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:08:29.462: INFO: Initial restart count of pod liveness-http is 0
Feb 28 08:08:43.918: INFO: Restart count of pod e2e-tests-container-probe-ssgqr/liveness-http is now 1 (14.455820058s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:08:43.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-ssgqr" for this suite.
Feb 28 08:08:50.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:50.683: INFO: namespace: e2e-tests-container-probe-ssgqr, resource: bindings, ignored listing per whitelist
Feb 28 08:08:52.387: INFO: namespace e2e-tests-container-probe-ssgqr deletion completed in 8.349550434s

• [SLOW TEST:29.783 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:08:52.387: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6v62m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-173c0333-3b30-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 08:08:54.692: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-17448924-3b30-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-6v62m" to be "success or failure"
Feb 28 08:08:54.749: INFO: Pod "pod-projected-configmaps-17448924-3b30-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 56.862002ms
Feb 28 08:08:56.806: INFO: Pod "pod-projected-configmaps-17448924-3b30-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.113792622s
STEP: Saw pod success
Feb 28 08:08:56.806: INFO: Pod "pod-projected-configmaps-17448924-3b30-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:08:56.864: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-configmaps-17448924-3b30-11e9-8ad6-42906abdb246 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:08:56.996: INFO: Waiting for pod pod-projected-configmaps-17448924-3b30-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:08:57.053: INFO: Pod pod-projected-configmaps-17448924-3b30-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:08:57.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6v62m" for this suite.
Feb 28 08:09:03.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:09:05.389: INFO: namespace: e2e-tests-projected-6v62m, resource: bindings, ignored listing per whitelist
Feb 28 08:09:05.571: INFO: namespace e2e-tests-projected-6v62m deletion completed in 8.42315725s

• [SLOW TEST:13.185 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:09:05.572: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-lp55f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 28 08:09:08.124: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-a,UID:1f49cb90-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9676,Generation:0,CreationTimestamp:2019-02-28 08:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:09:08.124: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-a,UID:1f49cb90-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9676,Generation:0,CreationTimestamp:2019-02-28 08:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 28 08:09:18.243: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-a,UID:1f49cb90-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9696,Generation:0,CreationTimestamp:2019-02-28 08:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 28 08:09:18.243: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-a,UID:1f49cb90-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9696,Generation:0,CreationTimestamp:2019-02-28 08:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 28 08:09:28.363: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-a,UID:1f49cb90-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9715,Generation:0,CreationTimestamp:2019-02-28 08:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:09:28.363: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-a,UID:1f49cb90-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9715,Generation:0,CreationTimestamp:2019-02-28 08:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 28 08:09:38.419: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-a,UID:1f49cb90-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9735,Generation:0,CreationTimestamp:2019-02-28 08:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:09:38.419: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-a,UID:1f49cb90-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9735,Generation:0,CreationTimestamp:2019-02-28 08:09:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 28 08:09:48.480: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-b,UID:3757106d-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9754,Generation:0,CreationTimestamp:2019-02-28 08:09:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:09:48.480: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-b,UID:3757106d-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9754,Generation:0,CreationTimestamp:2019-02-28 08:09:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 28 08:09:58.543: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-b,UID:3757106d-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9773,Generation:0,CreationTimestamp:2019-02-28 08:09:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:09:58.543: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-lp55f,SelfLink:/api/v1/namespaces/e2e-tests-watch-lp55f/configmaps/e2e-watch-test-configmap-b,UID:3757106d-3b30-11e9-ac40-06fd5a6cf138,ResourceVersion:9773,Generation:0,CreationTimestamp:2019-02-28 08:09:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:10:08.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-lp55f" for this suite.
Feb 28 08:10:14.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:10:16.808: INFO: namespace: e2e-tests-watch-lp55f, resource: bindings, ignored listing per whitelist
Feb 28 08:10:16.984: INFO: namespace e2e-tests-watch-lp55f deletion completed in 8.383487725s

• [SLOW TEST:71.412 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:10:16.984: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-t7zdx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:10:21.598: INFO: Waiting up to 5m0s for pod "client-envvars-4b11e14b-3b30-11e9-8ad6-42906abdb246" in namespace "e2e-tests-pods-t7zdx" to be "success or failure"
Feb 28 08:10:21.652: INFO: Pod "client-envvars-4b11e14b-3b30-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 53.837916ms
Feb 28 08:10:23.709: INFO: Pod "client-envvars-4b11e14b-3b30-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.110930451s
STEP: Saw pod success
Feb 28 08:10:23.710: INFO: Pod "client-envvars-4b11e14b-3b30-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:10:23.764: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod client-envvars-4b11e14b-3b30-11e9-8ad6-42906abdb246 container env3cont: <nil>
STEP: delete the pod
Feb 28 08:10:23.881: INFO: Waiting for pod client-envvars-4b11e14b-3b30-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:10:23.938: INFO: Pod client-envvars-4b11e14b-3b30-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:10:23.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-t7zdx" for this suite.
Feb 28 08:11:10.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:11:11.380: INFO: namespace: e2e-tests-pods-t7zdx, resource: bindings, ignored listing per whitelist
Feb 28 08:11:12.291: INFO: namespace e2e-tests-pods-t7zdx deletion completed in 48.298802612s

• [SLOW TEST:55.307 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:11:12.292: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-flh4g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-6aa10ca1-3b30-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 08:11:14.611: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6aa9c3cd-3b30-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-flh4g" to be "success or failure"
Feb 28 08:11:14.672: INFO: Pod "pod-projected-configmaps-6aa9c3cd-3b30-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 61.160593ms
Feb 28 08:11:16.726: INFO: Pod "pod-projected-configmaps-6aa9c3cd-3b30-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.115079728s
STEP: Saw pod success
Feb 28 08:11:16.726: INFO: Pod "pod-projected-configmaps-6aa9c3cd-3b30-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:11:16.780: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-configmaps-6aa9c3cd-3b30-11e9-8ad6-42906abdb246 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:11:16.896: INFO: Waiting for pod pod-projected-configmaps-6aa9c3cd-3b30-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:11:16.952: INFO: Pod pod-projected-configmaps-6aa9c3cd-3b30-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:11:16.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-flh4g" for this suite.
Feb 28 08:11:23.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:11:23.843: INFO: namespace: e2e-tests-projected-flh4g, resource: bindings, ignored listing per whitelist
Feb 28 08:11:25.252: INFO: namespace e2e-tests-projected-flh4g deletion completed in 8.245561092s

• [SLOW TEST:12.961 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:11:25.253: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rdkgt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Feb 28 08:11:27.481: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-rdkgt'
Feb 28 08:11:30.973: INFO: stderr: ""
Feb 28 08:11:30.973: INFO: stdout: "pod/pause created\n"
Feb 28 08:11:30.973: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 28 08:11:30.976: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-rdkgt" to be "running and ready"
Feb 28 08:11:31.030: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 54.441719ms
Feb 28 08:11:33.086: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.110807717s
Feb 28 08:11:33.087: INFO: Pod "pause" satisfied condition "running and ready"
Feb 28 08:11:33.087: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 28 08:11:33.087: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-rdkgt'
Feb 28 08:11:33.466: INFO: stderr: ""
Feb 28 08:11:33.466: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 28 08:11:33.467: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-rdkgt'
Feb 28 08:11:33.792: INFO: stderr: ""
Feb 28 08:11:33.792: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 28 08:11:33.792: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml label pods pause testing-label- --namespace=e2e-tests-kubectl-rdkgt'
Feb 28 08:11:34.165: INFO: stderr: ""
Feb 28 08:11:34.165: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 28 08:11:34.166: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-rdkgt'
Feb 28 08:11:34.505: INFO: stderr: ""
Feb 28 08:11:34.505: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Feb 28 08:11:34.505: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rdkgt'
Feb 28 08:11:34.887: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:11:34.887: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 28 08:11:34.887: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-rdkgt'
Feb 28 08:11:35.297: INFO: stderr: "No resources found.\n"
Feb 28 08:11:35.297: INFO: stdout: ""
Feb 28 08:11:35.298: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -l name=pause --namespace=e2e-tests-kubectl-rdkgt -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 08:11:35.603: INFO: stderr: ""
Feb 28 08:11:35.603: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:11:35.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rdkgt" for this suite.
Feb 28 08:11:41.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:11:43.226: INFO: namespace: e2e-tests-kubectl-rdkgt, resource: bindings, ignored listing per whitelist
Feb 28 08:11:43.924: INFO: namespace e2e-tests-kubectl-rdkgt deletion completed in 8.267487058s

• [SLOW TEST:18.672 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:11:43.925: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-wdx4n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 28 08:11:49.053: INFO: Pod name wrapped-volume-race-7f20b5b7-3b30-11e9-8ad6-42906abdb246: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7f20b5b7-3b30-11e9-8ad6-42906abdb246 in namespace e2e-tests-emptydir-wrapper-wdx4n, will wait for the garbage collector to delete the pods
Feb 28 08:11:53.649: INFO: Deleting ReplicationController wrapped-volume-race-7f20b5b7-3b30-11e9-8ad6-42906abdb246 took: 55.303354ms
Feb 28 08:11:53.749: INFO: Terminating ReplicationController wrapped-volume-race-7f20b5b7-3b30-11e9-8ad6-42906abdb246 pods took: 100.293622ms
STEP: Creating RC which spawns configmap-volume pods
Feb 28 08:12:37.847: INFO: Pod name wrapped-volume-race-9c325383-3b30-11e9-8ad6-42906abdb246: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9c325383-3b30-11e9-8ad6-42906abdb246 in namespace e2e-tests-emptydir-wrapper-wdx4n, will wait for the garbage collector to delete the pods
Feb 28 08:12:42.403: INFO: Deleting ReplicationController wrapped-volume-race-9c325383-3b30-11e9-8ad6-42906abdb246 took: 54.962101ms
Feb 28 08:12:42.505: INFO: Terminating ReplicationController wrapped-volume-race-9c325383-3b30-11e9-8ad6-42906abdb246 pods took: 101.681883ms
STEP: Creating RC which spawns configmap-volume pods
Feb 28 08:13:27.774: INFO: Pod name wrapped-volume-race-b9f8902c-3b30-11e9-8ad6-42906abdb246: Found 4 pods out of 5
Feb 28 08:13:32.836: INFO: Pod name wrapped-volume-race-b9f8902c-3b30-11e9-8ad6-42906abdb246: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b9f8902c-3b30-11e9-8ad6-42906abdb246 in namespace e2e-tests-emptydir-wrapper-wdx4n, will wait for the garbage collector to delete the pods
Feb 28 08:13:33.330: INFO: Deleting ReplicationController wrapped-volume-race-b9f8902c-3b30-11e9-8ad6-42906abdb246 took: 58.576501ms
Feb 28 08:13:33.430: INFO: Terminating ReplicationController wrapped-volume-race-b9f8902c-3b30-11e9-8ad6-42906abdb246 pods took: 100.232542ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:14:20.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-wdx4n" for this suite.
Feb 28 08:14:26.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:14:27.167: INFO: namespace: e2e-tests-emptydir-wrapper-wdx4n, resource: bindings, ignored listing per whitelist
Feb 28 08:14:28.873: INFO: namespace e2e-tests-emptydir-wrapper-wdx4n deletion completed in 8.437272459s

• [SLOW TEST:164.949 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:14:28.874: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-xtgwn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:14:31.096: INFO: Creating ReplicaSet my-hostname-basic-dfd07950-3b30-11e9-8ad6-42906abdb246
Feb 28 08:14:31.216: INFO: Pod name my-hostname-basic-dfd07950-3b30-11e9-8ad6-42906abdb246: Found 1 pods out of 1
Feb 28 08:14:31.216: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-dfd07950-3b30-11e9-8ad6-42906abdb246" is running
Feb 28 08:14:33.325: INFO: Pod "my-hostname-basic-dfd07950-3b30-11e9-8ad6-42906abdb246-7xdhs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:14:31 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:14:31 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-dfd07950-3b30-11e9-8ad6-42906abdb246]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:14:31 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-dfd07950-3b30-11e9-8ad6-42906abdb246]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:14:31 +0000 UTC Reason: Message:}])
Feb 28 08:14:33.325: INFO: Trying to dial the pod
Feb 28 08:14:38.574: INFO: Controller my-hostname-basic-dfd07950-3b30-11e9-8ad6-42906abdb246: Got expected result from replica 1 [my-hostname-basic-dfd07950-3b30-11e9-8ad6-42906abdb246-7xdhs]: "my-hostname-basic-dfd07950-3b30-11e9-8ad6-42906abdb246-7xdhs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:14:38.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-xtgwn" for this suite.
Feb 28 08:14:44.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:14:45.768: INFO: namespace: e2e-tests-replicaset-xtgwn, resource: bindings, ignored listing per whitelist
Feb 28 08:14:46.851: INFO: namespace e2e-tests-replicaset-xtgwn deletion completed in 8.222330183s

• [SLOW TEST:17.978 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:14:46.852: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-jq2bd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-jq2bd/configmap-test-ea955891-3b30-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 08:14:49.282: INFO: Waiting up to 5m0s for pod "pod-configmaps-ea9e3527-3b30-11e9-8ad6-42906abdb246" in namespace "e2e-tests-configmap-jq2bd" to be "success or failure"
Feb 28 08:14:49.344: INFO: Pod "pod-configmaps-ea9e3527-3b30-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 61.41352ms
Feb 28 08:14:51.399: INFO: Pod "pod-configmaps-ea9e3527-3b30-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.116189708s
STEP: Saw pod success
Feb 28 08:14:51.399: INFO: Pod "pod-configmaps-ea9e3527-3b30-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:14:51.453: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-configmaps-ea9e3527-3b30-11e9-8ad6-42906abdb246 container env-test: <nil>
STEP: delete the pod
Feb 28 08:14:51.576: INFO: Waiting for pod pod-configmaps-ea9e3527-3b30-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:14:51.632: INFO: Pod pod-configmaps-ea9e3527-3b30-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:14:51.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jq2bd" for this suite.
Feb 28 08:14:57.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:14:58.147: INFO: namespace: e2e-tests-configmap-jq2bd, resource: bindings, ignored listing per whitelist
Feb 28 08:15:00.223: INFO: namespace e2e-tests-configmap-jq2bd deletion completed in 8.534725935s

• [SLOW TEST:13.371 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:15:00.223: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-zp699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:15:06.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-zp699" for this suite.
Feb 28 08:15:29.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:15:29.902: INFO: namespace: e2e-tests-replication-controller-zp699, resource: bindings, ignored listing per whitelist
Feb 28 08:15:31.448: INFO: namespace e2e-tests-replication-controller-zp699 deletion completed in 24.424390905s

• [SLOW TEST:31.225 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:15:31.448: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-dbzh9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-dbzh9.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-dbzh9.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-dbzh9.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-dbzh9.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-dbzh9.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-dbzh9.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 08:15:51.462: INFO: DNS probes using e2e-tests-dns-dbzh9/dns-test-053a9873-3b31-11e9-8ad6-42906abdb246 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:15:51.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-dbzh9" for this suite.
Feb 28 08:15:57.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:15:58.404: INFO: namespace: e2e-tests-dns-dbzh9, resource: bindings, ignored listing per whitelist
Feb 28 08:16:00.067: INFO: namespace e2e-tests-dns-dbzh9 deletion completed in 8.482096292s

• [SLOW TEST:28.619 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:16:00.070: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-5mfw9
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 28 08:16:02.548: INFO: Waiting up to 5m0s for pod "pod-16493f0a-3b31-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-5mfw9" to be "success or failure"
Feb 28 08:16:02.608: INFO: Pod "pod-16493f0a-3b31-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.044197ms
Feb 28 08:16:04.670: INFO: Pod "pod-16493f0a-3b31-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121982463s
STEP: Saw pod success
Feb 28 08:16:04.670: INFO: Pod "pod-16493f0a-3b31-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:16:04.731: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-16493f0a-3b31-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 08:16:04.859: INFO: Waiting for pod pod-16493f0a-3b31-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:16:04.918: INFO: Pod pod-16493f0a-3b31-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:16:04.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5mfw9" for this suite.
Feb 28 08:16:11.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:16:13.138: INFO: namespace: e2e-tests-emptydir-5mfw9, resource: bindings, ignored listing per whitelist
Feb 28 08:16:13.430: INFO: namespace e2e-tests-emptydir-5mfw9 deletion completed in 8.449317747s

• [SLOW TEST:13.360 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:16:13.430: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-jxhpz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 08:16:18.783: INFO: Successfully updated pod "annotationupdate1e367b8f-3b31-11e9-8ad6-42906abdb246"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:16:20.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jxhpz" for this suite.
Feb 28 08:16:43.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:16:45.009: INFO: namespace: e2e-tests-downward-api-jxhpz, resource: bindings, ignored listing per whitelist
Feb 28 08:16:45.362: INFO: namespace e2e-tests-downward-api-jxhpz deletion completed in 24.385372313s

• [SLOW TEST:31.932 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:16:45.363: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-4p68t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:16:47.933: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"315189ac-3b31-11e9-ac40-06fd5a6cf138", Controller:(*bool)(0xc00220cf5e), BlockOwnerDeletion:(*bool)(0xc00220cf5f)}}
Feb 28 08:16:47.997: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"313f2506-3b31-11e9-ac40-06fd5a6cf138", Controller:(*bool)(0xc002232ce6), BlockOwnerDeletion:(*bool)(0xc002232ce7)}}
Feb 28 08:16:48.060: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"314879cb-3b31-11e9-ac40-06fd5a6cf138", Controller:(*bool)(0xc002232f66), BlockOwnerDeletion:(*bool)(0xc002232f67)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:16:53.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4p68t" for this suite.
Feb 28 08:16:59.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:17:00.062: INFO: namespace: e2e-tests-gc-4p68t, resource: bindings, ignored listing per whitelist
Feb 28 08:17:01.573: INFO: namespace e2e-tests-gc-4p68t deletion completed in 8.333722768s

• [SLOW TEST:16.210 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:17:01.573: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-r8tcr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:17:04.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3aefc71a-3b31-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-r8tcr" to be "success or failure"
Feb 28 08:17:04.095: INFO: Pod "downwardapi-volume-3aefc71a-3b31-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 57.495015ms
Feb 28 08:17:06.156: INFO: Pod "downwardapi-volume-3aefc71a-3b31-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118185533s
STEP: Saw pod success
Feb 28 08:17:06.156: INFO: Pod "downwardapi-volume-3aefc71a-3b31-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:17:06.218: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-3aefc71a-3b31-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 08:17:06.343: INFO: Waiting for pod downwardapi-volume-3aefc71a-3b31-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:17:06.399: INFO: Pod downwardapi-volume-3aefc71a-3b31-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:17:06.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-r8tcr" for this suite.
Feb 28 08:17:12.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:17:14.742: INFO: namespace: e2e-tests-projected-r8tcr, resource: bindings, ignored listing per whitelist
Feb 28 08:17:14.923: INFO: namespace e2e-tests-projected-r8tcr deletion completed in 8.4696945s

• [SLOW TEST:13.350 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:17:14.923: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-d6ht8
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:17:17.348: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:17:17.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-d6ht8" for this suite.
Feb 28 08:17:24.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:17:24.323: INFO: namespace: e2e-tests-custom-resource-definition-d6ht8, resource: bindings, ignored listing per whitelist
Feb 28 08:17:26.343: INFO: namespace e2e-tests-custom-resource-definition-d6ht8 deletion completed in 8.477274006s

• [SLOW TEST:11.420 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:17:26.344: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-tt4mt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:17:30.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-tt4mt" for this suite.
Feb 28 08:18:23.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:18:23.703: INFO: namespace: e2e-tests-kubelet-test-tt4mt, resource: bindings, ignored listing per whitelist
Feb 28 08:18:25.752: INFO: namespace e2e-tests-kubelet-test-tt4mt deletion completed in 54.715523947s

• [SLOW TEST:59.408 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:18:25.752: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rcwjd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:18:28.223: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d1e362c-3b31-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-rcwjd" to be "success or failure"
Feb 28 08:18:28.279: INFO: Pod "downwardapi-volume-6d1e362c-3b31-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 56.097883ms
Feb 28 08:18:30.350: INFO: Pod "downwardapi-volume-6d1e362c-3b31-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.126887427s
STEP: Saw pod success
Feb 28 08:18:30.350: INFO: Pod "downwardapi-volume-6d1e362c-3b31-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:18:30.409: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-6d1e362c-3b31-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 08:18:30.539: INFO: Waiting for pod downwardapi-volume-6d1e362c-3b31-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:18:30.601: INFO: Pod downwardapi-volume-6d1e362c-3b31-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:18:30.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rcwjd" for this suite.
Feb 28 08:18:36.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:18:38.061: INFO: namespace: e2e-tests-projected-rcwjd, resource: bindings, ignored listing per whitelist
Feb 28 08:18:39.174: INFO: namespace e2e-tests-projected-rcwjd deletion completed in 8.511146927s

• [SLOW TEST:13.422 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:18:39.174: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-vlhl7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 28 08:18:43.867: INFO: Pod pod-hostip-751ca738-3b31-11e9-8ad6-42906abdb246 has hostIP: 10.250.15.222
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:18:43.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-vlhl7" for this suite.
Feb 28 08:19:06.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:19:07.970: INFO: namespace: e2e-tests-pods-vlhl7, resource: bindings, ignored listing per whitelist
Feb 28 08:19:08.395: INFO: namespace e2e-tests-pods-vlhl7 deletion completed in 24.467152224s

• [SLOW TEST:29.222 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:19:08.396: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fwqrb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:19:10.772: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml version'
Feb 28 08:19:11.510: INFO: stderr: ""
Feb 28 08:19:11.510: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"archive\", BuildDate:\"2019-02-28T07:22:21Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:00:57Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:19:11.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fwqrb" for this suite.
Feb 28 08:19:17.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:19:18.349: INFO: namespace: e2e-tests-kubectl-fwqrb, resource: bindings, ignored listing per whitelist
Feb 28 08:19:20.012: INFO: namespace e2e-tests-kubectl-fwqrb deletion completed in 8.44342425s

• [SLOW TEST:11.617 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:19:20.013: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-b2w4n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 28 08:19:22.673: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 28 08:19:22.673: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:23.200: INFO: stderr: ""
Feb 28 08:19:23.200: INFO: stdout: "service/redis-slave created\n"
Feb 28 08:19:23.200: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 28 08:19:23.200: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:23.728: INFO: stderr: ""
Feb 28 08:19:23.728: INFO: stdout: "service/redis-master created\n"
Feb 28 08:19:23.728: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 28 08:19:23.728: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:24.226: INFO: stderr: ""
Feb 28 08:19:24.226: INFO: stdout: "service/frontend created\n"
Feb 28 08:19:24.226: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 28 08:19:24.226: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:24.760: INFO: stderr: ""
Feb 28 08:19:24.760: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 28 08:19:24.760: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 28 08:19:24.760: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:25.274: INFO: stderr: ""
Feb 28 08:19:25.274: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 28 08:19:25.274: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 28 08:19:25.274: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:25.763: INFO: stderr: ""
Feb 28 08:19:25.764: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 28 08:19:25.764: INFO: Waiting for all frontend pods to be Running.
Feb 28 08:19:50.865: INFO: Waiting for frontend to serve content.
Feb 28 08:19:51.011: INFO: Trying to add a new entry to the guestbook.
Feb 28 08:19:51.075: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 28 08:19:51.223: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:51.597: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:19:51.597: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:19:51.597: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:51.988: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:19:51.988: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:19:51.988: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:52.366: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:19:52.367: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:19:52.367: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:52.753: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:19:52.753: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:19:52.753: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:53.121: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:19:53.121: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:19:53.122: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b2w4n'
Feb 28 08:19:53.517: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:19:53.517: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:19:53.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-b2w4n" for this suite.
Feb 28 08:20:39.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:20:40.801: INFO: namespace: e2e-tests-kubectl-b2w4n, resource: bindings, ignored listing per whitelist
Feb 28 08:20:42.003: INFO: namespace e2e-tests-kubectl-b2w4n deletion completed in 48.426581835s

• [SLOW TEST:81.990 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:20:42.003: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-4dwbh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:20:44.373: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:20:47.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4dwbh" for this suite.
Feb 28 08:21:29.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:21:30.409: INFO: namespace: e2e-tests-pods-4dwbh, resource: bindings, ignored listing per whitelist
Feb 28 08:21:31.589: INFO: namespace e2e-tests-pods-4dwbh deletion completed in 44.414249229s

• [SLOW TEST:49.586 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:21:31.589: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-7qzvt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 28 08:21:38.587: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:21:38.646: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:21:40.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:21:40.704: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:21:42.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:21:42.707: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:21:44.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:21:44.707: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:21:46.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:21:46.709: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:21:48.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:21:48.706: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:21:50.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:21:50.704: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:21:52.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:21:52.707: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:21:54.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:21:54.705: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:21:56.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:21:56.704: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:21:58.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:21:58.707: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:22:00.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:22:00.709: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:22:02.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:22:02.709: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:22:04.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:22:04.709: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:22:06.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:22:06.702: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 08:22:08.647: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 08:22:08.705: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:22:08.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-7qzvt" for this suite.
Feb 28 08:22:30.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:22:31.155: INFO: namespace: e2e-tests-container-lifecycle-hook-7qzvt, resource: bindings, ignored listing per whitelist
Feb 28 08:22:33.136: INFO: namespace e2e-tests-container-lifecycle-hook-7qzvt deletion completed in 24.374058931s

• [SLOW TEST:61.547 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:22:33.137: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-b82dr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 28 08:22:35.901: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-b82dr,SelfLink:/api/v1/namespaces/e2e-tests-watch-b82dr/configmaps/e2e-watch-test-resource-version,UID:008c8183-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:11988,Generation:0,CreationTimestamp:2019-02-28 08:22:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:22:35.901: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-b82dr,SelfLink:/api/v1/namespaces/e2e-tests-watch-b82dr/configmaps/e2e-watch-test-resource-version,UID:008c8183-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:11989,Generation:0,CreationTimestamp:2019-02-28 08:22:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:22:35.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-b82dr" for this suite.
Feb 28 08:22:42.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:22:42.422: INFO: namespace: e2e-tests-watch-b82dr, resource: bindings, ignored listing per whitelist
Feb 28 08:22:44.694: INFO: namespace e2e-tests-watch-b82dr deletion completed in 8.737233505s

• [SLOW TEST:11.558 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:22:44.694: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-4zfmh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-4zfmh
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-4zfmh
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-4zfmh
Feb 28 08:22:47.239: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 28 08:22:57.296: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 28 08:22:57.354: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:22:58.588: INFO: stderr: ""
Feb 28 08:22:58.588: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:22:58.588: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:22:58.649: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 28 08:23:08.709: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:23:08.709: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:23:08.954: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999522s
Feb 28 08:23:10.019: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.939106987s
Feb 28 08:23:11.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.874283968s
Feb 28 08:23:12.151: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.802162881s
Feb 28 08:23:13.212: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.742416556s
Feb 28 08:23:14.273: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.6814104s
Feb 28 08:23:15.331: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.620118011s
Feb 28 08:23:16.392: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.561564446s
Feb 28 08:23:17.448: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.501160254s
Feb 28 08:23:18.521: INFO: Verifying statefulset ss doesn't scale past 1 for another 445.425873ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-4zfmh
Feb 28 08:23:19.583: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:23:20.551: INFO: stderr: ""
Feb 28 08:23:20.551: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:23:20.551: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:23:20.609: INFO: Found 1 stateful pods, waiting for 3
Feb 28 08:23:30.664: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:23:30.664: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:23:30.664: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 28 08:23:30.772: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:23:31.793: INFO: stderr: ""
Feb 28 08:23:31.793: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:23:31.793: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:23:31.793: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:23:32.828: INFO: stderr: ""
Feb 28 08:23:32.828: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:23:32.828: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:23:32.828: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:23:33.838: INFO: stderr: ""
Feb 28 08:23:33.838: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:23:33.838: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:23:33.838: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:23:33.895: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 28 08:23:44.018: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:23:44.018: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:23:44.018: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:23:44.196: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999345s
Feb 28 08:23:45.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.938622488s
Feb 28 08:23:46.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.877968497s
Feb 28 08:23:47.376: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.817230445s
Feb 28 08:23:48.437: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.758812544s
Feb 28 08:23:49.497: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.69781705s
Feb 28 08:23:50.555: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.637453589s
Feb 28 08:23:51.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.579784527s
Feb 28 08:23:52.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.517487286s
Feb 28 08:23:53.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 457.371013ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-4zfmh
Feb 28 08:23:54.796: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:23:55.704: INFO: stderr: ""
Feb 28 08:23:55.704: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:23:55.704: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:23:55.705: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:23:56.748: INFO: stderr: ""
Feb 28 08:23:56.748: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:23:56.748: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:23:56.748: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:23:57.419: INFO: rc: 1
Feb 28 08:23:57.419: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0020a3320 exit status 1 <nil> <nil> true [0xc00000f730 0xc00000f750 0xc00000f800] [0xc00000f730 0xc00000f750 0xc00000f800] [0xc00000f748 0xc00000f7f8] [0x933040 0x933040] 0xc001ff9320 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 28 08:24:07.419: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:24:07.836: INFO: rc: 1
Feb 28 08:24:07.837: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020a3830 exit status 1 <nil> <nil> true [0xc00000f870 0xc00000f900 0xc00000f948] [0xc00000f870 0xc00000f900 0xc00000f948] [0xc00000f8b8 0xc00000f938] [0x933040 0x933040] 0xc001ff9740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:24:17.837: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:24:18.183: INFO: rc: 1
Feb 28 08:24:18.183: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00151e960 exit status 1 <nil> <nil> true [0xc000304eb0 0xc000304f98 0xc000305160] [0xc000304eb0 0xc000304f98 0xc000305160] [0xc000304f28 0xc000305098] [0x933040 0x933040] 0xc001c35860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:24:28.183: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:24:28.543: INFO: rc: 1
Feb 28 08:24:28.543: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00151ec00 exit status 1 <nil> <nil> true [0xc0003051a0 0xc000305228 0xc000305270] [0xc0003051a0 0xc000305228 0xc000305270] [0xc0003051f8 0xc000305260] [0x933040 0x933040] 0xc001c35b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:24:38.543: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:24:39.057: INFO: rc: 1
Feb 28 08:24:39.057: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000a61320 exit status 1 <nil> <nil> true [0xc001220290 0xc0012202c8 0xc0012202f8] [0xc001220290 0xc0012202c8 0xc0012202f8] [0xc0012202b0 0xc0012202e8] [0x933040 0x933040] 0xc0021499e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:24:49.058: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:24:49.389: INFO: rc: 1
Feb 28 08:24:49.389: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c9a600 exit status 1 <nil> <nil> true [0xc000984d30 0xc000984dd0 0xc000984f18] [0xc000984d30 0xc000984dd0 0xc000984f18] [0xc000984db0 0xc000984ee8] [0x933040 0x933040] 0xc000d37ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:24:59.389: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:24:59.908: INFO: rc: 1
Feb 28 08:24:59.908: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c9a8a0 exit status 1 <nil> <nil> true [0xc000984f60 0xc000985020 0xc000985050] [0xc000984f60 0xc000985020 0xc000985050] [0xc000985010 0xc000985048] [0x933040 0x933040] 0xc0020b41e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:25:09.908: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:25:10.338: INFO: rc: 1
Feb 28 08:25:10.339: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00151ef90 exit status 1 <nil> <nil> true [0xc0003052c0 0xc000305360 0xc000305480] [0xc0003052c0 0xc000305360 0xc000305480] [0xc0003052e8 0xc000305448] [0x933040 0x933040] 0xc001c35e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:25:20.339: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:25:20.737: INFO: rc: 1
Feb 28 08:25:20.737: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001148300 exit status 1 <nil> <nil> true [0xc001220010 0xc001220030 0xc001220048] [0xc001220010 0xc001220030 0xc001220048] [0xc001220028 0xc001220040] [0x933040 0x933040] 0xc001c7c240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:25:30.737: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:25:31.164: INFO: rc: 1
Feb 28 08:25:31.164: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000ec0510 exit status 1 <nil> <nil> true [0xc00000e148 0xc00000e640 0xc00000e878] [0xc00000e148 0xc00000e640 0xc00000e878] [0xc00000e400 0xc00000e7b0] [0x933040 0x933040] 0xc000d36240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:25:41.165: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:25:41.573: INFO: rc: 1
Feb 28 08:25:41.574: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000ec0a50 exit status 1 <nil> <nil> true [0xc00000eac8 0xc00000ee10 0xc00000ef60] [0xc00000eac8 0xc00000ee10 0xc00000ef60] [0xc00000eb80 0xc00000ef30] [0x933040 0x933040] 0xc000d365a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:25:51.574: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:25:51.964: INFO: rc: 1
Feb 28 08:25:51.964: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001148630 exit status 1 <nil> <nil> true [0xc001220050 0xc001220080 0xc0012200d0] [0xc001220050 0xc001220080 0xc0012200d0] [0xc001220078 0xc0012200b8] [0x933040 0x933040] 0xc001c7c540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:26:01.964: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:26:02.424: INFO: rc: 1
Feb 28 08:26:02.424: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000ec0d20 exit status 1 <nil> <nil> true [0xc00000efa8 0xc00000f088 0xc00000f268] [0xc00000efa8 0xc00000f088 0xc00000f268] [0xc00000efc0 0xc00000f168] [0x933040 0x933040] 0xc000d379e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:26:12.424: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:26:12.887: INFO: rc: 1
Feb 28 08:26:12.887: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0011488a0 exit status 1 <nil> <nil> true [0xc0012200d8 0xc001220110 0xc001220138] [0xc0012200d8 0xc001220110 0xc001220138] [0xc0012200f0 0xc001220130] [0x933040 0x933040] 0xc001c7cba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:26:22.889: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:26:23.374: INFO: rc: 1
Feb 28 08:26:23.374: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001148b40 exit status 1 <nil> <nil> true [0xc001220140 0xc001220158 0xc001220170] [0xc001220140 0xc001220158 0xc001220170] [0xc001220150 0xc001220168] [0x933040 0x933040] 0xc0021480c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:26:33.375: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:26:33.792: INFO: rc: 1
Feb 28 08:26:33.792: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000309620 exit status 1 <nil> <nil> true [0xc000304180 0xc000304238 0xc000304270] [0xc000304180 0xc000304238 0xc000304270] [0xc000304228 0xc000304248] [0x933040 0x933040] 0xc001ff82a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:26:43.792: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:26:44.220: INFO: rc: 1
Feb 28 08:26:44.220: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001149200 exit status 1 <nil> <nil> true [0xc001220178 0xc001220198 0xc0012201b0] [0xc001220178 0xc001220198 0xc0012201b0] [0xc001220190 0xc0012201a8] [0x933040 0x933040] 0xc002148480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:26:54.220: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:26:54.618: INFO: rc: 1
Feb 28 08:26:54.618: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000ec11a0 exit status 1 <nil> <nil> true [0xc00000f288 0xc00000f2d8 0xc00000f338] [0xc00000f288 0xc00000f2d8 0xc00000f338] [0xc00000f2b8 0xc00000f318] [0x933040 0x933040] 0xc000d37d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:27:04.619: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:27:05.222: INFO: rc: 1
Feb 28 08:27:05.223: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000ec1440 exit status 1 <nil> <nil> true [0xc00000f378 0xc00000f400 0xc00000f510] [0xc00000f378 0xc00000f400 0xc00000f510] [0xc00000f3d0 0xc00000f4d8] [0x933040 0x933040] 0xc001c34060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:27:15.223: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:27:15.625: INFO: rc: 1
Feb 28 08:27:15.625: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001cb83f0 exit status 1 <nil> <nil> true [0xc000984000 0xc000984260 0xc000984308] [0xc000984000 0xc000984260 0xc000984308] [0xc000984058 0xc0009842c8] [0x933040 0x933040] 0xc0020b4240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:27:25.625: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:27:26.095: INFO: rc: 1
Feb 28 08:27:26.095: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000ec04e0 exit status 1 <nil> <nil> true [0xc00000e148 0xc00000e640 0xc00000e878] [0xc00000e148 0xc00000e640 0xc00000e878] [0xc00000e400 0xc00000e7b0] [0x933040 0x933040] 0xc000d36240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:27:36.095: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:27:36.486: INFO: rc: 1
Feb 28 08:27:36.486: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0011482d0 exit status 1 <nil> <nil> true [0xc001220008 0xc001220028 0xc001220040] [0xc001220008 0xc001220028 0xc001220040] [0xc001220018 0xc001220038] [0x933040 0x933040] 0xc001c7c240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:27:46.486: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:27:46.948: INFO: rc: 1
Feb 28 08:27:46.948: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000ec0a80 exit status 1 <nil> <nil> true [0xc00000eac8 0xc00000ee10 0xc00000ef60] [0xc00000eac8 0xc00000ee10 0xc00000ef60] [0xc00000eb80 0xc00000ef30] [0x933040 0x933040] 0xc000d365a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:27:56.949: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:27:57.361: INFO: rc: 1
Feb 28 08:27:57.361: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0003095f0 exit status 1 <nil> <nil> true [0xc000304180 0xc000304238 0xc000304270] [0xc000304180 0xc000304238 0xc000304270] [0xc000304228 0xc000304248] [0x933040 0x933040] 0xc001c34240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:28:07.361: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:28:07.826: INFO: rc: 1
Feb 28 08:28:07.826: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001cb80f0 exit status 1 <nil> <nil> true [0xc000304778 0xc0003047d0 0xc0003047f8] [0xc000304778 0xc0003047d0 0xc0003047f8] [0xc0003047c8 0xc0003047e8] [0x933040 0x933040] 0xc001c34540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:28:17.826: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:28:18.285: INFO: rc: 1
Feb 28 08:28:18.285: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002010270 exit status 1 <nil> <nil> true [0xc000984020 0xc000984730 0xc000984b40] [0xc000984020 0xc000984730 0xc000984b40] [0xc000984698 0xc000984998] [0x933040 0x933040] 0xc002148300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:28:28.285: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:28:28.697: INFO: rc: 1
Feb 28 08:28:28.697: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000ec0d80 exit status 1 <nil> <nil> true [0xc00000efa8 0xc00000f088 0xc00000f268] [0xc00000efa8 0xc00000f088 0xc00000f268] [0xc00000efc0 0xc00000f168] [0x933040 0x933040] 0xc000d379e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:28:38.698: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:28:39.059: INFO: rc: 1
Feb 28 08:28:39.059: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000ec11d0 exit status 1 <nil> <nil> true [0xc00000f288 0xc00000f2d8 0xc00000f338] [0xc00000f288 0xc00000f2d8 0xc00000f338] [0xc00000f2b8 0xc00000f318] [0x933040 0x933040] 0xc000d37d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:28:49.062: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:28:49.449: INFO: rc: 1
Feb 28 08:28:49.449: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001cb84e0 exit status 1 <nil> <nil> true [0xc000304850 0xc0003048e0 0xc000304930] [0xc000304850 0xc0003048e0 0xc000304930] [0xc0003048c8 0xc000304918] [0x933040 0x933040] 0xc001c34840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:28:59.449: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zfmh ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:28:59.866: INFO: rc: 1
Feb 28 08:28:59.876: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb 28 08:28:59.882: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:29:00.080: INFO: Deleting all statefulset in ns e2e-tests-statefulset-4zfmh
Feb 28 08:29:00.171: INFO: Scaling statefulset ss to 0
Feb 28 08:29:00.347: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:29:00.409: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:29:00.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-4zfmh" for this suite.
Feb 28 08:29:06.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:08.030: INFO: namespace: e2e-tests-statefulset-4zfmh, resource: bindings, ignored listing per whitelist
Feb 28 08:29:09.254: INFO: namespace e2e-tests-statefulset-4zfmh deletion completed in 8.589761244s

• [SLOW TEST:384.560 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:29:09.254: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-pvdwc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 08:29:11.670: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:29:15.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-pvdwc" for this suite.
Feb 28 08:29:21.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:22.135: INFO: namespace: e2e-tests-init-container-pvdwc, resource: bindings, ignored listing per whitelist
Feb 28 08:29:23.861: INFO: namespace e2e-tests-init-container-pvdwc deletion completed in 8.454887571s

• [SLOW TEST:14.607 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:29:23.862: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-4ghds
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:29:26.193: INFO: Creating deployment "nginx-deployment"
Feb 28 08:29:26.260: INFO: Waiting for observed generation 1
Feb 28 08:29:26.323: INFO: Waiting for all required pods to come up
Feb 28 08:29:26.721: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 28 08:29:30.955: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 28 08:29:31.087: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 28 08:29:31.205: INFO: Updating deployment nginx-deployment
Feb 28 08:29:31.205: INFO: Waiting for observed generation 2
Feb 28 08:29:31.356: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 28 08:29:31.450: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 28 08:29:31.777: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 28 08:29:31.965: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 28 08:29:31.965: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 28 08:29:32.024: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 28 08:29:32.142: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 28 08:29:32.142: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 28 08:29:32.262: INFO: Updating deployment nginx-deployment
Feb 28 08:29:32.262: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 28 08:29:32.385: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 28 08:29:32.451: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 08:29:32.571: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-4ghds,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4ghds/deployments/nginx-deployment,UID:f55a5637-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13056,Generation:3,CreationTimestamp:2019-02-28 08:29:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-02-28 08:29:32 +0000 UTC 2019-02-28 08:29:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-28 08:29:32 +0000 UTC 2019-02-28 08:29:26 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 28 08:29:32.639: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-4ghds,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4ghds/replicasets/nginx-deployment-65bbdb5f8,UID:f84d1816-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13055,Generation:3,CreationTimestamp:2019-02-28 08:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment f55a5637-3b32-11e9-ac40-06fd5a6cf138 0xc001725b87 0xc001725b88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:29:32.639: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 28 08:29:32.639: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-4ghds,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4ghds/replicasets/nginx-deployment-555b55d965,UID:f55afd56-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13053,Generation:3,CreationTimestamp:2019-02-28 08:29:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment f55a5637-3b32-11e9-ac40-06fd5a6cf138 0xc001725ac7 0xc001725ac8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 28 08:29:32.888: INFO: Pod "nginx-deployment-555b55d965-2wn8l" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2wn8l,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-2wn8l,UID:f55f19c5-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:12936,Generation:0,CreationTimestamp:2019-02-28 08:29:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.150/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc00065eb37 0xc00065eb38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00065eba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00065ebc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:100.96.1.150,StartTime:2019-02-28 08:29:26 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:29:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://7f312c3c27a2d25bc4568b8e329cc932ff771703e2c28701331e57cf2b8b6fcd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.888: INFO: Pod "nginx-deployment-555b55d965-58nhb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-58nhb,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-58nhb,UID:f8f255d6-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13070,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc00065f0a7 0xc00065f0a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00065f130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00065f150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.888: INFO: Pod "nginx-deployment-555b55d965-7wwwz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-7wwwz,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-7wwwz,UID:f55ea873-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:12951,Generation:0,CreationTimestamp:2019-02-28 08:29:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.151/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc00065fad7 0xc00065fad8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00065fb50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00065fb80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:100.96.1.151,StartTime:2019-02-28 08:29:26 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:29:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://a7d1d7581baf33ca31aedc5db62be8507714822c0be4bae86e2f77e927c6fe30}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.889: INFO: Pod "nginx-deployment-555b55d965-bvxln" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-bvxln,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-bvxln,UID:f55f0ae5-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:12930,Generation:0,CreationTimestamp:2019-02-28 08:29:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.38/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc00005c1d7 0xc00005c1d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00005c760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00005c870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:100.96.0.38,StartTime:2019-02-28 08:29:26 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:29:27 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://6fff8fbd0bf73599132de95c00b1026013fefa1d35853e1c6f11bd14419aa2f4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.889: INFO: Pod "nginx-deployment-555b55d965-dt5zq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dt5zq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-dt5zq,UID:f55f0a83-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:12939,Generation:0,CreationTimestamp:2019-02-28 08:29:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.149/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc00005d2e0 0xc00005d2e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00005d6d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00005d750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:100.96.1.149,StartTime:2019-02-28 08:29:26 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:29:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://27f165ff15cca7b407dc8124f70f64592318665100e5983b3527fe31b745297b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.897: INFO: Pod "nginx-deployment-555b55d965-dtj8w" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dtj8w,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-dtj8w,UID:f55fb4a8-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:12924,Generation:0,CreationTimestamp:2019-02-28 08:29:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.39/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc00005db57 0xc00005db58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00005dd40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00005de10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:100.96.0.39,StartTime:2019-02-28 08:29:26 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:29:27 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://44516ff0c7f6aa200877572f4a6a8e709f622f2d3d8157074457238964564f55}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.898: INFO: Pod "nginx-deployment-555b55d965-dvr8d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dvr8d,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-dvr8d,UID:f8f25931-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13057,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc0007e8380 0xc0007e8381}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007e8520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007e8550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.898: INFO: Pod "nginx-deployment-555b55d965-grqn8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-grqn8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-grqn8,UID:f8ef1eed-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13059,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc0007e8867 0xc0007e8868}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007e88d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007e8920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.898: INFO: Pod "nginx-deployment-555b55d965-h9rhb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-h9rhb,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-h9rhb,UID:f8f24e1d-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13067,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc0007e8d27 0xc0007e8d28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007e8dd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007e8e00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.898: INFO: Pod "nginx-deployment-555b55d965-j9dq6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-j9dq6,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-j9dq6,UID:f55fb819-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:12948,Generation:0,CreationTimestamp:2019-02-28 08:29:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.147/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc0007e8f27 0xc0007e8f28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007e8fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007e8fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:100.96.1.147,StartTime:2019-02-28 08:29:26 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:29:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://d66832d693c00230e2bcb14006a56011b4cc13e1be41337607890d7e76e49324}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.898: INFO: Pod "nginx-deployment-555b55d965-k6skl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-k6skl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-k6skl,UID:f55ea3cd-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:12933,Generation:0,CreationTimestamp:2019-02-28 08:29:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.37/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc0007e9137 0xc0007e9138}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007e91c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007e91e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:100.96.0.37,StartTime:2019-02-28 08:29:26 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:29:27 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://bd245aafc073aad3272e4aa34c73112b54a16122b1cc4df3f4614cf25385b167}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.898: INFO: Pod "nginx-deployment-555b55d965-kv2hl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-kv2hl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-kv2hl,UID:f8ef761e-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13036,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc0007e95c0 0xc0007e95c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000482000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000482020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.899: INFO: Pod "nginx-deployment-555b55d965-kxx4v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-kxx4v,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-kxx4v,UID:f8f25537-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13060,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc000482107 0xc000482108}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000482390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000482400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.899: INFO: Pod "nginx-deployment-555b55d965-lb7sh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lb7sh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-lb7sh,UID:f8f25ac0-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13068,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc000482707 0xc000482708}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004827f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000482820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.899: INFO: Pod "nginx-deployment-555b55d965-m9hm8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-m9hm8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-m9hm8,UID:f8ef7d28-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13037,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc0004829f7 0xc0004829f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000482a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000482a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.899: INFO: Pod "nginx-deployment-555b55d965-q4dhr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-q4dhr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-q4dhr,UID:f8ef182e-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13048,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc000483227 0xc000483228}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000483290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004832b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.900: INFO: Pod "nginx-deployment-555b55d965-shd4v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-shd4v,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-shd4v,UID:f8ef7afd-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13065,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc0004833d7 0xc0004833d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000483440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000483460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.900: INFO: Pod "nginx-deployment-555b55d965-wbbbz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-wbbbz,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-wbbbz,UID:f8ef7684-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13034,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc0004835c7 0xc0004835c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000483660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000483690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.900: INFO: Pod "nginx-deployment-555b55d965-xcnpm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-xcnpm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-xcnpm,UID:f8eec0ed-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13033,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc000483867 0xc000483868}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000483960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000483990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.900: INFO: Pod "nginx-deployment-555b55d965-z2dgr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-z2dgr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-555b55d965-z2dgr,UID:f55fb0ef-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:12927,Generation:0,CreationTimestamp:2019-02-28 08:29:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.40/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f55afd56-3b32-11e9-ac40-06fd5a6cf138 0xc000483bb7 0xc000483bb8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000483c40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000483c70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:26 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:100.96.0.40,StartTime:2019-02-28 08:29:26 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:29:27 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://eed362673d18f2b16e33569168907460c4af307cb80c6d033a38d4015da63145}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.900: INFO: Pod "nginx-deployment-65bbdb5f8-2t8hz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-2t8hz,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-2t8hz,UID:f8f23e95-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13066,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc000483ee0 0xc000483ee1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0003848e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0003849e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.900: INFO: Pod "nginx-deployment-65bbdb5f8-4xc2z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-4xc2z,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-4xc2z,UID:f84dd838-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13069,Generation:0,CreationTimestamp:2019-02-28 08:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.154/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc0003859c0 0xc0003859c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0000ce090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0000ce0c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:31 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.900: INFO: Pod "nginx-deployment-65bbdb5f8-97dkd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-97dkd,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-97dkd,UID:f863e06d-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:12996,Generation:0,CreationTimestamp:2019-02-28 08:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.42/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc0000ce6e0 0xc0000ce6e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0000ce760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0000ce790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:,StartTime:2019-02-28 08:29:31 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.901: INFO: Pod "nginx-deployment-65bbdb5f8-d4zj6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-d4zj6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-d4zj6,UID:f8f23ff6-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13061,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc0000ce8a0 0xc0000ce8a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0000ce9b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0000ce9d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.901: INFO: Pod "nginx-deployment-65bbdb5f8-dzwr2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-dzwr2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-dzwr2,UID:f8f23fcf-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13054,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc0000cec60 0xc0000cec61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0000ceda0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0000cedd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.901: INFO: Pod "nginx-deployment-65bbdb5f8-gkcbd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-gkcbd,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-gkcbd,UID:f8f24144-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13058,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc0000cf190 0xc0000cf191}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0000cf410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0000cf540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.901: INFO: Pod "nginx-deployment-65bbdb5f8-hnvw2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-hnvw2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-hnvw2,UID:f8ef0256-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13035,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc0000cf7d0 0xc0000cf7d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0000cff80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0000cffa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.901: INFO: Pod "nginx-deployment-65bbdb5f8-hwt4p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-hwt4p,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-hwt4p,UID:f84dd6c8-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:12994,Generation:0,CreationTimestamp:2019-02-28 08:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.41/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc00073e730 0xc00073e731}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00051a2b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00051a370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:,StartTime:2019-02-28 08:29:31 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.901: INFO: Pod "nginx-deployment-65bbdb5f8-m8wvk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-m8wvk,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-m8wvk,UID:f84d7273-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13047,Generation:0,CreationTimestamp:2019-02-28 08:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.152/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc00051ab20 0xc00051ab21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00051ae20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00051aec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:31 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.901: INFO: Pod "nginx-deployment-65bbdb5f8-ptvxp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-ptvxp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-ptvxp,UID:f863926e-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13063,Generation:0,CreationTimestamp:2019-02-28 08:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.153/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc00051b180 0xc00051b181}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00051b400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00051b430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:31 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.901: INFO: Pod "nginx-deployment-65bbdb5f8-qn5c5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-qn5c5,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-qn5c5,UID:f8ef6769-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13064,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc00051b5b0 0xc00051b5b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00051b6b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00051b700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.902: INFO: Pod "nginx-deployment-65bbdb5f8-wdzgj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-wdzgj,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-wdzgj,UID:f9017e15-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13071,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc00051b8f0 0xc00051b8f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00051b9d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00051ba10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:29:32.902: INFO: Pod "nginx-deployment-65bbdb5f8-wsqmb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-wsqmb,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4ghds,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4ghds/pods/nginx-deployment-65bbdb5f8-wsqmb,UID:f8ef6190-3b32-11e9-ac40-06fd5a6cf138,ResourceVersion:13032,Generation:0,CreationTimestamp:2019-02-28 08:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 f84d1816-3b32-11e9-ac40-06fd5a6cf138 0xc00051bbd0 0xc00051bbd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jsc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jsc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00051bde0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00051be20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:,StartTime:2019-02-28 08:29:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:29:32.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-4ghds" for this suite.
Feb 28 08:29:39.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:40.029: INFO: namespace: e2e-tests-deployment-4ghds, resource: bindings, ignored listing per whitelist
Feb 28 08:29:41.541: INFO: namespace e2e-tests-deployment-4ghds deletion completed in 8.582632861s

• [SLOW TEST:17.679 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:29:41.541: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7lqqv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:29:44.059: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fff042a9-3b32-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-7lqqv" to be "success or failure"
Feb 28 08:29:44.120: INFO: Pod "downwardapi-volume-fff042a9-3b32-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.680623ms
Feb 28 08:29:46.179: INFO: Pod "downwardapi-volume-fff042a9-3b32-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11968648s
STEP: Saw pod success
Feb 28 08:29:46.179: INFO: Pod "downwardapi-volume-fff042a9-3b32-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:29:46.238: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-fff042a9-3b32-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 08:29:46.368: INFO: Waiting for pod downwardapi-volume-fff042a9-3b32-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:29:46.429: INFO: Pod downwardapi-volume-fff042a9-3b32-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:29:46.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7lqqv" for this suite.
Feb 28 08:29:52.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:54.370: INFO: namespace: e2e-tests-projected-7lqqv, resource: bindings, ignored listing per whitelist
Feb 28 08:29:54.958: INFO: namespace e2e-tests-projected-7lqqv deletion completed in 8.467647461s

• [SLOW TEST:13.417 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:29:54.958: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qchmj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:29:57.288: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-qchmj'
Feb 28 08:30:01.095: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 08:30:01.095: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Feb 28 08:30:03.214: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-qchmj'
Feb 28 08:30:03.710: INFO: stderr: ""
Feb 28 08:30:03.710: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:30:03.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qchmj" for this suite.
Feb 28 08:30:25.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:30:27.931: INFO: namespace: e2e-tests-kubectl-qchmj, resource: bindings, ignored listing per whitelist
Feb 28 08:30:28.347: INFO: namespace e2e-tests-kubectl-qchmj deletion completed in 24.57753579s

• [SLOW TEST:33.389 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:30:28.348: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7dnsf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-1bd551dd-3b33-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 08:30:30.911: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1bdecfae-3b33-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-7dnsf" to be "success or failure"
Feb 28 08:30:30.971: INFO: Pod "pod-projected-configmaps-1bdecfae-3b33-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.042894ms
Feb 28 08:30:33.032: INFO: Pod "pod-projected-configmaps-1bdecfae-3b33-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121150795s
STEP: Saw pod success
Feb 28 08:30:33.032: INFO: Pod "pod-projected-configmaps-1bdecfae-3b33-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:30:33.090: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-configmaps-1bdecfae-3b33-11e9-8ad6-42906abdb246 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:30:33.215: INFO: Waiting for pod pod-projected-configmaps-1bdecfae-3b33-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:30:33.276: INFO: Pod pod-projected-configmaps-1bdecfae-3b33-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:30:33.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7dnsf" for this suite.
Feb 28 08:30:39.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:30:40.589: INFO: namespace: e2e-tests-projected-7dnsf, resource: bindings, ignored listing per whitelist
Feb 28 08:30:41.895: INFO: namespace e2e-tests-projected-7dnsf deletion completed in 8.49637754s

• [SLOW TEST:13.548 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:30:41.896: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-fvt28
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-23e28a63-3b33-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 08:30:44.411: INFO: Waiting up to 5m0s for pod "pod-secrets-23eb7899-3b33-11e9-8ad6-42906abdb246" in namespace "e2e-tests-secrets-fvt28" to be "success or failure"
Feb 28 08:30:44.481: INFO: Pod "pod-secrets-23eb7899-3b33-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 69.971473ms
Feb 28 08:30:46.540: INFO: Pod "pod-secrets-23eb7899-3b33-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.128921181s
STEP: Saw pod success
Feb 28 08:30:46.540: INFO: Pod "pod-secrets-23eb7899-3b33-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:30:46.598: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-secrets-23eb7899-3b33-11e9-8ad6-42906abdb246 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:30:46.724: INFO: Waiting for pod pod-secrets-23eb7899-3b33-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:30:46.783: INFO: Pod pod-secrets-23eb7899-3b33-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:30:46.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-fvt28" for this suite.
Feb 28 08:30:53.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:30:53.559: INFO: namespace: e2e-tests-secrets-fvt28, resource: bindings, ignored listing per whitelist
Feb 28 08:30:55.435: INFO: namespace e2e-tests-secrets-fvt28 deletion completed in 8.531674687s

• [SLOW TEST:13.539 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:30:55.435: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-c7nrl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:30:57.879: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2bf21851-3b33-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-c7nrl" to be "success or failure"
Feb 28 08:30:57.946: INFO: Pod "downwardapi-volume-2bf21851-3b33-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 67.06179ms
Feb 28 08:31:00.006: INFO: Pod "downwardapi-volume-2bf21851-3b33-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.127331158s
STEP: Saw pod success
Feb 28 08:31:00.007: INFO: Pod "downwardapi-volume-2bf21851-3b33-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:31:00.071: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-2bf21851-3b33-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 08:31:00.199: INFO: Waiting for pod downwardapi-volume-2bf21851-3b33-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:31:00.259: INFO: Pod downwardapi-volume-2bf21851-3b33-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:31:00.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-c7nrl" for this suite.
Feb 28 08:31:06.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:31:08.301: INFO: namespace: e2e-tests-projected-c7nrl, resource: bindings, ignored listing per whitelist
Feb 28 08:31:08.865: INFO: namespace e2e-tests-projected-c7nrl deletion completed in 8.543236317s

• [SLOW TEST:13.430 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:31:08.865: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-mn6lg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:31:35.511: INFO: Container started at 2019-02-28 08:31:12 +0000 UTC, pod became ready at 2019-02-28 08:31:34 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:31:35.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-mn6lg" for this suite.
Feb 28 08:31:57.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:31:58.307: INFO: namespace: e2e-tests-container-probe-mn6lg, resource: bindings, ignored listing per whitelist
Feb 28 08:32:00.023: INFO: namespace e2e-tests-container-probe-mn6lg deletion completed in 24.453404741s

• [SLOW TEST:51.158 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:32:00.024: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-l2sd4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-clng
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:32:02.676: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-clng" in namespace "e2e-tests-subpath-l2sd4" to be "success or failure"
Feb 28 08:32:02.732: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Pending", Reason="", readiness=false. Elapsed: 55.807681ms
Feb 28 08:32:04.794: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Running", Reason="", readiness=false. Elapsed: 2.117237636s
Feb 28 08:32:06.854: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Running", Reason="", readiness=false. Elapsed: 4.177951501s
Feb 28 08:32:08.912: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Running", Reason="", readiness=false. Elapsed: 6.23604213s
Feb 28 08:32:10.971: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Running", Reason="", readiness=false. Elapsed: 8.294586828s
Feb 28 08:32:13.037: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Running", Reason="", readiness=false. Elapsed: 10.360840751s
Feb 28 08:32:15.096: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Running", Reason="", readiness=false. Elapsed: 12.420099775s
Feb 28 08:32:17.161: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Running", Reason="", readiness=false. Elapsed: 14.484487878s
Feb 28 08:32:19.222: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Running", Reason="", readiness=false. Elapsed: 16.546016523s
Feb 28 08:32:21.281: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Running", Reason="", readiness=false. Elapsed: 18.604839332s
Feb 28 08:32:23.343: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Running", Reason="", readiness=false. Elapsed: 20.667044611s
Feb 28 08:32:25.403: INFO: Pod "pod-subpath-test-downwardapi-clng": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.726618258s
STEP: Saw pod success
Feb 28 08:32:25.403: INFO: Pod "pod-subpath-test-downwardapi-clng" satisfied condition "success or failure"
Feb 28 08:32:25.465: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-subpath-test-downwardapi-clng container test-container-subpath-downwardapi-clng: <nil>
STEP: delete the pod
Feb 28 08:32:25.595: INFO: Waiting for pod pod-subpath-test-downwardapi-clng to disappear
Feb 28 08:32:25.655: INFO: Pod pod-subpath-test-downwardapi-clng no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-clng
Feb 28 08:32:25.655: INFO: Deleting pod "pod-subpath-test-downwardapi-clng" in namespace "e2e-tests-subpath-l2sd4"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:32:25.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-l2sd4" for this suite.
Feb 28 08:32:31.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:32:32.192: INFO: namespace: e2e-tests-subpath-l2sd4, resource: bindings, ignored listing per whitelist
Feb 28 08:32:34.290: INFO: namespace e2e-tests-subpath-l2sd4 deletion completed in 8.514188717s

• [SLOW TEST:34.266 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:32:34.291: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-5nh5t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 28 08:32:36.734: INFO: Waiting up to 5m0s for pod "pod-66de6903-3b33-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-5nh5t" to be "success or failure"
Feb 28 08:32:36.794: INFO: Pod "pod-66de6903-3b33-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.076848ms
Feb 28 08:32:38.856: INFO: Pod "pod-66de6903-3b33-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.122151083s
STEP: Saw pod success
Feb 28 08:32:38.856: INFO: Pod "pod-66de6903-3b33-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:32:38.916: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-66de6903-3b33-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 08:32:39.047: INFO: Waiting for pod pod-66de6903-3b33-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:32:39.105: INFO: Pod pod-66de6903-3b33-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:32:39.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5nh5t" for this suite.
Feb 28 08:32:45.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:32:45.586: INFO: namespace: e2e-tests-emptydir-5nh5t, resource: bindings, ignored listing per whitelist
Feb 28 08:32:47.677: INFO: namespace e2e-tests-emptydir-5nh5t deletion completed in 8.510889209s

• [SLOW TEST:13.387 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:32:47.677: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-wqh85
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:32:50.244: INFO: Waiting up to 5m0s for pod "downward-api-6eebe945-3b33-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-wqh85" to be "success or failure"
Feb 28 08:32:50.303: INFO: Pod "downward-api-6eebe945-3b33-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 58.751832ms
Feb 28 08:32:52.364: INFO: Pod "downward-api-6eebe945-3b33-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.120358147s
STEP: Saw pod success
Feb 28 08:32:52.364: INFO: Pod "downward-api-6eebe945-3b33-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:32:52.426: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downward-api-6eebe945-3b33-11e9-8ad6-42906abdb246 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:32:52.551: INFO: Waiting for pod downward-api-6eebe945-3b33-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:32:52.649: INFO: Pod downward-api-6eebe945-3b33-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:32:52.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wqh85" for this suite.
Feb 28 08:32:59.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:33:00.950: INFO: namespace: e2e-tests-downward-api-wqh85, resource: bindings, ignored listing per whitelist
Feb 28 08:33:01.427: INFO: namespace e2e-tests-downward-api-wqh85 deletion completed in 8.60298587s

• [SLOW TEST:13.750 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:33:01.428: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-runtime-qltd4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:33:24.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-qltd4" for this suite.
Feb 28 08:33:30.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:33:31.384: INFO: namespace: e2e-tests-container-runtime-qltd4, resource: bindings, ignored listing per whitelist
Feb 28 08:33:33.238: INFO: namespace e2e-tests-container-runtime-qltd4 deletion completed in 8.677619982s

• [SLOW TEST:31.810 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:33:33.238: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ztq4z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Feb 28 08:33:35.573: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-ztq4z'
Feb 28 08:33:36.351: INFO: stderr: ""
Feb 28 08:33:36.351: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 28 08:33:37.417: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:33:37.417: INFO: Found 1 / 1
Feb 28 08:33:37.417: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 08:33:37.477: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:33:37.477: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 28 08:33:37.477: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml logs redis-master-89f65 redis-master --namespace=e2e-tests-kubectl-ztq4z'
Feb 28 08:33:37.880: INFO: stderr: ""
Feb 28 08:33:37.880: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 08:33:37.131 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 08:33:37.131 # Server started, Redis version 3.2.12\n1:M 28 Feb 08:33:37.131 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 08:33:37.131 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 28 08:33:37.880: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml log redis-master-89f65 redis-master --namespace=e2e-tests-kubectl-ztq4z --tail=1'
Feb 28 08:33:38.318: INFO: stderr: ""
Feb 28 08:33:38.318: INFO: stdout: "1:M 28 Feb 08:33:37.131 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 28 08:33:38.318: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml log redis-master-89f65 redis-master --namespace=e2e-tests-kubectl-ztq4z --limit-bytes=1'
Feb 28 08:33:38.737: INFO: stderr: ""
Feb 28 08:33:38.737: INFO: stdout: " "
STEP: exposing timestamps
Feb 28 08:33:38.737: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml log redis-master-89f65 redis-master --namespace=e2e-tests-kubectl-ztq4z --tail=1 --timestamps'
Feb 28 08:33:39.128: INFO: stderr: ""
Feb 28 08:33:39.128: INFO: stdout: "2019-02-28T08:33:37.131879576Z 1:M 28 Feb 08:33:37.131 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 28 08:33:41.628: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml log redis-master-89f65 redis-master --namespace=e2e-tests-kubectl-ztq4z --since=1s'
Feb 28 08:33:42.030: INFO: stderr: ""
Feb 28 08:33:42.030: INFO: stdout: ""
Feb 28 08:33:42.030: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml log redis-master-89f65 redis-master --namespace=e2e-tests-kubectl-ztq4z --since=24h'
Feb 28 08:33:42.424: INFO: stderr: ""
Feb 28 08:33:42.424: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 08:33:37.131 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 08:33:37.131 # Server started, Redis version 3.2.12\n1:M 28 Feb 08:33:37.131 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 08:33:37.131 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Feb 28 08:33:42.424: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ztq4z'
Feb 28 08:33:42.885: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:33:42.885: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 28 08:33:42.885: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-ztq4z'
Feb 28 08:33:43.282: INFO: stderr: "No resources found.\n"
Feb 28 08:33:43.282: INFO: stdout: ""
Feb 28 08:33:43.282: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -l name=nginx --namespace=e2e-tests-kubectl-ztq4z -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 08:33:43.599: INFO: stderr: ""
Feb 28 08:33:43.599: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:33:43.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ztq4z" for this suite.
Feb 28 08:33:49.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:33:50.605: INFO: namespace: e2e-tests-kubectl-ztq4z, resource: bindings, ignored listing per whitelist
Feb 28 08:33:52.164: INFO: namespace e2e-tests-kubectl-ztq4z deletion completed in 8.504753567s

• [SLOW TEST:18.926 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:33:52.165: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-n8bzw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-953e22c4-3b33-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 08:33:54.598: INFO: Waiting up to 5m0s for pod "pod-secrets-9547588b-3b33-11e9-8ad6-42906abdb246" in namespace "e2e-tests-secrets-n8bzw" to be "success or failure"
Feb 28 08:33:54.658: INFO: Pod "pod-secrets-9547588b-3b33-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 59.785313ms
Feb 28 08:33:56.719: INFO: Pod "pod-secrets-9547588b-3b33-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.120337955s
STEP: Saw pod success
Feb 28 08:33:56.719: INFO: Pod "pod-secrets-9547588b-3b33-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:33:56.782: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-secrets-9547588b-3b33-11e9-8ad6-42906abdb246 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:33:56.905: INFO: Waiting for pod pod-secrets-9547588b-3b33-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:33:56.962: INFO: Pod pod-secrets-9547588b-3b33-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:33:56.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-n8bzw" for this suite.
Feb 28 08:34:03.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:34:03.683: INFO: namespace: e2e-tests-secrets-n8bzw, resource: bindings, ignored listing per whitelist
Feb 28 08:34:05.534: INFO: namespace e2e-tests-secrets-n8bzw deletion completed in 8.515212093s

• [SLOW TEST:13.370 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:34:05.535: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-zxvqs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:34:10.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-zxvqs" for this suite.
Feb 28 08:35:02.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:35:03.565: INFO: namespace: e2e-tests-kubelet-test-zxvqs, resource: bindings, ignored listing per whitelist
Feb 28 08:35:04.788: INFO: namespace e2e-tests-kubelet-test-zxvqs deletion completed in 54.534118812s

• [SLOW TEST:59.253 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:35:04.788: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-k7nz6
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-c09c3e13-3b33-11e9-8ad6-42906abdb246
STEP: Creating configMap with name cm-test-opt-upd-c09c3f4d-3b33-11e9-8ad6-42906abdb246
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c09c3e13-3b33-11e9-8ad6-42906abdb246
STEP: Updating configmap cm-test-opt-upd-c09c3f4d-3b33-11e9-8ad6-42906abdb246
STEP: Creating configMap with name cm-test-opt-create-c09c3f63-3b33-11e9-8ad6-42906abdb246
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:35:12.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-k7nz6" for this suite.
Feb 28 08:35:34.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:35:35.182: INFO: namespace: e2e-tests-configmap-k7nz6, resource: bindings, ignored listing per whitelist
Feb 28 08:35:36.794: INFO: namespace e2e-tests-configmap-k7nz6 deletion completed in 24.496394174s

• [SLOW TEST:32.006 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:35:36.795: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-vf88m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:35:39.295: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 08:35:41.419: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 28 08:35:43.478: INFO: Creating deployment "test-rollover-deployment"
Feb 28 08:35:43.596: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 28 08:35:43.655: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 28 08:35:43.770: INFO: Ensure that both replica sets have 1 created replica
Feb 28 08:35:43.890: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 28 08:35:44.009: INFO: Updating deployment test-rollover-deployment
Feb 28 08:35:44.009: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 28 08:35:44.068: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 28 08:35:44.188: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 28 08:35:44.318: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:35:44.318: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:1, UpdatedReplicas:0, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939744, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:35:46.442: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:35:46.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939744, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:35:48.438: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:35:48.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939744, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:35:50.436: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:35:50.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939744, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:35:52.439: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:35:52.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939744, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:35:54.438: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:35:54.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939744, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939743, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:35:56.441: INFO: 
Feb 28 08:35:56.442: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 08:35:56.617: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-vf88m,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vf88m/deployments/test-rollover-deployment,UID:d63a4b19-3b33-11e9-ac40-06fd5a6cf138,ResourceVersion:14293,Generation:2,CreationTimestamp:2019-02-28 08:35:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-28 08:35:43 +0000 UTC 2019-02-28 08:35:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-28 08:35:54 +0000 UTC 2019-02-28 08:35:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 08:35:56.676: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-vf88m,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vf88m/replicasets/test-rollover-deployment-6b7f9d6597,UID:d6829e70-3b33-11e9-ac40-06fd5a6cf138,ResourceVersion:14286,Generation:2,CreationTimestamp:2019-02-28 08:35:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d63a4b19-3b33-11e9-ac40-06fd5a6cf138 0xc0017be4e7 0xc0017be4e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 08:35:56.676: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 28 08:35:56.677: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-vf88m,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vf88m/replicasets/test-rollover-controller,UID:d3a9fd4e-3b33-11e9-ac40-06fd5a6cf138,ResourceVersion:14292,Generation:2,CreationTimestamp:2019-02-28 08:35:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d63a4b19-3b33-11e9-ac40-06fd5a6cf138 0xc0017be2c7 0xc0017be2c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:35:56.677: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-vf88m,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vf88m/replicasets/test-rollover-deployment-6586df867b,UID:d63b7dfc-3b33-11e9-ac40-06fd5a6cf138,ResourceVersion:14258,Generation:2,CreationTimestamp:2019-02-28 08:35:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d63a4b19-3b33-11e9-ac40-06fd5a6cf138 0xc0017be387 0xc0017be388}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:35:56.737: INFO: Pod "test-rollover-deployment-6b7f9d6597-2sbs5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-2sbs5,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-vf88m,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vf88m/pods/test-rollover-deployment-6b7f9d6597-2sbs5,UID:d6840a71-3b33-11e9-ac40-06fd5a6cf138,ResourceVersion:14264,Generation:0,CreationTimestamp:2019-02-28 08:35:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.52/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 d6829e70-3b33-11e9-ac40-06fd5a6cf138 0xc0017bf757 0xc0017bf758}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-mk7s5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mk7s5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-mk7s5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-20-37.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017bf7c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017bf7e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:35:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:35:44 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:35:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:35:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.20.37,PodIP:100.96.0.52,StartTime:2019-02-28 08:35:43 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-28 08:35:44 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://fd4dab20daeca8292437d418d5f1f45ce686fa6f3c4771870bf1be8feced694b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:35:56.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-vf88m" for this suite.
Feb 28 08:36:02.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:36:04.078: INFO: namespace: e2e-tests-deployment-vf88m, resource: bindings, ignored listing per whitelist
Feb 28 08:36:05.263: INFO: namespace e2e-tests-deployment-vf88m deletion completed in 8.464368843s

• [SLOW TEST:28.469 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:36:05.264: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-7pb59
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 28 08:36:07.632: INFO: Waiting up to 5m0s for pod "client-containers-e492be95-3b33-11e9-8ad6-42906abdb246" in namespace "e2e-tests-containers-7pb59" to be "success or failure"
Feb 28 08:36:07.692: INFO: Pod "client-containers-e492be95-3b33-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 59.738961ms
Feb 28 08:36:09.753: INFO: Pod "client-containers-e492be95-3b33-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.120507394s
STEP: Saw pod success
Feb 28 08:36:09.753: INFO: Pod "client-containers-e492be95-3b33-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:36:09.813: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod client-containers-e492be95-3b33-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 08:36:09.939: INFO: Waiting for pod client-containers-e492be95-3b33-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:36:09.998: INFO: Pod client-containers-e492be95-3b33-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:36:09.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-7pb59" for this suite.
Feb 28 08:36:16.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:36:17.628: INFO: namespace: e2e-tests-containers-7pb59, resource: bindings, ignored listing per whitelist
Feb 28 08:36:18.579: INFO: namespace e2e-tests-containers-7pb59 deletion completed in 8.519767349s

• [SLOW TEST:13.315 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:36:18.579: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-hgvcm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-hgvcm
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 08:36:20.971: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 08:36:38.077: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.184:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-hgvcm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:36:38.077: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 08:36:38.884: INFO: Found all expected endpoints: [netserver-0]
Feb 28 08:36:38.942: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.53:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-hgvcm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:36:38.942: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 08:36:39.623: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:36:39.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-hgvcm" for this suite.
Feb 28 08:37:01.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:37:02.177: INFO: namespace: e2e-tests-pod-network-test-hgvcm, resource: bindings, ignored listing per whitelist
Feb 28 08:37:04.222: INFO: namespace e2e-tests-pod-network-test-hgvcm deletion completed in 24.541845686s

• [SLOW TEST:45.643 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:37:04.223: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-phgcs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 28 08:37:06.638: INFO: Waiting up to 5m0s for pod "client-containers-07bf6228-3b34-11e9-8ad6-42906abdb246" in namespace "e2e-tests-containers-phgcs" to be "success or failure"
Feb 28 08:37:06.695: INFO: Pod "client-containers-07bf6228-3b34-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 56.564086ms
Feb 28 08:37:08.754: INFO: Pod "client-containers-07bf6228-3b34-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.115255348s
STEP: Saw pod success
Feb 28 08:37:08.754: INFO: Pod "client-containers-07bf6228-3b34-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:37:08.813: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod client-containers-07bf6228-3b34-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 08:37:08.960: INFO: Waiting for pod client-containers-07bf6228-3b34-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:37:09.022: INFO: Pod client-containers-07bf6228-3b34-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:37:09.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-phgcs" for this suite.
Feb 28 08:37:15.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:37:16.976: INFO: namespace: e2e-tests-containers-phgcs, resource: bindings, ignored listing per whitelist
Feb 28 08:37:17.687: INFO: namespace e2e-tests-containers-phgcs deletion completed in 8.605968639s

• [SLOW TEST:13.465 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:37:17.688: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-t4g4t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 28 08:37:20.134: INFO: Waiting up to 5m0s for pod "var-expansion-0fc97eeb-3b34-11e9-8ad6-42906abdb246" in namespace "e2e-tests-var-expansion-t4g4t" to be "success or failure"
Feb 28 08:37:20.194: INFO: Pod "var-expansion-0fc97eeb-3b34-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.461325ms
Feb 28 08:37:22.254: INFO: Pod "var-expansion-0fc97eeb-3b34-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.120555135s
STEP: Saw pod success
Feb 28 08:37:22.255: INFO: Pod "var-expansion-0fc97eeb-3b34-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:37:22.311: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod var-expansion-0fc97eeb-3b34-11e9-8ad6-42906abdb246 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:37:22.437: INFO: Waiting for pod var-expansion-0fc97eeb-3b34-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:37:22.499: INFO: Pod var-expansion-0fc97eeb-3b34-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:37:22.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-t4g4t" for this suite.
Feb 28 08:37:28.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:37:30.025: INFO: namespace: e2e-tests-var-expansion-t4g4t, resource: bindings, ignored listing per whitelist
Feb 28 08:37:31.102: INFO: namespace e2e-tests-var-expansion-t4g4t deletion completed in 8.543587599s

• [SLOW TEST:13.414 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:37:31.102: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-dbwqp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:37:33.480: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:37:36.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-dbwqp" for this suite.
Feb 28 08:38:18.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:38:18.746: INFO: namespace: e2e-tests-pods-dbwqp, resource: bindings, ignored listing per whitelist
Feb 28 08:38:20.678: INFO: namespace e2e-tests-pods-dbwqp deletion completed in 44.408591477s

• [SLOW TEST:49.576 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:38:20.678: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-tx9b5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-rz9b
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:38:23.246: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rz9b" in namespace "e2e-tests-subpath-tx9b5" to be "success or failure"
Feb 28 08:38:23.312: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Pending", Reason="", readiness=false. Elapsed: 65.822091ms
Feb 28 08:38:25.371: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.124750409s
Feb 28 08:38:27.449: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Running", Reason="", readiness=false. Elapsed: 4.20271445s
Feb 28 08:38:29.507: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Running", Reason="", readiness=false. Elapsed: 6.261009703s
Feb 28 08:38:31.566: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Running", Reason="", readiness=false. Elapsed: 8.319858915s
Feb 28 08:38:33.629: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Running", Reason="", readiness=false. Elapsed: 10.382722464s
Feb 28 08:38:35.693: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Running", Reason="", readiness=false. Elapsed: 12.446975481s
Feb 28 08:38:37.754: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Running", Reason="", readiness=false. Elapsed: 14.508132221s
Feb 28 08:38:39.815: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Running", Reason="", readiness=false. Elapsed: 16.569003918s
Feb 28 08:38:41.878: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Running", Reason="", readiness=false. Elapsed: 18.631589091s
Feb 28 08:38:43.939: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Running", Reason="", readiness=false. Elapsed: 20.692589344s
Feb 28 08:38:46.000: INFO: Pod "pod-subpath-test-projected-rz9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.753783986s
STEP: Saw pod success
Feb 28 08:38:46.000: INFO: Pod "pod-subpath-test-projected-rz9b" satisfied condition "success or failure"
Feb 28 08:38:46.061: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-subpath-test-projected-rz9b container test-container-subpath-projected-rz9b: <nil>
STEP: delete the pod
Feb 28 08:38:46.193: INFO: Waiting for pod pod-subpath-test-projected-rz9b to disappear
Feb 28 08:38:46.253: INFO: Pod pod-subpath-test-projected-rz9b no longer exists
STEP: Deleting pod pod-subpath-test-projected-rz9b
Feb 28 08:38:46.253: INFO: Deleting pod "pod-subpath-test-projected-rz9b" in namespace "e2e-tests-subpath-tx9b5"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:38:46.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-tx9b5" for this suite.
Feb 28 08:38:52.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:38:54.631: INFO: namespace: e2e-tests-subpath-tx9b5, resource: bindings, ignored listing per whitelist
Feb 28 08:38:54.932: INFO: namespace e2e-tests-subpath-tx9b5 deletion completed in 8.557905516s

• [SLOW TEST:34.254 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:38:54.933: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-8cxh4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 28 08:39:00.202: INFO: Successfully updated pod "pod-update-49ba23c8-3b34-11e9-8ad6-42906abdb246"
STEP: verifying the updated pod is in kubernetes
Feb 28 08:39:00.321: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:39:00.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-8cxh4" for this suite.
Feb 28 08:39:22.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:39:24.427: INFO: namespace: e2e-tests-pods-8cxh4, resource: bindings, ignored listing per whitelist
Feb 28 08:39:24.896: INFO: namespace e2e-tests-pods-8cxh4 deletion completed in 24.513397938s

• [SLOW TEST:29.964 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:39:24.896: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-kvrd9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:39:27.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b9bb7ad-3b34-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-kvrd9" to be "success or failure"
Feb 28 08:39:27.402: INFO: Pod "downwardapi-volume-5b9bb7ad-3b34-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 59.408523ms
Feb 28 08:39:29.460: INFO: Pod "downwardapi-volume-5b9bb7ad-3b34-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118020546s
STEP: Saw pod success
Feb 28 08:39:29.460: INFO: Pod "downwardapi-volume-5b9bb7ad-3b34-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:39:29.520: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-5b9bb7ad-3b34-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 08:39:29.649: INFO: Waiting for pod downwardapi-volume-5b9bb7ad-3b34-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:39:29.710: INFO: Pod downwardapi-volume-5b9bb7ad-3b34-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:39:29.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kvrd9" for this suite.
Feb 28 08:39:35.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:39:37.638: INFO: namespace: e2e-tests-downward-api-kvrd9, resource: bindings, ignored listing per whitelist
Feb 28 08:39:38.497: INFO: namespace e2e-tests-downward-api-kvrd9 deletion completed in 8.726431693s

• [SLOW TEST:13.601 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:39:38.497: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-8kzgm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-8kzgm
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 08:39:40.859: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 08:39:57.908: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 100.96.1.192 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-8kzgm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:39:57.909: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 08:39:59.810: INFO: Found all expected endpoints: [netserver-0]
Feb 28 08:39:59.869: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 100.96.0.54 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-8kzgm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:39:59.869: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 08:40:01.551: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:40:01.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-8kzgm" for this suite.
Feb 28 08:40:23.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:40:25.581: INFO: namespace: e2e-tests-pod-network-test-8kzgm, resource: bindings, ignored listing per whitelist
Feb 28 08:40:26.114: INFO: namespace e2e-tests-pod-network-test-8kzgm deletion completed in 24.492085487s

• [SLOW TEST:47.617 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:40:26.115: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6llg7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 28 08:40:28.508: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml --namespace=e2e-tests-kubectl-6llg7 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 28 08:40:34.786: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 28 08:40:34.786: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:40:36.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6llg7" for this suite.
Feb 28 08:40:43.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:40:43.942: INFO: namespace: e2e-tests-kubectl-6llg7, resource: bindings, ignored listing per whitelist
Feb 28 08:40:45.490: INFO: namespace e2e-tests-kubectl-6llg7 deletion completed in 8.518243824s

• [SLOW TEST:19.376 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:40:45.490: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-k5nz4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:40:48.006: INFO: Waiting up to 5m0s for pod "downward-api-8bb0a6b5-3b34-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-k5nz4" to be "success or failure"
Feb 28 08:40:48.064: INFO: Pod "downward-api-8bb0a6b5-3b34-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 57.827147ms
Feb 28 08:40:50.122: INFO: Pod "downward-api-8bb0a6b5-3b34-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.116030358s
STEP: Saw pod success
Feb 28 08:40:50.122: INFO: Pod "downward-api-8bb0a6b5-3b34-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:40:50.180: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downward-api-8bb0a6b5-3b34-11e9-8ad6-42906abdb246 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:40:50.316: INFO: Waiting for pod downward-api-8bb0a6b5-3b34-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:40:50.378: INFO: Pod downward-api-8bb0a6b5-3b34-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:40:50.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-k5nz4" for this suite.
Feb 28 08:40:56.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:40:58.102: INFO: namespace: e2e-tests-downward-api-k5nz4, resource: bindings, ignored listing per whitelist
Feb 28 08:40:58.974: INFO: namespace e2e-tests-downward-api-k5nz4 deletion completed in 8.536388977s

• [SLOW TEST:13.484 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:40:58.982: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-sq7pn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-93a4c19e-3b34-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 08:41:01.434: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-93ad6b28-3b34-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-sq7pn" to be "success or failure"
Feb 28 08:41:01.493: INFO: Pod "pod-projected-secrets-93ad6b28-3b34-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 59.175866ms
Feb 28 08:41:03.552: INFO: Pod "pod-projected-secrets-93ad6b28-3b34-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117748199s
STEP: Saw pod success
Feb 28 08:41:03.552: INFO: Pod "pod-projected-secrets-93ad6b28-3b34-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:41:03.610: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-secrets-93ad6b28-3b34-11e9-8ad6-42906abdb246 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:41:03.746: INFO: Waiting for pod pod-projected-secrets-93ad6b28-3b34-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:41:03.806: INFO: Pod pod-projected-secrets-93ad6b28-3b34-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:41:03.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sq7pn" for this suite.
Feb 28 08:41:10.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:41:10.766: INFO: namespace: e2e-tests-projected-sq7pn, resource: bindings, ignored listing per whitelist
Feb 28 08:41:12.375: INFO: namespace e2e-tests-projected-sq7pn deletion completed in 8.508328827s

• [SLOW TEST:13.393 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:41:12.375: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vm59c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 28 08:41:14.782: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:15.479: INFO: stderr: ""
Feb 28 08:41:15.479: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 08:41:15.479: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:15.819: INFO: stderr: ""
Feb 28 08:41:15.819: INFO: stdout: "update-demo-nautilus-7k99d update-demo-nautilus-njn86 "
Feb 28 08:41:15.820: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-7k99d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:16.153: INFO: stderr: ""
Feb 28 08:41:16.153: INFO: stdout: ""
Feb 28 08:41:16.153: INFO: update-demo-nautilus-7k99d is created but not running
Feb 28 08:41:21.154: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:21.486: INFO: stderr: ""
Feb 28 08:41:21.486: INFO: stdout: "update-demo-nautilus-7k99d update-demo-nautilus-njn86 "
Feb 28 08:41:21.486: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-7k99d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:21.812: INFO: stderr: ""
Feb 28 08:41:21.812: INFO: stdout: "true"
Feb 28 08:41:21.812: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-7k99d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:22.141: INFO: stderr: ""
Feb 28 08:41:22.141: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:41:22.141: INFO: validating pod update-demo-nautilus-7k99d
Feb 28 08:41:22.289: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:41:22.292: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:41:22.292: INFO: update-demo-nautilus-7k99d is verified up and running
Feb 28 08:41:22.292: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-njn86 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:22.676: INFO: stderr: ""
Feb 28 08:41:22.676: INFO: stdout: "true"
Feb 28 08:41:22.676: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-njn86 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:23.004: INFO: stderr: ""
Feb 28 08:41:23.004: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:41:23.004: INFO: validating pod update-demo-nautilus-njn86
Feb 28 08:41:23.149: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:41:23.149: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:41:23.149: INFO: update-demo-nautilus-njn86 is verified up and running
STEP: scaling down the replication controller
Feb 28 08:41:23.162: INFO: scanned /root for discovery docs: <nil>
Feb 28 08:41:23.162: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:23.690: INFO: stderr: ""
Feb 28 08:41:23.690: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 08:41:23.690: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:24.078: INFO: stderr: ""
Feb 28 08:41:24.078: INFO: stdout: "update-demo-nautilus-7k99d update-demo-nautilus-njn86 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 28 08:41:29.079: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:29.436: INFO: stderr: ""
Feb 28 08:41:29.436: INFO: stdout: "update-demo-nautilus-njn86 "
Feb 28 08:41:29.436: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-njn86 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:29.758: INFO: stderr: ""
Feb 28 08:41:29.758: INFO: stdout: "true"
Feb 28 08:41:29.758: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-njn86 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:30.110: INFO: stderr: ""
Feb 28 08:41:30.110: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:41:30.110: INFO: validating pod update-demo-nautilus-njn86
Feb 28 08:41:30.170: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:41:30.170: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:41:30.170: INFO: update-demo-nautilus-njn86 is verified up and running
STEP: scaling up the replication controller
Feb 28 08:41:30.175: INFO: scanned /root for discovery docs: <nil>
Feb 28 08:41:30.176: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:30.712: INFO: stderr: ""
Feb 28 08:41:30.712: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 08:41:30.712: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:31.078: INFO: stderr: ""
Feb 28 08:41:31.078: INFO: stdout: "update-demo-nautilus-4ggj2 update-demo-nautilus-njn86 "
Feb 28 08:41:31.078: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-4ggj2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:31.464: INFO: stderr: ""
Feb 28 08:41:31.464: INFO: stdout: ""
Feb 28 08:41:31.464: INFO: update-demo-nautilus-4ggj2 is created but not running
Feb 28 08:41:36.464: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:36.791: INFO: stderr: ""
Feb 28 08:41:36.791: INFO: stdout: "update-demo-nautilus-4ggj2 update-demo-nautilus-njn86 "
Feb 28 08:41:36.791: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-4ggj2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:37.114: INFO: stderr: ""
Feb 28 08:41:37.114: INFO: stdout: "true"
Feb 28 08:41:37.114: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-4ggj2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:37.443: INFO: stderr: ""
Feb 28 08:41:37.443: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:41:37.443: INFO: validating pod update-demo-nautilus-4ggj2
Feb 28 08:41:37.591: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:41:37.591: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:41:37.591: INFO: update-demo-nautilus-4ggj2 is verified up and running
Feb 28 08:41:37.591: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-njn86 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:37.906: INFO: stderr: ""
Feb 28 08:41:37.906: INFO: stdout: "true"
Feb 28 08:41:37.906: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-njn86 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:38.230: INFO: stderr: ""
Feb 28 08:41:38.230: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:41:38.230: INFO: validating pod update-demo-nautilus-njn86
Feb 28 08:41:38.293: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:41:38.294: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:41:38.294: INFO: update-demo-nautilus-njn86 is verified up and running
STEP: using delete to clean up resources
Feb 28 08:41:38.294: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:38.697: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:41:38.697: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 28 08:41:38.697: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-vm59c'
Feb 28 08:41:39.086: INFO: stderr: "No resources found.\n"
Feb 28 08:41:39.086: INFO: stdout: ""
Feb 28 08:41:39.086: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-vm59c -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 08:41:39.433: INFO: stderr: ""
Feb 28 08:41:39.433: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:41:39.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vm59c" for this suite.
Feb 28 08:42:01.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:42:03.327: INFO: namespace: e2e-tests-kubectl-vm59c, resource: bindings, ignored listing per whitelist
Feb 28 08:42:03.977: INFO: namespace e2e-tests-kubectl-vm59c deletion completed in 24.479645084s

• [SLOW TEST:51.602 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:42:03.977: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-2mvcf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-2mvcf
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 28 08:42:06.664: INFO: Found 1 stateful pods, waiting for 3
Feb 28 08:42:16.725: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:42:16.725: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:42:16.725: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:42:16.906: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-2mvcf ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:42:18.073: INFO: stderr: ""
Feb 28 08:42:18.073: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:42:18.073: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 28 08:42:28.456: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 28 08:42:28.633: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-2mvcf ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:29.635: INFO: stderr: ""
Feb 28 08:42:29.635: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:42:29.635: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

STEP: Rolling back to a previous revision
Feb 28 08:42:59.988: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-2mvcf ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:43:01.034: INFO: stderr: ""
Feb 28 08:43:01.034: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:43:01.034: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:43:11.402: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 28 08:43:11.583: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-2mvcf ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:43:12.588: INFO: stderr: ""
Feb 28 08:43:12.588: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:43:12.588: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:43:22.955: INFO: Waiting for StatefulSet e2e-tests-statefulset-2mvcf/ss2 to complete update
Feb 28 08:43:22.955: INFO: Waiting for Pod e2e-tests-statefulset-2mvcf/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:43:33.073: INFO: Deleting all statefulset in ns e2e-tests-statefulset-2mvcf
Feb 28 08:43:33.133: INFO: Scaling statefulset ss2 to 0
Feb 28 08:43:53.370: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:43:53.428: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:43:53.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-2mvcf" for this suite.
Feb 28 08:43:59.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:44:01.555: INFO: namespace: e2e-tests-statefulset-2mvcf, resource: bindings, ignored listing per whitelist
Feb 28 08:44:02.085: INFO: namespace e2e-tests-statefulset-2mvcf deletion completed in 8.419936429s

• [SLOW TEST:118.108 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:44:02.085: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-fxfcc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-00c8f492-3b35-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 08:44:04.524: INFO: Waiting up to 5m0s for pod "pod-configmaps-00d26693-3b35-11e9-8ad6-42906abdb246" in namespace "e2e-tests-configmap-fxfcc" to be "success or failure"
Feb 28 08:44:04.581: INFO: Pod "pod-configmaps-00d26693-3b35-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 56.452772ms
Feb 28 08:44:06.642: INFO: Pod "pod-configmaps-00d26693-3b35-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117576852s
STEP: Saw pod success
Feb 28 08:44:06.642: INFO: Pod "pod-configmaps-00d26693-3b35-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:44:06.700: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-configmaps-00d26693-3b35-11e9-8ad6-42906abdb246 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:44:06.830: INFO: Waiting for pod pod-configmaps-00d26693-3b35-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:44:06.890: INFO: Pod pod-configmaps-00d26693-3b35-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:44:06.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-fxfcc" for this suite.
Feb 28 08:44:13.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:44:14.200: INFO: namespace: e2e-tests-configmap-fxfcc, resource: bindings, ignored listing per whitelist
Feb 28 08:44:15.439: INFO: namespace e2e-tests-configmap-fxfcc deletion completed in 8.488391755s

• [SLOW TEST:13.355 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:44:15.440: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-svpmb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:44:17.831: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08c1611e-3b35-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-svpmb" to be "success or failure"
Feb 28 08:44:17.890: INFO: Pod "downwardapi-volume-08c1611e-3b35-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 58.134909ms
Feb 28 08:44:19.950: INFO: Pod "downwardapi-volume-08c1611e-3b35-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118393792s
STEP: Saw pod success
Feb 28 08:44:19.950: INFO: Pod "downwardapi-volume-08c1611e-3b35-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:44:20.010: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-08c1611e-3b35-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 08:44:20.139: INFO: Waiting for pod downwardapi-volume-08c1611e-3b35-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:44:20.197: INFO: Pod downwardapi-volume-08c1611e-3b35-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:44:20.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-svpmb" for this suite.
Feb 28 08:44:26.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:44:27.741: INFO: namespace: e2e-tests-projected-svpmb, resource: bindings, ignored listing per whitelist
Feb 28 08:44:28.756: INFO: namespace e2e-tests-projected-svpmb deletion completed in 8.497294828s

• [SLOW TEST:13.316 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:44:28.756: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-gj9z7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-gj9z7
Feb 28 08:44:33.261: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-gj9z7
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:44:33.319: INFO: Initial restart count of pod liveness-http is 0
Feb 28 08:44:47.788: INFO: Restart count of pod e2e-tests-container-probe-gj9z7/liveness-http is now 1 (14.468154624s elapsed)
Feb 28 08:45:06.376: INFO: Restart count of pod e2e-tests-container-probe-gj9z7/liveness-http is now 2 (33.056329011s elapsed)
Feb 28 08:45:26.999: INFO: Restart count of pod e2e-tests-container-probe-gj9z7/liveness-http is now 3 (53.679774534s elapsed)
Feb 28 08:45:47.604: INFO: Restart count of pod e2e-tests-container-probe-gj9z7/liveness-http is now 4 (1m14.284519297s elapsed)
Feb 28 08:46:57.695: INFO: Restart count of pod e2e-tests-container-probe-gj9z7/liveness-http is now 5 (2m24.375441447s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:46:57.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-gj9z7" for this suite.
Feb 28 08:47:04.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:05.940: INFO: namespace: e2e-tests-container-probe-gj9z7, resource: bindings, ignored listing per whitelist
Feb 28 08:47:06.346: INFO: namespace e2e-tests-container-probe-gj9z7 deletion completed in 8.52564455s

• [SLOW TEST:157.590 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:47:06.346: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-97pf2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:47:08.671: INFO: Creating deployment "test-recreate-deployment"
Feb 28 08:47:08.731: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 28 08:47:08.852: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 28 08:47:08.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686940428, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686940428, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686940428, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686940428, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:47:10.971: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 28 08:47:11.090: INFO: Updating deployment test-recreate-deployment
Feb 28 08:47:11.090: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 08:47:11.250: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-97pf2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-97pf2/deployments/test-recreate-deployment,UID:6ea26480-3b35-11e9-ac40-06fd5a6cf138,ResourceVersion:16180,Generation:2,CreationTimestamp:2019-02-28 08:47:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-28 08:47:11 +0000 UTC 2019-02-28 08:47:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-28 08:47:11 +0000 UTC 2019-02-28 08:47:08 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 28 08:47:11.311: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-97pf2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-97pf2/replicasets/test-recreate-deployment-697fbf54bf,UID:700c19ee-3b35-11e9-ac40-06fd5a6cf138,ResourceVersion:16178,Generation:1,CreationTimestamp:2019-02-28 08:47:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 6ea26480-3b35-11e9-ac40-06fd5a6cf138 0xc001ec7217 0xc001ec7218}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:47:11.311: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 28 08:47:11.311: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-97pf2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-97pf2/replicasets/test-recreate-deployment-5dfdcc846d,UID:6ea31f72-3b35-11e9-ac40-06fd5a6cf138,ResourceVersion:16171,Generation:2,CreationTimestamp:2019-02-28 08:47:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 6ea26480-3b35-11e9-ac40-06fd5a6cf138 0xc001ec7127 0xc001ec7128}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:47:11.373: INFO: Pod "test-recreate-deployment-697fbf54bf-mpwmj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-mpwmj,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-97pf2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-97pf2/pods/test-recreate-deployment-697fbf54bf-mpwmj,UID:700c630f-3b35-11e9-ac40-06fd5a6cf138,ResourceVersion:16179,Generation:0,CreationTimestamp:2019-02-28 08:47:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 700c19ee-3b35-11e9-ac40-06fd5a6cf138 0xc001354437 0xc001354438}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-dgbtp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dgbtp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dgbtp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001354770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001354790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:47:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:47:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:47:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:,StartTime:2019-02-28 08:47:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:47:11.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-97pf2" for this suite.
Feb 28 08:47:17.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:19.281: INFO: namespace: e2e-tests-deployment-97pf2, resource: bindings, ignored listing per whitelist
Feb 28 08:47:19.932: INFO: namespace e2e-tests-deployment-97pf2 deletion completed in 8.497881733s

• [SLOW TEST:13.586 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:47:19.933: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-lfzvf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-lfzvf/secret-test-76c9af70-3b35-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 08:47:22.498: INFO: Waiting up to 5m0s for pod "pod-configmaps-76d2d686-3b35-11e9-8ad6-42906abdb246" in namespace "e2e-tests-secrets-lfzvf" to be "success or failure"
Feb 28 08:47:22.559: INFO: Pod "pod-configmaps-76d2d686-3b35-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 61.28941ms
Feb 28 08:47:24.621: INFO: Pod "pod-configmaps-76d2d686-3b35-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.122764754s
STEP: Saw pod success
Feb 28 08:47:24.621: INFO: Pod "pod-configmaps-76d2d686-3b35-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:47:24.679: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-configmaps-76d2d686-3b35-11e9-8ad6-42906abdb246 container env-test: <nil>
STEP: delete the pod
Feb 28 08:47:24.810: INFO: Waiting for pod pod-configmaps-76d2d686-3b35-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:47:24.872: INFO: Pod pod-configmaps-76d2d686-3b35-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:47:24.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-lfzvf" for this suite.
Feb 28 08:47:31.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:32.106: INFO: namespace: e2e-tests-secrets-lfzvf, resource: bindings, ignored listing per whitelist
Feb 28 08:47:33.433: INFO: namespace e2e-tests-secrets-lfzvf deletion completed in 8.498182902s

• [SLOW TEST:13.501 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:47:33.433: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-2hmsh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-ntbgt
STEP: Creating secret with name secret-test-7ef5f41b-3b35-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 08:47:36.603: INFO: Waiting up to 5m0s for pod "pod-secrets-7f3bc2c2-3b35-11e9-8ad6-42906abdb246" in namespace "e2e-tests-secrets-2hmsh" to be "success or failure"
Feb 28 08:47:36.664: INFO: Pod "pod-secrets-7f3bc2c2-3b35-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.476902ms
Feb 28 08:47:38.725: INFO: Pod "pod-secrets-7f3bc2c2-3b35-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121401641s
STEP: Saw pod success
Feb 28 08:47:38.725: INFO: Pod "pod-secrets-7f3bc2c2-3b35-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:47:38.787: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-secrets-7f3bc2c2-3b35-11e9-8ad6-42906abdb246 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:47:38.920: INFO: Waiting for pod pod-secrets-7f3bc2c2-3b35-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:47:38.981: INFO: Pod pod-secrets-7f3bc2c2-3b35-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:47:38.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2hmsh" for this suite.
Feb 28 08:47:45.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:45.515: INFO: namespace: e2e-tests-secrets-2hmsh, resource: bindings, ignored listing per whitelist
Feb 28 08:47:47.565: INFO: namespace e2e-tests-secrets-2hmsh deletion completed in 8.52384739s
STEP: Destroying namespace "e2e-tests-secret-namespace-ntbgt" for this suite.
Feb 28 08:47:53.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:54.571: INFO: namespace: e2e-tests-secret-namespace-ntbgt, resource: bindings, ignored listing per whitelist
Feb 28 08:47:56.051: INFO: namespace e2e-tests-secret-namespace-ntbgt deletion completed in 8.486708544s

• [SLOW TEST:22.618 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:47:56.052: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-vjrc7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:47:58.537: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c4df9ce-3b35-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-vjrc7" to be "success or failure"
Feb 28 08:47:58.596: INFO: Pod "downwardapi-volume-8c4df9ce-3b35-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 58.686571ms
Feb 28 08:48:00.654: INFO: Pod "downwardapi-volume-8c4df9ce-3b35-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117504685s
STEP: Saw pod success
Feb 28 08:48:00.655: INFO: Pod "downwardapi-volume-8c4df9ce-3b35-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:48:00.711: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-8c4df9ce-3b35-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 08:48:00.836: INFO: Waiting for pod downwardapi-volume-8c4df9ce-3b35-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:48:00.896: INFO: Pod downwardapi-volume-8c4df9ce-3b35-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:48:00.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vjrc7" for this suite.
Feb 28 08:48:07.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:48:08.179: INFO: namespace: e2e-tests-downward-api-vjrc7, resource: bindings, ignored listing per whitelist
Feb 28 08:48:10.354: INFO: namespace e2e-tests-downward-api-vjrc7 deletion completed in 9.397521776s

• [SLOW TEST:14.302 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:48:10.354: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-s6k5j
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:48:15.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-s6k5j" for this suite.
Feb 28 08:48:21.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:48:22.614: INFO: namespace: e2e-tests-emptydir-wrapper-s6k5j, resource: bindings, ignored listing per whitelist
Feb 28 08:48:24.003: INFO: namespace e2e-tests-emptydir-wrapper-s6k5j deletion completed in 8.559463253s

• [SLOW TEST:13.649 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:48:24.004: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kggc6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:48:26.638: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d0dfb6f-3b35-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-kggc6" to be "success or failure"
Feb 28 08:48:26.699: INFO: Pod "downwardapi-volume-9d0dfb6f-3b35-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 61.366039ms
Feb 28 08:48:28.761: INFO: Pod "downwardapi-volume-9d0dfb6f-3b35-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.123235821s
STEP: Saw pod success
Feb 28 08:48:28.761: INFO: Pod "downwardapi-volume-9d0dfb6f-3b35-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:48:28.823: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-9d0dfb6f-3b35-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 08:48:28.951: INFO: Waiting for pod downwardapi-volume-9d0dfb6f-3b35-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:48:29.013: INFO: Pod downwardapi-volume-9d0dfb6f-3b35-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:48:29.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kggc6" for this suite.
Feb 28 08:48:35.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:48:35.553: INFO: namespace: e2e-tests-projected-kggc6, resource: bindings, ignored listing per whitelist
Feb 28 08:48:37.663: INFO: namespace e2e-tests-projected-kggc6 deletion completed in 8.531183149s

• [SLOW TEST:13.660 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:48:37.664: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-687wk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-a50a6336-3b35-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 08:48:40.096: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5139e84-3b35-11e9-8ad6-42906abdb246" in namespace "e2e-tests-configmap-687wk" to be "success or failure"
Feb 28 08:48:40.154: INFO: Pod "pod-configmaps-a5139e84-3b35-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 58.305004ms
Feb 28 08:48:42.213: INFO: Pod "pod-configmaps-a5139e84-3b35-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.116870637s
STEP: Saw pod success
Feb 28 08:48:42.213: INFO: Pod "pod-configmaps-a5139e84-3b35-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:48:42.272: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-configmaps-a5139e84-3b35-11e9-8ad6-42906abdb246 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:48:42.396: INFO: Waiting for pod pod-configmaps-a5139e84-3b35-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:48:42.455: INFO: Pod pod-configmaps-a5139e84-3b35-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:48:42.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-687wk" for this suite.
Feb 28 08:48:48.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:48:50.123: INFO: namespace: e2e-tests-configmap-687wk, resource: bindings, ignored listing per whitelist
Feb 28 08:48:51.072: INFO: namespace e2e-tests-configmap-687wk deletion completed in 8.500805219s

• [SLOW TEST:13.408 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:48:51.073: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-74psp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 28 08:48:53.416: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:48:54.284: INFO: stderr: ""
Feb 28 08:48:54.284: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 08:48:54.284: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:48:54.631: INFO: stderr: ""
Feb 28 08:48:54.631: INFO: stdout: "update-demo-nautilus-qf5jn update-demo-nautilus-twwv5 "
Feb 28 08:48:54.631: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-qf5jn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:48:54.994: INFO: stderr: ""
Feb 28 08:48:54.994: INFO: stdout: ""
Feb 28 08:48:54.994: INFO: update-demo-nautilus-qf5jn is created but not running
Feb 28 08:48:59.994: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:49:00.375: INFO: stderr: ""
Feb 28 08:49:00.375: INFO: stdout: "update-demo-nautilus-qf5jn update-demo-nautilus-twwv5 "
Feb 28 08:49:00.375: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-qf5jn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:49:00.721: INFO: stderr: ""
Feb 28 08:49:00.721: INFO: stdout: "true"
Feb 28 08:49:00.721: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-qf5jn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:49:01.046: INFO: stderr: ""
Feb 28 08:49:01.046: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:49:01.046: INFO: validating pod update-demo-nautilus-qf5jn
Feb 28 08:49:01.190: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:49:01.190: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:49:01.190: INFO: update-demo-nautilus-qf5jn is verified up and running
Feb 28 08:49:01.190: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-twwv5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:49:01.566: INFO: stderr: ""
Feb 28 08:49:01.566: INFO: stdout: "true"
Feb 28 08:49:01.566: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-twwv5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:49:01.927: INFO: stderr: ""
Feb 28 08:49:01.927: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:49:01.927: INFO: validating pod update-demo-nautilus-twwv5
Feb 28 08:49:02.077: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:49:02.077: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:49:02.078: INFO: update-demo-nautilus-twwv5 is verified up and running
STEP: rolling-update to new replication controller
Feb 28 08:49:02.084: INFO: scanned /root for discovery docs: <nil>
Feb 28 08:49:02.084: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:49:19.182: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 28 08:49:19.183: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 08:49:19.183: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:49:19.521: INFO: stderr: ""
Feb 28 08:49:19.521: INFO: stdout: "update-demo-kitten-dnmjx update-demo-kitten-fnzmt "
Feb 28 08:49:19.521: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-kitten-dnmjx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:49:19.840: INFO: stderr: ""
Feb 28 08:49:19.840: INFO: stdout: "true"
Feb 28 08:49:19.840: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-kitten-dnmjx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:49:20.174: INFO: stderr: ""
Feb 28 08:49:20.174: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 28 08:49:20.174: INFO: validating pod update-demo-kitten-dnmjx
Feb 28 08:49:20.323: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 28 08:49:20.323: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 28 08:49:20.323: INFO: update-demo-kitten-dnmjx is verified up and running
Feb 28 08:49:20.323: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-kitten-fnzmt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:49:20.713: INFO: stderr: ""
Feb 28 08:49:20.713: INFO: stdout: "true"
Feb 28 08:49:20.713: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-kitten-fnzmt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-74psp'
Feb 28 08:49:21.082: INFO: stderr: ""
Feb 28 08:49:21.082: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 28 08:49:21.083: INFO: validating pod update-demo-kitten-fnzmt
Feb 28 08:49:21.233: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 28 08:49:21.233: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 28 08:49:21.233: INFO: update-demo-kitten-fnzmt is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:49:21.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-74psp" for this suite.
Feb 28 08:49:43.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:49:45.568: INFO: namespace: e2e-tests-kubectl-74psp, resource: bindings, ignored listing per whitelist
Feb 28 08:49:45.847: INFO: namespace e2e-tests-kubectl-74psp deletion completed in 24.497536525s

• [SLOW TEST:54.775 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:49:45.847: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jscs8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-cdb0ce91-3b35-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 08:49:48.293: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cdb9ff73-3b35-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-jscs8" to be "success or failure"
Feb 28 08:49:48.349: INFO: Pod "pod-projected-secrets-cdb9ff73-3b35-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 55.769383ms
Feb 28 08:49:50.410: INFO: Pod "pod-projected-secrets-cdb9ff73-3b35-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117304021s
STEP: Saw pod success
Feb 28 08:49:50.410: INFO: Pod "pod-projected-secrets-cdb9ff73-3b35-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:49:50.471: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-secrets-cdb9ff73-3b35-11e9-8ad6-42906abdb246 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:49:50.601: INFO: Waiting for pod pod-projected-secrets-cdb9ff73-3b35-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:49:50.660: INFO: Pod pod-projected-secrets-cdb9ff73-3b35-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:49:50.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jscs8" for this suite.
Feb 28 08:49:56.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:49:57.261: INFO: namespace: e2e-tests-projected-jscs8, resource: bindings, ignored listing per whitelist
Feb 28 08:49:59.473: INFO: namespace e2e-tests-projected-jscs8 deletion completed in 8.751200102s

• [SLOW TEST:13.626 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:49:59.473: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8qxf7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:50:01.967: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5dcde02-3b35-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-8qxf7" to be "success or failure"
Feb 28 08:50:02.044: INFO: Pod "downwardapi-volume-d5dcde02-3b35-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 76.159157ms
Feb 28 08:50:04.103: INFO: Pod "downwardapi-volume-d5dcde02-3b35-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.135782428s
STEP: Saw pod success
Feb 28 08:50:04.103: INFO: Pod "downwardapi-volume-d5dcde02-3b35-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:50:04.182: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-d5dcde02-3b35-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 08:50:04.312: INFO: Waiting for pod downwardapi-volume-d5dcde02-3b35-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:50:04.370: INFO: Pod downwardapi-volume-d5dcde02-3b35-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:50:04.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8qxf7" for this suite.
Feb 28 08:50:10.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:50:11.916: INFO: namespace: e2e-tests-projected-8qxf7, resource: bindings, ignored listing per whitelist
Feb 28 08:50:12.995: INFO: namespace e2e-tests-projected-8qxf7 deletion completed in 8.509030908s

• [SLOW TEST:13.522 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:50:12.996: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hmncc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-de143cd3-3b35-11e9-8ad6-42906abdb246
STEP: Creating secret with name secret-projected-all-test-volume-de143cc0-3b35-11e9-8ad6-42906abdb246
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 28 08:50:15.853: INFO: Waiting up to 5m0s for pod "projected-volume-de143c8e-3b35-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-hmncc" to be "success or failure"
Feb 28 08:50:15.911: INFO: Pod "projected-volume-de143c8e-3b35-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 58.378374ms
Feb 28 08:50:17.971: INFO: Pod "projected-volume-de143c8e-3b35-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118281188s
STEP: Saw pod success
Feb 28 08:50:17.971: INFO: Pod "projected-volume-de143c8e-3b35-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:50:18.030: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod projected-volume-de143c8e-3b35-11e9-8ad6-42906abdb246 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 28 08:50:18.159: INFO: Waiting for pod projected-volume-de143c8e-3b35-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:50:18.220: INFO: Pod projected-volume-de143c8e-3b35-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:50:18.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hmncc" for this suite.
Feb 28 08:50:24.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:50:26.054: INFO: namespace: e2e-tests-projected-hmncc, resource: bindings, ignored listing per whitelist
Feb 28 08:50:26.847: INFO: namespace e2e-tests-projected-hmncc deletion completed in 8.508227906s

• [SLOW TEST:13.851 seconds]
[sig-storage] Projected combined
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:50:26.847: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-z956h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 28 08:50:29.185: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:50:29.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-z956h" for this suite.
Feb 28 08:50:35.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:50:37.205: INFO: namespace: e2e-tests-kubectl-z956h, resource: bindings, ignored listing per whitelist
Feb 28 08:50:38.223: INFO: namespace e2e-tests-kubectl-z956h deletion completed in 8.551286867s

• [SLOW TEST:11.376 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:50:38.224: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-svm7l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-w2qh
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:50:41.154: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w2qh" in namespace "e2e-tests-subpath-svm7l" to be "success or failure"
Feb 28 08:50:41.216: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Pending", Reason="", readiness=false. Elapsed: 61.360997ms
Feb 28 08:50:43.277: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12272012s
Feb 28 08:50:45.338: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Running", Reason="", readiness=false. Elapsed: 4.184219838s
Feb 28 08:50:47.400: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Running", Reason="", readiness=false. Elapsed: 6.245694226s
Feb 28 08:50:49.463: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Running", Reason="", readiness=false. Elapsed: 8.308427633s
Feb 28 08:50:51.523: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Running", Reason="", readiness=false. Elapsed: 10.369023642s
Feb 28 08:50:53.584: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Running", Reason="", readiness=false. Elapsed: 12.430175904s
Feb 28 08:50:55.645: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Running", Reason="", readiness=false. Elapsed: 14.490383322s
Feb 28 08:50:57.703: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Running", Reason="", readiness=false. Elapsed: 16.549254923s
Feb 28 08:50:59.762: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Running", Reason="", readiness=false. Elapsed: 18.60808074s
Feb 28 08:51:01.827: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Running", Reason="", readiness=false. Elapsed: 20.673279331s
Feb 28 08:51:03.889: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Running", Reason="", readiness=false. Elapsed: 22.73467449s
Feb 28 08:51:05.951: INFO: Pod "pod-subpath-test-configmap-w2qh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.796687428s
STEP: Saw pod success
Feb 28 08:51:05.951: INFO: Pod "pod-subpath-test-configmap-w2qh" satisfied condition "success or failure"
Feb 28 08:51:06.011: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-subpath-test-configmap-w2qh container test-container-subpath-configmap-w2qh: <nil>
STEP: delete the pod
Feb 28 08:51:06.142: INFO: Waiting for pod pod-subpath-test-configmap-w2qh to disappear
Feb 28 08:51:06.201: INFO: Pod pod-subpath-test-configmap-w2qh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-w2qh
Feb 28 08:51:06.201: INFO: Deleting pod "pod-subpath-test-configmap-w2qh" in namespace "e2e-tests-subpath-svm7l"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:51:06.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-svm7l" for this suite.
Feb 28 08:51:12.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:51:14.573: INFO: namespace: e2e-tests-subpath-svm7l, resource: bindings, ignored listing per whitelist
Feb 28 08:51:14.871: INFO: namespace e2e-tests-subpath-svm7l deletion completed in 8.492600053s

• [SLOW TEST:36.648 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:51:14.872: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-vh4mx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 28 08:51:17.347: INFO: Waiting up to 5m0s for pod "pod-02cdddf1-3b36-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-vh4mx" to be "success or failure"
Feb 28 08:51:17.407: INFO: Pod "pod-02cdddf1-3b36-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.39647ms
Feb 28 08:51:19.469: INFO: Pod "pod-02cdddf1-3b36-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.122864938s
STEP: Saw pod success
Feb 28 08:51:19.470: INFO: Pod "pod-02cdddf1-3b36-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:51:19.530: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-02cdddf1-3b36-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 08:51:19.661: INFO: Waiting for pod pod-02cdddf1-3b36-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:51:19.721: INFO: Pod pod-02cdddf1-3b36-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:51:19.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vh4mx" for this suite.
Feb 28 08:51:26.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:51:27.380: INFO: namespace: e2e-tests-emptydir-vh4mx, resource: bindings, ignored listing per whitelist
Feb 28 08:51:28.340: INFO: namespace e2e-tests-emptydir-vh4mx deletion completed in 8.502223184s

• [SLOW TEST:13.469 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:51:28.341: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-chc47
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:51:32.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-chc47" for this suite.
Feb 28 08:52:19.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:52:21.085: INFO: namespace: e2e-tests-kubelet-test-chc47, resource: bindings, ignored listing per whitelist
Feb 28 08:52:21.635: INFO: namespace e2e-tests-kubelet-test-chc47 deletion completed in 48.541391507s

• [SLOW TEST:53.294 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:52:21.635: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qxsz7
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2aa42e0d-3b36-11e9-8ad6-42906abdb246
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-2aa42e0d-3b36-11e9-8ad6-42906abdb246
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:52:28.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qxsz7" for this suite.
Feb 28 08:52:50.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:52:53.208: INFO: namespace: e2e-tests-projected-qxsz7, resource: bindings, ignored listing per whitelist
Feb 28 08:52:53.269: INFO: namespace e2e-tests-projected-qxsz7 deletion completed in 24.528616315s

• [SLOW TEST:31.634 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:52:53.269: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5dx6v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 28 08:52:55.610: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml api-versions'
Feb 28 08:52:56.284: INFO: stderr: ""
Feb 28 08:52:56.284: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:52:56.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5dx6v" for this suite.
Feb 28 08:53:02.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:53:03.838: INFO: namespace: e2e-tests-kubectl-5dx6v, resource: bindings, ignored listing per whitelist
Feb 28 08:53:04.916: INFO: namespace e2e-tests-kubectl-5dx6v deletion completed in 8.569523762s

• [SLOW TEST:11.647 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:53:04.917: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-tk7j8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 08:53:10.269: INFO: Successfully updated pod "labelsupdate445ddd94-3b36-11e9-8ad6-42906abdb246"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:53:12.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tk7j8" for this suite.
Feb 28 08:53:34.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:53:36.476: INFO: namespace: e2e-tests-downward-api-tk7j8, resource: bindings, ignored listing per whitelist
Feb 28 08:53:37.015: INFO: namespace e2e-tests-downward-api-tk7j8 deletion completed in 24.49544975s

• [SLOW TEST:32.098 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:53:37.015: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-g5f5m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 28 08:53:39.700: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml proxy --unix-socket=/tmp/kubectl-proxy-unix798841799/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:53:39.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-g5f5m" for this suite.
Feb 28 08:53:46.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:53:46.633: INFO: namespace: e2e-tests-kubectl-g5f5m, resource: bindings, ignored listing per whitelist
Feb 28 08:53:48.456: INFO: namespace e2e-tests-kubectl-g5f5m deletion completed in 8.541263609s

• [SLOW TEST:11.441 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:53:48.457: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-n7zz8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-n7zz8
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-n7zz8
STEP: Deleting pre-stop pod
Feb 28 08:54:04.478: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:54:04.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-n7zz8" for this suite.
Feb 28 08:54:42.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:54:43.384: INFO: namespace: e2e-tests-prestop-n7zz8, resource: bindings, ignored listing per whitelist
Feb 28 08:54:45.227: INFO: namespace e2e-tests-prestop-n7zz8 deletion completed in 40.55107381s

• [SLOW TEST:56.770 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:54:45.228: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6rjbr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-8034f61e-3b36-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 08:54:47.797: INFO: Waiting up to 5m0s for pod "pod-secrets-803e3baf-3b36-11e9-8ad6-42906abdb246" in namespace "e2e-tests-secrets-6rjbr" to be "success or failure"
Feb 28 08:54:47.857: INFO: Pod "pod-secrets-803e3baf-3b36-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.713436ms
Feb 28 08:54:49.918: INFO: Pod "pod-secrets-803e3baf-3b36-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121235708s
STEP: Saw pod success
Feb 28 08:54:49.918: INFO: Pod "pod-secrets-803e3baf-3b36-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:54:49.977: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-secrets-803e3baf-3b36-11e9-8ad6-42906abdb246 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:54:50.106: INFO: Waiting for pod pod-secrets-803e3baf-3b36-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:54:50.164: INFO: Pod pod-secrets-803e3baf-3b36-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:54:50.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6rjbr" for this suite.
Feb 28 08:54:56.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:54:56.985: INFO: namespace: e2e-tests-secrets-6rjbr, resource: bindings, ignored listing per whitelist
Feb 28 08:54:58.820: INFO: namespace e2e-tests-secrets-6rjbr deletion completed in 8.534946276s

• [SLOW TEST:13.592 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:54:58.820: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-wnxcg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wnxcg A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wnxcg A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wnxcg.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wnxcg.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wnxcg.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-wnxcg.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wnxcg.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-wnxcg.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-wnxcg.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 217.202.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.202.217_udp@PTR;check="$$(dig +tcp +noall +answer +search 217.202.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.202.217_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wnxcg A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-wnxcg;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wnxcg A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wnxcg.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wnxcg.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wnxcg.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-wnxcg.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wnxcg.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-wnxcg.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-wnxcg.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 217.202.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.202.217_udp@PTR;check="$$(dig +tcp +noall +answer +search 217.202.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.202.217_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 08:55:03.687: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:03.754: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:03.821: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:03.885: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:03.955: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:04.021: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:04.087: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:04.147: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:04.571: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:04.634: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:04.696: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:04.757: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:04.829: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:04.889: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:04.948: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:05.006: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:05.384: INFO: Lookups using e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-wnxcg jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc]

Feb 28 08:55:10.445: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:10.506: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:10.567: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:10.627: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:10.688: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:10.750: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:10.810: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:10.870: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:11.360: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:11.428: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:11.490: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:11.552: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:11.614: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:11.676: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:11.739: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:11.806: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:12.205: INFO: Lookups using e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-wnxcg jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc]

Feb 28 08:55:15.447: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:15.513: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:15.569: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:15.630: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:15.690: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:15.752: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:15.810: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:15.869: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:16.296: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:16.361: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:16.421: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:16.483: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:16.547: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:16.604: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:16.663: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:16.723: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:17.102: INFO: Lookups using e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-wnxcg jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc]

Feb 28 08:55:20.447: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:20.505: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:20.564: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:20.619: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:20.677: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:20.735: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:20.793: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:20.852: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:21.286: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:21.621: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:21.685: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:21.749: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:21.811: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:21.873: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:21.936: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:21.999: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:22.371: INFO: Lookups using e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-wnxcg jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc]

Feb 28 08:55:25.446: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:25.513: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:25.572: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:25.634: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:25.694: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:25.752: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:25.813: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:25.872: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:26.310: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:26.373: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:26.435: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:26.523: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:26.587: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:26.646: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:26.709: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:26.770: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:27.179: INFO: Lookups using e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-wnxcg jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc]

Feb 28 08:55:30.451: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:30.515: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:30.577: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:30.641: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:30.703: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:30.763: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:30.825: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:30.887: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:31.316: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:31.374: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:31.432: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:31.489: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:31.549: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:31.608: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:31.667: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:31.726: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc from pod e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246: the server could not find the requested resource (get pods dns-test-88538444-3b36-11e9-8ad6-42906abdb246)
Feb 28 08:55:32.093: INFO: Lookups using e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg wheezy_udp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-wnxcg jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg jessie_udp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@dns-test-service.e2e-tests-dns-wnxcg.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wnxcg.svc]

Feb 28 08:55:37.112: INFO: DNS probes using e2e-tests-dns-wnxcg/dns-test-88538444-3b36-11e9-8ad6-42906abdb246 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:55:37.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-wnxcg" for this suite.
Feb 28 08:55:43.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:55:44.644: INFO: namespace: e2e-tests-dns-wnxcg, resource: bindings, ignored listing per whitelist
Feb 28 08:55:46.311: INFO: namespace e2e-tests-dns-wnxcg deletion completed in 8.813686165s

• [SLOW TEST:47.491 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:55:46.311: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-q58ld
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 28 08:55:48.841: INFO: Waiting up to 5m0s for pod "pod-a4a12b69-3b36-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-q58ld" to be "success or failure"
Feb 28 08:55:48.899: INFO: Pod "pod-a4a12b69-3b36-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 58.731855ms
Feb 28 08:55:50.961: INFO: Pod "pod-a4a12b69-3b36-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.120863103s
STEP: Saw pod success
Feb 28 08:55:50.962: INFO: Pod "pod-a4a12b69-3b36-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:55:51.023: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-a4a12b69-3b36-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 08:55:51.156: INFO: Waiting for pod pod-a4a12b69-3b36-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:55:51.218: INFO: Pod pod-a4a12b69-3b36-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:55:51.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-q58ld" for this suite.
Feb 28 08:55:57.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:55:59.695: INFO: namespace: e2e-tests-emptydir-q58ld, resource: bindings, ignored listing per whitelist
Feb 28 08:55:59.875: INFO: namespace e2e-tests-emptydir-q58ld deletion completed in 8.544607911s

• [SLOW TEST:13.564 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:55:59.875: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-gpwss
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0228 08:56:08.802576   30448 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 08:56:08.821: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:56:08.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-gpwss" for this suite.
Feb 28 08:56:15.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:56:16.609: INFO: namespace: e2e-tests-gc-gpwss, resource: bindings, ignored listing per whitelist
Feb 28 08:56:17.438: INFO: namespace e2e-tests-gc-gpwss deletion completed in 8.491630079s

• [SLOW TEST:17.563 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:56:17.438: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fv8kf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:56:19.771: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml version --client'
Feb 28 08:56:19.874: INFO: stderr: ""
Feb 28 08:56:19.875: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"archive\", BuildDate:\"2019-02-28T07:22:21Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 28 08:56:19.933: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-fv8kf'
Feb 28 08:56:22.932: INFO: stderr: ""
Feb 28 08:56:22.932: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 28 08:56:22.933: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-fv8kf'
Feb 28 08:56:23.482: INFO: stderr: ""
Feb 28 08:56:23.482: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 08:56:24.543: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:56:24.543: INFO: Found 1 / 1
Feb 28 08:56:24.543: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 08:56:24.601: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:56:24.601: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 08:56:24.601: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml describe pod redis-master-qzkxp --namespace=e2e-tests-kubectl-fv8kf'
Feb 28 08:56:25.073: INFO: stderr: ""
Feb 28 08:56:25.073: INFO: stdout: "Name:               redis-master-qzkxp\nNamespace:          e2e-tests-kubectl-fv8kf\nPriority:           0\nPriorityClassName:  <none>\nNode:               ip-10-250-15-222.eu-west-1.compute.internal/10.250.15.222\nStart Time:         Thu, 28 Feb 2019 08:56:22 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.1.237/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.1.237\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://8124543bb1ae4b49b1ed7de76c24a1b9cca56a9bc4433aef329b2b4945bb81a8\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 28 Feb 2019 08:56:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-q5f89 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-q5f89:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-q5f89\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                  Message\n  ----    ------     ----  ----                                                  -------\n  Normal  Scheduled  3s    default-scheduler                                     Successfully assigned e2e-tests-kubectl-fv8kf/redis-master-qzkxp to ip-10-250-15-222.eu-west-1.compute.internal\n  Normal  Pulled     2s    kubelet, ip-10-250-15-222.eu-west-1.compute.internal  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, ip-10-250-15-222.eu-west-1.compute.internal  Created container\n  Normal  Started    2s    kubelet, ip-10-250-15-222.eu-west-1.compute.internal  Started container\n"
Feb 28 08:56:25.073: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml describe rc redis-master --namespace=e2e-tests-kubectl-fv8kf'
Feb 28 08:56:25.630: INFO: stderr: ""
Feb 28 08:56:25.630: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-fv8kf\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-qzkxp\n"
Feb 28 08:56:25.630: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml describe service redis-master --namespace=e2e-tests-kubectl-fv8kf'
Feb 28 08:56:26.137: INFO: stderr: ""
Feb 28 08:56:26.137: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-fv8kf\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.69.186.11\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.237:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 28 08:56:26.257: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml describe node ip-10-250-15-222.eu-west-1.compute.internal'
Feb 28 08:56:26.793: INFO: stderr: ""
Feb 28 08:56:26.793: INFO: stdout: "Name:               ip-10-250-15-222.eu-west-1.compute.internal\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m4.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-west-1\n                    failure-domain.beta.kubernetes.io/zone=eu-west-1a\n                    kubernetes.io/hostname=ip-10-250-15-222.eu-west-1.compute.internal\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.15.222/19\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 28 Feb 2019 07:12:17 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 28 Feb 2019 08:56:23 +0000   Thu, 28 Feb 2019 07:12:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 28 Feb 2019 08:56:23 +0000   Thu, 28 Feb 2019 07:12:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 28 Feb 2019 08:56:23 +0000   Thu, 28 Feb 2019 07:12:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 28 Feb 2019 08:56:23 +0000   Thu, 28 Feb 2019 07:12:37 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.250.15.222\n  InternalDNS:  ip-10-250-15-222.eu-west-1.compute.internal\n  Hostname:     ip-10-250-15-222.eu-west-1.compute.internal\nCapacity:\n attachable-volumes-aws-ebs:  39\n cpu:                         2\n ephemeral-storage:           17897500Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      8169008Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  39\n cpu:                         1920m\n ephemeral-storage:           17410687987\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      6873069153\n pods:                        110\nSystem Info:\n Machine ID:                 0d6e9dd8e41a4bbd878da93c73097e47\n System UUID:                EC27C222-8E3C-A840-0E16-BE1AA6961FF0\n Boot ID:                    ce0447d6-e1d1-40cc-bbad-97bf9ad6b037\n Kernel Version:             4.14.96-coreos\n OS Image:                   Container Linux by CoreOS 1967.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.13.3\n Kube-Proxy Version:         v1.13.3\nPodCIDR:                     100.96.1.0/24\nProviderID:                  aws:///eu-west-1a/i-0e1d9d9927c5e0807\nNon-terminated Pods:         (4 in total)\n  Namespace                  Name                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                   ------------  ----------  ---------------  -------------  ---\n  e2e-tests-kubectl-fv8kf    redis-master-qzkxp     0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  kube-system                calico-node-9s67j      100m (5%)     500m (26%)  100Mi (1%)       700Mi (10%)    104m\n  kube-system                kube-proxy-bvswv       20m (1%)      900m (46%)  64Mi (0%)        200Mi (3%)     104m\n  kube-system                node-exporter-bsxsh    5m (0%)       15m (0%)    10Mi (0%)        50Mi (0%)      104m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         125m (6%)   1415m (73%)\n  memory                      174Mi (2%)  950Mi (14%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:                       <none>\n"
Feb 28 08:56:26.793: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml describe namespace e2e-tests-kubectl-fv8kf'
Feb 28 08:56:27.279: INFO: stderr: ""
Feb 28 08:56:27.279: INFO: stdout: "Name:         e2e-tests-kubectl-fv8kf\nLabels:       e2e-framework=kubectl\n              e2e-run=e8b8954a-3b29-11e9-8ad6-42906abdb246\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:56:27.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fv8kf" for this suite.
Feb 28 08:56:49.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:56:49.932: INFO: namespace: e2e-tests-kubectl-fv8kf, resource: bindings, ignored listing per whitelist
Feb 28 08:56:51.826: INFO: namespace e2e-tests-kubectl-fv8kf deletion completed in 24.486403978s

• [SLOW TEST:34.388 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:56:51.827: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-shxzt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:56:54.489: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 28 08:56:54.608: INFO: Number of nodes with available pods: 0
Feb 28 08:56:54.608: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 28 08:56:54.854: INFO: Number of nodes with available pods: 0
Feb 28 08:56:54.854: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:56:55.916: INFO: Number of nodes with available pods: 0
Feb 28 08:56:55.916: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:56:56.915: INFO: Number of nodes with available pods: 1
Feb 28 08:56:56.915: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 28 08:56:57.155: INFO: Number of nodes with available pods: 0
Feb 28 08:56:57.155: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 28 08:56:57.269: INFO: Number of nodes with available pods: 0
Feb 28 08:56:57.269: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:56:58.332: INFO: Number of nodes with available pods: 0
Feb 28 08:56:58.332: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:56:59.332: INFO: Number of nodes with available pods: 0
Feb 28 08:56:59.332: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:00.331: INFO: Number of nodes with available pods: 0
Feb 28 08:57:00.331: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:01.331: INFO: Number of nodes with available pods: 0
Feb 28 08:57:01.331: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:02.334: INFO: Number of nodes with available pods: 0
Feb 28 08:57:02.334: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:03.330: INFO: Number of nodes with available pods: 0
Feb 28 08:57:03.330: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:04.329: INFO: Number of nodes with available pods: 0
Feb 28 08:57:04.329: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:05.329: INFO: Number of nodes with available pods: 0
Feb 28 08:57:05.329: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:06.330: INFO: Number of nodes with available pods: 0
Feb 28 08:57:06.330: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:07.330: INFO: Number of nodes with available pods: 0
Feb 28 08:57:07.330: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:08.331: INFO: Number of nodes with available pods: 0
Feb 28 08:57:08.331: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:09.329: INFO: Number of nodes with available pods: 0
Feb 28 08:57:09.329: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:10.328: INFO: Number of nodes with available pods: 0
Feb 28 08:57:10.328: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:11.329: INFO: Number of nodes with available pods: 0
Feb 28 08:57:11.329: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:12.331: INFO: Number of nodes with available pods: 0
Feb 28 08:57:12.331: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:13.330: INFO: Number of nodes with available pods: 0
Feb 28 08:57:13.330: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:14.330: INFO: Number of nodes with available pods: 0
Feb 28 08:57:14.330: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:15.330: INFO: Number of nodes with available pods: 0
Feb 28 08:57:15.330: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:16.329: INFO: Number of nodes with available pods: 0
Feb 28 08:57:16.329: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:17.327: INFO: Number of nodes with available pods: 0
Feb 28 08:57:17.327: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:18.329: INFO: Number of nodes with available pods: 0
Feb 28 08:57:18.329: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:19.331: INFO: Number of nodes with available pods: 0
Feb 28 08:57:19.331: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:20.328: INFO: Number of nodes with available pods: 0
Feb 28 08:57:20.328: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:21.329: INFO: Number of nodes with available pods: 0
Feb 28 08:57:21.330: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:22.331: INFO: Number of nodes with available pods: 0
Feb 28 08:57:22.331: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:23.331: INFO: Number of nodes with available pods: 0
Feb 28 08:57:23.331: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:24.330: INFO: Number of nodes with available pods: 0
Feb 28 08:57:24.330: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:25.327: INFO: Number of nodes with available pods: 0
Feb 28 08:57:25.327: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:26.328: INFO: Number of nodes with available pods: 0
Feb 28 08:57:26.328: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:27.328: INFO: Number of nodes with available pods: 0
Feb 28 08:57:27.329: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:28.328: INFO: Number of nodes with available pods: 0
Feb 28 08:57:28.328: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:29.334: INFO: Number of nodes with available pods: 0
Feb 28 08:57:29.334: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:30.330: INFO: Number of nodes with available pods: 0
Feb 28 08:57:30.330: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:31.329: INFO: Number of nodes with available pods: 0
Feb 28 08:57:31.329: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:32.331: INFO: Number of nodes with available pods: 0
Feb 28 08:57:32.331: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:33.328: INFO: Number of nodes with available pods: 0
Feb 28 08:57:33.329: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:34.332: INFO: Number of nodes with available pods: 0
Feb 28 08:57:34.332: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:35.331: INFO: Number of nodes with available pods: 0
Feb 28 08:57:35.331: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:36.330: INFO: Number of nodes with available pods: 0
Feb 28 08:57:36.330: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:37.329: INFO: Number of nodes with available pods: 0
Feb 28 08:57:37.329: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:38.331: INFO: Number of nodes with available pods: 0
Feb 28 08:57:38.331: INFO: Node ip-10-250-15-222.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:57:39.329: INFO: Number of nodes with available pods: 1
Feb 28 08:57:39.329: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-shxzt, will wait for the garbage collector to delete the pods
Feb 28 08:57:39.679: INFO: Deleting DaemonSet.extensions daemon-set took: 59.339368ms
Feb 28 08:57:39.779: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.279701ms
Feb 28 08:58:17.639: INFO: Number of nodes with available pods: 0
Feb 28 08:58:17.639: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 08:58:17.699: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-shxzt/daemonsets","resourceVersion":"18129"},"items":null}

Feb 28 08:58:17.760: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-shxzt/pods","resourceVersion":"18129"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:58:18.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-shxzt" for this suite.
Feb 28 08:58:24.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:58:26.338: INFO: namespace: e2e-tests-daemonsets-shxzt, resource: bindings, ignored listing per whitelist
Feb 28 08:58:26.812: INFO: namespace e2e-tests-daemonsets-shxzt deletion completed in 8.641053792s

• [SLOW TEST:94.985 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:58:26.812: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-wptbz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 28 08:58:29.179: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 08:58:29.725: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 08:58:29.783: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-15-222.eu-west-1.compute.internal before test
Feb 28 08:58:29.912: INFO: calico-node-9s67j from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:29.912: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 08:58:29.912: INFO: kube-proxy-bvswv from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:29.912: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 08:58:29.912: INFO: node-exporter-bsxsh from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:29.912: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 08:58:29.912: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-20-37.eu-west-1.compute.internal before test
Feb 28 08:58:30.211: INFO: blackbox-exporter-86f6cf4cb7-q64tx from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:30.211: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 28 08:58:30.211: INFO: calico-node-clpwj from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:30.211: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 08:58:30.211: INFO: vpn-shoot-5fcfddb796-m7gkh from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:30.211: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 28 08:58:30.211: INFO: addons-nginx-ingress-controller-85496c84cf-v44st from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:30.211: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 08:58:30.211: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-f4cl8 from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:30.211: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 08:58:30.211: INFO: addons-kube-lego-69bbdc96b6-fspjg from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:30.212: INFO: 	Container kube-lego ready: true, restart count 0
Feb 28 08:58:30.212: INFO: node-exporter-kn6kk from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:30.212: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 08:58:30.212: INFO: coredns-67df79bbdd-7spjs from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:30.212: INFO: 	Container coredns ready: true, restart count 0
Feb 28 08:58:30.212: INFO: kube-proxy-bp6q8 from kube-system started at 2019-02-28 07:12:17 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:30.212: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 08:58:30.212: INFO: metrics-server-56cb9fb9c6-bjqsq from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:30.212: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 08:58:30.212: INFO: addons-kubernetes-dashboard-6579b646c5-mljz4 from kube-system started at 2019-02-28 07:12:36 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:30.212: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15877b6e5a295a48], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:58:31.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-wptbz" for this suite.
Feb 28 08:58:37.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:58:38.240: INFO: namespace: e2e-tests-sched-pred-wptbz, resource: bindings, ignored listing per whitelist
Feb 28 08:58:40.186: INFO: namespace e2e-tests-sched-pred-wptbz deletion completed in 8.484950623s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:13.374 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:58:40.187: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-mcdwz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 28 08:58:42.641: INFO: Waiting up to 5m0s for pod "pod-0c38a50a-3b37-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-mcdwz" to be "success or failure"
Feb 28 08:58:42.703: INFO: Pod "pod-0c38a50a-3b37-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 61.765412ms
Feb 28 08:58:44.763: INFO: Pod "pod-0c38a50a-3b37-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.122059335s
STEP: Saw pod success
Feb 28 08:58:44.763: INFO: Pod "pod-0c38a50a-3b37-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:58:44.822: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-0c38a50a-3b37-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 08:58:44.950: INFO: Waiting for pod pod-0c38a50a-3b37-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:58:45.012: INFO: Pod pod-0c38a50a-3b37-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:58:45.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mcdwz" for this suite.
Feb 28 08:58:51.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:58:53.089: INFO: namespace: e2e-tests-emptydir-mcdwz, resource: bindings, ignored listing per whitelist
Feb 28 08:58:53.559: INFO: namespace e2e-tests-emptydir-mcdwz deletion completed in 8.487268257s

• [SLOW TEST:13.373 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:58:53.560: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qv6pf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:58:55.883: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-qv6pf'
Feb 28 08:58:56.428: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 08:58:56.428: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 28 08:58:56.543: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-l2wbh]
Feb 28 08:58:56.543: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-l2wbh" in namespace "e2e-tests-kubectl-qv6pf" to be "running and ready"
Feb 28 08:58:56.600: INFO: Pod "e2e-test-nginx-rc-l2wbh": Phase="Pending", Reason="", readiness=false. Elapsed: 57.437526ms
Feb 28 08:58:58.662: INFO: Pod "e2e-test-nginx-rc-l2wbh": Phase="Running", Reason="", readiness=true. Elapsed: 2.119291459s
Feb 28 08:58:58.662: INFO: Pod "e2e-test-nginx-rc-l2wbh" satisfied condition "running and ready"
Feb 28 08:58:58.662: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-l2wbh]
Feb 28 08:58:58.662: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-qv6pf'
Feb 28 08:58:59.134: INFO: stderr: ""
Feb 28 08:58:59.134: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Feb 28 08:58:59.134: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-qv6pf'
Feb 28 08:58:59.543: INFO: stderr: ""
Feb 28 08:58:59.544: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:58:59.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qv6pf" for this suite.
Feb 28 08:59:21.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:59:22.261: INFO: namespace: e2e-tests-kubectl-qv6pf, resource: bindings, ignored listing per whitelist
Feb 28 08:59:24.131: INFO: namespace e2e-tests-kubectl-qv6pf deletion completed in 24.527249789s

• [SLOW TEST:30.572 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:59:24.132: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pb7dd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-26661f6f-3b37-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 08:59:26.621: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-266f80a2-3b37-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-pb7dd" to be "success or failure"
Feb 28 08:59:26.678: INFO: Pod "pod-projected-secrets-266f80a2-3b37-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 57.767625ms
Feb 28 08:59:28.738: INFO: Pod "pod-projected-secrets-266f80a2-3b37-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117258939s
STEP: Saw pod success
Feb 28 08:59:28.738: INFO: Pod "pod-projected-secrets-266f80a2-3b37-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 08:59:28.795: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-secrets-266f80a2-3b37-11e9-8ad6-42906abdb246 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:59:28.920: INFO: Waiting for pod pod-projected-secrets-266f80a2-3b37-11e9-8ad6-42906abdb246 to disappear
Feb 28 08:59:28.980: INFO: Pod pod-projected-secrets-266f80a2-3b37-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:59:28.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pb7dd" for this suite.
Feb 28 08:59:35.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:59:35.812: INFO: namespace: e2e-tests-projected-pb7dd, resource: bindings, ignored listing per whitelist
Feb 28 08:59:37.483: INFO: namespace e2e-tests-projected-pb7dd deletion completed in 8.441601271s

• [SLOW TEST:13.351 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:59:37.483: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-jzw6v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 28 08:59:44.468: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:59:44.528: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:59:46.529: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:59:46.589: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:59:48.529: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:59:48.590: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:59:48.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-jzw6v" for this suite.
Feb 28 09:00:10.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:00:11.797: INFO: namespace: e2e-tests-container-lifecycle-hook-jzw6v, resource: bindings, ignored listing per whitelist
Feb 28 09:00:13.161: INFO: namespace e2e-tests-container-lifecycle-hook-jzw6v deletion completed in 24.50612582s

• [SLOW TEST:35.678 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:00:13.162: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-j74f4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-6lnfm in namespace e2e-tests-proxy-j74f4
I0228 09:00:15.688556   30448 runners.go:184] Created replication controller with name: proxy-service-6lnfm, namespace: e2e-tests-proxy-j74f4, replica count: 1
I0228 09:00:16.789135   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 09:00:17.789373   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 09:00:18.789769   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 09:00:19.789997   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 09:00:20.790246   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 09:00:21.790438   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 09:00:22.790707   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 09:00:23.791030   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 09:00:24.791261   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 09:00:25.791507   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 09:00:26.791722   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 09:00:27.792265   30448 runners.go:184] proxy-service-6lnfm Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 28 09:00:27.854: INFO: setup took 12.285853855s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 28 09:00:27.924: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 70.093289ms)
Feb 28 09:00:27.924: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 70.020718ms)
Feb 28 09:00:27.924: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 70.244553ms)
Feb 28 09:00:27.924: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 70.141596ms)
Feb 28 09:00:27.924: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 70.115521ms)
Feb 28 09:00:27.924: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 70.105237ms)
Feb 28 09:00:27.925: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 70.551697ms)
Feb 28 09:00:27.925: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 70.764513ms)
Feb 28 09:00:27.931: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 77.395383ms)
Feb 28 09:00:27.931: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 77.192218ms)
Feb 28 09:00:27.931: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 77.146327ms)
Feb 28 09:00:27.974: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 120.497095ms)
Feb 28 09:00:27.975: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 120.515272ms)
Feb 28 09:00:27.975: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 120.747397ms)
Feb 28 09:00:27.975: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 121.293539ms)
Feb 28 09:00:27.977: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 122.604688ms)
Feb 28 09:00:28.039: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 61.354084ms)
Feb 28 09:00:28.039: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 61.414021ms)
Feb 28 09:00:28.039: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.807796ms)
Feb 28 09:00:28.039: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 62.312263ms)
Feb 28 09:00:28.039: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.90963ms)
Feb 28 09:00:28.039: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 62.35757ms)
Feb 28 09:00:28.040: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 62.062814ms)
Feb 28 09:00:28.040: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 62.022009ms)
Feb 28 09:00:28.040: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 62.022058ms)
Feb 28 09:00:28.040: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 62.020647ms)
Feb 28 09:00:28.040: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 62.74061ms)
Feb 28 09:00:28.040: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 62.197177ms)
Feb 28 09:00:28.040: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 62.904734ms)
Feb 28 09:00:28.040: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 63.01053ms)
Feb 28 09:00:28.040: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 62.71695ms)
Feb 28 09:00:28.041: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 63.232752ms)
Feb 28 09:00:28.101: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 60.1275ms)
Feb 28 09:00:28.102: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 60.194804ms)
Feb 28 09:00:28.102: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 60.380686ms)
Feb 28 09:00:28.102: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 61.432469ms)
Feb 28 09:00:28.102: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 60.643962ms)
Feb 28 09:00:28.102: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 60.644471ms)
Feb 28 09:00:28.102: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 61.16442ms)
Feb 28 09:00:28.102: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 60.562704ms)
Feb 28 09:00:28.102: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 60.6084ms)
Feb 28 09:00:28.102: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 60.980055ms)
Feb 28 09:00:28.102: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 60.950202ms)
Feb 28 09:00:28.103: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 60.983349ms)
Feb 28 09:00:28.103: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 61.571584ms)
Feb 28 09:00:28.103: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 62.122918ms)
Feb 28 09:00:28.104: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 62.613843ms)
Feb 28 09:00:28.104: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 62.975946ms)
Feb 28 09:00:28.164: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 59.862009ms)
Feb 28 09:00:28.164: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 60.101099ms)
Feb 28 09:00:28.165: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 60.408243ms)
Feb 28 09:00:28.165: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 60.619975ms)
Feb 28 09:00:28.165: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 60.376248ms)
Feb 28 09:00:28.165: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 60.488927ms)
Feb 28 09:00:28.165: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 60.630821ms)
Feb 28 09:00:28.166: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 61.417799ms)
Feb 28 09:00:28.166: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 61.483559ms)
Feb 28 09:00:28.166: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 61.580481ms)
Feb 28 09:00:28.166: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 61.417671ms)
Feb 28 09:00:28.166: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 61.338204ms)
Feb 28 09:00:28.166: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 61.768977ms)
Feb 28 09:00:28.167: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 62.359043ms)
Feb 28 09:00:28.167: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 62.310418ms)
Feb 28 09:00:28.167: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 62.848387ms)
Feb 28 09:00:28.230: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 62.953284ms)
Feb 28 09:00:28.230: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 62.886302ms)
Feb 28 09:00:28.230: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 62.899419ms)
Feb 28 09:00:28.230: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 62.974742ms)
Feb 28 09:00:28.230: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 62.368816ms)
Feb 28 09:00:28.230: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 62.311777ms)
Feb 28 09:00:28.230: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 62.405325ms)
Feb 28 09:00:28.230: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 62.481732ms)
Feb 28 09:00:28.230: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 62.434041ms)
Feb 28 09:00:28.231: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 63.515579ms)
Feb 28 09:00:28.231: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 63.552294ms)
Feb 28 09:00:28.231: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 63.079259ms)
Feb 28 09:00:28.232: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 64.280273ms)
Feb 28 09:00:28.232: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 64.309141ms)
Feb 28 09:00:28.232: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 63.680229ms)
Feb 28 09:00:28.232: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 65.041003ms)
Feb 28 09:00:28.294: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 61.355857ms)
Feb 28 09:00:28.294: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 60.64987ms)
Feb 28 09:00:28.294: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 61.52121ms)
Feb 28 09:00:28.294: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 61.605432ms)
Feb 28 09:00:28.294: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.354779ms)
Feb 28 09:00:28.295: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 61.831785ms)
Feb 28 09:00:28.295: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 62.016772ms)
Feb 28 09:00:28.295: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 61.715334ms)
Feb 28 09:00:28.295: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 61.360118ms)
Feb 28 09:00:28.295: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.307335ms)
Feb 28 09:00:28.295: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 61.605897ms)
Feb 28 09:00:28.295: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 61.425376ms)
Feb 28 09:00:28.295: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 62.337032ms)
Feb 28 09:00:28.296: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 63.080185ms)
Feb 28 09:00:28.296: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 62.936649ms)
Feb 28 09:00:28.296: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 62.594594ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 61.737801ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.862902ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 61.969113ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 61.920707ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 61.893096ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 62.106762ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 61.965225ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 62.002299ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 62.068543ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 62.053145ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 62.149169ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 62.054385ms)
Feb 28 09:00:28.358: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 62.202199ms)
Feb 28 09:00:28.359: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 62.705418ms)
Feb 28 09:00:28.359: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 62.754049ms)
Feb 28 09:00:28.359: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 62.686351ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 62.832913ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 62.926201ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 63.064728ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 62.887419ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 62.994556ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 63.046516ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 62.975413ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 62.874553ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 63.128704ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 62.9705ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 62.935423ms)
Feb 28 09:00:28.422: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 62.97897ms)
Feb 28 09:00:28.464: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 104.324305ms)
Feb 28 09:00:28.464: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 104.650552ms)
Feb 28 09:00:28.464: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 104.479189ms)
Feb 28 09:00:28.464: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 104.570195ms)
Feb 28 09:00:28.526: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 62.014606ms)
Feb 28 09:00:28.526: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 62.173353ms)
Feb 28 09:00:28.526: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 62.494456ms)
Feb 28 09:00:28.526: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 62.269716ms)
Feb 28 09:00:28.527: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 62.606111ms)
Feb 28 09:00:28.527: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 62.79336ms)
Feb 28 09:00:28.527: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 62.890792ms)
Feb 28 09:00:28.527: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 62.895514ms)
Feb 28 09:00:28.527: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 62.958042ms)
Feb 28 09:00:28.527: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 62.996582ms)
Feb 28 09:00:28.527: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 63.040831ms)
Feb 28 09:00:28.527: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 62.884381ms)
Feb 28 09:00:28.527: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 63.094323ms)
Feb 28 09:00:28.528: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 63.954558ms)
Feb 28 09:00:28.528: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 64.151537ms)
Feb 28 09:00:28.528: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 64.049781ms)
Feb 28 09:00:28.589: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 61.088029ms)
Feb 28 09:00:28.590: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 61.561901ms)
Feb 28 09:00:28.590: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 61.33812ms)
Feb 28 09:00:28.590: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 61.497998ms)
Feb 28 09:00:28.590: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 61.056822ms)
Feb 28 09:00:28.590: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.41498ms)
Feb 28 09:00:28.590: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.401603ms)
Feb 28 09:00:28.590: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 61.670959ms)
Feb 28 09:00:28.590: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 61.508108ms)
Feb 28 09:00:28.590: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 61.534914ms)
Feb 28 09:00:28.590: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 61.726281ms)
Feb 28 09:00:28.590: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 61.745535ms)
Feb 28 09:00:28.591: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 62.221107ms)
Feb 28 09:00:28.592: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 63.149973ms)
Feb 28 09:00:28.592: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 63.217559ms)
Feb 28 09:00:28.592: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 63.339968ms)
Feb 28 09:00:28.656: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 64.309015ms)
Feb 28 09:00:28.656: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 64.558285ms)
Feb 28 09:00:28.657: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 64.177227ms)
Feb 28 09:00:28.657: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 64.152314ms)
Feb 28 09:00:28.657: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 64.293443ms)
Feb 28 09:00:28.657: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 64.054922ms)
Feb 28 09:00:28.657: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 64.775424ms)
Feb 28 09:00:28.657: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 64.528658ms)
Feb 28 09:00:28.657: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 64.147319ms)
Feb 28 09:00:28.657: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 64.770864ms)
Feb 28 09:00:28.657: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 64.673336ms)
Feb 28 09:00:28.657: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 64.067458ms)
Feb 28 09:00:28.658: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 65.566903ms)
Feb 28 09:00:28.658: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 65.459697ms)
Feb 28 09:00:28.658: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 65.195044ms)
Feb 28 09:00:28.659: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 66.052078ms)
Feb 28 09:00:28.721: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 62.218736ms)
Feb 28 09:00:28.721: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 62.470005ms)
Feb 28 09:00:28.721: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.653735ms)
Feb 28 09:00:28.721: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 62.175469ms)
Feb 28 09:00:28.721: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 62.10018ms)
Feb 28 09:00:28.721: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 61.946116ms)
Feb 28 09:00:28.721: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 62.371063ms)
Feb 28 09:00:28.721: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 62.07298ms)
Feb 28 09:00:28.721: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 61.971888ms)
Feb 28 09:00:28.721: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 61.877307ms)
Feb 28 09:00:28.721: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 61.91988ms)
Feb 28 09:00:28.722: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 62.287153ms)
Feb 28 09:00:28.722: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 62.600117ms)
Feb 28 09:00:28.723: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 63.094489ms)
Feb 28 09:00:28.723: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 63.049153ms)
Feb 28 09:00:28.723: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 63.202602ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 62.949898ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 62.984791ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 63.091634ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 63.013364ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 63.206366ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 63.198792ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 63.336393ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 63.344917ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 63.297278ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 63.294469ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 63.403396ms)
Feb 28 09:00:28.786: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 63.406569ms)
Feb 28 09:00:28.787: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 64.125797ms)
Feb 28 09:00:28.788: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 64.716246ms)
Feb 28 09:00:28.788: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 64.531209ms)
Feb 28 09:00:28.789: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 65.904319ms)
Feb 28 09:00:28.853: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 63.068835ms)
Feb 28 09:00:28.853: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 63.350404ms)
Feb 28 09:00:28.853: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 63.429408ms)
Feb 28 09:00:28.853: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 63.145238ms)
Feb 28 09:00:28.853: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 63.753474ms)
Feb 28 09:00:28.853: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 63.517785ms)
Feb 28 09:00:28.853: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 63.715266ms)
Feb 28 09:00:28.853: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 63.057896ms)
Feb 28 09:00:28.853: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 63.694745ms)
Feb 28 09:00:28.853: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 64.155335ms)
Feb 28 09:00:28.854: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 64.638289ms)
Feb 28 09:00:28.854: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 65.247658ms)
Feb 28 09:00:28.855: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 65.38834ms)
Feb 28 09:00:28.855: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 65.473629ms)
Feb 28 09:00:28.855: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 65.847933ms)
Feb 28 09:00:28.855: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 66.152521ms)
Feb 28 09:00:28.979: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 123.948306ms)
Feb 28 09:00:28.979: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 124.064327ms)
Feb 28 09:00:28.979: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 124.206107ms)
Feb 28 09:00:28.979: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 124.182835ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 124.216706ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 124.145477ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 124.173861ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 124.234317ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 124.333696ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 124.113623ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 124.251688ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 124.314868ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 124.191576ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 124.189851ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 124.284187ms)
Feb 28 09:00:28.980: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 124.267689ms)
Feb 28 09:00:29.040: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 60.095441ms)
Feb 28 09:00:29.040: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 60.210953ms)
Feb 28 09:00:29.040: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 60.102775ms)
Feb 28 09:00:29.040: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 60.146041ms)
Feb 28 09:00:29.040: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 60.153184ms)
Feb 28 09:00:29.040: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 60.107554ms)
Feb 28 09:00:29.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 61.802928ms)
Feb 28 09:00:29.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 61.842052ms)
Feb 28 09:00:29.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 61.947245ms)
Feb 28 09:00:29.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 61.833032ms)
Feb 28 09:00:29.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 61.822356ms)
Feb 28 09:00:29.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 61.925637ms)
Feb 28 09:00:29.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 61.908117ms)
Feb 28 09:00:29.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 61.875306ms)
Feb 28 09:00:29.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 62.385721ms)
Feb 28 09:00:29.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 62.339021ms)
Feb 28 09:00:29.103: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 60.682686ms)
Feb 28 09:00:29.103: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 60.765546ms)
Feb 28 09:00:29.103: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 60.654451ms)
Feb 28 09:00:29.103: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 60.777325ms)
Feb 28 09:00:29.103: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 60.725056ms)
Feb 28 09:00:29.103: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 60.900023ms)
Feb 28 09:00:29.103: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 61.000266ms)
Feb 28 09:00:29.103: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 61.128702ms)
Feb 28 09:00:29.104: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 61.123909ms)
Feb 28 09:00:29.104: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 61.18676ms)
Feb 28 09:00:29.104: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 61.147824ms)
Feb 28 09:00:29.104: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 61.171686ms)
Feb 28 09:00:29.104: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 61.59466ms)
Feb 28 09:00:29.104: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 61.586041ms)
Feb 28 09:00:29.104: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 61.607447ms)
Feb 28 09:00:29.105: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 62.197281ms)
Feb 28 09:00:29.169: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 64.107481ms)
Feb 28 09:00:29.169: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 64.228105ms)
Feb 28 09:00:29.169: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 64.167919ms)
Feb 28 09:00:29.169: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 64.139429ms)
Feb 28 09:00:29.169: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 64.725305ms)
Feb 28 09:00:29.169: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 64.673315ms)
Feb 28 09:00:29.169: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 64.755791ms)
Feb 28 09:00:29.170: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 65.35309ms)
Feb 28 09:00:29.170: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 65.475306ms)
Feb 28 09:00:29.170: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 65.523895ms)
Feb 28 09:00:29.170: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 65.57237ms)
Feb 28 09:00:29.170: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 65.480917ms)
Feb 28 09:00:29.170: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 65.720117ms)
Feb 28 09:00:29.170: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 65.674275ms)
Feb 28 09:00:29.171: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 66.291059ms)
Feb 28 09:00:29.171: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 66.437492ms)
Feb 28 09:00:29.233: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.681417ms)
Feb 28 09:00:29.233: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 61.776754ms)
Feb 28 09:00:29.233: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.743113ms)
Feb 28 09:00:29.233: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 61.672131ms)
Feb 28 09:00:29.233: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 61.739292ms)
Feb 28 09:00:29.233: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 61.744239ms)
Feb 28 09:00:29.233: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 62.023068ms)
Feb 28 09:00:29.233: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 62.010308ms)
Feb 28 09:00:29.233: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 62.132078ms)
Feb 28 09:00:29.233: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 62.1301ms)
Feb 28 09:00:29.233: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 62.210137ms)
Feb 28 09:00:29.234: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 62.45331ms)
Feb 28 09:00:29.234: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 63.110981ms)
Feb 28 09:00:29.234: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 62.966664ms)
Feb 28 09:00:29.234: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 63.223498ms)
Feb 28 09:00:29.235: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 63.521732ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:443/proxy/... (200; 61.957681ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:462/proxy/: tls qux (200; 62.238043ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.664645ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 62.338259ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc/proxy/rewriteme"... (200; 61.986676ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:1080/proxy/... (200; 62.093556ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:1080/proxy/rewri... (200; 61.523304ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/https:proxy-service-6lnfm-nxhdc:460/proxy/: tls baz (200; 61.831486ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/http:proxy-service-6lnfm-nxhdc:162/proxy/: bar (200; 61.737097ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/pods/proxy-service-6lnfm-nxhdc:160/proxy/: foo (200; 62.195544ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname2/proxy/: tls qux (200; 61.982217ms)
Feb 28 09:00:29.297: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/https:proxy-service-6lnfm:tlsportname1/proxy/: tls baz (200; 61.777124ms)
Feb 28 09:00:29.298: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname2/proxy/: bar (200; 62.615156ms)
Feb 28 09:00:29.299: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname1/proxy/: foo (200; 63.178593ms)
Feb 28 09:00:29.299: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/http:proxy-service-6lnfm:portname2/proxy/: bar (200; 63.433491ms)
Feb 28 09:00:29.299: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-j74f4/services/proxy-service-6lnfm:portname1/proxy/: foo (200; 64.036172ms)
STEP: deleting ReplicationController proxy-service-6lnfm in namespace e2e-tests-proxy-j74f4, will wait for the garbage collector to delete the pods
Feb 28 09:00:29.521: INFO: Deleting ReplicationController proxy-service-6lnfm took: 60.229969ms
Feb 28 09:00:29.621: INFO: Terminating ReplicationController proxy-service-6lnfm pods took: 100.490144ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:00:37.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-j74f4" for this suite.
Feb 28 09:00:43.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:00:44.558: INFO: namespace: e2e-tests-proxy-j74f4, resource: bindings, ignored listing per whitelist
Feb 28 09:00:46.186: INFO: namespace e2e-tests-proxy-j74f4 deletion completed in 8.546494401s

• [SLOW TEST:33.024 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:00:46.187: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-5xf76
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 28 09:00:50.990: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-57520f3d-3b37-11e9-8ad6-42906abdb246", GenerateName:"", Namespace:"e2e-tests-pods-5xf76", SelfLink:"/api/v1/namespaces/e2e-tests-pods-5xf76/pods/pod-submit-remove-57520f3d-3b37-11e9-8ad6-42906abdb246", UID:"57677b5f-3b37-11e9-ac40-06fd5a6cf138", ResourceVersion:"18573", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686941248, loc:(*time.Location)(0x791ea40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"574675176"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.246/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-hztx4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0019ec500), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hztx4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0016073e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-250-15-222.eu-west-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0013a91a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001607530)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001607550)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001607558), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00160755c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686941248, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686941250, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686941250, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686941248, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.15.222", PodIP:"100.96.1.246", StartTime:(*v1.Time)(0xc000a19e20), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc000a19e40), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://4413607fab0af627ac6d2fb5c1cd233de56e0c29c71740bc6e8133632146716f"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:00:57.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-5xf76" for this suite.
Feb 28 09:01:03.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:01:04.252: INFO: namespace: e2e-tests-pods-5xf76, resource: bindings, ignored listing per whitelist
Feb 28 09:01:06.096: INFO: namespace e2e-tests-pods-5xf76 deletion completed in 8.498958929s

• [SLOW TEST:19.910 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:01:06.097: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-9gfw4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 09:01:08.579: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-9gfw4'
Feb 28 09:01:09.164: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 09:01:09.164: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Feb 28 09:01:09.291: INFO: scanned /root for discovery docs: <nil>
Feb 28 09:01:09.291: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-9gfw4'
Feb 28 09:01:22.133: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 28 09:01:22.133: INFO: stdout: "Created e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22\nScaling up e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 28 09:01:22.133: INFO: stdout: "Created e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22\nScaling up e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 28 09:01:22.133: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-9gfw4'
Feb 28 09:01:22.464: INFO: stderr: ""
Feb 28 09:01:22.464: INFO: stdout: "e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22-l9wjm "
Feb 28 09:01:22.464: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22-l9wjm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9gfw4'
Feb 28 09:01:22.812: INFO: stderr: ""
Feb 28 09:01:22.813: INFO: stdout: "true"
Feb 28 09:01:22.813: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22-l9wjm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9gfw4'
Feb 28 09:01:23.138: INFO: stderr: ""
Feb 28 09:01:23.138: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb 28 09:01:23.138: INFO: e2e-test-nginx-rc-8e776a9235566a72f55b3c1364323f22-l9wjm is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Feb 28 09:01:23.138: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-9gfw4'
Feb 28 09:01:23.530: INFO: stderr: ""
Feb 28 09:01:23.530: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:01:23.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9gfw4" for this suite.
Feb 28 09:01:29.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:01:30.187: INFO: namespace: e2e-tests-kubectl-9gfw4, resource: bindings, ignored listing per whitelist
Feb 28 09:01:32.075: INFO: namespace e2e-tests-kubectl-9gfw4 deletion completed in 8.483036846s

• [SLOW TEST:25.979 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:01:32.075: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-r6mr8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 09:01:34.499: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 09:01:36.620: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 09:01:39.089: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-r6mr8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-r6mr8/deployments/test-cleanup-deployment,UID:740a72f5-3b37-11e9-ac40-06fd5a6cf138,ResourceVersion:18767,Generation:1,CreationTimestamp:2019-02-28 09:01:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-28 09:01:36 +0000 UTC 2019-02-28 09:01:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-28 09:01:38 +0000 UTC 2019-02-28 09:01:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-7dbbfcf846" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 09:01:39.146: INFO: New ReplicaSet "test-cleanup-deployment-7dbbfcf846" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846,GenerateName:,Namespace:e2e-tests-deployment-r6mr8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-r6mr8/replicasets/test-cleanup-deployment-7dbbfcf846,UID:740ba957-3b37-11e9-ac40-06fd5a6cf138,ResourceVersion:18759,Generation:1,CreationTimestamp:2019-02-28 09:01:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 740a72f5-3b37-11e9-ac40-06fd5a6cf138 0xc001820d97 0xc001820d98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 09:01:39.201: INFO: Pod "test-cleanup-deployment-7dbbfcf846-9z9cq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846-9z9cq,GenerateName:test-cleanup-deployment-7dbbfcf846-,Namespace:e2e-tests-deployment-r6mr8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-r6mr8/pods/test-cleanup-deployment-7dbbfcf846-9z9cq,UID:740bf4e5-3b37-11e9-ac40-06fd5a6cf138,ResourceVersion:18758,Generation:0,CreationTimestamp:2019-02-28 09:01:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.250/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-7dbbfcf846 740ba957-3b37-11e9-ac40-06fd5a6cf138 0xc0018214b7 0xc0018214b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vwxjv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vwxjv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-vwxjv true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-15-222.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001821520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001821540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:01:36 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:01:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:01:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:01:36 +0000 UTC  }],Message:,Reason:,HostIP:10.250.15.222,PodIP:100.96.1.250,StartTime:2019-02-28 09:01:36 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-28 09:01:37 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://3ad550674c86d1505cb5d15f3002cc7b3e513a1b061435e86f859ded2fc5d866}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:01:39.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-r6mr8" for this suite.
Feb 28 09:01:45.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:01:47.152: INFO: namespace: e2e-tests-deployment-r6mr8, resource: bindings, ignored listing per whitelist
Feb 28 09:01:47.746: INFO: namespace e2e-tests-deployment-r6mr8 deletion completed in 8.48709947s

• [SLOW TEST:15.671 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:01:47.746: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-tkgf8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-7c093f59-3b37-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 09:01:50.295: INFO: Waiting up to 5m0s for pod "pod-configmaps-7c1272e8-3b37-11e9-8ad6-42906abdb246" in namespace "e2e-tests-configmap-tkgf8" to be "success or failure"
Feb 28 09:01:50.356: INFO: Pod "pod-configmaps-7c1272e8-3b37-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 61.030853ms
Feb 28 09:01:52.414: INFO: Pod "pod-configmaps-7c1272e8-3b37-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.119014506s
STEP: Saw pod success
Feb 28 09:01:52.414: INFO: Pod "pod-configmaps-7c1272e8-3b37-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:01:52.472: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-configmaps-7c1272e8-3b37-11e9-8ad6-42906abdb246 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 09:01:52.594: INFO: Waiting for pod pod-configmaps-7c1272e8-3b37-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:01:52.651: INFO: Pod pod-configmaps-7c1272e8-3b37-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:01:52.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-tkgf8" for this suite.
Feb 28 09:01:58.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:02:00.575: INFO: namespace: e2e-tests-configmap-tkgf8, resource: bindings, ignored listing per whitelist
Feb 28 09:02:01.293: INFO: namespace e2e-tests-configmap-tkgf8 deletion completed in 8.58336619s

• [SLOW TEST:13.547 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:02:01.293: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-hdfl2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 28 09:02:03.736: INFO: Waiting up to 5m0s for pod "pod-841529f2-3b37-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-hdfl2" to be "success or failure"
Feb 28 09:02:03.796: INFO: Pod "pod-841529f2-3b37-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.598619ms
Feb 28 09:02:05.854: INFO: Pod "pod-841529f2-3b37-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118456984s
STEP: Saw pod success
Feb 28 09:02:05.854: INFO: Pod "pod-841529f2-3b37-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:02:05.913: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-841529f2-3b37-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 09:02:06.042: INFO: Waiting for pod pod-841529f2-3b37-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:02:06.101: INFO: Pod pod-841529f2-3b37-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:02:06.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hdfl2" for this suite.
Feb 28 09:02:12.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:02:13.231: INFO: namespace: e2e-tests-emptydir-hdfl2, resource: bindings, ignored listing per whitelist
Feb 28 09:02:14.650: INFO: namespace e2e-tests-emptydir-hdfl2 deletion completed in 8.488225245s

• [SLOW TEST:13.357 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:02:14.651: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-c5mpt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-c5mpt
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 28 09:02:17.171: INFO: Found 1 stateful pods, waiting for 3
Feb 28 09:02:27.236: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 09:02:27.236: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 09:02:27.236: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 28 09:02:27.531: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 28 09:02:27.785: INFO: Updating stateful set ss2
Feb 28 09:02:27.907: INFO: Waiting for Pod e2e-tests-statefulset-c5mpt/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 28 09:02:38.217: INFO: Found 2 stateful pods, waiting for 3
Feb 28 09:02:48.278: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 09:02:48.278: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 09:02:48.278: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 28 09:02:48.529: INFO: Updating stateful set ss2
Feb 28 09:02:48.650: INFO: Waiting for Pod e2e-tests-statefulset-c5mpt/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 28 09:02:58.905: INFO: Updating stateful set ss2
Feb 28 09:02:59.024: INFO: Waiting for StatefulSet e2e-tests-statefulset-c5mpt/ss2 to complete update
Feb 28 09:02:59.025: INFO: Waiting for Pod e2e-tests-statefulset-c5mpt/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 28 09:03:09.141: INFO: Waiting for StatefulSet e2e-tests-statefulset-c5mpt/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 09:03:19.143: INFO: Deleting all statefulset in ns e2e-tests-statefulset-c5mpt
Feb 28 09:03:19.205: INFO: Scaling statefulset ss2 to 0
Feb 28 09:03:49.446: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 09:03:49.503: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:03:49.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-c5mpt" for this suite.
Feb 28 09:03:55.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:03:56.872: INFO: namespace: e2e-tests-statefulset-c5mpt, resource: bindings, ignored listing per whitelist
Feb 28 09:03:58.293: INFO: namespace e2e-tests-statefulset-c5mpt deletion completed in 8.553876916s

• [SLOW TEST:103.642 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:03:58.293: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-sr4mx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-c9d77705-3b37-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 09:04:00.829: INFO: Waiting up to 5m0s for pod "pod-secrets-c9e076ff-3b37-11e9-8ad6-42906abdb246" in namespace "e2e-tests-secrets-sr4mx" to be "success or failure"
Feb 28 09:04:00.888: INFO: Pod "pod-secrets-c9e076ff-3b37-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 59.458125ms
Feb 28 09:04:02.950: INFO: Pod "pod-secrets-c9e076ff-3b37-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.120694924s
STEP: Saw pod success
Feb 28 09:04:02.950: INFO: Pod "pod-secrets-c9e076ff-3b37-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:04:03.011: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-secrets-c9e076ff-3b37-11e9-8ad6-42906abdb246 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 09:04:03.144: INFO: Waiting for pod pod-secrets-c9e076ff-3b37-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:04:03.201: INFO: Pod pod-secrets-c9e076ff-3b37-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:04:03.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-sr4mx" for this suite.
Feb 28 09:04:09.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:04:11.516: INFO: namespace: e2e-tests-secrets-sr4mx, resource: bindings, ignored listing per whitelist
Feb 28 09:04:12.333: INFO: namespace e2e-tests-secrets-sr4mx deletion completed in 9.070952368s

• [SLOW TEST:14.040 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:04:12.333: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-fbnsf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W0228 09:04:15.265460   30448 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 09:04:15.265: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:04:15.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-fbnsf" for this suite.
Feb 28 09:04:21.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:04:23.369: INFO: namespace: e2e-tests-gc-fbnsf, resource: bindings, ignored listing per whitelist
Feb 28 09:04:23.905: INFO: namespace e2e-tests-gc-fbnsf deletion completed in 8.518044679s

• [SLOW TEST:11.572 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:04:23.907: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-5txqx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:04:30.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-5txqx" for this suite.
Feb 28 09:04:36.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:04:37.513: INFO: namespace: e2e-tests-kubelet-test-5txqx, resource: bindings, ignored listing per whitelist
Feb 28 09:04:39.038: INFO: namespace e2e-tests-kubelet-test-5txqx deletion completed in 8.47008054s

• [SLOW TEST:15.131 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:04:39.038: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-5zd4s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 09:04:41.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e21501b2-3b37-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-5zd4s" to be "success or failure"
Feb 28 09:04:41.500: INFO: Pod "downwardapi-volume-e21501b2-3b37-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 58.524955ms
Feb 28 09:04:43.560: INFO: Pod "downwardapi-volume-e21501b2-3b37-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118947496s
STEP: Saw pod success
Feb 28 09:04:43.560: INFO: Pod "downwardapi-volume-e21501b2-3b37-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:04:43.620: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-e21501b2-3b37-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 09:04:43.753: INFO: Waiting for pod downwardapi-volume-e21501b2-3b37-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:04:43.812: INFO: Pod downwardapi-volume-e21501b2-3b37-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:04:43.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5zd4s" for this suite.
Feb 28 09:04:50.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:04:51.351: INFO: namespace: e2e-tests-downward-api-5zd4s, resource: bindings, ignored listing per whitelist
Feb 28 09:04:52.361: INFO: namespace e2e-tests-downward-api-5zd4s deletion completed in 8.489334328s

• [SLOW TEST:13.323 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:04:52.361: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-c5pm7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 09:04:57.721: INFO: Successfully updated pod "annotationupdateea087939-3b37-11e9-8ad6-42906abdb246"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:04:59.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-c5pm7" for this suite.
Feb 28 09:05:22.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:05:23.475: INFO: namespace: e2e-tests-projected-c5pm7, resource: bindings, ignored listing per whitelist
Feb 28 09:05:24.434: INFO: namespace e2e-tests-projected-c5pm7 deletion completed in 24.519008244s

• [SLOW TEST:32.073 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:05:24.435: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-zn7bw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 09:05:26.939: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd3395bc-3b37-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-zn7bw" to be "success or failure"
Feb 28 09:05:27.000: INFO: Pod "downwardapi-volume-fd3395bc-3b37-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.980735ms
Feb 28 09:05:29.060: INFO: Pod "downwardapi-volume-fd3395bc-3b37-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121063352s
STEP: Saw pod success
Feb 28 09:05:29.060: INFO: Pod "downwardapi-volume-fd3395bc-3b37-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:05:29.117: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-fd3395bc-3b37-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 09:05:29.244: INFO: Waiting for pod downwardapi-volume-fd3395bc-3b37-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:05:29.305: INFO: Pod downwardapi-volume-fd3395bc-3b37-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:05:29.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zn7bw" for this suite.
Feb 28 09:05:35.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:05:36.843: INFO: namespace: e2e-tests-projected-zn7bw, resource: bindings, ignored listing per whitelist
Feb 28 09:05:37.858: INFO: namespace e2e-tests-projected-zn7bw deletion completed in 8.490640563s

• [SLOW TEST:13.424 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:05:37.858: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-c4jtr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-0521a5a5-3b38-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 09:05:40.297: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-052a477e-3b38-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-c4jtr" to be "success or failure"
Feb 28 09:05:40.619: INFO: Pod "pod-projected-configmaps-052a477e-3b38-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 322.200422ms
Feb 28 09:05:42.679: INFO: Pod "pod-projected-configmaps-052a477e-3b38-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.381972447s
STEP: Saw pod success
Feb 28 09:05:42.679: INFO: Pod "pod-projected-configmaps-052a477e-3b38-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:05:42.738: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-configmaps-052a477e-3b38-11e9-8ad6-42906abdb246 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 09:05:42.868: INFO: Waiting for pod pod-projected-configmaps-052a477e-3b38-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:05:42.929: INFO: Pod pod-projected-configmaps-052a477e-3b38-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:05:42.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-c4jtr" for this suite.
Feb 28 09:05:49.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:05:51.357: INFO: namespace: e2e-tests-projected-c4jtr, resource: bindings, ignored listing per whitelist
Feb 28 09:05:51.534: INFO: namespace e2e-tests-projected-c4jtr deletion completed in 8.486798572s

• [SLOW TEST:13.676 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:05:51.535: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-48zph
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 09:05:54.016: INFO: Waiting up to 5m0s for pod "downward-api-0d578560-3b38-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-48zph" to be "success or failure"
Feb 28 09:05:54.077: INFO: Pod "downward-api-0d578560-3b38-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.635348ms
Feb 28 09:05:56.139: INFO: Pod "downward-api-0d578560-3b38-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.122298558s
STEP: Saw pod success
Feb 28 09:05:56.139: INFO: Pod "downward-api-0d578560-3b38-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:05:56.200: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downward-api-0d578560-3b38-11e9-8ad6-42906abdb246 container dapi-container: <nil>
STEP: delete the pod
Feb 28 09:05:56.332: INFO: Waiting for pod downward-api-0d578560-3b38-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:05:56.395: INFO: Pod downward-api-0d578560-3b38-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:05:56.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-48zph" for this suite.
Feb 28 09:06:02.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:06:03.097: INFO: namespace: e2e-tests-downward-api-48zph, resource: bindings, ignored listing per whitelist
Feb 28 09:06:04.904: INFO: namespace e2e-tests-downward-api-48zph deletion completed in 8.449444911s

• [SLOW TEST:13.370 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:06:04.905: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8fd88
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-1547daa7-3b38-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 09:06:07.397: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-15511c6a-3b38-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-8fd88" to be "success or failure"
Feb 28 09:06:07.458: INFO: Pod "pod-projected-secrets-15511c6a-3b38-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.799589ms
Feb 28 09:06:09.518: INFO: Pod "pod-projected-secrets-15511c6a-3b38-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121215919s
STEP: Saw pod success
Feb 28 09:06:09.518: INFO: Pod "pod-projected-secrets-15511c6a-3b38-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:06:09.579: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-secrets-15511c6a-3b38-11e9-8ad6-42906abdb246 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 09:06:09.708: INFO: Waiting for pod pod-projected-secrets-15511c6a-3b38-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:06:09.769: INFO: Pod pod-projected-secrets-15511c6a-3b38-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:06:09.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8fd88" for this suite.
Feb 28 09:06:16.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:06:17.325: INFO: namespace: e2e-tests-projected-8fd88, resource: bindings, ignored listing per whitelist
Feb 28 09:06:18.286: INFO: namespace e2e-tests-projected-8fd88 deletion completed in 8.456923365s

• [SLOW TEST:13.382 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:06:18.287: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-mnm82
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-1d42e54c-3b38-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 09:06:20.786: INFO: Waiting up to 5m0s for pod "pod-configmaps-1d4c4226-3b38-11e9-8ad6-42906abdb246" in namespace "e2e-tests-configmap-mnm82" to be "success or failure"
Feb 28 09:06:20.845: INFO: Pod "pod-configmaps-1d4c4226-3b38-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 59.47026ms
Feb 28 09:06:22.904: INFO: Pod "pod-configmaps-1d4c4226-3b38-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118680225s
STEP: Saw pod success
Feb 28 09:06:22.904: INFO: Pod "pod-configmaps-1d4c4226-3b38-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:06:22.962: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-configmaps-1d4c4226-3b38-11e9-8ad6-42906abdb246 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 09:06:23.087: INFO: Waiting for pod pod-configmaps-1d4c4226-3b38-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:06:23.145: INFO: Pod pod-configmaps-1d4c4226-3b38-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:06:23.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mnm82" for this suite.
Feb 28 09:06:29.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:06:31.564: INFO: namespace: e2e-tests-configmap-mnm82, resource: bindings, ignored listing per whitelist
Feb 28 09:06:31.743: INFO: namespace e2e-tests-configmap-mnm82 deletion completed in 8.53820509s

• [SLOW TEST:13.456 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:06:31.743: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-znk48
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-25406d56-3b38-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume secrets
Feb 28 09:06:34.192: INFO: Waiting up to 5m0s for pod "pod-secrets-25499fbd-3b38-11e9-8ad6-42906abdb246" in namespace "e2e-tests-secrets-znk48" to be "success or failure"
Feb 28 09:06:34.253: INFO: Pod "pod-secrets-25499fbd-3b38-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 60.295673ms
Feb 28 09:06:36.312: INFO: Pod "pod-secrets-25499fbd-3b38-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.120123842s
STEP: Saw pod success
Feb 28 09:06:36.313: INFO: Pod "pod-secrets-25499fbd-3b38-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:06:36.371: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-secrets-25499fbd-3b38-11e9-8ad6-42906abdb246 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 09:06:36.501: INFO: Waiting for pod pod-secrets-25499fbd-3b38-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:06:36.561: INFO: Pod pod-secrets-25499fbd-3b38-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:06:36.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-znk48" for this suite.
Feb 28 09:06:42.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:06:43.998: INFO: namespace: e2e-tests-secrets-znk48, resource: bindings, ignored listing per whitelist
Feb 28 09:06:45.049: INFO: namespace e2e-tests-secrets-znk48 deletion completed in 8.428877006s

• [SLOW TEST:13.306 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:06:45.049: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cd7vt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 09:06:47.383: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-cd7vt'
Feb 28 09:06:50.143: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 09:06:50.143: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Feb 28 09:06:50.200: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-cd7vt'
Feb 28 09:06:50.588: INFO: stderr: ""
Feb 28 09:06:50.588: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:06:50.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cd7vt" for this suite.
Feb 28 09:06:56.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:06:58.034: INFO: namespace: e2e-tests-kubectl-cd7vt, resource: bindings, ignored listing per whitelist
Feb 28 09:06:59.227: INFO: namespace e2e-tests-kubectl-cd7vt deletion completed in 8.582688722s

• [SLOW TEST:14.178 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:06:59.228: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-blp4w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 09:07:01.508: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-blp4w'
Feb 28 09:07:01.824: INFO: stderr: ""
Feb 28 09:07:01.824: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 28 09:07:06.925: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-blp4w -o json'
Feb 28 09:07:07.243: INFO: stderr: ""
Feb 28 09:07:07.243: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.17/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-02-28T09:07:01Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-blp4w\",\n        \"resourceVersion\": \"19843\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-blp4w/pods/e2e-test-nginx-pod\",\n        \"uid\": \"35c474f2-3b38-11e9-ac40-06fd5a6cf138\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-x7jww\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-250-15-222.eu-west-1.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-x7jww\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-x7jww\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T09:07:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T09:07:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T09:07:02Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T09:07:01Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://0bd199fbafdf5141f03885b1de78b4e2c77dfd2a3fe7dd0a7cb6765e975ba4f8\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-28T09:07:02Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.15.222\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.17\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-28T09:07:01Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 28 09:07:07.243: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml replace -f - --namespace=e2e-tests-kubectl-blp4w'
Feb 28 09:07:07.967: INFO: stderr: ""
Feb 28 09:07:07.967: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Feb 28 09:07:08.028: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-blp4w'
Feb 28 09:07:10.107: INFO: stderr: ""
Feb 28 09:07:10.107: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:07:10.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-blp4w" for this suite.
Feb 28 09:07:16.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:07:17.502: INFO: namespace: e2e-tests-kubectl-blp4w, resource: bindings, ignored listing per whitelist
Feb 28 09:07:18.619: INFO: namespace e2e-tests-kubectl-blp4w deletion completed in 8.452874949s

• [SLOW TEST:19.391 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:07:18.619: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-29dcz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4126192b-3b38-11e9-8ad6-42906abdb246
STEP: Creating a pod to test consume configMaps
Feb 28 09:07:20.996: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-412fcca3-3b38-11e9-8ad6-42906abdb246" in namespace "e2e-tests-projected-29dcz" to be "success or failure"
Feb 28 09:07:21.054: INFO: Pod "pod-projected-configmaps-412fcca3-3b38-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 58.374338ms
Feb 28 09:07:23.113: INFO: Pod "pod-projected-configmaps-412fcca3-3b38-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.116816366s
STEP: Saw pod success
Feb 28 09:07:23.113: INFO: Pod "pod-projected-configmaps-412fcca3-3b38-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:07:23.172: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-projected-configmaps-412fcca3-3b38-11e9-8ad6-42906abdb246 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 09:07:23.304: INFO: Waiting for pod pod-projected-configmaps-412fcca3-3b38-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:07:23.365: INFO: Pod pod-projected-configmaps-412fcca3-3b38-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:07:23.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-29dcz" for this suite.
Feb 28 09:07:29.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:07:31.118: INFO: namespace: e2e-tests-projected-29dcz, resource: bindings, ignored listing per whitelist
Feb 28 09:07:31.890: INFO: namespace e2e-tests-projected-29dcz deletion completed in 8.463192689s

• [SLOW TEST:13.272 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:07:31.891: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-np7nt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 28 09:07:34.317: INFO: Waiting up to 5m0s for pod "pod-4920d2e1-3b38-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-np7nt" to be "success or failure"
Feb 28 09:07:34.372: INFO: Pod "pod-4920d2e1-3b38-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 55.27836ms
Feb 28 09:07:36.433: INFO: Pod "pod-4920d2e1-3b38-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11584288s
STEP: Saw pod success
Feb 28 09:07:36.433: INFO: Pod "pod-4920d2e1-3b38-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:07:36.492: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-4920d2e1-3b38-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 09:07:36.621: INFO: Waiting for pod pod-4920d2e1-3b38-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:07:36.680: INFO: Pod pod-4920d2e1-3b38-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:07:36.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-np7nt" for this suite.
Feb 28 09:07:42.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:07:44.390: INFO: namespace: e2e-tests-emptydir-np7nt, resource: bindings, ignored listing per whitelist
Feb 28 09:07:45.231: INFO: namespace e2e-tests-emptydir-np7nt deletion completed in 8.489885891s

• [SLOW TEST:13.340 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:07:45.231: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-pcckq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 28 09:07:47.636: INFO: Waiting up to 5m0s for pod "pod-51106b53-3b38-11e9-8ad6-42906abdb246" in namespace "e2e-tests-emptydir-pcckq" to be "success or failure"
Feb 28 09:07:47.695: INFO: Pod "pod-51106b53-3b38-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 59.114888ms
Feb 28 09:07:49.752: INFO: Pod "pod-51106b53-3b38-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.116516572s
STEP: Saw pod success
Feb 28 09:07:49.752: INFO: Pod "pod-51106b53-3b38-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:07:49.812: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod pod-51106b53-3b38-11e9-8ad6-42906abdb246 container test-container: <nil>
STEP: delete the pod
Feb 28 09:07:49.933: INFO: Waiting for pod pod-51106b53-3b38-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:07:49.991: INFO: Pod pod-51106b53-3b38-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:07:49.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pcckq" for this suite.
Feb 28 09:07:56.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:07:57.063: INFO: namespace: e2e-tests-emptydir-pcckq, resource: bindings, ignored listing per whitelist
Feb 28 09:07:58.443: INFO: namespace e2e-tests-emptydir-pcckq deletion completed in 8.395346393s

• [SLOW TEST:13.212 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:07:58.443: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-cw9jb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 09:08:00.771: INFO: PodSpec: initContainers in spec.initContainers
Feb 28 09:08:44.983: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-58ee0c6f-3b38-11e9-8ad6-42906abdb246", GenerateName:"", Namespace:"e2e-tests-init-container-cw9jb", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-cw9jb/pods/pod-init-58ee0c6f-3b38-11e9-8ad6-42906abdb246", UID:"58f1b5e2-3b38-11e9-ac40-06fd5a6cf138", ResourceVersion:"20115", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686941680, loc:(*time.Location)(0x791ea40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"771395333"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp", "cni.projectcalico.org/podIP":"100.96.1.21/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-2g6kp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000efd680), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2g6kp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2g6kp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2g6kp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0013b8688), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-250-15-222.eu-west-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0017b57a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0013b8700)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0013b8720)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0013b8728), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0013b872c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686941680, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686941680, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686941680, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686941680, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.15.222", PodIP:"100.96.1.21", StartTime:(*v1.Time)(0xc000e71a60), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0003f16c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0003f1730)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://17c4009cd94ac04b309f9f98e820d33ac7baafbf2948d9fac16b1c1fa7ce4ee5"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e71aa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e71a80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:08:44.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-cw9jb" for this suite.
Feb 28 09:09:07.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:09:08.314: INFO: namespace: e2e-tests-init-container-cw9jb, resource: bindings, ignored listing per whitelist
Feb 28 09:09:09.503: INFO: namespace e2e-tests-init-container-cw9jb deletion completed in 24.458646812s

• [SLOW TEST:71.060 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:09:09.504: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-snpsr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 09:09:11.754: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:09:14.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-snpsr" for this suite.
Feb 28 09:09:20.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:09:21.037: INFO: namespace: e2e-tests-init-container-snpsr, resource: bindings, ignored listing per whitelist
Feb 28 09:09:22.543: INFO: namespace e2e-tests-init-container-snpsr deletion completed in 8.343787685s

• [SLOW TEST:13.039 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:09:22.543: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-dc8s7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-dc8s7
I0228 09:09:24.927397   30448 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-dc8s7, replica count: 1
I0228 09:09:26.028012   30448 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 09:09:27.028255   30448 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 28 09:09:27.191: INFO: Created: latency-svc-8svjt
Feb 28 09:09:27.195: INFO: Got endpoints: latency-svc-8svjt [67.407611ms]
Feb 28 09:09:27.258: INFO: Created: latency-svc-b99d7
Feb 28 09:09:27.264: INFO: Created: latency-svc-g8fbk
Feb 28 09:09:27.264: INFO: Got endpoints: latency-svc-b99d7 [68.403891ms]
Feb 28 09:09:27.265: INFO: Got endpoints: latency-svc-g8fbk [69.470585ms]
Feb 28 09:09:27.311: INFO: Created: latency-svc-tq7tl
Feb 28 09:09:27.311: INFO: Created: latency-svc-8pgws
Feb 28 09:09:27.311: INFO: Created: latency-svc-lksdn
Feb 28 09:09:27.311: INFO: Created: latency-svc-6w4cs
Feb 28 09:09:27.311: INFO: Created: latency-svc-md887
Feb 28 09:09:27.311: INFO: Created: latency-svc-6mmv2
Feb 28 09:09:27.311: INFO: Created: latency-svc-9qx4j
Feb 28 09:09:27.311: INFO: Created: latency-svc-trgzj
Feb 28 09:09:27.311: INFO: Created: latency-svc-kjgrz
Feb 28 09:09:27.311: INFO: Got endpoints: latency-svc-kjgrz [115.176421ms]
Feb 28 09:09:27.312: INFO: Got endpoints: latency-svc-8pgws [115.777318ms]
Feb 28 09:09:27.312: INFO: Got endpoints: latency-svc-lksdn [116.275838ms]
Feb 28 09:09:27.312: INFO: Got endpoints: latency-svc-6w4cs [116.087159ms]
Feb 28 09:09:27.312: INFO: Got endpoints: latency-svc-md887 [116.176251ms]
Feb 28 09:09:27.312: INFO: Got endpoints: latency-svc-6mmv2 [116.317461ms]
Feb 28 09:09:27.312: INFO: Got endpoints: latency-svc-9qx4j [116.516088ms]
Feb 28 09:09:27.313: INFO: Got endpoints: latency-svc-trgzj [116.515308ms]
Feb 28 09:09:27.315: INFO: Created: latency-svc-fq77n
Feb 28 09:09:27.315: INFO: Got endpoints: latency-svc-fq77n [119.145964ms]
Feb 28 09:09:27.315: INFO: Got endpoints: latency-svc-tq7tl [119.310757ms]
Feb 28 09:09:27.330: INFO: Created: latency-svc-m8pfk
Feb 28 09:09:27.332: INFO: Created: latency-svc-x7fz2
Feb 28 09:09:27.333: INFO: Got endpoints: latency-svc-m8pfk [68.464415ms]
Feb 28 09:09:27.334: INFO: Got endpoints: latency-svc-x7fz2 [68.530108ms]
Feb 28 09:09:27.376: INFO: Created: latency-svc-rxp68
Feb 28 09:09:27.381: INFO: Created: latency-svc-btq4p
Feb 28 09:09:27.381: INFO: Got endpoints: latency-svc-rxp68 [69.614093ms]
Feb 28 09:09:27.383: INFO: Got endpoints: latency-svc-btq4p [70.938888ms]
Feb 28 09:09:27.387: INFO: Created: latency-svc-fd6gd
Feb 28 09:09:27.453: INFO: Got endpoints: latency-svc-fd6gd [141.098733ms]
Feb 28 09:09:27.453: INFO: Created: latency-svc-n9rxh
Feb 28 09:09:27.453: INFO: Got endpoints: latency-svc-n9rxh [140.80552ms]
Feb 28 09:09:27.453: INFO: Created: latency-svc-v85zq
Feb 28 09:09:27.453: INFO: Got endpoints: latency-svc-v85zq [140.696489ms]
Feb 28 09:09:27.453: INFO: Created: latency-svc-4v7jl
Feb 28 09:09:27.453: INFO: Created: latency-svc-tm7rc
Feb 28 09:09:27.454: INFO: Created: latency-svc-sbcj9
Feb 28 09:09:27.454: INFO: Created: latency-svc-dhx26
Feb 28 09:09:27.454: INFO: Created: latency-svc-rtgl7
Feb 28 09:09:27.454: INFO: Created: latency-svc-5lw8d
Feb 28 09:09:27.453: INFO: Created: latency-svc-lsrrx
Feb 28 09:09:27.453: INFO: Created: latency-svc-d9cp6
Feb 28 09:09:27.454: INFO: Created: latency-svc-mbnjp
Feb 28 09:09:27.454: INFO: Got endpoints: latency-svc-rtgl7 [257.562745ms]
Feb 28 09:09:27.454: INFO: Got endpoints: latency-svc-4v7jl [141.154174ms]
Feb 28 09:09:27.454: INFO: Got endpoints: latency-svc-tm7rc [138.236258ms]
Feb 28 09:09:27.454: INFO: Got endpoints: latency-svc-dhx26 [141.35424ms]
Feb 28 09:09:27.454: INFO: Got endpoints: latency-svc-d9cp6 [257.540925ms]
Feb 28 09:09:27.454: INFO: Got endpoints: latency-svc-mbnjp [121.059366ms]
Feb 28 09:09:27.454: INFO: Got endpoints: latency-svc-lsrrx [71.661616ms]
Feb 28 09:09:27.454: INFO: Got endpoints: latency-svc-5lw8d [73.47256ms]
Feb 28 09:09:27.454: INFO: Got endpoints: latency-svc-sbcj9 [120.90475ms]
Feb 28 09:09:27.455: INFO: Created: latency-svc-f9bk4
Feb 28 09:09:27.455: INFO: Created: latency-svc-lb8jb
Feb 28 09:09:27.455: INFO: Got endpoints: latency-svc-lb8jb [258.532031ms]
Feb 28 09:09:27.455: INFO: Got endpoints: latency-svc-f9bk4 [139.21627ms]
Feb 28 09:09:27.455: INFO: Created: latency-svc-t5gkq
Feb 28 09:09:27.455: INFO: Got endpoints: latency-svc-t5gkq [142.393966ms]
Feb 28 09:09:27.515: INFO: Created: latency-svc-jfxdg
Feb 28 09:09:27.518: INFO: Created: latency-svc-f9s59
Feb 28 09:09:27.518: INFO: Got endpoints: latency-svc-jfxdg [65.120678ms]
Feb 28 09:09:27.519: INFO: Got endpoints: latency-svc-f9s59 [65.820434ms]
Feb 28 09:09:27.522: INFO: Created: latency-svc-7hqp9
Feb 28 09:09:27.563: INFO: Created: latency-svc-bnvpg
Feb 28 09:09:27.563: INFO: Created: latency-svc-7tngj
Feb 28 09:09:27.563: INFO: Created: latency-svc-8dxb5
Feb 28 09:09:27.563: INFO: Created: latency-svc-wmv2d
Feb 28 09:09:27.563: INFO: Got endpoints: latency-svc-bnvpg [109.669923ms]
Feb 28 09:09:27.564: INFO: Got endpoints: latency-svc-7hqp9 [111.083141ms]
Feb 28 09:09:27.564: INFO: Created: latency-svc-vrvh5
Feb 28 09:09:27.645: INFO: Created: latency-svc-9qjfx
Feb 28 09:09:27.646: INFO: Got endpoints: latency-svc-8dxb5 [192.61012ms]
Feb 28 09:09:27.647: INFO: Got endpoints: latency-svc-7tngj [193.15983ms]
Feb 28 09:09:27.649: INFO: Created: latency-svc-86hcf
Feb 28 09:09:27.656: INFO: Created: latency-svc-fgttq
Feb 28 09:09:27.659: INFO: Got endpoints: latency-svc-wmv2d [204.151816ms]
Feb 28 09:09:27.671: INFO: Created: latency-svc-mbqbf
Feb 28 09:09:27.675: INFO: Created: latency-svc-6j2bp
Feb 28 09:09:27.747: INFO: Created: latency-svc-z44mm
Feb 28 09:09:27.748: INFO: Got endpoints: latency-svc-vrvh5 [293.588587ms]
Feb 28 09:09:27.752: INFO: Created: latency-svc-fvsgv
Feb 28 09:09:27.757: INFO: Created: latency-svc-fhmqk
Feb 28 09:09:27.761: INFO: Created: latency-svc-mnh4g
Feb 28 09:09:27.761: INFO: Got endpoints: latency-svc-9qjfx [307.378407ms]
Feb 28 09:09:27.765: INFO: Created: latency-svc-hjkr9
Feb 28 09:09:27.768: INFO: Created: latency-svc-vnkl9
Feb 28 09:09:27.773: INFO: Created: latency-svc-grb8t
Feb 28 09:09:27.777: INFO: Created: latency-svc-bt8zw
Feb 28 09:09:27.783: INFO: Created: latency-svc-hktd5
Feb 28 09:09:27.808: INFO: Created: latency-svc-nn5fl
Feb 28 09:09:27.809: INFO: Got endpoints: latency-svc-86hcf [354.573623ms]
Feb 28 09:09:27.821: INFO: Created: latency-svc-mwz4t
Feb 28 09:09:27.858: INFO: Got endpoints: latency-svc-fgttq [403.975158ms]
Feb 28 09:09:27.868: INFO: Created: latency-svc-dqv27
Feb 28 09:09:27.912: INFO: Got endpoints: latency-svc-mbqbf [457.435672ms]
Feb 28 09:09:27.922: INFO: Created: latency-svc-pvv4v
Feb 28 09:09:27.963: INFO: Got endpoints: latency-svc-6j2bp [508.071139ms]
Feb 28 09:09:27.976: INFO: Created: latency-svc-hmj9k
Feb 28 09:09:28.015: INFO: Got endpoints: latency-svc-z44mm [560.285586ms]
Feb 28 09:09:28.028: INFO: Created: latency-svc-4dgsh
Feb 28 09:09:28.061: INFO: Got endpoints: latency-svc-fvsgv [606.232514ms]
Feb 28 09:09:28.077: INFO: Created: latency-svc-tdw5m
Feb 28 09:09:28.110: INFO: Got endpoints: latency-svc-fhmqk [546.176471ms]
Feb 28 09:09:28.123: INFO: Created: latency-svc-kmgzj
Feb 28 09:09:28.158: INFO: Got endpoints: latency-svc-mnh4g [639.995898ms]
Feb 28 09:09:28.169: INFO: Created: latency-svc-pk2f9
Feb 28 09:09:28.212: INFO: Got endpoints: latency-svc-hjkr9 [693.29231ms]
Feb 28 09:09:28.218: INFO: Created: latency-svc-x6m5l
Feb 28 09:09:28.259: INFO: Got endpoints: latency-svc-vnkl9 [694.714664ms]
Feb 28 09:09:28.275: INFO: Created: latency-svc-6n55j
Feb 28 09:09:28.311: INFO: Got endpoints: latency-svc-grb8t [664.866541ms]
Feb 28 09:09:28.321: INFO: Created: latency-svc-ftwch
Feb 28 09:09:28.364: INFO: Got endpoints: latency-svc-bt8zw [716.582559ms]
Feb 28 09:09:28.377: INFO: Created: latency-svc-gpkhn
Feb 28 09:09:28.413: INFO: Got endpoints: latency-svc-hktd5 [753.685287ms]
Feb 28 09:09:28.428: INFO: Created: latency-svc-6rm2t
Feb 28 09:09:28.464: INFO: Got endpoints: latency-svc-nn5fl [716.801073ms]
Feb 28 09:09:28.480: INFO: Created: latency-svc-8rm4n
Feb 28 09:09:28.513: INFO: Got endpoints: latency-svc-mwz4t [751.26144ms]
Feb 28 09:09:28.531: INFO: Created: latency-svc-6hmkn
Feb 28 09:09:28.560: INFO: Got endpoints: latency-svc-dqv27 [751.385502ms]
Feb 28 09:09:28.574: INFO: Created: latency-svc-cm5z9
Feb 28 09:09:28.611: INFO: Got endpoints: latency-svc-pvv4v [752.747217ms]
Feb 28 09:09:28.622: INFO: Created: latency-svc-849vg
Feb 28 09:09:28.661: INFO: Got endpoints: latency-svc-hmj9k [748.734371ms]
Feb 28 09:09:28.673: INFO: Created: latency-svc-5rnkq
Feb 28 09:09:28.712: INFO: Got endpoints: latency-svc-4dgsh [749.466209ms]
Feb 28 09:09:28.723: INFO: Created: latency-svc-qwbvx
Feb 28 09:09:28.765: INFO: Got endpoints: latency-svc-tdw5m [749.365435ms]
Feb 28 09:09:28.779: INFO: Created: latency-svc-vn5qj
Feb 28 09:09:28.814: INFO: Got endpoints: latency-svc-kmgzj [753.116813ms]
Feb 28 09:09:28.830: INFO: Created: latency-svc-bsc84
Feb 28 09:09:28.864: INFO: Got endpoints: latency-svc-pk2f9 [754.439936ms]
Feb 28 09:09:28.879: INFO: Created: latency-svc-v8bk8
Feb 28 09:09:28.914: INFO: Got endpoints: latency-svc-x6m5l [756.292272ms]
Feb 28 09:09:28.931: INFO: Created: latency-svc-qxl6m
Feb 28 09:09:28.965: INFO: Got endpoints: latency-svc-6n55j [753.101807ms]
Feb 28 09:09:28.980: INFO: Created: latency-svc-bjs6n
Feb 28 09:09:29.013: INFO: Got endpoints: latency-svc-ftwch [754.265829ms]
Feb 28 09:09:29.030: INFO: Created: latency-svc-ms44n
Feb 28 09:09:29.065: INFO: Got endpoints: latency-svc-gpkhn [753.568996ms]
Feb 28 09:09:29.079: INFO: Created: latency-svc-xf8cp
Feb 28 09:09:29.114: INFO: Got endpoints: latency-svc-6rm2t [749.983953ms]
Feb 28 09:09:29.130: INFO: Created: latency-svc-m2xp2
Feb 28 09:09:29.165: INFO: Got endpoints: latency-svc-8rm4n [752.462147ms]
Feb 28 09:09:29.180: INFO: Created: latency-svc-4bpxw
Feb 28 09:09:29.215: INFO: Got endpoints: latency-svc-6hmkn [751.024335ms]
Feb 28 09:09:29.230: INFO: Created: latency-svc-zhbqq
Feb 28 09:09:29.265: INFO: Got endpoints: latency-svc-cm5z9 [752.234412ms]
Feb 28 09:09:29.278: INFO: Created: latency-svc-lwrlm
Feb 28 09:09:29.312: INFO: Got endpoints: latency-svc-849vg [751.186976ms]
Feb 28 09:09:29.328: INFO: Created: latency-svc-6sw6b
Feb 28 09:09:29.362: INFO: Got endpoints: latency-svc-5rnkq [751.279685ms]
Feb 28 09:09:29.376: INFO: Created: latency-svc-6tbvd
Feb 28 09:09:29.412: INFO: Got endpoints: latency-svc-qwbvx [751.41023ms]
Feb 28 09:09:29.425: INFO: Created: latency-svc-sgzj2
Feb 28 09:09:29.466: INFO: Got endpoints: latency-svc-vn5qj [753.998174ms]
Feb 28 09:09:29.475: INFO: Created: latency-svc-m9ksz
Feb 28 09:09:29.512: INFO: Got endpoints: latency-svc-bsc84 [747.665217ms]
Feb 28 09:09:29.530: INFO: Created: latency-svc-zdh4x
Feb 28 09:09:29.564: INFO: Got endpoints: latency-svc-v8bk8 [749.607054ms]
Feb 28 09:09:29.576: INFO: Created: latency-svc-cbcss
Feb 28 09:09:29.612: INFO: Got endpoints: latency-svc-qxl6m [747.598307ms]
Feb 28 09:09:29.628: INFO: Created: latency-svc-5fdvn
Feb 28 09:09:29.663: INFO: Got endpoints: latency-svc-bjs6n [748.505907ms]
Feb 28 09:09:29.677: INFO: Created: latency-svc-bzjwj
Feb 28 09:09:29.711: INFO: Got endpoints: latency-svc-ms44n [745.725698ms]
Feb 28 09:09:29.725: INFO: Created: latency-svc-7nn2t
Feb 28 09:09:29.761: INFO: Got endpoints: latency-svc-xf8cp [747.711692ms]
Feb 28 09:09:29.778: INFO: Created: latency-svc-g7vgq
Feb 28 09:09:29.812: INFO: Got endpoints: latency-svc-m2xp2 [746.706603ms]
Feb 28 09:09:29.824: INFO: Created: latency-svc-gdzqh
Feb 28 09:09:29.863: INFO: Got endpoints: latency-svc-4bpxw [749.165258ms]
Feb 28 09:09:29.875: INFO: Created: latency-svc-wvmp6
Feb 28 09:09:29.913: INFO: Got endpoints: latency-svc-zhbqq [747.43669ms]
Feb 28 09:09:29.930: INFO: Created: latency-svc-ln2sh
Feb 28 09:09:29.964: INFO: Got endpoints: latency-svc-lwrlm [748.839322ms]
Feb 28 09:09:29.978: INFO: Created: latency-svc-jml7p
Feb 28 09:09:30.015: INFO: Got endpoints: latency-svc-6sw6b [749.609673ms]
Feb 28 09:09:30.028: INFO: Created: latency-svc-jg68r
Feb 28 09:09:30.062: INFO: Got endpoints: latency-svc-6tbvd [750.124366ms]
Feb 28 09:09:30.079: INFO: Created: latency-svc-w2wc4
Feb 28 09:09:30.111: INFO: Got endpoints: latency-svc-sgzj2 [748.806621ms]
Feb 28 09:09:30.125: INFO: Created: latency-svc-9z6mc
Feb 28 09:09:30.162: INFO: Got endpoints: latency-svc-m9ksz [749.691201ms]
Feb 28 09:09:30.175: INFO: Created: latency-svc-dxqmf
Feb 28 09:09:30.213: INFO: Got endpoints: latency-svc-zdh4x [746.633506ms]
Feb 28 09:09:30.226: INFO: Created: latency-svc-766t8
Feb 28 09:09:30.262: INFO: Got endpoints: latency-svc-cbcss [749.924827ms]
Feb 28 09:09:30.277: INFO: Created: latency-svc-tvrjd
Feb 28 09:09:30.313: INFO: Got endpoints: latency-svc-5fdvn [749.601067ms]
Feb 28 09:09:30.328: INFO: Created: latency-svc-4kgjw
Feb 28 09:09:30.364: INFO: Got endpoints: latency-svc-bzjwj [752.277146ms]
Feb 28 09:09:30.380: INFO: Created: latency-svc-kpkvt
Feb 28 09:09:30.413: INFO: Got endpoints: latency-svc-7nn2t [750.060949ms]
Feb 28 09:09:30.428: INFO: Created: latency-svc-lgwr2
Feb 28 09:09:30.462: INFO: Got endpoints: latency-svc-g7vgq [751.202296ms]
Feb 28 09:09:30.477: INFO: Created: latency-svc-bq6tr
Feb 28 09:09:30.510: INFO: Got endpoints: latency-svc-gdzqh [749.433601ms]
Feb 28 09:09:30.524: INFO: Created: latency-svc-tt5lf
Feb 28 09:09:30.560: INFO: Got endpoints: latency-svc-wvmp6 [748.178408ms]
Feb 28 09:09:30.572: INFO: Created: latency-svc-vgnqb
Feb 28 09:09:30.611: INFO: Got endpoints: latency-svc-ln2sh [747.618781ms]
Feb 28 09:09:30.622: INFO: Created: latency-svc-bz7rv
Feb 28 09:09:30.662: INFO: Got endpoints: latency-svc-jml7p [749.328158ms]
Feb 28 09:09:30.671: INFO: Created: latency-svc-ff5mk
Feb 28 09:09:30.711: INFO: Got endpoints: latency-svc-jg68r [746.07654ms]
Feb 28 09:09:30.725: INFO: Created: latency-svc-n74cs
Feb 28 09:09:30.760: INFO: Got endpoints: latency-svc-w2wc4 [745.725565ms]
Feb 28 09:09:30.772: INFO: Created: latency-svc-d9bls
Feb 28 09:09:30.813: INFO: Got endpoints: latency-svc-9z6mc [750.583869ms]
Feb 28 09:09:30.825: INFO: Created: latency-svc-8p8z8
Feb 28 09:09:30.862: INFO: Got endpoints: latency-svc-dxqmf [751.108213ms]
Feb 28 09:09:30.877: INFO: Created: latency-svc-26jqr
Feb 28 09:09:30.911: INFO: Got endpoints: latency-svc-766t8 [749.056521ms]
Feb 28 09:09:30.926: INFO: Created: latency-svc-fbm84
Feb 28 09:09:30.961: INFO: Got endpoints: latency-svc-tvrjd [747.738944ms]
Feb 28 09:09:30.974: INFO: Created: latency-svc-fcqb4
Feb 28 09:09:31.014: INFO: Got endpoints: latency-svc-4kgjw [751.736785ms]
Feb 28 09:09:31.030: INFO: Created: latency-svc-ctd88
Feb 28 09:09:31.061: INFO: Got endpoints: latency-svc-kpkvt [747.856633ms]
Feb 28 09:09:31.077: INFO: Created: latency-svc-dh79x
Feb 28 09:09:31.112: INFO: Got endpoints: latency-svc-lgwr2 [747.851731ms]
Feb 28 09:09:31.125: INFO: Created: latency-svc-rkcfh
Feb 28 09:09:31.163: INFO: Got endpoints: latency-svc-bq6tr [750.132872ms]
Feb 28 09:09:31.177: INFO: Created: latency-svc-xmqqs
Feb 28 09:09:31.214: INFO: Got endpoints: latency-svc-tt5lf [751.860386ms]
Feb 28 09:09:31.232: INFO: Created: latency-svc-vzrdw
Feb 28 09:09:31.263: INFO: Got endpoints: latency-svc-vgnqb [752.81595ms]
Feb 28 09:09:31.278: INFO: Created: latency-svc-cs226
Feb 28 09:09:31.308: INFO: Got endpoints: latency-svc-bz7rv [747.463254ms]
Feb 28 09:09:31.321: INFO: Created: latency-svc-vgxvt
Feb 28 09:09:31.359: INFO: Got endpoints: latency-svc-ff5mk [748.272772ms]
Feb 28 09:09:31.367: INFO: Created: latency-svc-h52x2
Feb 28 09:09:31.408: INFO: Got endpoints: latency-svc-n74cs [745.698938ms]
Feb 28 09:09:31.418: INFO: Created: latency-svc-m54c5
Feb 28 09:09:31.460: INFO: Got endpoints: latency-svc-d9bls [749.689454ms]
Feb 28 09:09:31.470: INFO: Created: latency-svc-pdqdg
Feb 28 09:09:31.512: INFO: Got endpoints: latency-svc-8p8z8 [751.297733ms]
Feb 28 09:09:31.522: INFO: Created: latency-svc-64q7b
Feb 28 09:09:31.560: INFO: Got endpoints: latency-svc-26jqr [747.175302ms]
Feb 28 09:09:31.576: INFO: Created: latency-svc-nf928
Feb 28 09:09:31.611: INFO: Got endpoints: latency-svc-fbm84 [748.496732ms]
Feb 28 09:09:31.622: INFO: Created: latency-svc-v9ltk
Feb 28 09:09:31.661: INFO: Got endpoints: latency-svc-fcqb4 [750.210566ms]
Feb 28 09:09:31.675: INFO: Created: latency-svc-n8fsc
Feb 28 09:09:31.711: INFO: Got endpoints: latency-svc-ctd88 [750.165016ms]
Feb 28 09:09:31.723: INFO: Created: latency-svc-zzhrl
Feb 28 09:09:31.761: INFO: Got endpoints: latency-svc-dh79x [746.703534ms]
Feb 28 09:09:31.773: INFO: Created: latency-svc-dm4dd
Feb 28 09:09:31.814: INFO: Got endpoints: latency-svc-rkcfh [752.512945ms]
Feb 28 09:09:31.827: INFO: Created: latency-svc-8rd2p
Feb 28 09:09:31.864: INFO: Got endpoints: latency-svc-xmqqs [751.915724ms]
Feb 28 09:09:31.879: INFO: Created: latency-svc-kjvsf
Feb 28 09:09:31.914: INFO: Got endpoints: latency-svc-vzrdw [750.574548ms]
Feb 28 09:09:31.930: INFO: Created: latency-svc-tfk2r
Feb 28 09:09:31.967: INFO: Got endpoints: latency-svc-cs226 [752.43078ms]
Feb 28 09:09:31.979: INFO: Created: latency-svc-4nx9k
Feb 28 09:09:32.011: INFO: Got endpoints: latency-svc-vgxvt [747.449445ms]
Feb 28 09:09:32.026: INFO: Created: latency-svc-j4bp8
Feb 28 09:09:32.060: INFO: Got endpoints: latency-svc-h52x2 [751.94697ms]
Feb 28 09:09:32.072: INFO: Created: latency-svc-lbkxp
Feb 28 09:09:32.108: INFO: Got endpoints: latency-svc-m54c5 [748.891175ms]
Feb 28 09:09:32.118: INFO: Created: latency-svc-x8gkq
Feb 28 09:09:32.157: INFO: Got endpoints: latency-svc-pdqdg [749.317347ms]
Feb 28 09:09:32.168: INFO: Created: latency-svc-4gbx9
Feb 28 09:09:32.208: INFO: Got endpoints: latency-svc-64q7b [747.244476ms]
Feb 28 09:09:32.215: INFO: Created: latency-svc-q9sx9
Feb 28 09:09:32.257: INFO: Got endpoints: latency-svc-nf928 [745.201061ms]
Feb 28 09:09:32.272: INFO: Created: latency-svc-9n4pk
Feb 28 09:09:32.310: INFO: Got endpoints: latency-svc-v9ltk [749.927419ms]
Feb 28 09:09:32.318: INFO: Created: latency-svc-xzwnw
Feb 28 09:09:32.364: INFO: Got endpoints: latency-svc-n8fsc [752.95153ms]
Feb 28 09:09:32.373: INFO: Created: latency-svc-5smz9
Feb 28 09:09:32.415: INFO: Got endpoints: latency-svc-zzhrl [753.343343ms]
Feb 28 09:09:32.430: INFO: Created: latency-svc-ns5w4
Feb 28 09:09:32.461: INFO: Got endpoints: latency-svc-dm4dd [749.618262ms]
Feb 28 09:09:32.476: INFO: Created: latency-svc-vfqqh
Feb 28 09:09:32.507: INFO: Got endpoints: latency-svc-8rd2p [745.913431ms]
Feb 28 09:09:32.519: INFO: Created: latency-svc-6w2ck
Feb 28 09:09:32.559: INFO: Got endpoints: latency-svc-kjvsf [744.99134ms]
Feb 28 09:09:32.568: INFO: Created: latency-svc-q26mp
Feb 28 09:09:32.611: INFO: Got endpoints: latency-svc-tfk2r [746.680028ms]
Feb 28 09:09:32.621: INFO: Created: latency-svc-v9mn2
Feb 28 09:09:32.662: INFO: Got endpoints: latency-svc-4nx9k [747.928853ms]
Feb 28 09:09:32.675: INFO: Created: latency-svc-sm9qh
Feb 28 09:09:32.711: INFO: Got endpoints: latency-svc-j4bp8 [744.038113ms]
Feb 28 09:09:32.724: INFO: Created: latency-svc-xzg2l
Feb 28 09:09:32.761: INFO: Got endpoints: latency-svc-lbkxp [750.226665ms]
Feb 28 09:09:32.777: INFO: Created: latency-svc-cjbvw
Feb 28 09:09:32.811: INFO: Got endpoints: latency-svc-x8gkq [750.963493ms]
Feb 28 09:09:32.823: INFO: Created: latency-svc-tvw6k
Feb 28 09:09:32.862: INFO: Got endpoints: latency-svc-4gbx9 [754.1842ms]
Feb 28 09:09:32.874: INFO: Created: latency-svc-sfpc7
Feb 28 09:09:32.911: INFO: Got endpoints: latency-svc-q9sx9 [753.4988ms]
Feb 28 09:09:32.924: INFO: Created: latency-svc-66668
Feb 28 09:09:32.963: INFO: Got endpoints: latency-svc-9n4pk [755.159192ms]
Feb 28 09:09:32.973: INFO: Created: latency-svc-dp4v7
Feb 28 09:09:33.008: INFO: Got endpoints: latency-svc-xzwnw [750.528124ms]
Feb 28 09:09:33.022: INFO: Created: latency-svc-rpzkl
Feb 28 09:09:33.058: INFO: Got endpoints: latency-svc-5smz9 [747.87983ms]
Feb 28 09:09:33.068: INFO: Created: latency-svc-pwwtk
Feb 28 09:09:33.112: INFO: Got endpoints: latency-svc-ns5w4 [747.850457ms]
Feb 28 09:09:33.122: INFO: Created: latency-svc-7qhcm
Feb 28 09:09:33.164: INFO: Got endpoints: latency-svc-vfqqh [749.303001ms]
Feb 28 09:09:33.174: INFO: Created: latency-svc-zfgx5
Feb 28 09:09:33.214: INFO: Got endpoints: latency-svc-6w2ck [753.156109ms]
Feb 28 09:09:33.227: INFO: Created: latency-svc-ck67k
Feb 28 09:09:33.263: INFO: Got endpoints: latency-svc-q26mp [755.864033ms]
Feb 28 09:09:33.280: INFO: Created: latency-svc-46d2f
Feb 28 09:09:33.312: INFO: Got endpoints: latency-svc-v9mn2 [752.584539ms]
Feb 28 09:09:33.326: INFO: Created: latency-svc-zmzv8
Feb 28 09:09:33.361: INFO: Got endpoints: latency-svc-sm9qh [749.607245ms]
Feb 28 09:09:33.373: INFO: Created: latency-svc-z46j8
Feb 28 09:09:33.413: INFO: Got endpoints: latency-svc-xzg2l [750.581767ms]
Feb 28 09:09:33.427: INFO: Created: latency-svc-q6wpc
Feb 28 09:09:33.461: INFO: Got endpoints: latency-svc-cjbvw [749.686171ms]
Feb 28 09:09:33.475: INFO: Created: latency-svc-gw5sh
Feb 28 09:09:33.511: INFO: Got endpoints: latency-svc-tvw6k [750.134587ms]
Feb 28 09:09:33.528: INFO: Created: latency-svc-wr6n9
Feb 28 09:09:33.563: INFO: Got endpoints: latency-svc-sfpc7 [752.261746ms]
Feb 28 09:09:33.576: INFO: Created: latency-svc-dwh77
Feb 28 09:09:33.613: INFO: Got endpoints: latency-svc-66668 [750.799059ms]
Feb 28 09:09:33.627: INFO: Created: latency-svc-drkqn
Feb 28 09:09:33.662: INFO: Got endpoints: latency-svc-dp4v7 [751.383207ms]
Feb 28 09:09:33.682: INFO: Created: latency-svc-cqptg
Feb 28 09:09:33.715: INFO: Got endpoints: latency-svc-rpzkl [751.957085ms]
Feb 28 09:09:33.727: INFO: Created: latency-svc-vtm2n
Feb 28 09:09:33.763: INFO: Got endpoints: latency-svc-pwwtk [755.012786ms]
Feb 28 09:09:33.781: INFO: Created: latency-svc-wqb57
Feb 28 09:09:33.813: INFO: Got endpoints: latency-svc-7qhcm [754.98361ms]
Feb 28 09:09:33.827: INFO: Created: latency-svc-np9d6
Feb 28 09:09:33.862: INFO: Got endpoints: latency-svc-zfgx5 [749.997383ms]
Feb 28 09:09:33.878: INFO: Created: latency-svc-wbmng
Feb 28 09:09:33.912: INFO: Got endpoints: latency-svc-ck67k [747.744163ms]
Feb 28 09:09:33.927: INFO: Created: latency-svc-mbnf7
Feb 28 09:09:33.965: INFO: Got endpoints: latency-svc-46d2f [751.374353ms]
Feb 28 09:09:33.978: INFO: Created: latency-svc-pks26
Feb 28 09:09:34.011: INFO: Got endpoints: latency-svc-zmzv8 [747.98921ms]
Feb 28 09:09:34.027: INFO: Created: latency-svc-4w6mc
Feb 28 09:09:34.059: INFO: Got endpoints: latency-svc-z46j8 [747.607314ms]
Feb 28 09:09:34.072: INFO: Created: latency-svc-2mdpv
Feb 28 09:09:34.109: INFO: Got endpoints: latency-svc-q6wpc [748.078394ms]
Feb 28 09:09:34.121: INFO: Created: latency-svc-8j7nt
Feb 28 09:09:34.160: INFO: Got endpoints: latency-svc-gw5sh [747.390409ms]
Feb 28 09:09:34.171: INFO: Created: latency-svc-8zxdm
Feb 28 09:09:34.212: INFO: Got endpoints: latency-svc-wr6n9 [751.326878ms]
Feb 28 09:09:34.225: INFO: Created: latency-svc-xbbgc
Feb 28 09:09:34.263: INFO: Got endpoints: latency-svc-dwh77 [752.04339ms]
Feb 28 09:09:34.274: INFO: Created: latency-svc-cr2xh
Feb 28 09:09:34.312: INFO: Got endpoints: latency-svc-drkqn [748.69307ms]
Feb 28 09:09:34.327: INFO: Created: latency-svc-fb7cz
Feb 28 09:09:34.364: INFO: Got endpoints: latency-svc-cqptg [751.08908ms]
Feb 28 09:09:34.377: INFO: Created: latency-svc-qfwm2
Feb 28 09:09:34.413: INFO: Got endpoints: latency-svc-vtm2n [750.28586ms]
Feb 28 09:09:34.428: INFO: Created: latency-svc-8nj5t
Feb 28 09:09:34.469: INFO: Got endpoints: latency-svc-wqb57 [753.644293ms]
Feb 28 09:09:34.476: INFO: Created: latency-svc-7l2g4
Feb 28 09:09:34.510: INFO: Got endpoints: latency-svc-np9d6 [747.079214ms]
Feb 28 09:09:34.530: INFO: Created: latency-svc-7plwj
Feb 28 09:09:34.562: INFO: Got endpoints: latency-svc-wbmng [748.123266ms]
Feb 28 09:09:34.574: INFO: Created: latency-svc-t7lsq
Feb 28 09:09:34.612: INFO: Got endpoints: latency-svc-mbnf7 [749.321549ms]
Feb 28 09:09:34.626: INFO: Created: latency-svc-z9nlz
Feb 28 09:09:34.662: INFO: Got endpoints: latency-svc-pks26 [749.609178ms]
Feb 28 09:09:34.677: INFO: Created: latency-svc-ssl6j
Feb 28 09:09:34.710: INFO: Got endpoints: latency-svc-4w6mc [744.152953ms]
Feb 28 09:09:34.723: INFO: Created: latency-svc-48khm
Feb 28 09:09:34.760: INFO: Got endpoints: latency-svc-2mdpv [749.03518ms]
Feb 28 09:09:34.771: INFO: Created: latency-svc-fv46x
Feb 28 09:09:34.813: INFO: Got endpoints: latency-svc-8j7nt [753.121086ms]
Feb 28 09:09:34.824: INFO: Created: latency-svc-bvj47
Feb 28 09:09:34.862: INFO: Got endpoints: latency-svc-8zxdm [752.508865ms]
Feb 28 09:09:34.877: INFO: Created: latency-svc-8fbmj
Feb 28 09:09:34.912: INFO: Got endpoints: latency-svc-xbbgc [751.639749ms]
Feb 28 09:09:34.950: INFO: Created: latency-svc-57z6d
Feb 28 09:09:35.049: INFO: Got endpoints: latency-svc-fb7cz [785.996944ms]
Feb 28 09:09:35.050: INFO: Got endpoints: latency-svc-cr2xh [837.678844ms]
Feb 28 09:09:35.055: INFO: Created: latency-svc-mbt2r
Feb 28 09:09:35.061: INFO: Got endpoints: latency-svc-qfwm2 [748.932585ms]
Feb 28 09:09:35.111: INFO: Created: latency-svc-hbvsc
Feb 28 09:09:35.111: INFO: Got endpoints: latency-svc-8nj5t [746.642344ms]
Feb 28 09:09:35.114: INFO: Created: latency-svc-qcj9s
Feb 28 09:09:35.162: INFO: Got endpoints: latency-svc-7l2g4 [749.196182ms]
Feb 28 09:09:35.213: INFO: Got endpoints: latency-svc-7plwj [743.756241ms]
Feb 28 09:09:35.264: INFO: Got endpoints: latency-svc-t7lsq [753.863023ms]
Feb 28 09:09:35.316: INFO: Got endpoints: latency-svc-z9nlz [754.431843ms]
Feb 28 09:09:35.362: INFO: Got endpoints: latency-svc-ssl6j [750.774066ms]
Feb 28 09:09:35.412: INFO: Got endpoints: latency-svc-48khm [750.458184ms]
Feb 28 09:09:35.465: INFO: Got endpoints: latency-svc-fv46x [754.699386ms]
Feb 28 09:09:35.515: INFO: Got endpoints: latency-svc-bvj47 [754.419794ms]
Feb 28 09:09:35.563: INFO: Got endpoints: latency-svc-8fbmj [750.145189ms]
Feb 28 09:09:35.614: INFO: Got endpoints: latency-svc-57z6d [752.108259ms]
Feb 28 09:09:35.664: INFO: Got endpoints: latency-svc-mbt2r [752.031818ms]
Feb 28 09:09:35.715: INFO: Got endpoints: latency-svc-hbvsc [665.674873ms]
Feb 28 09:09:35.764: INFO: Got endpoints: latency-svc-qcj9s [713.9158ms]
Feb 28 09:09:35.764: INFO: Latencies: [65.120678ms 65.820434ms 68.403891ms 68.464415ms 68.530108ms 69.470585ms 69.614093ms 70.938888ms 71.661616ms 73.47256ms 109.669923ms 111.083141ms 115.176421ms 115.777318ms 116.087159ms 116.176251ms 116.275838ms 116.317461ms 116.515308ms 116.516088ms 119.145964ms 119.310757ms 120.90475ms 121.059366ms 138.236258ms 139.21627ms 140.696489ms 140.80552ms 141.098733ms 141.154174ms 141.35424ms 142.393966ms 192.61012ms 193.15983ms 204.151816ms 257.540925ms 257.562745ms 258.532031ms 293.588587ms 307.378407ms 354.573623ms 403.975158ms 457.435672ms 508.071139ms 546.176471ms 560.285586ms 606.232514ms 639.995898ms 664.866541ms 665.674873ms 693.29231ms 694.714664ms 713.9158ms 716.582559ms 716.801073ms 743.756241ms 744.038113ms 744.152953ms 744.99134ms 745.201061ms 745.698938ms 745.725565ms 745.725698ms 745.913431ms 746.07654ms 746.633506ms 746.642344ms 746.680028ms 746.703534ms 746.706603ms 747.079214ms 747.175302ms 747.244476ms 747.390409ms 747.43669ms 747.449445ms 747.463254ms 747.598307ms 747.607314ms 747.618781ms 747.665217ms 747.711692ms 747.738944ms 747.744163ms 747.850457ms 747.851731ms 747.856633ms 747.87983ms 747.928853ms 747.98921ms 748.078394ms 748.123266ms 748.178408ms 748.272772ms 748.496732ms 748.505907ms 748.69307ms 748.734371ms 748.806621ms 748.839322ms 748.891175ms 748.932585ms 749.03518ms 749.056521ms 749.165258ms 749.196182ms 749.303001ms 749.317347ms 749.321549ms 749.328158ms 749.365435ms 749.433601ms 749.466209ms 749.601067ms 749.607054ms 749.607245ms 749.609178ms 749.609673ms 749.618262ms 749.686171ms 749.689454ms 749.691201ms 749.924827ms 749.927419ms 749.983953ms 749.997383ms 750.060949ms 750.124366ms 750.132872ms 750.134587ms 750.145189ms 750.165016ms 750.210566ms 750.226665ms 750.28586ms 750.458184ms 750.528124ms 750.574548ms 750.581767ms 750.583869ms 750.774066ms 750.799059ms 750.963493ms 751.024335ms 751.08908ms 751.108213ms 751.186976ms 751.202296ms 751.26144ms 751.279685ms 751.297733ms 751.326878ms 751.374353ms 751.383207ms 751.385502ms 751.41023ms 751.639749ms 751.736785ms 751.860386ms 751.915724ms 751.94697ms 751.957085ms 752.031818ms 752.04339ms 752.108259ms 752.234412ms 752.261746ms 752.277146ms 752.43078ms 752.462147ms 752.508865ms 752.512945ms 752.584539ms 752.747217ms 752.81595ms 752.95153ms 753.101807ms 753.116813ms 753.121086ms 753.156109ms 753.343343ms 753.4988ms 753.568996ms 753.644293ms 753.685287ms 753.863023ms 753.998174ms 754.1842ms 754.265829ms 754.419794ms 754.431843ms 754.439936ms 754.699386ms 754.98361ms 755.012786ms 755.159192ms 755.864033ms 756.292272ms 785.996944ms 837.678844ms]
Feb 28 09:09:35.764: INFO: 50 %ile: 748.891175ms
Feb 28 09:09:35.764: INFO: 90 %ile: 753.343343ms
Feb 28 09:09:35.764: INFO: 99 %ile: 785.996944ms
Feb 28 09:09:35.764: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:09:35.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-dc8s7" for this suite.
Feb 28 09:09:47.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:09:48.897: INFO: namespace: e2e-tests-svc-latency-dc8s7, resource: bindings, ignored listing per whitelist
Feb 28 09:09:50.330: INFO: namespace e2e-tests-svc-latency-dc8s7 deletion completed in 14.50756033s

• [SLOW TEST:27.788 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:09:50.331: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-7dnxr
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-9ba0840a-3b38-11e9-8ad6-42906abdb246
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-9ba0840a-3b38-11e9-8ad6-42906abdb246
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:09:57.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7dnxr" for this suite.
Feb 28 09:10:19.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:10:21.443: INFO: namespace: e2e-tests-configmap-7dnxr, resource: bindings, ignored listing per whitelist
Feb 28 09:10:21.697: INFO: namespace e2e-tests-configmap-7dnxr deletion completed in 24.419526275s

• [SLOW TEST:31.366 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:10:21.697: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4cstc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 09:10:24.036: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae4999a5-3b38-11e9-8ad6-42906abdb246" in namespace "e2e-tests-downward-api-4cstc" to be "success or failure"
Feb 28 09:10:24.093: INFO: Pod "downwardapi-volume-ae4999a5-3b38-11e9-8ad6-42906abdb246": Phase="Pending", Reason="", readiness=false. Elapsed: 56.509599ms
Feb 28 09:10:26.158: INFO: Pod "downwardapi-volume-ae4999a5-3b38-11e9-8ad6-42906abdb246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.122288908s
STEP: Saw pod success
Feb 28 09:10:26.158: INFO: Pod "downwardapi-volume-ae4999a5-3b38-11e9-8ad6-42906abdb246" satisfied condition "success or failure"
Feb 28 09:10:26.224: INFO: Trying to get logs from node ip-10-250-15-222.eu-west-1.compute.internal pod downwardapi-volume-ae4999a5-3b38-11e9-8ad6-42906abdb246 container client-container: <nil>
STEP: delete the pod
Feb 28 09:10:26.357: INFO: Waiting for pod downwardapi-volume-ae4999a5-3b38-11e9-8ad6-42906abdb246 to disappear
Feb 28 09:10:26.415: INFO: Pod downwardapi-volume-ae4999a5-3b38-11e9-8ad6-42906abdb246 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:10:26.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4cstc" for this suite.
Feb 28 09:10:32.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:10:33.739: INFO: namespace: e2e-tests-downward-api-4cstc, resource: bindings, ignored listing per whitelist
Feb 28 09:10:34.902: INFO: namespace e2e-tests-downward-api-4cstc deletion completed in 8.427431741s

• [SLOW TEST:13.205 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:10:34.903: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-9lfjs
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-b6330c01-3b38-11e9-8ad6-42906abdb246
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:10:39.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9lfjs" for this suite.
Feb 28 09:10:57.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:10:58.689: INFO: namespace: e2e-tests-configmap-9lfjs, resource: bindings, ignored listing per whitelist
Feb 28 09:11:00.175: INFO: namespace e2e-tests-configmap-9lfjs deletion completed in 20.438623928s

• [SLOW TEST:25.272 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:11:00.175: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gfqjp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 28 09:11:02.498: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml cluster-info'
Feb 28 09:11:03.081: INFO: stderr: ""
Feb 28 09:11:03.081: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.pub-aws-e7h74.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:11:03.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gfqjp" for this suite.
Feb 28 09:11:09.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:11:09.490: INFO: namespace: e2e-tests-kubectl-gfqjp, resource: bindings, ignored listing per whitelist
Feb 28 09:11:11.554: INFO: namespace e2e-tests-kubectl-gfqjp deletion completed in 8.415619052s

• [SLOW TEST:11.379 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSFeb 28 09:11:11.554: INFO: Running AfterSuite actions on all nodes
Feb 28 09:11:11.554: INFO: Running AfterSuite actions on node 1
Feb 28 09:11:11.554: INFO: Skipping dumping logs from cluster

Ran 200 of 2161 Specs in 6391.192 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Flaked | 0 Pending | 1961 Skipped PASS

Ginkgo ran 1 suite in 1h46m32.17846s
Test Suite Passed
